Search Result Diversiﬁcation in Resource Selection for

Federated Search

Dzung Hong

Department of Computer Science

Purdue University

250 N. University Street

West Lafayette, IN 47907, USA
dthong@cs.purdue.edu

Luo Si

Department of Computer Science

Purdue University

250 N. University Street

West Lafayette, IN 47907, USA

lsi@cs.purdue.edu

ABSTRACT
Prior research in resource selection for federated search mainly
focused on selecting a small number of information sources
that are most relevant to a user query. However, result nov-
elty and diversiﬁcation are largely unexplored, which does
not reﬂect the various kinds of information needs of users in
real world applications.

This paper proposes two general approaches to model both
result relevance and diversiﬁcation in selecting sources, in or-
der to provide more comprehensive coverage of multiple as-
pects of a user query. The ﬁrst approach focuses on diversify-
ing the document ranking on a centralized sample database
before selecting information sources under the framework
of Relevant Document Distribution Estimation (ReDDE).
The second approach ﬁrst evaluates the relevance of infor-
mation sources with respect to each aspect of the query, and
then ranks the sources based on the novelty and relevance
that they oﬀer. Both approaches can be applied with a
wide range of existing resource selection algorithms such as
ReDDE, CRCS, CORI and Big Document. Moreover, this
paper proposes a learning based approach to combine mul-
tiple resource selection algorithms for result diversiﬁcation,
which can further improve the performance. We propose a
set of new metrics for resource selection in federated search
to evaluate the diversiﬁcation performance of diﬀerent ap-
proaches. To our best knowledge, this is the ﬁrst piece of
work that addresses the problem of search result diversiﬁca-
tion in federated search. The eﬀectiveness of the proposed
approaches has been demonstrated by an extensive set of
experiments on the federated search testbed of the Clueweb
dataset.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Keywords
Federated Search, Resource Selection, Diversiﬁcation

1.

INTRODUCTION

Federated search, also known as distributed information
retrieval [28, 4, 14], focuses on searching information dis-
tributed across multiple information sources such as local
repositories or verticals. There are three major sub-problems
in federated search: resource representation obtains infor-
mation about contents and other key properties of each indi-
vidual information source, resource selection selects a small
number of most useful sources given a user query, and re-
sult merging integrates individual ranked lists from selected
sources into a single ﬁnal list. A large body of research has
been conducted for resource selection in federated search [4,
28]. However, little is known about selecting a set of sources
that balances relevance and novelty. This substantially lim-
its the usability of federated search in many applications.

On the other side, search result diversiﬁcation has been
studied extensively in ad hoc search in order to oﬀer more
coverage for ambiguous and multifaceted queries.
In sev-
eral occasions, users’ intents in their queries may not be
expressed explicitly. For example, an ambiguous query such
as “Jaguar” may refer to an animal or a car model; or a
multifaceted query such as “Batman” may refer to a name
of a movie, a comic character, or the comic itself. Search
result diversiﬁcation circumvents this problem by explicitly
or implicitly considering probable aspects of the query and
presenting the search results in a way that is easier for users
to ﬁnd the needed information. Since 2009, the TREC Web
track has incorporated diversiﬁcation in the evaluation of
the Web track [9]. Several evaluation metrics have been
developed in ad hoc search to measure the eﬀectiveness of
diﬀerent approaches of search result diversiﬁcation.

Search result diversiﬁcation in federated search may not
be as simple as diversifying the ﬁnal ranked list obtained
from the selected sources. In a federated environment where
documents of a same source cover similar topics, selecting a
set of sources that balances relevance and novelty becomes
crucial. As for the example of the query “Jaguar” above,
if the sources related to “Jaguar” as a car dominate the re-
source selection result, it will be much harder to obtain a
diversiﬁed ranked list in the end.

This paper proposes new approaches in diversifying results
of resource selection in federated search. To the best of our
knowledge, this is the ﬁrst study that tackles the issue. First,
a set of new metrics is designed for measuring result diver-

613siﬁcation in resource selection. The metrics can incorporate
any diversity measure that has been developed in ad hoc
search, including the intent-aware expected reciprocal rank
(ERR) [7], α-nDCG@k [13] and MAP-IA [1]. Second, two
general approaches are proposed for diversifying resource se-
lection. The ﬁrst approach extends the ReDDE framework
and utilizes a ranked list of documents on the centralized
sample database. By reranking the sample documents with
respect to result diversiﬁcation, a better set of sources can
be obtained in term of relevance and aspect coverage. The
second approach oﬀers a diﬀerent view. Instead of reranking
the sample documents based on their relevance to the query
aspects, the information sources are reranked in a similar
process. This can be done by estimating the relevance of
each information source with respect to diﬀerent aspects of
the query by any existing resource selection algorithm. Fur-
thermore, a learning based classiﬁcation approach is pro-
posed to combine multiple resource selection algorithms for
a better estimation of source relevance with respect to query
aspects. With some training data, the learning based clas-
siﬁcation approach can improve the eﬀectiveness of resource
selection for search result diversiﬁcation.

An extensive set of experiments has been conducted with
the federated search testbed of the Clueweb dataset to eval-
uate the proposed research. In many experimental settings,
the new approaches can successfully improve result diversi-
ﬁcation over traditional approaches that only consider doc-
ument relevance. In particular, the new approaches provide
superior performance on two test levels: source-level results
of resource selection and document-level results of the ﬁ-
nal ranked list of federated document retrieval. Finally, the
learning based approach, which combines results from mul-
tiple resource selection algorithms, outperforms each indi-
vidual algorithm in result diversiﬁcation.

The rest of the paper is organized as follows. Section
2 oﬀers the literature review on both resource selection in
federated search and result diversiﬁcation in ad hoc search.
Section 3 discusses our proposed metric to measure the di-
versity of selected sources. Section 4 proposes the two result
diversiﬁcation approaches for resource selection in federated
search. Section 5 presents the learning based classiﬁcation
approach. The new proposed research is examined by an
extensive set of experiments in Sections 6 and 7. Section 8
concludes our work and points out some potential research
directions in the future.

2. RELATED WORK

Considerable research have been conducted for all three
sub-problems of federated search as resource representation,
resource selection and result merging [4, 28, 15]. This section
provides a discussion of prior research on resource selection,
as well as a brief review on resource representation and re-
sult merging. We also discuss some popular ad hoc search
algorithms for search result diversiﬁcation.

Resource representation is the ﬁrst step of federated search
for obtaining important properties of distributed informa-
tion sources such as content and size statistics. Query-based
sampling [4] is a common approach as it obtains sample
documents from available sources with randomly generated
queries. The sample documents obtained in this process can
be placed together in a centralized sample database.

Resource selection selects a small number of most relevant
information sources for a user query. Some early resource

selection algorithms such as CORI [4], CVV [37] and KL
[36] treat each source as a big document and derive useful
statistics to rank available sources with respect to a user
query. However, these algorithms have limitation of los-
ing the boundaries of individual documents, and thus may
underestimate a big source with many relevant documents.
Topic modeling has been proposed by recent work to over-
come this limitation [3].

Other resource selection algorithms such as ReDDE [32],
DTF [17], CRCS [27] and SUSHI [33] step away from the big
document assumption by modeling individual documents of
a source. Several selection algorithms in this category rely
on the centralized sample database to build a ranked list of
sample documents for a user query, and then assign a rele-
vance score to available sources based on the scores of their
sample documents in the list. Diﬀerent algorithms use diﬀer-
ent methods for aggregating document contribution to avail-
able sources. Recent work by Markov et al. follows a sim-
ilar approach, but attempts to minimize uncertainty in the
centralized sample database by sampling diﬀerent queries,
retrieval systems, or rankings [22].

Learning based models have also been proposed for re-
source selection. They treat resource selection in federated
search [20], or vertical search [2] as a classiﬁcation prob-
lem. In particular, given a set of training queries and some
relevance judgment, a classiﬁcation model can learn to pre-
dict the relevance of an information source.
In some ex-
periments, the classiﬁcation approaches have been shown to
provide more accurate resource selection results than tradi-
tional algorithms without the training process.

Result merging is the last step in federated search, which
merges documents returned by selected sources into a single
ranked list. Modern methods such as SSL [31] and SAFE
[30] both attempt to merge documents by approximating the
centralize retrieval results in diﬀerent ways.

Existing research in federated search have not explored an
important issue of result novelty and diversiﬁcation, which
limits their abilities in representing the various information
needs of users. The research work in [29] estimates the de-
gree of document overlap among available sources, but its
focus is only on duplicate documents and does not directly
address the diversiﬁcation problem related with multiple as-
pects of user queries. Other research work in [25] and [39] ad-
dress diversiﬁcation in aggregated search, which is similar to
federated search, but operates in cooperative environments.
Most importantly, those work do not target diversiﬁcation
in selecting relevant verticals (or sources) directly.

On the other hand, result diversiﬁcation has been a pop-
ular research topic in ad hoc search.
Its goal is to make
a trade-oﬀ between relevance and novelty in ranking docu-
ments [38, 5, 1, 6, 26]. In order to achieve the desired eﬀect
of covering suﬃcient aspects of a user query (so that user
will likely ﬁnd the sought information), diversiﬁcation al-
gorithms target on discovering novel aspects that have not
been covered in the ranked list, and reducing the redundancy
information shared between multiple documents.

The earlier generation of diversiﬁcation algorithms do not
explicitly consider multiple aspects of a query [5, 38]. In-
stead, they build the ranked list from top to bottom, and
make a choice of whether to include a document based on its
similarity with existing documents in the list. More recent
diversiﬁcation algorithms directly incorporate query aspects
into consideration. Agrawal et al. proposed the use of tax-

614onomy to classify query aspects, in order to discover novel
and redundant information [1]. Carterette and Chandar di-
rectly optimized the ranked list with respect to evaluation
measures based on diversity [6]. Santos et al. proposed the
eXplitcit Query Aspect Diversiﬁcation (xQuAD) method,
which estimates the surplus information that a document
can add to a ranked list, using the probability of relevance
with respect to all aspects of the query [26]. Recently, Dang
and Croft proposed a diﬀerent view on diversiﬁcation by
preserving the proportionality of document presence with
respect to each aspect of the query [16]. Their proposed algo-
rithm PM-2 has proven to achieve superior performance over
several other algorithms of the same category. Other recent
work on search result diversiﬁcation include a combination
of implicit and explicit topic representations [19], personal-
ized diversiﬁcation [34], and explicit relevance model [35].
3. DIVERSIFICATION METRICS IN RE-

SOURCE SELECTION FOR FEDERATED
SEARCH

There exists several standard diversiﬁcation metrics for
ad hoc search. However, no evaluation metric has been de-
veloped to compare the diversiﬁcation results of diﬀerent re-
source selection algorithms in federated search. This section
proposes a new metric in order to ﬁll that gap. Our metric
is based on the popular R-metric [4] for resource selection
in federated search.
It is calculated as the ratio between
the number of relevant documents contained in sources se-
lected by a particular algorithm, over the number of rele-
vance documents in sources selected by an ideal algorithm.
In particular, the R-metric is deﬁned as:

(cid:80)k
(cid:80)k

i=1 Ei
i=1 Bi

Rk =

where Ei denotes the number of relevant documents of the i-
th source according to the ranking E by a particular resource
selection algorithm, and Bi denotes the same quantity with
respect to the optimal ranking B. In this case, the optimal
ranking B should order sources by the true number of their
relevant documents. We adopt the idea to our new generic
metric, which is called R-based diversiﬁcation metric, as fol-
lows.

M(optimal ranking of documents in S)

RM(S) =
M(optimal ranking of documents in all sources)
where S is the set of selected sources for comparison and M
is a diversity metric of a ranked list of documents such as
ERR-IA, α-nDCG, Prec-IA, S-Recall and NRBP. The op-
timal ranking of documents in S is the list that achieves
the best score with respect to metric M. For most of the
aforementioned metrics, ﬁnding the optimal ranked list is an
NP-hard problem, but this can be acceptably approximated
by a greedy algorithm (i.e., repeatedly select the next docu-
ment that maximizes the metric given the current ranking;
cf. [8, 13]). The intuition of the proposed metric is that, if
we can select a minimal set of sources that contains enough
diversiﬁed documents to reach the optimal measure, then the
R-based diversiﬁcation metric is maximized to 1. Otherwise,
it gives us an estimation of how far our selected sources are
from the optimal ones.

Like R-metric, the proposed R-based diversiﬁcation met-
ric is independent of the retrieval algorithm utilized by each

source. The R-based diversiﬁcation metric returns 1 if all
available sources are selected.
In general, for comparison
between diﬀerent resource selection algorithms, the maxi-
mum number of sources is determined beforehand.

4. TWO APPROACHES FOR DIVERSIFICA-

TION IN RESOURCE SELECTION

This section proposes two approaches for selecting diver-
siﬁed information sources. The ﬁrst approach extends the
ReDDE framework by ranking sample documents with con-
sideration to diversity. The second approach estimates the
source relevance to each aspect of the query and ranks the
sources based on the estimations of their aspect relevance.
4.1 Diversiﬁcation on Sample Documents un-

der ReDDE framework

We ﬁrst describe the Relevant Document Distribution Es-
timation (ReDDE) framework [32] for ranking sources based
on the centralized sample database.
In this framework, a
given query is issued to the centralized sample database to
retrieve a ranked list of sample documents. ReDDE makes
an assumption that each sample document in this list repre-
sents a number of (unseen) documents from the source that
it belongs to. Based on that, a source score is calculated
by aggregating all contribution from its sample documents.
The amount of contribution is scaled up depending on the
source’s size. Original ReDDE assigns a constant score for
all documents on the top part of the returned list, and mul-
tiplies that constant with the ratio of the estimated source
size over the sample size. The obtained quantity is then
used for aggregating source scores. The CRCS resource se-
lection algorithm [27] follows the same approach, but varies
the amount of contribution of each document by an exponen-
tial decay function, as documents further down the ranked
list have less contribution to its source.

ReDDE and many other algorithms of the same family
such as CRCS and SUSHI [33] utilize a ranking on the cen-
tralized sample database to estimate the relevance of avail-
able sources. For algorithms of this family, they mostly vary
in the way of deﬁning a utility function for each document in
the list. Having said that, the original ranked list from the
centralized sample database plays an important role. Non-
diversiﬁcation algorithms target on building a centralized
ranked list that covers as many relevant documents to the
query as possible. In many cases, this may not pay enough
attention to sources that cover multiple query aspects. For
diversiﬁcation purpose, we should be careful when select-
ing a source that mainly contains documents relevant to an
aspect that has been covered before.

The above observation suggests a way to achieve good
search diversiﬁcation results by constructing a ranked list
that covers several aspects of the query. We call this ap-
proach Diversiﬁcation approach based on sample Documents
(DivD). Instead of building a centralized ranked list that fo-
cuses only on relevance, we construct a ranked list that oﬀers
more diversity. The goal is to reduce the contribution of a
document (on behalf of its source) that may be relevant to
the query, but oﬀers less novelty in the overall ranking.

This approach can combine a wide range of resource selec-
tion algorithms with any diversiﬁcation algorithm that has
been developed before, for instance, PM-2 [16] and xQuAD
[26], which were mentioned in Section 2. A typical example

615Algorithm 1 Diversiﬁcation Approach based on Sample
Documents using ReDDE and PM-2

1: Initialize scores of all sources to 0
2: Rank documents of the centralized sample database by

an eﬀective retrieval algorithm, e.g. Indri

3: Rerank that list by PM-2
4: for each document on top of the ranked list do
5: Add a constant score c to the source containing the

document

6: end for
7: return the ranked list of all sources based on their

scores

of combining the standard ReDDE and PM-2 is shown in
Algorithm 1.

Some resource selection algorithms utilize the relevance
score of each document in the centralized sample ranked list
for ranking sources (e.g. ReDDE.top [2]). In the diversiﬁca-
tion approach that has been discussed so far, this relevance
score can be replaced by the diversity score given by a diver-
siﬁcation algorithm for ad hoc search. There are diﬀerent
interpretations about those diversity scores, depending on
the assumptions made by the diversiﬁcation algorithms. In
our experiments, diversity scores can be used with most re-
source selection algorithms eﬀectively.
4.2 Diversiﬁcation Approach based on Source-

level Estimation

The second approach follows a diﬀerent strategy than the
ﬁrst one. As presented in the previous section, the diversiﬁ-
cation approach based on sample documents works directly
on the ranked list of sample documents, which is not a nat-
ural component for resource selection algorithms similar to
CORI. Indeed, several resource selection algorithms utilize
summary statistics of a source to estimate its relevance to
a query.
It is not straightforward to apply a diversiﬁca-
tion method based on individual sample documents for those
kinds of algorithms.

This paper proposes another diversiﬁcation approach for
resource selection that operates at the source level. More
speciﬁcally, many existing diversiﬁcation algorithms for ad
hoc search rank documents by estimating their relevance
with respect to each aspect of the query, and then harvest-
ing this information in order to produce a ranked list that
balances multiple query aspects.
It is possible to design
a similar process that uses the estimated source relevance
with respect to each aspect of the query. More speciﬁcally,
instead of building a diversiﬁed list of documents for a given
query as in ad hoc search, we can build a diversiﬁed list
of available sources in a similar manner. All estimations
with respect to the documents can be replaced by estima-
tions with respect to the sources. This resource selection
approach for results diversiﬁcation is called Diversiﬁcation
approach based on Source-level estimation (DivS).

An important step in DivS is therefore to compute the
probability of relevance of a source with respect to a query
aspect. Many existing resource selection algorithms are able
to provide such information. CORI can directly provide a
relevance estimation based on the big document assump-
tion. ReDDE and CRCS do not provide direct estimations,
but it is possible to use their source scores aggregated from
sample documents for such a purpose. An example of the

Algorithm 2 Source-based Diversiﬁcation using ReDDE
and PM-2
1: Rank all sources using standard ReDDE
2: for each aspect qi of query q and each source sj do
3:

Estimate P (sj|qi), the probability of relevance of
source sj with respect to qi using the ReDDE algo-
rithm
4: end for
5: Rerank the list obtained in step 1 using PM-2 algorithm

with the estimated P (sj|qi) as inputs

6: return the ranked list of all sources based on their

diversiﬁcation scores

diversiﬁcation approach based on source-level estimation us-
ing ReDDE and PM-2 algorithms is presented in Algorithm
2.

Compared with diversiﬁcation approach based on sample
documents, the diversiﬁcation approach based on source-
level estimation can work with a wider range of resource
selection algorithms. On the other hand, it requires multiple
runs of a resource selection algorithm for all diﬀerent aspects
of a query, which is more time consuming than the former
approach. In real-world applications, it is possible to design
a parallel solution for multiple resource selection runs to
speed up the process.

5. A CLASSIFICATION APPROACH FOR

COMBINING DIVERSIFICATION
RESULTS

The two diversiﬁcation approaches based on sample doc-
uments and source-level estimation both utilize a speciﬁc
resource selection algorithm and a diversiﬁcation algorithm
for ad hoc search. It is possible to combine the results of
multiple resource selection algorithms for search result di-
versiﬁcation, which may provide better results by modeling
complementary results from diﬀerent algorithms. This sec-
tion proposes a learning based classiﬁcation approach. With
some training information, this method can learn how to
combine evidence supporting available sources from diﬀer-
ent resource selection algorithms for result diversiﬁcation.

In particular, we adapt the classiﬁcation-based approach
that has been used for both vertical search [2] and federated
search [20]. For collecting training information in learning
the classiﬁcation model, this approach generates a pseudo-
relevant judgment of a source given a query by counting
the number of relevant documents that the source contains.
If the number is higher than some threshold value τ , the
source is considered to be relevant. In this paper, the train-
ing dataset for result diversiﬁcation consists of multiple in-
stances, each of them represents a pair of a source and a
query aspect. A source is considered relevant to a query
aspect if it contains at least one document relevant to that
aspect of the query. For each pair of a source and a query
aspect, we utilize the source score information from the fol-
lowing resource selection algorithms as features:

• ReDDE.top [2] This is quite similar to the original
ReDDE algorithm, but uses the relevance scores of
documents in calculating source scores, instead of a
step function as in traditional ReDDE. ReDDE.top has

616been chosen since it provides more consistent results
than ReDDE in our dataset.

• CRCS [27] with exponential decay function for esti-
mating probability of relevance of sample documents.
This method has been reported by several studies to
be better than the linear decay version [33].

• Big Document This is a traditional approach that
collapses all sample documents of a source into a big
document, which represents this source and is used
for resource selection. Our retrieval algorithm for this
approach is Indri [23].

• CORI [4] CORI is another type of big document selec-
tion approach with a diﬀerent tf.idf weighting scheme.
It shares the same assumption that considers each source
as a big document of its sample documents.

More speciﬁcally, the learning based approach attempts
to naturally integrate evidence from two diﬀerent views of
resource selection algorithms, one based on big document
assumption (Big Document and CORI), and the other based
on aggregated information of sample documents (ReDDE.top
and CRCS). In the experimental section, we will provide
more analysis and some examples about why this combina-
tion strategy may outperform each individual method.

All the features provided by the above algorithms are nor-
malized for each query in order to achieve more consistency.
Given the training dataset, it is possible to design a learning
method that estimates P (s|qk) for a new query aspect qk.
We choose logistic regression model as it has been shown to
be among the best in many practical applications such as
text categorization [18].

Let sj

i be the binary variable that indicates the relevance
of the i-th source to the query qj, i.e. sj
i = 1 indicates rele-
vance, and sj
i be the vector
of all features returned by the resource selection algorithms
mentioned above. We can then represent the relevance prob-
ability of source si given f j

i = 0 indicates otherwise. Let f j

P (sj

i = 1|qj) =

i by a sigmoid function σ:
exp(f j
i · w)

= σ(f j

i · w)
i · w)

1 + exp(f j

where w denotes the combination weight vector.

Learning the combination weight w can be conducted by
maximizing the log-likelihood function using the iterative re-
weighted least squares method. The learned parameter can
be then used to estimate the relevance probability P (s|qk)
for any particular aspect of a new user query. This probabil-
ity becomes inputs for the diversiﬁcation approach based on
source-level estimation to rank the sources. Our hypothesis
is that, if the learning model can provide a more accurate
estimation than those produced by a single resource selec-
tion algorithm, we can expect the learning based approach
to generate more accurate results. This approach is denoted
as LR-DivS, as it applies logistic regression within the di-
versiﬁcation approach based on source-level estimation.

6. EXPERIMENTAL METHODOLOGY

Dataset.

The experiments in the paper have been conducted on
the federated search testbed of the Clueweb English dataset.

Table 1: Document statistics of the federated search testbed
based on Clueweb English

# of

sources
2,780

total #
of docs

min #
of docs

max #
of docs

avg #
of docs

151,161,188

48

3,417,805

54,364.27

The Clueweb dataset1 is a large collection of web pages that
has been used in several oﬃcial tasks in the Text REtrieval
Conference (TREC) tracks. Furthermore, the available in-
formation of queries with multiple aspects and the corre-
sponding relevance judgment has enabled the evaluation of
search result diversiﬁcation. The federated search testbed
derived from Clueweb is publicly available2 as an attempt to
oﬀer a large and realistic testing environment for federated
search. This collection contains 2, 780 information sources
and about 151 million documents, which is much larger than
most other testbeds that have been previously used in fed-
erated search.
Information sources are created by collect-
ing web documents of the same domains (e.g. blogspot.com,
about.com), and the Wikipedia documents are clustered into
100 collections using the K-means clustering algorithm with
two passes/iterations. More statistics information of this
dataset is given in Table 1. It is noticed that Clueweb has
also been used in distributed environment, albeit in a diﬀer-
ent problem setting [21].

We are aware of a recent dataset that has been proposed
for federated search in web search environment [24]. How-
ever, this dataset does not provide relevance judgments on
multiple aspects of a query, and thus does not fully support
experiments of this research at the moment.

Each information source in the testbed has been assigned
a retrieval algorithm, which was chosen from a set of al-
gorithms such as Inquery, Language Model and tf.idf in a
round-robin manner. This strategy simulates the behav-
ior that information sources in real world applications may
use diﬀerent types of retrieval algorithms. In order to build
the centralized sample database, 300 documents have been
sampled from each source via the source’s speciﬁc retrieval
algorithm. The Indri retrieval algorithm [23] was used in all
retrieval processes on the centralized sample database.

The queries used in our experiments consist of 148 queries
from the TREC Web Track 2009[9], 2010[10], and 2011[11].

Diversiﬁcation Algorithm.

Diversity by Proportionality [16] (PM-2) and eXplicit Query

Aspect Diversiﬁcation (xQuAD) [26], which are among the
state-of-the-art diversiﬁcation algorithms, have been exam-
ined in our study. In particular, we notice that the perfor-
mance of PM-2 tends to be better than xQuAD in most of
the metrics, thus we only report our results based on PM-2.
In our implementation of this algorithm, we have chosen to
rerank the top K = 500 documents in the centralized sample
database, as well as in the ﬁnal step of diversifying results
from the centralized complete database (i.e., all documents
in available sources). The parameter λ in PM-2 is set to be
0.5 in all settings.

1http://boston.lti.cs.cmu.edu/Data/clueweb09
2http://www.cs.purdue.edu/homes/dthong/clueweb/

617An important component of PM-2 and many other search
result diversiﬁcation algorithms is the estimation of P (d|qi),
which is the relevance score of a document d with respect
to a particular aspect qi of a query. These aspects are usu-
ally not available in real-world applications. We follow the
work of [16, 26] to report our results on two scenarios: all
the aspects of a query are provided; or we can retrieve the
aspects from a commercial search engine such as Google or
Bing. For the ﬁrst scenario, we use subtopics that come
with TREC queries as aspects. For the second scenario, we
send the original query to the search engine (Google in our
experiments), and adopt its suggestions as query aspects.
The second set contains 144 queries, as 6 original queries do
not have search engine’s suggestions at the moment of our
work. We name the ﬁrst scenario “Given Subtopics” and the
second “Suggestions” in the results presented in Section 7.

Resource Selection Algorithms.

A set of commonly used resource selection algorithms as
described in section 5 has been utilized in the new research
of result diversiﬁcation in resource selection. They consist
of ReDDE.top, CRCS with exponential decay function, Big
Document and CORI.

With CRCS, the top 500 sample documents returned by
the centralized sample database are considered. The expo-
nential decay function of CRCS makes it more stable on the
Clueweb collection, which exhibits a highly skewed distri-
bution of source sizes. On the other side, ReDDE.top is
more sensitive to noise in such an environment, as a not-
so-relevant sample document from a really big source may
result in too much bias in favor of that source and aﬀects the
ﬁnal ranked list. Therefore, for ReDDE.top, we set the num-
ber of top sample documents for each query to be smaller
than CRCS’s, chosen from the set {50, 100, 150,···}. We
report the results using top 50 documents for ReDDE.top
as it provides the most consistent performance.

To name the diﬀerent methods, we use a preﬁx “D” for
a diversiﬁcation resource selection algorithm to indicate an
approach based on sample documents, and a preﬁx “S” to
indicate an approach based on source-level estimation. As
discussed before, the Big Document and CORI algorithms
have only the S versions. For all methods reported in the
next section, we select up to 10 sources for each query.

Training and Testing.

All the proposed resource selection algorithms for search
result diversiﬁcation do not need any training data except
the ﬁnal approach of combining multiple resource selection
algorithms in a learning model (LR-DivS). Therefore, for
the algorithms other than LR-DivS, we report the results
on all 148 queries from TREC Web Track 2009-2011. For
LR-DivS, since it requires a training dataset, we use queries
with TREC id less than or equal to 75 for training, and the
rest for testing. We also report the results on the testing
set of all other algorithms for comparison. For the scenario
with suggested aspects of user queries, since there is no cor-
responding relevance judgment, we train our model using
the provided query subtopics/aspects and their correspond-
ing relevance judgments. However, in the testing phase, the
model is applied with features derived from the suggested
aspects, i.e., we estimate P (s|qi) where qi is a query as-
pect suggested by a commercial search engine. Finally, we
evaluate all approaches with respect to TREC’s provided

subtopics/aspects. This strategy is consistent with the eval-
uation process in TREC Web Track [11].

Evaluation Metric.

The proposed new research has been evaluated at two lev-

els: source selection and federated document retrieval.

• Diversiﬁcation results with R-based diversiﬁca-
tion metric for source selection: We evaluate the
resource selection results using the R-based diversiﬁ-
cation metric described in Section 3. In particular, ﬁve
popular metrics in result diversiﬁcation such as ERR-
IA[7], α-nDCG[13], NRBP[12], P-IA (intent aware pre-
cision [1]) and S-Recall (subtopic recall, for the number
of subtopics/aspects covered by top documents) have
been adopted with the R metric in resource selection
and used in the experiments.

• Diversiﬁcation results for federated document
retrieval: To make the evaluation independent from
a speciﬁc result merging algorithm, the Indri algorithm
is used to perform document retrieval on the central-
ized complete database. Only documents from the se-
lected sources for each query have been retained, which
is consistent with prior research in [32]. This ranked
list of documents is reranked again using the PM-2 al-
gorithm. We then evaluate the ﬁnal ranked list by the
ﬁve metrics mentioned above as ERR-IA, α-nDCG,
NRBP, P-IA and S-Recall. All of these metrics are
computed at the top 20 documents, which is consistent
with the oﬃcial TREC evaluation of search result di-
versiﬁcation for ad hoc search [11] and consistent with
the commonly used high-precision metric in federated
document retrieval.

7. EXPERIMENTAL RESULTS

An extensive set of experiments has been conducted for
evaluating several approaches proposed in this paper, which
are: the approach based on sample documents (DivD), the
approach based on source-level estimation (DivS) and the
learning based approach (LR-DivS). We conduct experiments
on two levels for diﬀerent purposes: source-level results for
resource selection and document-level results for federated
document retrieval. More speciﬁcally, the ﬁrst subsection
compares the performance of a diversiﬁcation approach with
standard resource selection algorithms. The second set of
experiments in 7.2 compares multiple resource selection al-
gorithms adapting DivD and DivS approaches. The third
set of experiments in 7.3 demonstrates the advantage of the
learning based approach (LR-DivS) over approaches with a
single resource selection algorithm. The last set of exper-
iments in 7.4 compares the document-level diversiﬁcation
results across all proposed approaches.
7.1 Diversiﬁcation versus Standard Resource

Selection Algorithms

This subsection compares the performance of two stan-
dard resource selection algorithms ReDDE.top and CRCS
with their diversiﬁcation counterparts at source selection
level. In particular, we choose to study the ﬁrst diversiﬁ-
cation approach based on sample documents (DivD) in this
subsection as they are more related with standard ReDDE.top
and CRCS algorithms, while more results of both diversiﬁ-
cation approaches will be presented shortly.

618Table 2 shows the performance using all the R-based diver-
siﬁcation metrics described in the previous section. Without
diversiﬁcation, it can be observed that the standard CRCS
signiﬁcantly outperforms standard ReDDE.top in all met-
rics. This may be attributed to the fact that CRCS gener-
ally selects more relevant sources to the query, which leads
to a wider range of aspects being covered. The advantage
of CRCS may come from using the exponential decay func-
tion for document utility, which tends to be better than
using document score as utility in ReDDE.top. When the
document-based diversiﬁcation approach is applied, it fur-
ther increases the performance of the standard algorithms:
D-ReDDE.top signiﬁcantly outperforms ReDDE.top in its
capacity of selecting diversiﬁed sources. As for CRCS, its di-
versiﬁcation version (i.e., D-CRCS) is also consistently bet-
ter than the standard CRCS algorithm. The same observa-
tion can be seen in both scenarios when the query aspects
are given, or suggested by Web search.
7.2 Diversiﬁcation with Different Resource Se-

lection Algorithms

This subsection studies the performance of the two pro-
posed resource selection approaches (i.e., DivD and DivS)
using several resource selection algorithms, including ReDDE-
top, CRCS, Big Document and CORI. The results are pre-
sented in Table 3. In all settings, the standard Big Docu-
ment and standard CORI algorithms are outperformed by
the other methods. Furthermore, both S-Big Document and
S-CORI, which are under the same assumption of collaps-
ing sample documents within a source, are inferior to S-
ReDDE.top and S-CRCS. These results indicate that ReDDE-
top and CRCS tend to be more eﬀective in resource se-
lection for result diversiﬁcation than Big Document and
CORI, which is consistent with previous research in feder-
ated search for resource selection without diversiﬁcation.

Both DivD and DivS approaches produce comparable re-
sults when applied to ReDDE.top and CRCS. The D-CRCS
version based on sample documents is better than its coun-
terpart based on source-level estimation (i.e., S-CRCS), whereas
the contrary is observed for ReDDE.top. In case of ReDDE.top,
the diﬀerence is signiﬁcant, which may be explained by the
fact that when the original ranked list is short (only 50 for
ReDDE.top), it is more diﬃcult for the diversiﬁcation algo-
rithm to ﬁnd a document that covers many query aspects,
rather than ﬁnding a source that covers many aspects.

The results using the provided query aspects and sug-
gested ones reveal an interesting observation. We notice
that the performance of the two settings are quite compara-
ble with little diﬀerence. One possible explanation is that,
in federated environment, it may not need a perfect set of
query aspects for selecting a diversiﬁed set of information
sources, as sources are already somehow divided by diﬀerent
types of semantic topics. Since our goal is to select sources
that can cover as many query aspects as possible, the re-
source selection algorithms can do a reasonably good work
as long as the suggestions of query aspects provide some
meaningful interpretations of diﬀerent aspects of the query.
7.3 Classiﬁcation Approach for Combining Di-

versiﬁcation Results

proach requires a set of training queries and does the test-
ing on another set, we also report the results of all previ-
ous methods on the testing queries for comparison. Table
4 presents the results. The performance of the standard
resource selection algorithms and their diversiﬁcation coun-
terparts are better on the set of testing queries than on the
set of all queries, due to the particular division of training
and testing queries. The comparison between standard algo-
rithms’ performance and those of diversiﬁcation approaches
on the testing queries raises similar observations as men-
tioned in the previous subsections 7.1 and 7.2.

It can be seen that the classiﬁcation approach provides
the best performance over all metrics. It can be attributed
to the fact that the learning based classiﬁcation approach
can harness the advantage of diﬀerent algorithms, and com-
bine them in an eﬀective way. A typical example from
our training set is the query “Obama family tree” with its
provided subtopic “Find the TIME magazine photo essay
Barack Obama’s Family Tree”. For ReDDE.top and CRCS,
it is almost impossible to ﬁnd a sample document containing
all the keywords of the subtopic/aspect, if such a document
does not exist in the sample database. On the other hand,
Big Document and CORI can provide some useful hints for
selecting sources by looking at all sample documents from
each source as whole. For example, several diﬀerent sam-
ple documents of a source may contain various parts of the
query. This is particularly useful for sources that cover a
wide range of topics, for instance, Wikipedia. Another ex-
ample is the query “Volvo” with its provided subtopic “Find
a Volvo dealer”. Big Document and CORI can give hint to
the sources that contain the words “Volvo” and “dealer” from
diﬀerent sample documents. For those sources, the classiﬁ-
cation approach can utilize the complementary results for
improving search results diversiﬁcation.
7.4 Diversiﬁcation Results of Federated Doc-

ument Retrieval

This subsection compares the search diversiﬁcation results
in document-level of federated document retrieval. Given
the top ten sources selected by the proposed algorithms for
each query, the ﬁnal ranked list generated from the sources
is evaluated. One set of results compares algorithms without
training over all queries. The other set of results is included
for the test queries to compare LR-DivS with other algo-
rithms. We omit the results of Big Document and CORI on
the ﬁrst set due to space limitation.

Table 5 provides the results on all queries. The standard
ReDDE.top algorithm falls behind the other models in its
diversiﬁcation capacity. Among all the methods without
training, D-CRCS consistently outperforms the other meth-
ods, which is consistent with its performance on the R-based
diversiﬁcation metrics in the source-level.

Table 6 provides the results on the test queries. Again,
the same trend with algorithms without training can be
observed. When some training information is available to
learn how to combine multiple evidence, the classiﬁcation
approach LR-DivS consistently provides the best document-
level diversiﬁcation performance among all models.

8. CONCLUSION AND FUTURE WORK

This subsection compares the performance of the learning
based classiﬁcation approach with all other diversiﬁcation
approaches mentioned above. Since the classiﬁcation ap-

Resource selection is an important research problem in
federated search for selecting a small number of relevance
sources for a given query. Various algorithms have been pro-

619Table 2: R-based diversiﬁcation metric of resource selection on all 148 queries. Symbols † and ‡ indicate statistical signiﬁcance
under paired t-test with respect to ReDDE.top baseline and CRCS baseline (p < 0.05).

n
e
v
i
G

s
c
i
p
o
t
b
u
S

s
n
o
i
t
s
e
g
g
u
S

Baseline

DDiv

Baseline

DDiv

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
ReDDE.top
CRCS
D-ReDDE.top
D-CRCS

R-ERR R-α-nDCG R-NRBP R-P-IA R-S-Recall
0.549
0.677†
0.624†
0.699†
0.542
0.673†
0.614†
0.698†

0.406
0.484†
0.438†
0.499†‡
0.396
0.477†
0.434†
0.491†‡

0.566
0.698†
0.645†
0.721†
0.558
0.694†
0.635†
0.720†

0.545
0.658†
0.612†
0.674†
0.541
0.659†
0.604†
0.677†

0.529
0.652†
0.598†
0.672†
0.522
0.649†
0.589†
0.671†

Table 3: R-based diversiﬁcation metric on resource selection on multiple diversiﬁcation approaches. Letters r, c, R, C, B, O
indicate statistical signiﬁcance under paired t-test to D-ReDDE.top, D-CRCS, S-ReDDE.top, S-CRCS, S-BigDoc, and S-CORI
respectively (p < 0.05).

DDiv

SDiv

s
c
i
p
o
t
b
u
S

n
e
v
i
G

s DDiv
n
o
i
t
s
e
g
g
u
S

SDiv

D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI

R-ERR
0.624B
0.699r B O
0.680r B O
0.675r B O
0.490
0.569B
0.614B
0.698r B O
0.677r B O
0.673r B O
0.490
0.566B

R-α-nDCG R-NRBP
0.598B O
0.672r B O
0.656r B O
0.649r B O
0.468
0.541B
0.589B
0.671r B O
0.652r B O
0.647r B O
0.469
0.538B

0.645B
0.721r B O
0.701r B O
0.698r B O
0.506
0.591B
0.635B
0.720r B O
0.698r B O
0.695r B O
0.508
0.587B

R-P-IA
0.438B O
0.499r C B O
0.496r C B O
0.462r B O
0.346
0.351B
0.434B O
0.491r C B O
0.485r C B O
0.458B O
0.342
0.347B

R-S-Recall
0.612B
0.674r B O
0.664r B O
0.664r B O
0.486
0.561B
0.604B
0.677r B O
0.666r B O
0.665r B O
0.490
0.562B

Table 4: R-based diversiﬁcation metric of resource selection on multiple diversiﬁcation approaches on test queries. Symbols
†,‡, r, c, R, C, B, O indicate signiﬁcant improvement under paired t-test to ReDDE.top, CRCS, D-ReDDE.top, D-CRCS, S-
ReDDE.top, S-CRCS, S-BigDoc, and S-CORI respectively. (p < 0.05).

s
c
i
p
o
t
b
u
S

n
e
v
i
G

s
n
o
i
t
s
e
g
g
u
S

Baseline

DDiv

SDiv

LR-DivS

Baseline

DDiv

SDiv

LR-DivS

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI

†‡
rcRCBO

R-ERR
0.618
†
0.769
rBO
†
0.711
B
†
0.778
rBO
†
0.775
rBO
†
0.786
rBO
0.615
0.653
0.873
0.614
†
0.768
BO
†
0.724
B
†
0.781
rBO
†
0.799
rBO
†
0.787
rBO
0.614
0.647
0.821

†‡
rCBO

†‡
rcRCBO

R-α-nDCG
0.604
†
0.751
rBO
†
0.694
B
†
0.762
rBO
†
0.757
rBO
†
0.769
rBO
0.594
0.634
0.853
0.600
†
0.751
BO
†
0.705
B
†
0.765
rBO
†
0.779
rBO
†
0.771
rBO
0.593
0.629
0.805

†‡
rCBO

†‡
rcRCBO

R-NRBP
0.630
†
0.783
rBO
†
0.725
B
†
0.791
rBO
†
0.789
rBO
†
0.800
rBO
0.632
0.669
0.889
0.626
†
0.782
BO
†
0.739
B
†
0.794
rBO
†
0.814
rBO
†
0.801
rBO
0.632
0.662
0.834

†‡
rBO

R-P-IA
0.461
†
0.550
rBO
†
0.507
BO
†
0.557
rBO
†
0.567
rcCBO
†
0.544
rBO
0.415
0.396
0.705
0.463
†
0.555
BO
†
0.525
BO
†
0.558
rBO
†‡
0.590
rcCBO
†
0.550
rBO
0.411
0.396
0.629

†‡
rcRCBO

†‡
rcRCBO

†‡
rcRCBO

R-S-Recall
0.635
†
0.768
BO
†
0.726
B
†
0.782
rBO
†
0.778
rCBO
†
0.802
rRBO
0.621
0.676
0.856
0.629
†
0.768
BO
†
0.735
B
†
0.787
rBO
†
0.800
rBO
†
0.803
rBO
0.624
0.672
0.827

†‡
rBO

620Table 5: Diversiﬁcation results on document retrieval on all queries. The symbols †,‡, r, c, R, C indicate statistical signiﬁcance
under paired t-test to ReDDE.top baseline, CRCS baseline, D-ReDDE.top, D-CRCS, S-ReDDE.top, and S-CRCS respectively
(p < 0.05).

s
c
i
p
o
t
b
u
S

n
e
v
i
G

Baseline

DDiv

SDiv

s Baseline
n
o
i
t
s
e
g
g
u
S

DDiv

SDiv

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS

r

ERR
0.246
†
0.380
rR
0.319†
†
0.383
rR
0.357†
0.365†
r
0.249
0.339†
0.304†
†
0.356
rR
0.323†
0.342†

r

r

r

r

α-nDCG NRBP
0.276
0.406†
0.345†
†
0.410
rR
0.387†
0.395†
r
0.277
0.378†
0.333†
†
0.390
rR
0.359†
0.376†

0.226
†
0.365
rR
0.304†
†
0.365
rR
0.340†
0.346†
r
0.230
0.315†
0.286†
†‡
0.335
rR
0.300†
0.320†

r

r

r

r

r

P-IA
0.111
0.152rR
0.123†
†
0.151
rR
0.141†
0.142†
r
0.114
†
0.157
rR
0.129†
†
0.157
rR
0.144†
0.147†

r

r

r

r

r

S-Recall
0.409
0.530†
0.477†
0.535†
0.526†
0.534†
r
0.400
0.535†
0.469†
†
0.530
rR
0.512†
0.530†

r

r

r

Table 6: Diversiﬁcation results on document retrieval on test queries. The symbols †,‡, r, c, R, C, B, O indicate statistical
signiﬁcance under paired t-test to ReDDE.top baseline, CRCS baseline, D-ReDDE.top, D-CRCS, S-ReDDE.top, S-CRCS,
S-BigDoc, and S-CORI respectively (p < 0.05).

s
c
i
p
o
t
b
u
S

n
e
v
i
G

s
n
o
i
t
s
e
g
g
u
S

Baseline

DDiv

SDiv

LR-DivS

Baseline

DDiv

SDiv

LR-DivS

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI

ReDDE.top
CRCS
D-ReDDE.top
D-CRCS
S-ReDDE.top
S-CRCS
S-BigDoc
S-CORI

ERR
0.304
†
0.478
rBO
0.422†
†
0.485
rBO
†
0.463
BO
†
0.473
rBO
0.370
0.368
0.515
0.337
†
0.469
rBO
†
0.434
B
†
0.487
rBO
†
0.462
BO
†
0.472
BO
0.347
0.370
0.509

†
rRCBO

†‡
rRCBO

α-nDCG
0.345
†
0.512
rBO
0.457†
†
0.523
rBO
†
0.501
rBO
†
0.515
rBO
0.390
0.393
†
0.555
rRCBO
0.373
†
0.509
rBO
†
0.471
BO
†
0.524
rBO
†
0.504
rBO
†
0.512
rBO
0.374
0.400
0.543

†‡
rRCBO

NRBP
0.278
†
0.457
rBO
0.400†
†
0.460
rBO
†
0.438
BO
†
0.446
BO
0.359
0.354
†
0.489
rRCBO
0.311
†
0.445
BO
†
0.410
B
†
0.463
rBO
†
0.435
BO
†
0.448
BO
0.330
0.349
0.487

†‡
rRBO

†‡
rcRCBO

P-IA
0.151
†
0.217
rRBO
†
0.178
O
†
0.219
rRBO
†
0.200
rBO
†
0.206
rBO
0.151
0.129
0.243
0.167
†
0.235
rBO
†
0.202
O
†
0.233
rBO
†
0.224
BO
†
0.219
BO
0.157
0.150
†
0.249
rBO

†‡
rRBO

S-Recall
0.509
†
0.668
BO
†
0.622
B
†
0.677
rBO
†
0.678
rBO
†
0.687
rBO
0.519
0.545
0.722
0.518
†
0.671
BO
†
0.637
B
†
0.675
BO
†
0.674
BO
†
0.675
BO
0.511
0.560
†
0.690
rBO

posed for resource selection in federated search, but limited
attention has been paid to result novelty and diversiﬁcation,
which aﬀects the eﬀectiveness of existing algorithms. As far
as we know, this paper proposes the ﬁrst piece of research
for incorporating search result diversiﬁcation in resource se-
lection for federated search.

A family of new evaluation metrics is ﬁrst proposed for
measuring search result diversiﬁcation in resource selection,
which combines some popular diversiﬁcation metrics in ad
hoc search with the recall-based evaluation metric in re-
source selection. Two general approaches are then proposed
for diversiﬁcation in selecting relevant sources. The ﬁrst ap-
proach is based on sample documents, which ranks sample
documents with respect to result diversiﬁcation, and then

utilizes the ReDDE framework for ranking the sources. The
second approach is based on source-level estimation, which
directly ranks each information source as a whole for result
diversiﬁcation. Furthermore, a learning based classiﬁcation
approach is proposed to combine multiple resource selection
algorithms for more accurate diversiﬁcation results.

An intensive set of empirical studies has been conducted
to evaluate the proposed research on the Clueweb feder-
ated search dataset. Both the approach based on sample
documents and on source-level estimation can outperform
traditional resource selection algorithms in result diversi-
ﬁcation in both source-level for resource selection and in
the document-level for federated document retrieval. More-
over, the learning based approach, which combines outputs

621of multiple resource selection algorithms for result diversi-
ﬁcation, has been shown to generate the best results when
some training data is available.

There are several possible directions to pursue in the fu-
ture. The learning based method in this paper utilizes a
simple model for combining outputs of multiple algorithms
for result diversiﬁcation, while a more sophisticated learning
method may be more eﬀective. Furthermore, it is an inter-
esting topic to design new result merging algorithms with
the focus on result diversiﬁcation.

9. ACKNOWLEDGMENTS

This work is partially supported by NSF research grants
IIS-0746830, CNS- 1012208 and IIS-1017837. This work is
also partially supported by the Vietnam Education Foun-
dation, the Center for Science of Information (CSoI), an
NSF Science and Technology Center, under grant agreement
CCF-0939370, and a travel grant from the ACM Special In-
terest Group on Information Retrieval. Any opinions, ﬁnd-
ings and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily re-
ﬂect those of the sponsors.

10. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.

Diversifying search results. In Proceedings of the Second
ACM International Conference on Web Search and Data
Mining pages 5–14, 2009.

[2] J. Arguello, J. Callan, and F. Diaz. Classiﬁcation-based

resource selection. CIKM’09, pages 1277–1286, 2009.

[3] M. Baillie, M. Carman, and F. Crestani. A multi-collection

latent topic model for federated search. Information
Retrieval, 14(4):390–412, 2011.

[4] J. Callan. Distributed information retrieval. Advances in

Information Retrieval, pages 127–150, 2000.

[5] J. Carbonell and J. Goldstein. The use of MMR,

diversity-based reranking for reordering documents and
producing summaries. In SIGIR’98, pages 335–336, 1998.

[6] B. Carterette and P. Chandar. Probabilistic models of
ranking novel documents for faceted topic retrieval. In
CIKM’09, pages 1287–1296, 2009.

[7] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.

Expected reciprocal rank for graded relevance. In
CIKM’09, pages 621–630. ACM, 2009.

[8] H. Chen and D. Karger. Less is more: probabilistic models
for retrieving fewer relevant documents. In SIGIR’06, pages
429–436, 2006.

[9] C. Clarke, N. Craswell, and I. Soboroﬀ. Overview of the

TREC 2009 Web Track. TREC, pages 1–9, Jan. 2009.

[10] C. Clarke, N. Craswell, I. Soboroﬀ, and G. V. Cormack.

Overview of the TREC 2010 Web Track. TREC, pages 1–9,
Jan. 2010.

[11] C. Clarke, N. Craswell, I. Soboroﬀ, and E. Voorhees.

Overview of the TREC 2011 Web Track. pages 1–9, Jan.
2011.

[12] C. Clarke, M. Kolla, and O. Vechtomova. An eﬀectiveness

measure for ambiguous and underspeciﬁed queries.
Advances in Information Retrieval Theory, pages 188–199,
2009.

[13] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova,

A. Ashkan, S. B¨uttcher, and I. MacKinnon. Novelty and
diversity in information retrieval evaluation. In SIGIR’08,
pages 659–666, 2008.

[14] N. Craswell. Methods for Distributed Information Retrieval.

PhD thesis, The Australian National University, 2000.

[15] F. Crestani and I. Markov. Distributed Information

Retrieval and Applications. In Proceedings of ECIR, Jan.
2013.

[16] V. Dang and W. B. Croft. Diversity by proportionality: an
election-based approach to search result diversiﬁcation. In
SIGIR’12, pages 65–74. ACM, 2012.

[17] N. Fuhr. Resource Discovery in Distributed Digital

Libraries. In In Digital Libraries ’99: Advanced Methods
and Technologies, Digital Collections, 1999.

[18] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale

Bayesian logistic regression for text categorization.
Technometrics, 49(3):291–304, 2007.

[19] J. He, V. Hollink, and A. de Vries. Combining implicit and

explicit topic representations for result diversiﬁcation. In
SIGIR’12, pages 851–860. ACM, 2012.

[20] D. Hong, L. Si, P. Bracke, M. Witt, and T. Juchcinski. A

joint probabilistic classiﬁcation model for resource
selection. SIGIR’10, pages 98–105, 2010.

[21] A. Kulkarni and J. Callan. Document allocation policies for

selective searching of distributed indexes. CIKM’10, pages
449–458, 2010.

[22] I. Markov, L. Azzopardi, and F. Crestani. Reducing the

Uncertainty in Resource Selection. In Proceedings of ECIR,
2013.

[23] D. Metzler and W. B. Croft. Combining the language model
and inference network approaches to retrieval. Information
Processing and Management, 40(5):735–750, 2004.

[24] D. Nguyen, T. Demeester, D. Trieschnigg, and D. Hiemstra.

Federated Search in the Wild. In CIKM ’12, pages
1874–1878, 2012.

[25] R. L. Santos, C. Macdonald, and I. Ounis. Aggregated
search result diversiﬁcation. Advances in Information
Retrieval Theory, pages 250–261, 2011.

[26] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting
query reformulations for web search result diversiﬁcation.
In Proceedings of the 19th international conference on
World wide web, pages 881–890. ACM, 2010.

[27] M. Shokouhi. Central-rank-based collection selection in

uncooperative distributed information retrieval. Advances
in Information Retrieval, 2007.

[28] M. Shokouhi and L. Si. Federated Search. 2011.
[29] M. Shokouhi and J. Zobel. Federated Text Retrieval From

Uncooperative Overlapped Collections. SIGIR’07, pages
789–790, 2007.

[30] M. Shokouhi and J. Zobel. Robust result merging using

sample-based score estimates. ACM Transactions on
Information Systems (TOIS), 27(3):1–29, 2009.

[31] L. Si and J. Callan. A semisupervised learning method to

merge search engine results. ACM Transactions on
Information Systems (TOIS), 21(4):457–491, 2003.
[32] L. Si and J. Callan. Relevant document distribution

estimation method for resource selection. SIGIR’03, pages
298–305, 2003.

[33] P. Thomas and M. Shokouhi. Sushi: Scoring scaled samples

for server selection. In SIGIR’09, pages 419–426. ACM,
2009.

[34] D. Vallet and P. Castells. Personalized diversiﬁcation of
search results. In SIGIR’12, pages 841–850. ACM, 2012.

[35] S. Vargas, P. Castells, and D. Vallet. Explicit relevance

models in intent-oriented information retrieval
diversiﬁcation. In SIGIR’12, pages 75–84. ACM, 2012.

[36] J. Xu and W. B. Croft. Cluster-based language models for

distributed retrieval. In SIGIR’99, pages 254–261, 1999.
[37] B. Yuwono and D. L. Lee. Server ranking for distributed

text retrieval systems on the internet. In Proceedings of the
Fifth International Conference on Database Systems for
Advanced Applications (DASFAA), pages 41–50, 1997.

[38] C. X. Zhai, W. Cohen, and J. Laﬀerty. Beyond independent

relevance: methods and evaluation metrics for subtopic
retrieval. In SIGIR’03, pages 10–17, 2003.

[39] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose.

Evaluating aggregated search pages. In SIGIR’12, pages
115–124, 2012.

622