Addressing Cold-Start in App Recommendation:

Latent User Models Constructed from Twitter Followers

Jovian Lin1,2

Kazunari Sugiyama1

Min-Yen Kan1,2

Tat-Seng Chua1,2

1School of Computing, National University of Singapore, Singapore

2Interactive and Digital Media Institute, National University of Singapore, Singapore

jovian.lin@gmail.com

{sugiyama,kanmy,chuats}@comp.nus.edu.sg

ABSTRACT
As a tremendous number of mobile applications (apps) are readily
available, users have difﬁculty in identifying apps that are relevant
to their interests. Recommender systems that depend on previous
user ratings (i.e., collaborative ﬁltering, or CF) can address this
problem for apps that have sufﬁcient ratings from past users. But
for apps that are newly released, CF does not have any user ratings
to base recommendations on, which leads to the cold-start problem.
In this paper, we describe a method that accounts for nascent in-
formation culled from Twitter to provide relevant recommendation
in such cold-start situations. We use Twitter handles to access an
app’s Twitter account and extract the IDs of their Twitter-followers.
We create pseudo-documents that contain the IDs of Twitter users
interested in an app and then apply latent Dirichlet allocation to
generate latent groups. At test time, a target user seeking recom-
mendations is mapped to these latent groups. By using the tran-
sitive relationship of latent groups to apps, we estimate the prob-
ability of the user liking the app. We show that by incorporating
information from Twitter, our approach overcomes the difﬁculty of
cold-start app recommendation and signiﬁcantly outperforms other
state-of-the-art recommendation techniques by up to 33%.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information ﬁltering,
Search process

General Terms
Algorithms, Experimentation, Performance

Keywords
Recommender systems, Collaborative ﬁltering, Cold-start problem,
Mobile apps, Latent user models, Twitter

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

Mobile applications (apps) for smartphones are soaring in pop-
ularity and creating economic opportunities for app developers,
companies, and marketers. At present, users are able to access a
substantial number of apps via App Stores; furthermore, the selec-
tion available in app stores is growing rapidly as new apps are ap-
proved and released daily (Apple recently reported that more than
40 billion apps have been downloaded for its devices1). While this
growth has provided users with a myriad of unique and useful apps,
the sheer number of choices also makes it more difﬁcult for users
to ﬁnd apps that are relevant to their interests.

Historically, recommender systems have been introduced to al-
leviate this type of information overload by helping users ﬁnd rele-
vant items (i.e., apps). One effective and widely used recommender
system technique, collaborative ﬁltering (CF), employs user ratings
to recommend items. However, CF is ineffective in the case of new
apps as they have no prior ratings to base recommendations on.
This is an important manifestation of the cold-start problem, in
which the performance of CF systems suffers for items with few
or no prior ratings. The only way for the systems to provide au-
tomatic recommendation is to either wait for sufﬁcient ratings to
be supplied by users—which will take some time—or use content-
based ﬁltering (CBF).

CBF algorithms seek to recommend items based on similar con-
tent. For apps, this would be the metadata such as textual de-
scriptions and genre information. An obvious drawback of CBF
is that the recommended items are similar to the user’s previously-
consumed items [23]. For example, if a user has only downloaded
weather-related apps, CBF would recommend other apps that are
related to weather. This lack of diversity results in unsatisfactory
recommendations.

With the rise of social networking services (SNS), people broad-
cast and message friends, colleagues, and the general public about
many different matters in a short and informal manner [25]. They
do it often as the overhead of broadcasting such short messages
is low. For instance, Twitter experienced several record-breaking
events in 2012, such as having one million accounts created daily
and having 175 million tweets sent out per day2. Additionally,
SNS often include rich information about its users [33], such as
posted user-generated content, user logs, and user-to-user connec-
tions (e.g., Alice follows Bob).

People use SNS to talk about many subjects—including apps.
An interesting possibility thus arises: Can we merge information

1App Store Tops 40 Billion Downloads with Almost Half in 2012. Retrieved Jan 10,
2013, from http://goo.gl/fvND9.
2100 Fascinating Social Media Statistics and Figures From 2012. Retrieved Jan 10,
2013, from http://goo.gl/IvSJW.

283Figure 1: For two months since its release on the iTunes App Store, the Evernote app did not have any ratings. However, its Twitter
account already had active tweets and followers. This shows that despite the cold-start, there is still information out there about the
app, particularly on social networking services like Twitter.

mined from the rich data in SNS to enhance the performance of
app recommendation? More formally, can we address the cold-
start weaknesses of the proprietary user models of recommender
systems (e.g., the user proﬁles in an app store) by using nascent
signals about apps from SNS (e.g., the user proﬁles in Twitter)?

Through our case studies on the correlation and lag between SNS
and formal reviews in app stores, we veriﬁed that the answer to this
question is indeed “yes.” How then, would one go about captur-
ing and encoding data from SNS? Through our observation of app
stores, we note that the descriptions of some apps contain refer-
ences to their Twitter accounts; by having a Twitter account, an
app’s developer or the organization can interact with its users on
Twitter and market themselves, such as announcing new apps or
updates. For example, within the descriptions in its Google Play3
and iTunes4 app stores, the “Angry Birds Star Wars” app has a line
that says “follow @angrybirds5 on Twitter.”

We also observed that app mentions in SNS can precede for-
mal user ratings in app stores. This is important as it asserts that
an early signal for app ranking (and thus recommendation) can be
present in SNS. For example, Figure 1 shows that the “Evernote”
app that was released in May 2012 had no ratings in the iTunes App
Store for two months. It was only until July 2012 that the ﬁrst few
ratings started coming in. However, even before May 2012, Ever-
note’s Twitter account6 already had more than 120,000 followers
and 1,300 tweets. We take advantage of this active yet indirect in-
formation that is present in Twitter and use it to alleviate the cold-
start problem that besets newly released apps.

We integrate these ﬁndings into a novel approach to app rec-
ommendation that leverages on information from SNS (in speciﬁc,
Twitter) to drive recommender systems in cold-start situations. By
extracting follower information from an app’s (ofﬁcial) Twitter ac-
count, we create a set of pseudo-documents that contains Twitter-
follower information; these pseudo-documents are different from
the standard documents written in natural language. We then uti-
lize latent Dirichlet allocation (LDA) [6] to construct latent groups
of “Twitter personalities” from the pseudo-documents. By harness-
ing information from the linked topology of Twitter accounts, we
can infer a probability of how likely a target user will like a newly
released app—even when it has no ofﬁcial ratings.

We conduct extensive experiments and compare the usage of
Twitter-follower information with other types of features, such as
app genres, app developers, and text from app descriptions. In or-

3“Angry Birds Star Wars” in Google Play Store. Retrieved Jan 10, 2013, from http:
//goo.gl/Dn0T1.
4“Angry Birds Star Wars” in iTunes App Store. Retrieved Jan 10, 2013, from http:
//goo.gl/9ov2K.
5http://twitter.com/angrybirds
6http://twitter.com/evernote

der to show that our approach excels not because of the use of Twit-
ter information alone, we compare our approach with a non-LDA
technique that also employs the same information from Twitter. Fi-
nally, we combine our Twitter-follower feature with other features
gleaned from app metadata through the use of gradient boosted
trees (GBT) [10] and compare our approach with other state-of-
the-art techniques. We show that our approach signiﬁcantly out-
performs these techniques.

2. RELATED WORK
2.1 Addressing the Cold-Start

As the lack of ratings (i.e., the cold-start) hinders the use of CF
techniques [29], various alternatives have been employed to over-
come the problem. For example, Zhou et al. [35] experimented
with eliciting new user preferences by using decision trees with
collaborative ﬁltering. Moshfeghi et al. [21] proposed a method for
combining content features such as semantic and emotion informa-
tion with ratings information for the recommendation task. Liu et
al. [19] identiﬁed representative users whose linear combinations
of tastes are able to approximate other users. Likewise, many other
works attempt to overcome the cold-start by ﬁnding radical ways
of using proprietary user models and content.
2.2 Latent Factor Models

Another effective approach to the cold-start is to use latent fac-
tor models [17] that map users and items into a dense and reduced
latent space that captures their most salient features. These mod-
els provide better recommendations than traditional neighborhood
methods [11] as they reduce the level of sparsity and improve scal-
ability [16]. Notable latent factor models include probabilistic la-
tent semantic analysis (PLSA) [12], principal component analysis
(PCA) [15], neural networks such as the restricted Boltzmann ma-
chine (RBM) [27], and singular vector decomposition (SVD) [30].
However, latent factor models have the following two major disad-
vantages in recommendation tasks: Firstly, the learned latent space
is not easy to interpret. Secondly, many latent factor models rely
on other user ratings, which may be lacking if the dataset is sparse.
2.3 Probabilistic Topic Models

In order to better understand learned latent groups, a few recent
recommender systems employ probabilistic topic models. These
topic models [5] discover a set of “topics” from a large collection
of documents, where a topic is a distribution over terms that is bi-
ased around those associated under a single theme [31]. One of
the widely used probabilistic topic models is latent Dirichlet allo-
cation (LDA) [6], which can be seen as a dimensionality reduction
technique like latent factor models, but with proper underlying gen-

284erative probabilistic semantics that make sense for the type of data
that it models.

Topic models provide an interpretable, low-dimensional repre-
sentation of the documents [7] and may aid in the recommendation
process. In recent works, Ramage et al. [24] used a variant of LDA
to characterize microblogs in order to recommend which users to
follow. They used both tweets and hashtags to help identify promi-
nent topics for Twitter users, and used these topics to characterize
the users. Wang and Blei [31] combined CF based on latent factor
models and content based on probabilistic topic modeling to rec-
ommend scientiﬁc papers. LDA is also employed in [21], which
we described in Section 2.1.
2.4 User Proﬁling Using Social Networks

With the emergence of social networks, recommender systems
that rely on SNS have started to gain popularity. For example, Said
et al. [26] explored movie recommendations based on user ratings
and a social network (that comes with the user ratings data) where
users can befriend one another. Jamali and Ester [14] incorporated
trust propagation mechanisms into the matrix factorization tech-
nique in order to recommend items. We note that the aforemen-
tioned works rely on social networks within the dataset itself; that
is, they do not rely on external SNS. As many domains and datasets
do not incorporate a social network of their own, it is more realistic
if we can glean useful data from an external and ubiquitous source,
such as Twitter.

In addition, given an external social network, a standard way of
proﬁling users is through the identiﬁcation of inﬂuence [3], which
can be measured in a number of ways. Kwak et al. [18] compared
three different measures of inﬂuence on Twitter (number of fol-
lowers, PageRank [22], and number of retweets) and found that
the ranking of the most inﬂuential users differed depending on the
measure. Similarly, Cha et al. [9] also compared three different
measures of inﬂuence (number of followers, number of retweets,
and number of mentions) and found that all three captured differ-
ent perspectives of inﬂuence. We note that “following” information
has generally been a popular indicator for measuring inﬂuence.

Our work differs from the ones mentioned in Section 2.1 in that
instead of using proprietary user models and content, we use infor-
mation from Twitter (i.e., non-proprietary content from SNS) and
construct latent groups of “Twitter personalities” to predict recom-
mendations under the cold-start. Although textual features have
generally been a popular source of alternative data to substitute for
the lack of ratings—such that even state-of-the-art techniques (Sec-
tion 2.2 and 2.3) are primarily dependent on—it is not a universal
solution as not all domains contain reliable textual data. Addition-
ally, as mentioned in Section 2.4, it is more realistic to rely on ex-
ternal SNS. Therefore, our work is unique in that it uses the “who
follows whom” information on Twitter as its primary source of data,
as textual features in the app domain are inherently noisy and unre-
liable (in contrast to the cases of movies or scholarly papers).

3. OUR APPROACH

We ﬁrst explain the kind of problem we address. Then we de-
scribe the relation between apps and Twitter-followers, and how
we use them in our work. Next, we construct pseudo-documents
and pseudo-words using data from users, apps, users’ preferences,
and Twitter-followers. Thereafter, we generate latent groups from
the pseudo-documents, whereby a latent group represents the com-
bined “interests” of various Twitter-followers. Finally, the set of
latent groups is used as a crucial component of our algorithm to
estimate the probability of a target user liking an app.

Figure 2: Difference between (a) in-matrix prediction and (b)
out-of-matrix prediction.

3.1 Targeting the Cold-Start Problem

There are two types of cold-start problems in CF: in-matrix pre-
diction and out-of-matrix prediction [31]. Figure 2(a) illustrates
in-matrix prediction, which refers to the problem of recommend-
ing items that have been rated by at least one user in the system.
This is the task that CF researchers have often addressed [8, 1, 13,
28, 34, 4, 27, 17].

On the other hand, Figure 2(b) illustrates out-of-matrix predic-
tion, where newly released items (e.g., items 4 and 5) have never
been rated. Traditional CF algorithms cannot predict ratings of
items in the out-of-matrix prediction as they rely on user ratings,
which are unavailable in this scenario.

Our work focuses on this second, more challenging problem.
Hereafter, we use “cold-start problem” to refer to “out-of-matrix
prediction,” for ease of reference.
3.2 Apps and their Twitter-Followers

Apps have textual descriptions displayed in their app store en-
tries; some of these descriptions further contain links to the apps’
ofﬁcial Twitter account (i.e., Twitter handle). For example, the
“Angry Birds” franchise contains a link to its Twitter handle (@an-
grybirds). From the handle, we can identify the IDs of Twitter-
followers who follow the app. We note that by following an app’s
Twitter handle, the Twitter-followers subscribe to the tweets that
are related to the particular app, which can be seen as an indica-
tor of interest [9]. Figure 3 illustrates the relation between users,
apps, and Twitter-followers. By using information from the apps’
Twitter-followers, we can construct “latent personalities” from two
sources of data: the app store and Twitter. Using these latent per-
sonalities, our algorithm is able to recommend newly released apps
in the absence of ratings7 as shown in Figure 2(b).

Given that an app has a set of Twitter-followers, our approach
estimates the probability that user u—deﬁned by his or her past
ratings—likes app a that is followed by Twitter-follower t (i.e.,
Twitter-follower t follows app a’s Twitter account):

p(+|t, u),

(1)

where “+” denotes the binary event that a user likes an app.

Furthermore, as an app is represented by a set of its Twitter-
followers, it is necessary to aggregate the probability in Equation (1)
over the various Twitter-followers of app a. This allows us to esti-
mate the probability of how likely the target user will like the app:

p(+|a, u).

(2)

7Note that although other types of information can be extracted from Twitter, such as
the textual tweets and hashtags, we only focus on the Twitter-followers in this work as
early experiments have shown that other types of information are noisy and potentially
ambiguous.

285Figure 3: Instead of solely relying on the ratings of users, our
approach also makes use of the Twitter IDs that follow the apps
(red oval).

3.3 Pseudo-Documents and Pseudo-Words

In order to estimate the probability p(+|a, u) in Equation (2), we
build upon latent Dirichlet allocation (LDA)—a generative proba-
bilistic model for discovering latent semantics that has been mainly
used on textual corpora. Given a set of textual documents, LDA
generates a probability distribution over latent “topics” for each
document in the corpus, where each topic is a distribution over
words. Documents that have similar topics share the same latent
topic distribution.

We adapt LDA for the purpose of collaborative ﬁltering. As
mentioned in Section 3.2, users download apps and apps may have
Twitter-followers. Hence, user u and the Twitter-followers (of the
apps that user u has downloaded) are analogous to a document and
the words in the document, respectively. For the sake of clarity, we
will call them pseudo-documents from now on as our “documents”
actually do not contain natural language words in our work.

Drawing on the parallelism, we formally deﬁne the following

terms:

• We ﬁrst assume that user u likes app a, and app a has a set of
Twitter-followers {t1, . . . , tn} following its Twitter handle.
• A pseudo-document represents user u. Because of this, we

use u to represent both pseudo-document and user.

• A pseudo-word is a “word” in pseudo-document u that cor-
responds to the ID of a Twitter-follower t. If user u has liked
the apps a1, a2, and a3, the pseudo-document u will then
contain all the IDs of the Twitter-followers that are following
the Twitter handles of apps a1, a2, and a3. Note that there
may be duplicated pseudo-words as a Twitter-follower t may
be following more than one app’s Twitter handle.

However, the problem of only considering the “liked” apps is
in that LDA will indirectly assign higher probabilities to apps that
many users liked. In other words, LDA will indirectly give high
probabilities to popular apps. For example, suppose that two apps
are judged the same number of times. The probability given by
LDA will rank the two apps in order of their probability to be
“liked,” which we desire.
In contrast, if the ﬁrst app has been
judged by all the users and half of them liked the app, it will have
the same probability as another app that was judged by only half of
the users but liked all the time, which is undesirable.

Figure 4: A pseudo-document is constructed based on informa-
tion from a user, apps, Twitter-followers, and binary (“liked”
or “disliked”) preference indicators. A pseudo-document con-
tains pseudo-words; each pseudo-word is represented as a tuple
containing a Twitter-follower ID and a binary preference indi-
cator.

To address this popularity problem, we incorporate the magni-
tude of negative information as it indirectly allows us to account
for the frequency of the whole judging group (i.e., “liked” apps +
“disliked” apps = total apps). In addition, this solution allows user
groups to not only reﬂect the Twitter-followers that appear in the
apps that they like, but also in the apps that they dislike, thus pro-
viding richer information. We now formally deﬁne a pseudo-word:
• A pseudo-word is a “word” in pseudo-document u that con-
tains the ID of a Twitter-follower t and its associated binary
(“liked” or “disliked”) preference indicator.

Figure 4 illustrates how a pseudo-document is constructed based
on pseudo-words, which in turn are constructed based on the IDs of
Twitter-followers and the binary preference indicators from users.
Furthermore, in order to obtain the binary preferences, it is manda-
tory to convert the user ratings (i.e., the 5-point Likert scale) into
binary like/dislike indicators (see Section 4.2).

Note that the concept of pseudo-documents and pseudo-words
does not apply exclusively to Twitter-followers; it can also be ap-
plied to other sources of information such as the app genres, app
developers, and the words in the app descriptions. For example,
instead of using the IDs of Twitter-followers, we can also con-
struct pseudo-words based on the IDs of app developers. We fo-
cus on Twitter-followers as our goal is to assess the effectiveness
of this new source of data. In Section 5.1, we will create different
sets of pseudo-documents based on different features (i.e., Twitter-
followers, app genres, app developers, and words) and identify the
most effective feature in the recommendation of apps.
3.4 Constructing Latent Groups

Given the set of pseudo-documents {u1, . . . , um}, LDA can gen-
erate a probability distribution over latent groups for each pseudo-
document, where each latent group is represented as a distribution
over Twitter-follower IDs and binary preference indicators. Fig-
ure 5 illustrates this framework.

By using the information on “which apps are followed by which
Twitter-followers,” we can estimate the probability of target user u
liking app a by taking into account the IDs of the Twitter-followers
following app a, and marginalizing over the different latent groups
of pseudo-document u.
Formally speaking, given the set of tuples of pseudo-words Tu =
{(t1, d1), . . . , (tn, dn)}, where ti and di are a Twitter-follower ID

286Equation (4) is then reduced to the estimation of two quantities:

1. the probability that user u likes app a given that app a has

Twitter-follower t, i.e., p(+|t, u), and

2. the probability of considering Twitter-follower t given app a,

i.e., p(t|a).

p(+|t, u) is straightforward to estimate as it can be rewritten as:

p(+|t, u) =

p(+, t|u)

p(+, t|u) + p(−, t|u)

,

where p(+, t|u) and p(−, t|u) are derived from LDA in Equa-
tion (3), which is the probability that Twitter-follower t occurs in
an app that is liked (or disliked) by user u.
As for the probability p(t|a), we explore the following two ways:
(I) Assume a uniform distribution over the various Twitter-
followers present in app a

This framework is deﬁned as follows:
p(t|a) =

#T (a)

if t is a Twitter-follower of app a
otherwise,

(cid:40) 1

0

(5)

where T (a) and #T (a) denote the set of Twitter-followers follow-
ing app a and the set cardinality, respectively.
(II) Give more importance to inﬂuential Twitter-followers

In this alternative framework, we compute the inﬂuence of Twitter-

followers by using TwitterRank [32]. Let T R(t) be the Twitter-
Rank score of Twitter-follower t. With that, for each Twitter-follower
t, we retrieve its TwitterRank score and normalize it with the scores
obtained by the other Twitter-followers following app a. That is:

p(t|a) =

T R(t)

t(cid:48)∈T (a) T R(t(cid:48))

0

if t is a Twitter-follower of app a
otherwise,

(6)

(cid:40)

(cid:80)

where T (a) denotes the set of Twitter-followers following app a.

Prior tests on our dataset have shown that there is no signiﬁcant
difference in performance between (I) and (II) above. This is be-
cause the Twitter-followers that we get (note that each latent group
consists of Twitter-followers) are generally not prominent, inﬂuen-
tial people. Rather, they are the average users on Twitter. There-
fore, even if we use TwitterRank, the inﬂuence of each Twitter-
follower eventually converges into the uniform distribution. We
thus adopt the uniform distribution deﬁned by Equation (5) in our
evaluation for its simplicity.

4. EVALUATION PRELIMINARIES

We preface our evaluation proper by detailing how we constructed
our dataset, our settings for the dataset and the LDA model, and
how we chose our evaluation metric.
4.1 Dataset

We constructed our dataset by crawling from Apple’s iTunes
App Store and Twitter during September to December 2012. The
dataset consists of the following three elements:

1. App Metadata. These include an app’s ID, title, description,
genre, and its developer ID. The metadata is collected by ﬁrst get-
ting all the app IDs from the iTunes App Store, and then retrieving
the metadata for each app via the iTunes Search API. The metadata
is the source for the genre, developer, and word features mentioned
at the end of Section 3.3.

Figure 5: Given the set of pseudo-documents {u1, . . . , um},
LDA generates a probability distribution over latent groups
for each pseudo-document, where each latent group z is rep-
resented as a distribution over pseudo-words. A pseudo-word
is represented as a tuple containing a Twitter-follower ID and a
binary preference indicator.

and its associated binary (“liked” or “disliked”) preference indica-
tor, respectively, we deﬁne LDA as a generative process that creates
a series of tuples:

p(Tu|α, β) =

p(θ|α)

p(z|θ)p(ti, di|βz)

dθ,

(cid:90)

(cid:32) n(cid:89)

K(cid:88)

i=1

z=1

(cid:33)

where K is the number of latent groups, θ follows the Dirichlet
distribution of hyper-parameters α, and the latent group z follows
a multinomial distribution given by βz. The model is fully spec-
iﬁed by α and βz for each possible latent group z. Those hyper-
parameters are learned by maximizing the likelihood of the dataset.
The LDA model is used to compute the probability that the pres-
ence of a Twitter-follower t indicates that it is “liked” (+) (or “dis-
liked” (−)) by user u, given user u’s past interaction Tu and the
learned parameters α and β. Hence, we get the following equation:

p(±, t|u) = p(±, t|Tu, α, β)

=

p(±, t|z)p(z|u),

(3)

(cid:88)

z∈Z

where Z is the set of latent groups, p(±, t|z) is computed from the
per-topic word distribution of LDA, and p(z|u) is computed from
the per-document topic distribution of LDA.
3.5 Estimation of the Probability of How Likely

the Target User Will Like the App

Our approach is based on a simple “averaging” method where
the probability of how likely the target user will like the app is the
expectation of how the Twitter-followers like the app. Given a set
of Twitter-followers T , the probability that user u likes app a is
deﬁned as follows:

p(+|a, u) =

=

p(+, t|a, u)

p(+|t, u)P (t|a),

(4)

(cid:88)
(cid:88)

t∈T (a)

t∈T (a)

where T (a) is the set of possible Twitter-followers following app a,
in which we assume that: (i) Twitter-followers are examined one at
a time to make a decision about whether an app is liked or disliked.
In this case, t and t(cid:48) are disjoint events whenever t (cid:54)= t(cid:48); (ii) when
the Twitter-follower is known, the judgement does not depend on
the app any more, i.e., p(+|t, a, u) = p(+|t, u); (iii) the fact
that given a user and an app, there is no judgement involved, i.e.,
p(u, a) = p(u)p(a); (iv) the fact that an app has a given Twitter-
follower is independent from the user, i.e., p(t|a, u) = p(t|a).

2872. Ratings. For each app, we built a separate crawler to retrieve
its reviews from the iTunes App Store. A review contains the app’s
ID, its rating, the reviewer’s ID, the subject, and the review com-
ments. This is the source of the rating feature.

3. Related Twitter IDs. We used two methods to collect app-
related Twitter IDs. The ﬁrst way is to get the IDs that follow an
app’s Twitter account. We scanned through each app’s description
to identify its Twitter handle. For each app’s Twitter handle, we
used Twitter’s API to search for every Twitter ID following its han-
dle. The second method uses Twitter’s Streaming API to receive
tweets in real-time. To retrieve tweets that are related to apps, we
only kept tweets that contain hyperlinks to apps in the iTunes App
Store, and stored the Twitter IDs (who wrote the tweet) as well as
the app IDs that were mentioned in the tweets.

Altogether, we collected 1,289,668 ratings for 7,116 apps (that
have accounts on Twitter) from 10,133 users. The user-item ratings
matrix has a sparsity of 98.2%. We also collected 5,681,197 unique
Twitter IDs following the apps in our dataset. With respect to app
ratings and the number of related Twitter-followers per app, we
restrict that each user gives at least 10 ratings and each Twitter ID
is related to at least 5 apps, respectively. On average, each user
has rated 26 apps—ranging from a minimum of 10 rated apps to a
maximum of 271.
4.2 Experimental Settings

In order to train the LDA model, we require binary relevance
judgements to convert the user ratings to binary preference indica-
tors, as well as two sets of hyper-parameters—namely, the number
of latent groups K and the priors α and β.

We performed per-user normalization of the 5-point Likert scale
ratings when converting them to the binary like/dislike values re-
quired by our LDA application. This is because the average rating
for different users can vary signiﬁcantly [16].

Let ravg(u) and ru,a be user u’s average rating among all apps
and user u’s rating for app a, respectively. The normalized rating
is thus rn(u,a) = ru,a − ravg(u), where if rn(u,a) ≥ 0, the rating
is converted to a “like” (+), or “dislike” (−), otherwise.

We set the priors α and β as proposed in [20]. The number of
latent groups has a crucial inﬂuence on the performance of the LDA
approach. We used the likelihood over a held out set of training
data to ﬁnd the relevant number of latent groups. We tried several
settings for K (i.e., the number of latent groups), namely 10, 20,
35, 50, 80, 100, 120, 150, and 200. The ﬁnal number of latent
groups was selected by maximizing the likelihood of observations
over the development set.

In order to simulate the cold-start, we selected 10% of the apps
which were then the held out set for all users (i.e., we removed their
ratings in the training set). Therefore, each user has the same set of
within-fold apps and we can guarantee that none of these apps are
in the training set of any user.

We performed 10-fold cross validation where in each fold, we
used 70% of the apps to train the LDA model, 20% to identify the
number of latent groups for LDA (i.e., the development set), and
the remaining 10% as test data.
4.3 Evaluation Metric

Our system outputs M apps for each test user, which are sorted
by their probability of liking. We evaluate the algorithms based on
which of these apps were actually downloaded and liked (i.e., the
normalized rating of the test user).

This methodology leads to two possible evaluation metrics: pre-
cision and recall. However, a missing rating in training is ambigu-

ous as it may either mean that the user is not interested in the app
(negative), or that the user does not know about the app (truly miss-
ing). This makes it difﬁcult to accurately compute precision [31].
But since the known ratings are true positives, we feel that recall is
a more pertinent measure as it only considers the positively rated
apps within the top M, i.e., a high recall with a lower M will be
a better system. We thus chose Recall@M as our primary evalua-
tion metric. Let nu and Nu represent the number of apps the user
likes in the top M and the total number of apps the user likes, re-
spectively. Recall@M is then deﬁned as their ratio: nu/Nu. We
compare systems using average recall, where the average is com-
puted over all test users.

5. EXPERIMENTS

As our approach speciﬁcally attempts to address the cold-start,
we work on the speciﬁc scenario when a new set of apps is released
and users have yet to rate them (shown in Figure 2(b)). The goal of
our experiments is to answer the following research questions:

RQ1 How does the performance of the Twitter-followers feature
compare with other features, such as the app genres, app de-
velopers, and words in the app descriptions? Can we obtain
better recommendation accuracy by combining them?

RQ2 How does our proposed method compare with other state-of-

the-art techniques?

RQ3 Do the latent groups make any sense? If so, what can we

learn from them?

5.1 Comparison of Features (RQ1)

We benchmark how the signal from Twitter-followers affects per-
formance in comparison to other sources of data. We compare the
Twitter-followers feature (hereafter, T) with the other features: app
genres (G), app developers (D), and words in the app descriptions
(W). To perform this comparison in a fair manner, for each of these
four features, we constructed a set of pseudo-documents that con-
tains a set of feature-related pseudo-words (see Section 3.3).

Additionally, we assessed the effectiveness of these features in
combination; we combine multiple features (i.e., Twitter-followers,
genres, developers, and words) through the use of gradient boosted
trees (GBT) [10]. We also performed ablation testing where we
removed features from the combined feature set to determine the
importance of each feature. The features given to GBT are a set
of probabilities deﬁned by Equation (4). In GBT, we set the maxi-
mum number of trees and maximum tree depth to 2000 and 3, re-
spectively, and used the least-squares regression as a cost function.

Results. Figure 6 shows the results of the ﬁrst experiment, which
compares the overall performance between features (words (W),
developers (D), genres (G), Twitter-followers (T), and all features
(All) combined) when we vary the number of returned apps M =
20, . . . , 200. Fixing M = 100, we ablated individual features from
our combined method and show the results in Table 1.

The results are quite consistent. In the overall comparison over
all ranges of M, the individual feature of Twitter-followers gives
the best individual performance, followed by genres, developers,
and words in the app descriptions. From the combination and ab-
lation study in Table 1, we see that all features are necessary for
the optimal results. Matching the results in Figure 6, Table 1 also
shows that the removal of the best (worst) individual features leads
to the corresponding largest (smallest) drop in recall.

It may be surprising that the developer (D) and genre (G) fea-
tures are more effective than words in the app description (W).

288Figure 6: Recall obtained by different individual features
(dashed lines), as well as our method that combines all features
(solid). The baseline vector space model (VSM), using the app
description word vocabulary is also shown (dotted). The verti-
cal line marks model performance at M = 100 (cf. Table 1).

Table 1: Recall levels in our feature ablation study at M = 100.
TGDW and individual feature (T, G, D, W) performances in
Figure 6 are also shown.
Feature
All features (TGDW)
All, excluding Twitter-followers (GDW)
All, excluding Genres (TDW)
All, excluding Developers (TGW)
All, excluding Words (TGD)
Twitter-followers (T)
Genres (G)
Developers (D)
Words (W)

R@100
0.513
0.452
0.491
0.498
0.507
0.478
0.435
0.395
0.373

We observe that users may favor developer brands; possible causes
could be that the user recognizes the brand, or that the apps them-
selves may promote sister apps that are made by the same devel-
oper.
In addition, some apps complement one another. For ex-
ample, “Google Chrome,” “Gmail,” and “Google Maps” form a
complementary set. Also, the genres of apps may correlate with
download behavior. Figure 7 shows the distribution of app genres,
and indeed we observe that the “Games” genre dominates the dis-
tribution. This indicates that users often download apps that belong
to the “Games” genre.

From a practical standpoint, the most straightforward way to rec-
ommend apps in a cold-start would be to use the textual descrip-
tions in a content-based ﬁltering system, as in our (W) system. But
it performs the worst among the four individual features. Why do
words in app descriptions (W) perform the least well?

Carrying out a more detailed inspection, we ﬁnd that app descrip-
tions do not give informative hints about the app’s role; rather, they
are more focused on self-promotion as many apps boast about the
reviews that they have received in their app descriptions. Figure 8
is a screenshot of an app description that demonstrates this fact. In
addition, according to [2], users pay more attention to screenshots
instead of descriptions, which suggests that the information from

Figure 7: Distribution of app genres within our dataset.

Figure 8: A screenshot of an app description that illustrates
why word features may not be effective as it largely boasts
about endorsements received.

app descriptions is not as useful. This result also further explains
why text-dependent baselines such as CTR and LDA did not per-
form much better (cf. the next section on RQ2).

Features aside, we also assessed whether our use of the LDA-
based pseudo-document method is an important factor in recall per-
formance. We compared a straightforward use of the same data
with a standard vector space model (VSM) where we used the
same app description words (W) to build a vector of standard tf.idf
weighted words to represent each app.

Figure 6 also shows this result in the bottom two lines. We see
that the pseudo-document use of words (W) greatly outperforms
the VSM-based version (VSM). A two-tailed t-test at M = 100
shows that the improvement is statistically signiﬁcant (p < 0.01).
This validates our LDA-based pseudo-document approach.
5.2 Comparison Against Baselines (RQ2)

As we focus on the cold-start problem, we did not consider other
well-known recommender techniques that require user ratings, such
as matrix factorization or latent factor models. We compare our
approach with four baselines. The ﬁrst two baselines are VSM-
based—the ﬁrst is the app description-based VSM recapped from
RQ1 (i.e., “VSM (Words)”) while the second uses the IDs of Twitter-
followers as its vocabulary (i.e., “VSM (Twitter)”). The VSM (Twit-
ter) baseline evaluates whether our LDA-based approach of using
the Twitter-follower data betters the simpler VSM method. The
third baseline is the LDA model, which is equivalent to using our
pseudo-documents model on words in app descriptions. Lastly,
as a much stronger baseline, we show the performance of a re-
implementation of Wang and Blei’s [31] collaborative topic regres-
sion (CTR) model—a state-of-the-art CF algorithm that can also
make predictions in cold-start scenarios.

289We also observe that between the two models that only use Twit-
ter features (i.e., “VSM (Twitter)” and “Pseudo-Docs (Twitter)”),
our model signiﬁcantly outperforms the VSM model. This again
validates our earlier ﬁndings that our method’s performance is not
just due to the use of new data, but also how we make use of it.

Another important observation is that under a sparser cold-start
environment where we limit our training data to 15 apps per user
at maximum, there is an overall drop in recall, which is expected.
However, we note that our model (both using Twitter feature alone
and multi-features) still outperforms all of the baselines. This in-
dicates that using Twitter features is more robust than using app
descriptions under sparse conditions, as the IDs of the Twitter-
followers are independent from the apps—misleading or absent app
descriptions do not directly inﬂuence the Twitter-followers.

5.3 Analysis of Latent Groups (RQ3)

We use LDA and the notion of Twitter users as pseudo-words to
achieve good recall. A natural question to ask is whether the latent
groups discovered by LDA (from the individual features of Twitter-
followers, genres, and developers) have any meaning. We observe
interesting points for the developer (D) and Twitter-follower (T)
features.

For the developer feature, in most latent groups, the developers
who have received a substantial number of ratings for all their apps
are different from the independent (“indie”) developers. This is due
to the fact that these developers usually have apps that users “liked,”
whereas the indie developers sometimes have apps that users “dis-
liked” as well as apps that users “liked.” This coincides with our
hypothesis about brand loyalty, as the developers with a substantial
number of ratings either have created a large number of apps (such
as EA Games) or have created a small number of well-received
apps (such as the apps by Google). We also observe that the clus-
tered developers are classiﬁed into the same genre or strong com-
petitors like Facebook and Twitter.

In order to understand why Twitter-followers work best, we scru-
tinized the latent groups discovered by LDA at the optimal per-
formance point of K = 120. Each group consists of Twitter-
followers; and each Twitter-follower follows at least 5 apps in our
dataset. We then manually visited the Twitter pages of the top 5
Twitter-followers in each of the 120 latent groups, and then veri-
ﬁed their proﬁle descriptions and their latest tweets. We observe
that more than 95% of these Twitter-followers exhibit consistent
interest in apps. This shows that our approach accurately distin-
guishes between Twitter users who have implicit/explicit interest
in apps and regular Twitter users. Our approach assigns low prob-
abilities to Twitter users who have little or no correlation to apps,
effectively ﬁltering out their inﬂuence as noise.

To illustrate the quality of the latent groups, we performed an
additional level of micro-analysis. We selected the 3 most impor-
tant latent groups among the 120 groups based on the expectation
of the probability of each latent group over the set of training data
(i.e., the pseudo-documents), along with their corresponding top 5
Twitter individuals’ public proﬁle (Figure 10). The top-most row
in Figure 10 shows the top genres and examples of apps in each
of the top 3 latent groups. We see that the apps in each group
coincides with the Twitter proﬁles of the top 5 individuals in the
same group. Latent Group 1 is composed of family-oriented Twit-
ter users who also download family-oriented apps. Such a latent
group would be difﬁcult to describe if we were to use genres alone,
as this group consists of a non-discrete mix of children-friendly
apps. In Latent Group 2, we see that this group consists of profes-
sional music-creation apps, which also coincides with the type of
Twitter users who are either actual musicians or people interested

Figure 9: Recall varying the number of recommendations on
the full (top) and sparse (bottom) datasets. “*” and “**” de-
note statistically signiﬁcant improvements over the best base-
line (CTR) at p < 0.05 and p < 0.01, respectively.

Additionally, in order to study the impact of sparsity on our mod-
els, in a separate experimental condition, we randomly removed
some ratings from the training set so that the maximum number of
rated apps per user was 15, which represents a sparser environment.

Results. Figure 9 plots the results of this set of experiments
when we vary the number of returned apps M = 20, . . . , 200. We
observe that our approach that combines various pseudo-documents
using GBT consistently and signiﬁcantly outperforms other models
(at p < 0.01 on the full dataset), particularly the CTR model that is
the best among all baselines. This shows that we can achieve sig-
niﬁcant improvement in recommendation accuracy by integrating
multiple sources of information. We also observe that our model
of using pseudo-documents with Twitter data alone (i.e., “Pseudo-
Docs (Twitter)” in Figure 9) outperforms other models. This val-
idates our observation that Twitter is indeed a good source of in-
formation to address the cold-start in app recommendation. It also
indirectly points out that the textual features are less effective in the
app domain, which is obvious from the performances of the CTR
and LDA models that solely rely on textual data. We also note that
the performance of CTR is fairly similar to LDA. This is due to the
fact that the matrix-factorization component of CTR cannot per-
form recommendation in the cold-start. Therefore, like LDA, its
recommendation is based entirely on content.

290Figure 10: The top 3 latent groups; each group shows the top 5 Twitter-followers and their public proﬁle.

in creating music. This is in contrast with the “Music” genre in app
stores, which is in some sense too diverse as it may refer to music-
player apps, music-streaming apps, or trivial music-making apps.
Lastly, Latent Group 3 captures games that are of a light-hearted,
indie nature. We can also relate the Twitter users to the downloaded
apps, which are difﬁcult to describe using genres or words alone.
In short, our Twitter-follower feature is able to capture the person-
alities of Twitter-individuals. Therefore, even when an app does
not have user ratings, our system can still provide relevant recom-
mendation based on information about who is following the app’s
Twitter account.

6. CONCLUSION

By taking advantage of the unique property of apps and their
corresponding Twitter proﬁles, we identify Twitter-followers of the
Twitter proﬁles of apps. Pseudo-documents are then created to
represent users, where each pseudo-document contains the IDs of
Twitter-followers of the apps that a user has previously downloaded.
Thereafter, LDA is applied to the set of pseudo-documents to gen-
erate latent groups which is then used in the recommendation pro-
cess. By combining the feature of Twitter-followers with other fea-
tures based on various app metadata such as genre, developer, and
the words in the app description, we can generate a much more
accurate estimation of how likely a target user will like an app. Ex-
perimental results show that features extracted from Twitter consis-
tently and signiﬁcantly outperform state-of-the-art baselines which
rely on (potentially misleading) textual information distilled from
app descriptions. This also shows that follower information from
Twitter helps us discover valuable signals that is effective in allevi-
ating the cold-start situation.

In our future work, we plan to expand the use of data from SNS.
For instance, second-degree relationships such as Twitter-followers

following the current set of Twitter-followers may be useful, as
would using data from other SNS, such as Facebook. Finally, we
note that apps have a unique trait—they can be updated, which usu-
ally improves the app by incorporating new features or bug ﬁxes.
We plan to explore the use of this kind of temporal information to
enhance our app recommender system.

7. ACKNOWLEDGEMENTS

This research is supported by the Singapore National Research
Foundation under its International Research Centre @ Singapore
Funding Initiative and administered by the IDM Programme Ofﬁce.

8. REFERENCES
[1] C. C. Aggarwal, J. L. Wolf, K.-L. Wu, and P. S. Yu. Horting

Hatches an Egg: A New Graph-theoretic Approach to
Collaborative Filtering. In Proc. of the 5th ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining (KDD’99), pages 201–212, 1999.

[2] R. Ayalew. Consumer Behaviour in Apple’s App Store. PhD

thesis, Uppsala University, 2011.

[3] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.

Everyone’s an Inﬂuencer: Quantifying Inﬂuence on Twitter.
In Proc. of the 4th ACM International Conference on Web
Search and Data Mining (WSDM’11), pages 65–74, 2011.
[4] R. Bell, Y. Koren, and C. Volinsky. Modeling Relationships

at Multiple Scales to Improve Accuracy of Large
Recommender Systems. In Proc. of the 13th ACM SIGKDD
International Conference on Knowledge Discovery and Data
Mining (KDD’07), pages 95–104, 2007.

[5] D. M. Blei and J. D. Lafferty. Topic Models. Text mining:
Classiﬁcation, Clustering, and Applications, 10:71, 2009.

291[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet

Allocation. Journal of Machine Learning Research,
3:993–1022, 2003.

[7] J. Boyd-Graber, J. Chang, S. Gerrish, C. Wang, and D. M.

Blei. Reading Tea Leaves: How Humans Interpret Topic
Models. In Proc. of the 23rd Annual Conference on Neural
Information Processing Systems (NIPS’09), 2009.

[8] J. S. Breese, D. Heckerman, and C. Kadie. Empirical
Analysis of Predictive Algorithms for Collaborative
Filtering. In Proc. of the 14th Conference on Uncertainty in
Artiﬁcial Intelligence (UAI’98), pages 43–52, 1998.

[9] M. Cha, H. Haddadi, F. Benevenuto, and K. P. Gummadi.

Measuring User Inﬂuence in Twitter: The Million Follower
Fallacy. In Proc. of the 4th International AAAI Conference on
Weblogs and Social Media (ICWSM’10), pages 10–17, 2010.

[10] J. H. Friedman. Greedy Function Approximation: A
Gradient Boosting Machine. The Annals of Statistics,
29:1189–1232, 2001.

[11] J. L. Herlocker, J. A. Konstan, A. Borchers, and J. Riedl. An

Algorithmic Framework for Performing Collaborative
Filtering. In Proc. of the 22nd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’99), pages 230–237, 1999.
[12] T. Hofmann. Latent Semantic Models for Collaborative

Filtering. ACM Transactions on Information Systems (TOIS),
22:89–115, 2004.

[13] T. Hofmann and J. Puzicha. Latent Class Models for

Collaborative Filtering. In Proc. of the 16th International
Joint Conference on Artiﬁcial Intelligence (IJCAI’99), pages
688–693, 1999.

[14] M. Jamali and M. Ester. A Matrix Factorization Technique

with Trust Propagation for Recommendation in Social
Networks. In Proc. of the 4th ACM Conference on
Recommender Systems (RecSys’10), pages 135–142, 2010.

[15] D. Kim and B.-J. Yum. Collaborative Filtering Based on

Iterative Principal Component Analysis. Expert Systems with
Applications, 28(4):823–830, 2005.

[16] Y. Koren. Factorization Meets the Neighborhood: A

Multifaceted Collaborative Filtering Model. In Proc. of the
14th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (KDD’08), pages 426–434,
2008.

[17] Y. Koren and R. Bell. Advances in Collaborative Filtering.
Recommender Systems Handbook, pages 145–186, 2011.

[18] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a

Social Network or a News Media? In Proc. of the 19th
International Conference on World Wide Web (WWW’10),
pages 591–600, 2010.

[19] N. N. Liu, X. Meng, C. Liu, and Q. Yang. Wisdom of the

Better Few: Cold-Start Recommendation via Representative
based Rating Elicitation. In Proc. of the 5th ACM Conference
on Recommender Systems (RecSys’11), pages 37–44, 2011.

[20] H. Misra, O. Cappé, and F. Yvon. Using LDA to Detect
Semantically Incoherent Documents. In Proc. of the 12th
Conference on Computational Natural Language Learning
(CoNLL’08), pages 41–48, 2008.

[21] Y. Moshfeghi, B. Piwowarski, and J. M. Jose. Handling Data

Sparsity in Collaborative Filtering using Emotion and
Semantic-based Features. In Proc. of the 34th International

ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’11), pages 625–634, 2011.

[22] L. Page, S. Brin, R. Motwani, and T. Winograd. The

PageRank Citation Ranking: Bringing Order to the Web.
Technical Report SIDL-WP-1999-0120, Stanford Digital
Library Technologies Project, 1998.

[23] S.-T. Park and W. Chu. Pairwise Preference Regression for

Cold-Start Recommendation. In Proc. of the 3rd ACM
Conference on Recommender Systems (RecSys’09), pages
21–28, 2009.

[24] D. Ramage, S. Dumais, and D. Liebling. Characterizing

Microblogs with Topic Models. In Proc. of the 4th
International AAAI Conference on Weblogs and Social
Media (ICWSM’10), pages 130–137, 2010.

[25] R. Recuero, R. Araujo, and G. Zago. How Does Social

Capital Affect Retweets. In Proc. of the 5th International
AAAI Conference on Weblogs and Social Media
(ICWSM’11), pages 305–312, 2011.

[26] A. Said, E. W. De Luca, and S. Albayrak. How Social

Relationships Affect User Similarities. In Proc. of the 2010
Workshop on Social Recommender Systems, pages 1–4, 2010.

[27] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted

Boltzmann Machines for Collaborative Filtering. In Proc. of
the 24th International Conference on Machine Learning
(ICML’07), pages 791–798, 2007.

[28] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based

Collaborative Filtering Recommendation Algorithms. In
Proc. of the 10th International Conference on World Wide
Web (WWW’01), pages 285–295, 2001.

[29] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock.
Methods and Metrics for Cold-Start Recommendations. In
Proc. of the 25th Annual International ACM SIGIR
Conference on Research and Development in Information
Retrieval (SIGIR’02), pages 253–260, 2002.

[30] G. Takács, I. Pilászy, B. Németh, and D. Tikk. Matrix

Factorization and Neighbor based Algorithms for the Netﬂix
Prize Problem. In Proc. of the 2nd ACM Conference on
Recommender Systems (RecSys’08), pages 267–274, 2008.
[31] C. Wang and D. M. Blei. Collaborative Topic Modeling for

Recommending Scientiﬁc Articles. In Proc. of the 17th ACM
SIGKDD International Conference on Knowledge Discovery
and Data Mining (KDD’11), pages 448–456, 2011.

[32] J. Weng, E.-P. Lim, J. Jiang, and Q. He. TwitterRank:

Finding Topic-sensitive Inﬂuential Twitterers. In Proc. of the
3rd ACM International Conference on Web Search and Data
Mining (WSDM’10), pages 261–270, 2010.

[33] S. Wu, J. M. Hofman, W. A. Mason, and D. J. Watts. Who

Says What to Whom on Twitter. In Proc. of the 20th
International Conference on World Wide Web (WWW’11),
pages 705–714, 2011.

[34] G.-R. Xue, C. Lin, Q. Yang, W. Xi, H.-J. Zeng, Y. Yu, and

Z. Chen. Scalable Collaborative Filtering using Cluster-based
Smoothing. In Proc. of the 28th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’05), pages 114–121, 2005.

[35] K. Zhou, S.-H. Yang, and H. Zha. Functional Matrix

Factorizations for Cold-Start Recommendation. In Proc. of
the 34th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
(SIGIR’11), pages 315–324, 2011.

292