The Cluster Hypothesis for Entity Oriented Search

Hadas Raviv, Oren Kurland

Faculty of Industrial Engineering and

Management, Technion, Haifa 32000, Israel

hadasrv@tx.technion.ac.il,kurland@ie.technion.ac.il

David Carmel

Yahoo! Research, Haifa 31905, Israel

david.carmel@ymail.com

ABSTRACT
In this work we study the cluster hypothesis for entity ori-
ented search (EOS). Speciﬁcally, we show that the hypothe-
sis can hold to a substantial extent for several entity similar-
ity measures. We also demonstrate the retrieval eﬀectiveness
merits of using clusters of similar entities for EOS.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models

Keywords: cluster hypothesis, entity oriented search

1.

INTRODUCTION

The entity oriented search (EOS) task has attracted much
research attention lately. The main goal is to rank entities
in response to a query by their presumed relevance to the
information need that the query expresses. For example, the
goal in the TREC’s expert search task was to rank employees
in the enterprise by their expertise in a topic [6]. The goal
in INEX entity ranking track was to retrieve entities that
pertain to a topic in the English Wikipedia [7, 8, 9]. The
goal in TREC’s entity track was to rank Web entities with
respect to a given entity by their relationships [2, 3, 4].

The EOS task is diﬀerent than the standard ad-hoc doc-
ument retrieval task as entities are somewhat more complex
than (ﬂat) documents. That is, entities are characterized
by diﬀerent properties such as name, type (e.g., place or
person), and potentially, an associated document (e.g., a
homepage or a Wikipedia page). Despite the fundamental
diﬀerence between the two tasks, we set as a goal to study
whether an important principle in ad-hoc document retrieval
also holds for the EOS task; namely, the cluster hypothesis
[22]. We present the ﬁrst study of the cluster hypothesis
for EOS, where the hypothesis is that “closely associated
entities tend to be relevant to the same requests”.

We use several inter-entity similarity measures to quan-
tify the association between entities, which is a key point
in the hypothesis. These measures are based on the entity
type which is a highly important source of information [18,
14]. We then show that the cluster hypothesis, tested using

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Voorhees’ nearest neighbor test [23], can hold to a substan-
tial extent for EOS for several of the similarity measures.

Motivated by the ﬁndings about the cluster hypothesis,
we explore the merits of using clusters of similar entities
for entity ranking. We show that ranking entity clusters
by the percentage of relevant entities that they contain can
be used to produce extremely eﬀective entity ranking. We
also demonstrate the eﬀectiveness of using cluster ranking
techniques that are based on estimating the percentage of
relevant entities in the clusters for entity ranking.

Our main contributions are three fold: (i) showing that for
several inter-entity similarity measures the cluster hypothe-
sis holds for EOS to a substantial extent as determined by
the nearest neighbor test; (ii) demonstrating the consider-
able potential of using clusters of similar entities for EOS;
and, (iii) showing that using simple cluster ranking meth-
ods can help to improve retrieval performance with respect
to that of an eﬀective initial search.

2. RELATED WORK

Any entity in the INEX entity ranking track, which we
use for our experiments, has a Wikipedia page. Hence, some
previously proposed estimates for the entity-query similar-
ity are based on Wikipedia’s category information [20, 1,
18, 14]. Similarly, we measure the similarity between two
entities based on several measures of the similarity between
the sets of categories of the two entity pages.

In Cao et al.’s work on expert search [5] the retrieval score
assigned to an entity was smoothed with the retrieval score
assigned to a cluster constructed from the entity. In con-
trast, we explore a retrieval paradigm that ranks entity clus-
ters and transforms the ranking to entity ranking. Work on
document retrieval showed that cluster-based (score) smooth-
ing and cluster ranking are complementary [16]. We leave
the exploration of this ﬁnding for the EOS task for future
work.

There are several tests of the cluster hypothesis for doc-
ument retrieval [13, 10, 23, 19]. We use Voorhees’ nearest-
neighbor test for the EOS task as it is directly connected
with the nearest neighbor clusters we use for ranking.

The ﬁndings we present for the EOS task echo those re-
ported for document retrieval. Namely, the extent to which
the cluster hypothesis holds [23], and the (potential) merits
of using cluster ranking [12, 21, 15, 17].

3. THE CLUSTER HYPOTHESIS

Our ﬁrst goal is to explore the extent to which the cluster
hypothesis holds for EOS. To this end, we use the nearest

841neighbor test [23]. Let L[n]
q be the list of n entities that
are the highest ranked by an initial search performed in
response to query q. For each relevant entity in L[n]
q , we
record the percentage of relevant entities among its K near-
est neighbors in L[n]
q . The nearest neighbors are determined
using one of the inter-entity similarity measures speciﬁed in
Section 5.1. The test result is the average of the recorded
percentages over all relevant entities in L[n]
q , averaged over
all test queries.

Some of the inter-entity similarity measures assign dis-
crete values including 0. Hence, for some relevant entities
there could be less than K neighbors as we do not consider
neighbors with a 0 similarity value. In addition, a relevant
entity might be assigned with more than K nearest neigh-
bors due to ties in the similarity measure. That is, we keep
collecting all entities having the same similarity value as that
of the last one in the K neighbors list.

4. CLUSTER-BASED ENTITY RANKING

q ) be the set of clusters created from L[n]

Our second goal is studying the potential merits of us-
ing entity clusters to induce entity ranking. We re-rank the
initial entity list L[n]
q using a cluster-based paradigm which
is very common in work on document retrieval [17]. Let
Cl(L[n]
q using some
clustering method. The inter-entity similarity measures used
for creating clusters are those used for testing the cluster hy-
pothesis. (See Section 5.1 for further technical details.) The
clusters in Cl(L[n]
q ) are ranked by the presumed percentage
of relevant entities that they contain. Below we describe
two cluster ranking methods. Then, each cluster is replaced
with its constituent entities while omitting repeats. Within
cluster entity ranking is based on the initial entity retrieval
scores which were used to create the list L[n]
q .

The MeanScore cluster ranking method scores cluster
c by the mean retrieval score of its constituent entities:
1
|c| Pe∈c Sinit(e; q); Sinit(e; q) is the initial retrieval score
of entity e; |c| is the number of entities in c.

When Sinit(e; q) is a rank equivalent estimate to that of
log(P r(q, e)) [18], the cluster score assigned by MeanScore
is rank equivalent to the geometric mean of the joint query-
entity probabilities’ estimates in the cluster. Using a geometric-
mean-based representation for document clusters was shown
to be highly eﬀective for ranking document clusters [17].

The regularized mean score method, RegMeanScore in

e∈L

[n]
q

short, which is novel to this study, smoothes c’s score:
Pe∈c Sinit(e;q)+ 1

Sinit(e;q)

n P

|c|+1

. The cluster score is the
mean retrieval score of a cluster composed of c’s entities
and an additional “pseudo” entity whose score is the mean
score in the initial list. This method helps to address, among
others, cluster-size bias issues.

5. EVALUATION

5.1 Experimental setup

We conducted experiments with the datasets of the INEX
entity ranking track of 2007 [7], 2008 [8], and 2009 [9]. Table
1 provides a summary of the datasets. The tracks for 2007
and 2008 used the English Wikipedia dataset from 2006,
while the 2009 track used the English Wikipedia from 2008.
The set of test topics for 2007 is composed of 21 topics that

Data set

Collection size # of documents # of test topics

2007
2008
2009

4.4 GB
4.4 GB
50.7 GB

659, 388
659, 388

2, 666, 190

46
35
55

Table 1: INEX entity ranking datasets.

were derived from the ad hoc 2007 assessments, and addi-
tional 25 topics that were created by the participants specif-
ically for the track.
In 2008, 35 topics were created and
used for testing. The topics used for testing in 2009 were 55
topics out of the 60 test topics used in 2007 and 2008.

We used Lucene (http://lucene.apache.org/core/) for ex-
periments. The data was pre-processed using Lucene, in-
cluding tokenization, stopword removal, and Porter stem-
ming.

Inter-entity similarity measures. The inter-entity simi-
larity measures that we use utilize Wikipedia categories.
Speciﬁcally, the categories associated with the Wikipedia
page of the entity, henceforth referred to as its category set,
serve as the entity type.

x (·)||p[µ]

The Tree similarity between two entities e1 and e2 is
exp(−αd(e1, e2)) where d(e1, e2) is the minimum distance
over Wikipedia’s categories graph between a category in e1’s
category set and a category in e2’s category set; α is a decay
constant determined as in [18]. The SharedCat measure is
the cosine similarity between the binary vectors representing
two entities. An entity vector is deﬁned over the categories
space. An entry in the vector is 1 if the corresponding cat-
egory is associated with the entity and 0 otherwise. Thus,
SharedCat measures the (normalized) number of categories
shared by the two entities [20]. The CE measure is based
on measuring the language-model-based similarity between
the documents associated with the category sets of two en-
tities [14]. More speciﬁcally, each category is represented
in this case by the text that results from concatenating all
Wikipedia pages associated with the category. The similar-
ity between the texts x and y that represent two categories is
exp(−CE(p[0]
y (·))); CE is the cross entropy measure;
p[µ]
z (·) is the Dirichlet-smoothed unigram language model in-
duced from z with the smoothing parameter µ (=1000). The
CE similarity between two entities is deﬁned as the maximal
similarity, over all pairs of categories, one in the ﬁrst entity’s
category set and the other in the second entity’s category set,
of the texts representing the categories. Finally, the ESA
(Explicit Semantic Analysis) [11] similarity measure is the
cosine between two vectors, each represents the category set
of an entity. The vectors representing the category sets are
deﬁned over the entities space. The value of an entry in the
vector is the number of the categories in the given category
set that are associated with the corresponding entity. Using
ESA to measure inter-entity similarity is novel to this study.
q , are used
for both the cluster hypothesis test and cluster-based rank-
ing. The lists are created in response to the query using
highly eﬀective entity retrieval methods [18]. The ﬁrst list,
LDoc, is created by representing an entity with its Wikipedia
document (page). The documents are ranked in response
to the query using the standard language-model-based ap-
proach with Dirichlet-smoothed unigram language models
and the cross entropy similarity measure. The second list,

Three diﬀerent initially retrieved entity lists, L[n]

842Similarity measure

Tree

SharedCat

CE

ESA

Initial list
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame

2007
30.0
29.8
32.7
35.7
33.5
37.9
33.4
34.5
37.5
34.3
33.7
37.3

2008
32.0
35.5
37.7
41.0
45.5
44.3
36.2
38.6
41.7
36.2
41.0
39.1

2009
42.7
44.9
44.9
45.4
52.2
52.7
46.0
50.3
49.7
46.2
49.5
49.0

Table 2: The cluster hypothesis test: the average
percentage of relevant entities among the 5 nearest
neighbors of a relevant entity.

LDoc;T ype, is created by scoring entities with an interpola-
tion of two scores. The ﬁrst is that used to create the list
LDoc. The second is the similarity between the category
set of the entity and the query target type (the set of cat-
egories that are relevant to the query, as deﬁned by INEX
topics). The Tree estimate described above is used for mea-
suring similarity between the two category sets. The third
list, LDoc;T ype;N ame, is created by scoring an entity with an
interpolation of the score used to create LDoc;T ype, and an
estimate for the proximity-based association [18] between
the query terms and the entity name (i.e., the title of its
Wikipedia page) in the corpus. We employ the same train-
test approach as in [18] to set the free-parameter values of
the ranking methods used to create the initial lists. The
number of entities in each initial list L[n]

is n = 50.

q

q

and the K (= 5) entities in L[n]

We use a simple nearest neighbor clustering method to
cluster entities in the initial list L[n]
q . Speciﬁcally, each en-
tity in L[n]
that are the
most similar to it, according to the inter-entity similarity
measures described above, form a cluster. Using such small
overlapping clusters was shown to be highly eﬀective for
cluster-based document retrieval [16, 15, 17]. We note that
not all clusters necessarily contain K + 1 documents due to
the reasons speciﬁed in Section 3. For consistency, we also
use K = 5 in the cluster hypothesis test.

q

Following the INEX guidelines, the evaluation metric for
INEX 2007 is mean average precision (MAP) while that for
INEX 2008 and 2009 is infAP. We also report the precision of
the top 5 entities (p@5). Statistically signiﬁcant diﬀerences
of retrieval performance are determined using the two tailed
paired t-test with a 95% conﬁdence level.

5.2 Experimental results

5.2.1 The cluster hypothesis

Table 2 presents the results of the nearest neighbor cluster
hypothesis test that was described in Section 3. The test is
performed on the diﬀerent initially retrieved entity lists us-
ing the various inter-entity similarity measures. We see that
the average percentage of relevant entities among the near-
est neighbors of a relevant entity ranges between 30% and
53% across the various experimental settings. We also found
out that, on average, the percentage of relevant entities in
a list is often lower than 25% and can be as low as 10%.
Thus, due to the relatively high percentage of relevant enti-

ties among the nearest neighbors of relevant entities, we can
conclude that the cluster hypothesis holds to a substantial
extent, according to the nearest neighbor test, with various
inter-entity similarity measures.

Table 2 also shows that for most of the data sets and
similarity measures the test results for the LDoc;T ype and
LDoc;T ype;N ame lists are higher than for LDoc. This ﬁnding
is not surprising as LDoc;T ype and LDoc;T ype;N ame were cre-
ated using entity-query similarity measures that account for
category information, while the similarity measure used to
create LDoc does not use this information. The highest test
results are obtained for the SharedCat similarity measure
which, as noted above, measures the (normalized) number
of shared categories between two entities.

5.2.2 Cluster-based entity ranking

Table 3 presents the results of employing cluster-based
entity re-ranking, as described in Section 4, upon the three
initial entity lists. The various inter-entity similarity mea-
sures are used for creating the clusters.
’Initial’ refers to
the initial ranking of a list.
’Oracle’ is the ranking of enti-
ties that results from employing the cluster-based re-ranking
paradigm described in Section 4; the clusters are ranked by
the true percentage of relevant entities that they contain.

The high performance numbers for Oracle, which are sub-
stantially and statistically signiﬁcantly better than those for
Initial, attest to the existence of clusters that contain a very
high percentage of relevant entities. More generally, these
numbers attest to the incredible potential of employing ef-
fective cluster ranking methods to rank entities.

RegMeanScore, which outperforms MeanScore due to the
regularization discussed in Section 4, is in quite a few cases
more eﬀective than Initial; speciﬁcally, using the Tree and
SharedCat inter-entity similarity measures. While the im-
provements for LDoc are often statistically signiﬁcant, this
is not the case for LDoc;T ype and LDoc;T ype;N ame. Natu-
rally, the more eﬀective the initial ranking (Initial), the more
challenging the re-ranking task. Yet, the very high Oracle
numbers for LDoc;T ype and LDoc;T ype;N ame imply that eﬀec-
tive cluster ranking methods can yield performance that is
much better than that of the initial ranking. Finally, for
both LDoc;T ype and LDoc;T ype;N ame the best performance is
in most cases attained by using RegMeanScore.

6. CONCLUSIONS AND FUTURE WORK

We showed that the cluster hypothesis can hold to a sub-
stantial extent for the entity oriented search (EOS) task with
several inter-entity similarity measures. We also demon-
strated the potential merits of using a cluster-based retrieval
paradigm for EOS that relies on ranking entity clusters. De-
vising improved cluster ranking techniques is a future venue
we intend to explore.

7. ACKNOWLEDGMENTS

We thank the reviewers for their comments. Part of the
work reported here was done while David Carmel was at
IBM. This paper is based on work that has been supported
in part by the Israel Science Foundation under grant no.
433/12 and by Google’s faculty research award. Any opin-
ions, ﬁndings and conclusions or recommendations expressed
in this material are the authors’ and do not necessarily re-
ﬂect those of the sponsors.

8438. REFERENCES

[1] K. Balog, M. Bron, and M. De Rijke. Query modeling for entity

search based on terms, categories, and examples. ACM Trans.
Inf. Syst., 29(4), 2011.

[2] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and

T. Westerveld. Overview of the trec 2009 entity track. In
Proceedings of TREC, 2009.

[3] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the

trec 2010 entity track. In Proceedings of TREC, 2010.

[4] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the

trec 2011 entity track. In Proceedings of TREC, 2011.

[5] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at

enterprise track of TREC 2005. In Proceedings of TREC,
volume 14, 2005.

[6] N. Craswell, A. P. de Vries, and I. Soboroﬀ. Overview of the

trec 2005 enterprise track. In Proceedings of TREC, 2005.

[7] A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and
M. Lalmas. Overview of the INEX 2007 entity ranking track. In
Proceedings of INEX, pages 245–251, 2007.

[8] G. Demartini, A. P. de Vries, T. Iofciu, and J. Zhu. Overview of

the INEX 2008 entity ranking track. In Proceedings of INEX,
pages 243–252, 2008.

[9] G. Demartini, T. Iofciu, and A. P. de Vries. Overview of the

INEX 2009 entity ranking track. In Proceedings of INEX,
pages 254–264, 2009.

[10] A. El-Hamdouchi and P. Willett. Techniques for the

measurement of clustering tendency in document retrieval
systems. Journal of Information Science, 13:361–365, 1987.

[11] E. Gabrilovich and S. Markovitch. Computing semantic

relatedness using wikipedia-based explicit semantic analysis. In
Proceedings of IJCAI, pages 1606–1611, 2007.

[12] M. A. Hearst and J. O. Pedersen. Reexamining the cluster

hypothesis: Scatter/Gather on retrieval results. In Proceedings
of SIGIR, pages 76–84, 1996.

[13] N. Jardine and C. J. van Rijsbergen. The use of hierarchic

clustering in information retrieval. Information Storage and
Retrieval, 7(5):217–240, 1971.

[14] R. Kaptein and J. Kamps. Exploiting the category structure of

wikipedia for entity ranking. Artiﬁcial Intelligence, 0:111 –
129, 2013.

[15] O. Kurland and C. Domshlak. A rank-aggregation approach to
searching for optimal query-speciﬁc clusters. In Proceedings of
SIGIR, pages 547–554, 2008.

[16] O. Kurland and L. Lee. Corpus structure, language models, and

ad hoc information retrieval. In Proceedings of SIGIR, pages
194–201, 2004.

[17] X. Liu and W. B. Croft. Evaluating text representations for
retrieval of the best group of documents. In Proceedings of
ECIR, pages 454–462, 2008.

LDoc

2007

2008

MAP
20.2

p@5
26.1

infAP
12.6

31.8i
21.6
21.6

36.2i
22.5
23.1i

32.3i
22.6
22.0

33.7i
20.9
21.9

Tree

51.3i
26.1
26.0
SharedCat

22.3i
13.3
13.4

60.0i
23.9
27.8i

CE

53.5i
27.4
26.5

26.8i
12.6
13.4i

23.3i
13.5
13.5

ESA

57.0i
22.6
23.9

26.0i
12.8
12.9

LDoc;Type

p@5
19.4

49.7i
17.1
18.9

66.3i
18.9
20.6i

53.7i
20.6
20.6

60.6i
14.9
14.9

2009

infAP
19.1

p@5
35.3

25.5i
20.2i
20.2i

30.5i
19.7
19.9

28.0i
19.1
19.1

28.8i
19.2
19.2

68.0i
36.7
36.7

83.3i
37.1
38.5

74.2i
36.7
36.7

80.0i
35.6
35.3

2007

2008

2009

MAP
30.8

p@5
37.4

infAP
28.2

p@5
44.0

infAP
23.8

p@5
43.6

37.7i
31.6
31.6

43.8i
30.8
31.1

39.0i
31.3
31.0

42.1i
28.6
29.1

Tree

58.7i
40.0
40.0

32.5i
27.7
28.1

SharedCat

65.7i
36.1
37.0

60.0i
38.3
37.4

CE

38.3i
28.7
28.9

34.3i
28.5
28.7

ESA

64.3i
34.3
34.8

37.7i
28.4
28.9

50.9i
37.7
40.6

65.1i
42.3
42.3

58.3i
40.6
40.0

66.9i
42.3
44.0

29.5i
23.6
23.4

34.1i
23.2
23.6

31.3i
23.7
23.7

32.9i
22.8
22.7

70.2i
39.6
39.3

87.6i
44.0
45.1

75.6i
42.2
42.2

83.6i
42.5
41.5

Initial

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Initial

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

LDoc;Type;Name

2007

2008

MAP
33.3

p@5
40.4

infAP
35.4

[18] H. Raviv, D. Carmel, and O. Kurland. A ranking framework for

Initial

entity oriented search using markov random ﬁelds. In
Proceedings of the 1st Joint International Workshop on
Entity-Oriented and Semantic Search, 2012.

[19] M. D. Smucker and J. Allan. A new measure of the cluster
hypothesis. In Proceedings of ICTIR, pages 281–288, 2009.

[20] J. A. Thom, J. Pehcevski, and A.-M. Vercoustre. Use of

wikipedia categories in entity ranking. CoRR, abs/0711.2917,
2007.

[21] A. Tombros, R. Villa, and C. Van Rijsbergen. The eﬀectiveness

of query-speciﬁc hierarchic clustering in information retrieval.
Information Processing & management, 38(4):559–582, 2002.

[22] C. J. van Rijsbergen. Information Retrieval. Butterworths,

second edition, 1979.

[23] E. M. Voorhees. The cluster hypothesis revisited. In

Proceedings of SIGIR, pages 188–196, 1985.

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

Oracle
MeanScore
RegMeanScore

39.6i
34.1
34.0

47.4i
32.9
33.1

40.7i
33.6
33.6

44.7i
33.9
34.0

Tree

57.4i
43.5
42.6

42.3i
35.7
35.7

SharedCat

70.4i
38.7
39.1

59.1i
38.7
38.7

CE

47.8i
33.5
34.8

43.1i
34.5
34.5

ESA

66.5i
41.3
42.2

45.7i
33.7
34.2

p@5
46.9

62.3i
42.3
43.4

74.9i
42.9
44.0

63.4i
45.1
45.1

70.3i
43.4
44.6

2009

infAP
24.4

p@5
44.0

30.3i
24.7
24.6

34.3i
24.6
24.9

31.6i
24.6
24.8

32.9i
23.0
23.3

72.0i
42.9
42.2

87.i3
41.5
42.9

76.4i
43.3
43.6

81.5i
35.3
35.6

Table 3: Retrieval performance. The best result in
a column (excluding that of Oracle) per an initial
list is boldfaced.
’i’ marks statistically signiﬁcant
diﬀerences with Initial.

844