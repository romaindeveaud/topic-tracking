Timeline Generation with Social Attention

Wayne Xin Zhao1, Yanwei Guo1, Rui Yan1, Yulan He2 and Xiaoming Li1
1School of Electronic Engineering and Computer Science,Peking University, China

2School of Engineering & Applied Science, Aston University, UK

batmanﬂy@gmail.com, pkuguoyw@gmail.com, rui.yan.peking@gmail.com, y.he@cantab.net, lxm@pku.edu.cn

ABSTRACT
Timeline generation is an important research task which can help
users to have a quick understanding of the overall evolution of any
given topic. It thus attracts much attention from research commu-
nities in recent years. Nevertheless, existing work on timeline gen-
eration often ignores an important factor, the attention attracted to
topics of interest (hereafter termed “social attention”). Without tak-
ing into consideration social attention, the generated timelines may
not reﬂect users’ collective interests. In this paper, we study how
to incorporate social attention in the generation of timeline sum-
maries. In particular, for a given topic, we capture social attention
by learning users’ collective interests in the form of word distri-
butions from Twitter, which are subsequently incorporated into a
uniﬁed framework for timeline summary generation. We construct
four evaluation sets over six diverse topics. We demonstrate that
our proposed approach is able to generate both informative and in-
teresting timelines. Our work sheds light on the feasibility of in-
corporating social attention into traditional text mining tasks.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous

General Terms
Algorithms, Performance, Evaluation

Keywords
Timeline, social media attention, user interest

1.

INTRODUCTION

Timelines [3, 1, 5] provide temporal summaries of the evolution
of news stories related to a topic, which are often desirable for users
who do not closely follow news and want to quickly gain an overall
picture of the major events related to a topic. Typically, sentences
which describe major events are extracted in chronological order to
form timeline summaries. We show an example timeline presenta-
tion of “Nobel Prize" in Table 1 created by professional editors1.

Recently, Yan et al. proposed a task of automatically generating
evolutionary timeline summaries [5, 4]. They formally formulated

1http://timelinesdb.com

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

the task as an optimization problem via iterative sentence substi-
tution in order to maximize the objective function with four fac-
tors: relevance, coverage, coherence and cross-date diversity [5].
In [4], Yan et al.
further extended the work by modeling inter-
date and intra-date dependencies between timestamped sentences,
and incorporating these two kinds of dependencies into a sentence
ranking function.

The timeline summaries generated in [5, 4] are mainly based on
semantic information and do not necessarily reﬂect the public at-
tention an event has attracted. In Table 1, we can observe that not
all the events described by the summary sentences receive equal so-
cial attention, as evidenced by the number of related tweets listed
in the “Hotness” column. Since the timeline summaries are usually
kept succinct, it is desirable to present sentences which are likely
to attract the attention of the majority of users. With today’s social
web, it is possible to obtain the social attention signals from social
media content such as Twitter feeds.

In this paper, we study how to capture social attention and incor-
porate it into the generation of timeline summaries. The “Hotness”
column in Table 1 reveals that there is a very skewed social at-
tention distribution over the events described by the summary sen-
tences in the example timeline. Millions of users engage in a di-
verse range of activities on the social web such as posting status
messages and interacting with items generated by others, for exam-
ple, forwarding messages. These activities are often interest driv-
en. Thus learning from online social media could be a good way to
capture social attention. In our proposed method, users’ collective
interests are learned from the Twitter data, and are represented as
pseudo sentences. We run a modiﬁed graph-based method to prop-
agate collective interest biased scores, which represent a trade-off
between informativeness and interestingness, i.e., users’ collective
interests.

To the best of our knowledge, it is the ﬁrst study which utilizes
social attention to improve both the informativeness and interest-
ingness of timeline summaries. We evaluate our method on four
datasets constructed from Twitter and compare it with a number
of state-of-the-art timeline generation methods. Our experimen-
tal results show that by considering social attention, the proposed
method for timeline summary generation gives better performance
in terms of both informativeness and interestingness.

2. PROBLEM FORMULATION

For our timeline summarization problem, we assume the follow-

ing input data are available.

ts and an end time te.

Time span: A time span I = (ts, te) is deﬁned by a start time
Query: A user issues a query Q = {q1, q2, . . . , q|Q|} within
I to the timeline summarization system, where qi denotes a query
word in the vocabulary V.
News articles: We assume that news articles related to Q within
I have been retrieved. We represent these relevant news articles
as a set of sentences C. Each sentence in C has a timestamp (e.g.

1061Table 1: Timeline summaries between July and December 2009
on “Nobel Prize" generated by professional editors.

Summaries

Hotness*

Date
Jul. 19

Aug. 18
Oct. 6

Oct. 8
Oct. 9

Dec. 10

Frank McCourt, former NYC teacher and Irish-born

author,died of cancer.

Former South Korean Pres. Kim Dae-jung (85) died.
Hilary Mantel won the 2009 Man Booker Prize for

her historical novel “Wolf Hall."

Herta Mueller won the Nobel Prize in literature.

The Nobel Peace Prize was awarded to

US President Barack Obama.

In Oslo(Norway), President Barack Obama accepted

the Nobel Peace Prize

512

199
450

362
21760

2448

*Here “Hotness" is measured by # of related tweets (retrieved by manually generated

query keywords) in our Twitter dataset and not part of the timeline.

also has a timestamp.

the publish date) and Ct denotes a collection of sentences at the tth
time point.
Tweets: We also assume that we can retrieve a set of tweets
published within I and relevant to Q. We use C(cid:2)
to represent these
tweets. Similarly, each tweet in C(cid:2)
Given the input speciﬁed above, the system is expected to gen-
erate the following output: for each time point t ∈ I, the system
generates a summary Rt, which consists of a set of news sentences
from Ct. All the generated summaries are expected to capture the
most important information and meanwhile attract considerable so-
cial attention about the topic within I. It is worth noting that we do
not select sentences from tweets since our aim here is to generate
a timeline summary which has a good coverage of the most promi-
nent events happened related to a speciﬁc topic. We only use tweets
to derive social attention signals.

3. OUR PROPOSED ALGORITHM

We propose a novel approach to capture social attention from
tweets through learning users’ collective interests by a generative
mixture model. We then show how to incorporate the learned col-
lective interests into a state-of-the-art timeline generation algorith-
m.
3.1 Learning Users’ Collective Interests

Formally, let C(cid:2)

Existing timeline generation methods only consider news stream-
s. Hence, the timeline summaries generated do not reﬂect users’
collective interests in the real world. We propose to learn users’
collective interests from the Twitter. We assume that we can obtain
a set of relevant tweets relating to users’ queries at each time point
in a time span I. It is worth mentioning that we do not attempt to
discover individual user’s interests, instead, we aim to identify the
most prominent collective interests of the majority.
t denote all the tweets at time point t, our aim
is to learn a collective interest model θU,t from C(cid:2)
t.2 We notice
that tweets are often noisy and contain many background words
(home, good, time, lol, love, etc.) which are not relating to users’
topical interest with respect to a given query . Thus, words in C(cid:2)
t
can be clustered into two groups, words closely related to a spe-
ciﬁc query topic and general background words. We assume that
words in C(cid:2)
t are generated either from a model θU which represents
users’ collective topical interest or from a general background mod-
el θB. With this model, we can reduce the effects of background
words and learn a model which better captures words concentrat-
ing around users’ collective interests. We further assume that both
θU and θB are represented as multinomial distributions over the
vocabulary. This kind of two-mixture model has been shown to be
effective in pseudo relevance feedback for information retrieval [6].
With the two-mixture model, we have

p(C(cid:2)

t

|θU , θB) =

(cid:3)

t|(cid:2)
|C(cid:2)

i=1

(1 − η)p(wi|θU ) +ηp (wi|θB)

(cid:4)

,

(1)

2To simplify notation, we represent θU,t as θU by dropping the time index.

where η is the probability (weight) that a word wi in C(cid:2)
t is generated
from the general background model. A large η tends to make the
interest-related language model more discriminative because more
general words are generated from the background model. In prac-
tice, we can set η empirically and ﬁx it during the learning process.
t and V be the vocabulary
Let c(w,C(cid:2)
of the corpus, we then have
(cid:4)

t) be the count of word w in C(cid:2)

(cid:3)
(1− η)p(w|θU ) +ηp (w|θB)

|θU , θB) =

c(w, C(cid:2)

log p(C(cid:2)

t

t) log

.

(cid:5)
w∈V

Usually the background model θB can be estimated directly from
the entire tweet collection using maximum likelihood estimation.
After deriving θB and ﬁxing η, we can use the expectation max-
imization (EM) algorithm to estimate the collective interest lan-
guage model. The updating formulae of the E-step and the M-step
are shown below:

E-step: α(n+1)(w) =

M-step: p(n+1)(w|θU ) =

(1 − η)p(n)(w|θU )

(1 − η)p(n)(w|θU ) +ηp (n)(w|θB)

,

(cid:6)

c(w, C(cid:2)
w(cid:2)∈V c(w(cid:2), C(cid:2)

t)α(n+1)(w)

t)α(n+1)(w(cid:2))

.

After obtaining the interest-related model θU , we can easily trans-
form it into a vector by simply setting the weight of each word
dimension to the word’s corresponding probability in θU . Such a
vector can be treated as a pseudo sentence which describes users’
collective interests at a time point t.3

3.2 Timeline Summary Generation

We will ﬁrst give a brief introduction of a timeline generation
algorithm, denoted as ETTS, which was proposed in [4]. Then
we will present how to incorporate the learnt collective interests
into ETTS. It is noteworthy that our proposed method can be easily
incorporated into any other graph based summarization algorithms.
Given a news collection C partitioned according to the start/end
time speciﬁed in a time span I, i.e. C = {Ct}t∈I, we split each
news article into sentences for each Ct. At a time point t, we refer
to sentences with timestamp t as local sentences and sentences with
other timestamps as global sentences. ETTS models the inter-date
and intra-date dependencies between the timestamped sentences
and incorporate these two types of correlations into a sentence rank-
ing function.

Specially, ETTS constructs two probability transition matrices.
One is for modeling global afﬁnity and the other is for modeling
local afﬁnity. For global afﬁnity, it means that summary sentences
should be correlative with sentences from neighboring dates to cap-
ture the overall topic evolution patterns, i.e., inter-date dependency.
To allow the modeling of global afﬁnity, global sentences are tem-
porally projected onto time point t such that links between local and
global sentences can be built. For local afﬁnity, it means intra-date
dependency, i.e., the timeline summary at time point t should be
informative within Ct, and links are built between local sentences.
With these two transition matrices, both the global and local afﬁnity
propagation can be run using the standard LexRank algorithm. At
time point t, a set of global ranking Rg = {r(g)
} and local rank-
ing Rl = {r(l)
} of sentences in Ct are obtained. ETTS then uses
an optimization algorithm to combine both the global and the local
rankings, and the ﬁnal ranking of sentence si ∈ Ct is a weighted
combination between its global ranking and local ranking

i

i

3Note that depending on how we set the time point, there might be very few tweets
relating to a query topic at some time points, e.g., fewer than 100 tweets. In this case,
we do not learn users’ collective interests from tweets.

1062α

β

r(l)
i

,

r(g)
i +

ri =

α + β

(2)
where 0 ≤ α, 0 ≤ β. α and β can be tuned on different date sets
to make a tradeoff between global scores and local scores. See [4]
for a detailed description of ETTS.

α + β

Now we study how to incorporate users’ collective interests into
ETTS. Note that regardless of the modeling of either global afﬁn-
ity or local afﬁnity, we can formulate both problems in a standard
LexRank form

λ = μ · λ · M + (1− μ) · y,

(3)

where λ is the saliency score vector of sentences, M is the transi-
tion probability matrix and y is the restart probability vector usu-
ally set to be uniform. The main idea is that instead of using a
uniform restart distribution y, we use an interest biased restart dis-
tribution. Recall that at each time point, we have modeled users’
collective interests as a pseudo sentence. We can add a new vertex
which represents such a pseudo sentence at that time point. It can
be naturally incorporated into the above LexRank algorithm. We
build the similarity links between the interest-related pseudo sen-
tence and all the local sentences using the cosine similarity mea-
surement. At the beginning of each iteration of LexRank, this pseu-
do sentence has a large restart probability to be visited. During the
learning process, it gradually propagates the interest-related score
to other similar vertices. Letting v0 to denote the vertex represent-
ing users’ collective interest, each entry of y(cid:2) = [y0, y] can be
modiﬁed as follows

(cid:2)

y(cid:2)
i =

τ,
1−τ
N

,

if i = 0,
otherwise.

(4)

where τ is a positive factor and N is the number of candidate sen-
tences. For v0, i.e., the pseudo sentence representing users’ collec-
tive interest, it has a large restart probability of τ , while any of the
other vertices has a smaller restart probability of 1−τ
N . τ essentially
controls the trade-off between informativeness and interestingness;
the larger it is, the more emphasis we put on interestingness. We
use τg and τl to denote the corresponding values of τ for glob-
al afﬁnity ranking and local afﬁnity ranking. In our experiments,
we simply set τg = τl. In traditional LexRank, the ranking score
only indicates informativeness, while our interest biased LexRank
makes a trade-off between informativeness and interestingness.

4. EXPERIMENTS AND EVALUATION
4.1 Experimental Setup
Data collection. Since our summarization algorithm is query de-
pendent, we selected six topics to cover a few important news events
according to the Rule of Interpretation (ROI) category (Table 2).
We then constructed corpora of news articles and tweets for each
of the topics. For news articles, we submitted the topic queries in-
to Google News4 and crawled all the news articles from the three
major sources: China Daily5, New York Times and BBC News
between July, 2009 and December, 2009. For tweets, we use the
shared data set within the same time period.6 Next we split news
articles into sentences and ﬁltered those news sentences and tweets
with too many or too few word tokens. The statistics of the datasets
is summarized in Table 2.
Gold standard generation. We manually constructed the gold s-
tandard for these six topics, including both the informativeness-
oriented and the interestingness-oriented sets. Informativeness- ori-
ented set, similar to the evaluation of timeline summaries [4] and
4https://news.google.com/
5http://english.peopledaily.com.cn/
6http://an.kaist.ac.kr/traces/WWW2010.html

Table 2: Statistics of the datasets. We used the topic words in
the ﬁrst column as the queries to obtain relevant tweets and
news sentences.

Topics

#news articles

#tweets

ROI category

#Gold

sentences

Inﬂuenza A

Climate Change

Noble Prize
Flight Crash
Earthquake
Urumqi Riots

1291
3006
343
612
773
912

487258
174939
88816
231268
53118
3867

35
33
30
25
27
25

Science, Finance
Science, Finance
Science, Politics

Accidents, Disasters
Accidents, Disasters
Legal Cases, Politics

traditional summarization [2], requires a system to return informa-
tive and relevant news sentences to cover important aspects given a
query topic. We select various authoritative sources to generate the
gold standard timelines, including mainstream news media (Chi-
na Daily, BBC, New York Times) and the timeline Web database7.
For each topic, we ﬁrst extracted the timelines from the profession-
al editors in at least two kinds of resources mentioned above. Then
we invited a human judge to merge the information from different
resources by removing redundant sentences. Finally, 25 ∼ 35 sen-
tences were kept for each topic as gold standard. We denote this set
as D.

Since the informativeness-oriented set does not consider the so-
cial attention of the generated timelines, it may not reﬂect users’
collective interests. We thus further constructed interestingness-
oriented sets by removing less “interesting" sentences from D. A
sentence is considered to be interesting if it attracts a considerable
amount of social attention. We invited 6 graduate students major in
journalism for evaluation. Every volunteer was asked to remove top
K least interesting sentences from D for each topic query. For each
sentence in D, a volunteer was required to refer to news portals (for
report volume) and online social websites (for social attention) be-
fore making the judgement. We merged the results from six judges
and reordered the sentences according to their total votes. To eval-
uate interestingness at different levels, we set K to 5, 10 and 15,
for which we respectively removed the top 5, 10 and 15 sentences
that the judges considered to be less interesting from D. In such
a way we ended up with another three gold standard sets, D−5,
D−10 and D−15. For each topic, we computed the average pair-
wise agreement of six judges over the top 5/10/15 less interesting
sentences, and obtained the values of 0.84/0.8/0.75 indicating good
agreement.
Following [5], we used the F scores of
Evaluation metrics.
unigram-based ROUGE-1 (R-1), bigram-based ROUGE-2 (R-2),
and the weighted longest common subsequence based ROUGE-
W (R-W,W=1.2) as metrics.
Methods to compare. We used the following widely used multi-
document summarization or timeline generation algorithms as the
baseline systems.

1) CHIEU: Chieu et al. [1] presented a similar timeline system
with a different methodology by utilizing burstiness ranking tex-
t feature. 2) CENTROID: The method applies MEAD algorithm
(Radev et al., 2004) to extract sentences according to the follow-
ing parameters: centroid value, positional value, and ﬁrst-sentence
overlap. 3) LexRank: LexRank [2] ﬁrst constructs a sentence con-
nectivity graph based on cosine similarity and then selects impor-
tant sentences based on the concept of eigenvector centrality. 4)
LexRank+i: Since LexRank is a graph based method, we can al-
so incorporate users’ collective interests similarly. 5) ETTS: ETTS
proposed in [4] is an algorithm with optimized a combination of
global and local biased summarization. 6) ETTS+i: our proposed
algorithm, which incorporates users’ collective interests into ETTS.
Setup details. For fairness of comparison, we performed the same
preprocessing steps and adopted Maximal Marginal Relevance to
reduce redundancy for all the aforementioned baselines. We par-
titioned the entire sentence collection C into local sentence col-
lections according to the timestamps of each sentence, i.e., C =
7http://timelinesdb.com/

1063Table 3: Performance comparison on both informativeness and interestingness oriented data sets. D−5, D−10 and D−15 are used to
evaluate interestingness at different levels. Their compression rates compared with D are about 83%, 67% and 50% respectively,
with smaller rates indicating more emphasis on interestingness.

Informativeness

D

D−5

Interestingness

D−10

D−15

Methods
CHIEU
CENTROID
LexRank
LexRank+i
ETTS
ETTS+i

R-1
0.250
0.253
0.266
0.277 (+4.1%)
0.277
0.282 (+1.8%)

R-2
0.053
0.057
0.060
0.062
0.067
0.070

R-W
0.086
0.087
0.090
0.093
0.095
0.095

R-1
0.239
0.238
0.251
0.265 (+5.6%)
0.259
0.268 (+3.5%)

R-2
0.051
0.054
0.057
0.062
0.061
0.066

R-W
0.086
0.085
0.090
0.094
0.093
0.095

R-1
0.227
0.225
0.234
0.251 (+7.2%)
0.245
0.256 (+4.5%)

R-2
0.050
0.052
0.054
0.059
0.060
0.064

R-W
0.087
0.087
0.089
0.095
0.093
0.097

R-1
0.201
0.197
0.206
0.221 (+7.3%)
0.212
0.224 (+5.7%)

R-2
0.041
0.044
0.046
0.048
0.046
0.055

R-W
0.083
0.083
0.084
0.089
0.087
0.093

∪T
i=1Ci. Following [1], we applied a simple mechanism to select
sentences by extracting more sentences for important dates while
fewer sentences for others. The allocation rate on ti was set to
φi = |Ci|
|C| . The total number of selected sentences for each query
is set to the number of sentences in the gold standard.

The parameters of all the baselines and our proposed algorithm
were tuned in a way similar to cross-validation. For each query, we
ﬁrst found the parameters which lead to the optimal performance
on this query, and then applied the model learned with these pa-
rameters on the other ﬁve queries. And ﬁnally, we averaged results
over six such runs. For η in the two-mixture model (Eq. 1), we ﬁnd
that a value in 0.8 ∼ 0.95 usually gives good performance, which
can effectively remove noisy and background words. For the restart
probability of the interest-related pseudo sentence τ , we found that
a value in0 .4 ∼ 0.6 usually leads to an optimal performance for
both LexRank and ETTS. A large value of τ will hurt the diversity
of summary sentences while a small value of τ essentially ignores
the effect of social attention.

4.2 Experimental Results

We present the results of various methods on both the informa-
tiveness and interestingness oriented evaluation sets in Table 3. To
better check the improvement when incorporating social attention,
we also present the relative improvement on R-1 for LexRank+i
over LexRank and ETTS+i over ETTS.
Evaluation of informativeness. We ﬁrst examine the results
on the informativeness oriented set D, which consists of summa-
ry sentences obtained from professional editors. We notice that the
incorporation of users’ collective interest yields improvement of in-
formativeness, i.e. ETTS+i > ETTS and LexRank+i > LexRank. It
indicates that users do read important sentences, and the incorpora-
tion of user interests does not hurt informativeness. A noteworthy
point that users’ collective interests may affect the diversity due to
the fact that our method force the generated summaries to match
collective users’ interests.
Evaluation of interestingness. For the results on the three in-
terestingness oriented sets, D−5, D−10 and D−15, we can see the
improvement LexRank+i over LexRank and ETTS+i over ETTS be-
comes more signiﬁcant with the decreasing number of summary
sentences in the gold standards, D−5 < D−10 < D−15. This
shows that taking into account of social attention, our proposed
method is indeed able to generate timeline summaries more tailored
to users’ collective interests. The advantage of our method is more
prominent when generating more succinct summaries. Finally, we
notice that our proposed method is able to close the performance
gap between LexRank and ETTS. For example, when tested on the
D−5 set, ETTS outperforms LexRank by 3% (Δ = 0.006) in the
R-1 measure. But with our proposed method incorporated, the gap
between ETTS+i and LexRank+i (Δ = 0.003) is reduced to 1%.
To get an intuitive idea of why our method works, we present
an example timeline generated by ETTS+i in Table 4. We list the
top words ranked by probabilities learnt from the two-mixture user
interest-related model. Clearly, these words are very meaningful
and broadly reﬂect the social attention at that period. With them as
supervision, the graph based summarization methods tend to give
higher weight to sentences closely matched public interest. Com-

Table 4: Sample timeline generated by ETTS+i on “Nobel
Prize" from July 2009 to December 2009. Due to space limit,
we only present summary sentences related to the news “Oba-
ma won Nobel Peace Prize 2009". Top interest-related words
are marked in Italic.

OCT. 09, 2009 (cid:2) nobel peace prize obama president win
•The Nobel Peace Prize was awarded to US President Barack Obama.
•The Nobel Peace Prize Committee has spoken out in defense of its
decision to give the award to US President Barack Obama.
NOV. 27, 2009 (cid:2) nobel prize peace obama attend oslo
•Obama will attend the start of the conference on Dec 9
before heading to Oslo to accept the Nobel Peace Prize.
DEC. 10, 2009 (cid:2) nobel prize peace obama accept speech oslo
• US President Barack Obama, gives his Nobel speech after
receiving the Nobel Peace Prize at City Hall in Oslo.

pared to the results in Table 1, ETTS+i ﬁnds more news reﬂect-
ing users’ collective interest about the topic “President Obama won
Nobel Peace Prize".

5. CONCLUSIONS

We present a graph based approach to consider collective users’
interests in timeline generation. Experiment results show that the
incorporation of users’ interests is helpful to improve both infor-
mativeness and interestingness. The generated summaries become
more user favoring compared to traditional timeline summaries. As
future work, we will explore learning user interests by dynamical-
ly adjusting the time span of a given topic instead of using a ﬁxed
time interval.
ACKNOWLEDGEMENTS. We thank Jing Jiang and Qiaoling
Liu for helping to improve the work. We thank the anonymous
reviewers for the constructive comments. The work was partially
supported by NSFC Grant 61272340 and 60933004. Yulan He was
partially supported by the EPSRC grant EP/J020427/1. Xin Zhao
was supported by Microsoft Research Asia Fellowship.

6. REFERENCES
[1] H. L. Chieu and Y. K. Lee. Query based event extraction along

a timeline. In SIGIR ’04.

[2] G. Erkan and D. Radev. Lexpagerank: Prestige in

multi-document text summarization. In EMNLP, 2004.

[3] R. Swan and J. Allan. Automatic generation of overview

timelines. In SIGIR’00.

[4] R. Yan, L. Kong, C. Huang, X. Wan, X. Li, and Y. Zhang.
Timeline generation through evolutionary trans-temporal
summarization. In EMNLP, 2011.

[5] R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and Y. Zhang.
Evolutionary timeline summarization: a balanced optimization
framework via iterative substitution. In SIGIR, 2011.

[6] C. Zhai and J. Lafferty. Model-based feedback in the language

modeling approach to information retrieval. In CIKM, 2001.

1064