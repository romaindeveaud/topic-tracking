Flat vs. Hierarchical Phrase-Based Translation Models for

Cross-Language Information Retrieval

Ferhan Ture1,2, Jimmy Lin3,2,1

1Dept. of Computer Science, 2Institute for Advanced Computer Studies, 3The iSchool

University of Maryland, College Park

fture@cs.umd.edu, jimmylin@umd.edu

ABSTRACT
Although context-independent word-based approaches re-
main popular for cross-language information retrieval, many
recent studies have shown that integrating insights from
modern statistical machine translation systems can lead to
substantial improvements in eﬀectiveness. In this paper, we
compare ﬂat and hierarchical phrase-based translation mod-
els for query translation. Both approaches yield signiﬁcantly
better results than either a token-based or a one-best trans-
lation baseline on standard test collections. The choice of
model manifests interesting tradeoﬀs in terms of eﬀective-
ness, eﬃciency, and model compactness.
Categories and Subject Descriptors: H.3.3 [Information
Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: SCFG, query translation

1.

INTRODUCTION

Despite the prevalence of context-independent word-based
approaches for cross-language information retrieval (CLIR)
derived from the IBM translation models [4], recent stud-
ies have shown that exploiting ideas from machine transla-
tion (MT) for context-sensitive query translation produces
higher-quality results [17, 19, 24]. State-of-the-art MT sys-
tems take advantage of sophisticated models with “deeper”
representations of translation units, e.g., phrase-based [13],
syntax-based [25, 27], and even semantics-based [11] models.
In particular, hierarchical phrase-based machine translation
(PBMT) systems [5] provide a middle ground between ef-
ﬁcient “ﬂat” phrase-based models and expressive but slow
syntax-based models.
In terms of translation quality, ef-
ﬁciency, and practicality, ﬂat and hierarchical PBMT sys-
tems have become very popular, partly due to successful
open-source implementations.

This paper explores ﬂat and hierarchical PBMT systems
for query translation in CLIR. Previously, we have shown
that integrating techniques from hierarchical models lead to

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior speciﬁc permission and/or a fee.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

signiﬁcant gains in eﬀectiveness—however, it is unclear if
such gains could have been achieved from “ﬂat” represen-
tations. This question is interesting because it opens up a
diﬀerent region in the design space: ﬂat representations are
faster, more scalable, and exhibit less complexity—encoding
a diﬀerent tradeoﬀ between eﬃciency and eﬀectiveness.

There are two main contributions to this work: First, we
test the robustness of query translation techniques intro-
duced in earlier work [24] by comparing ﬂat and hierarchical
phrase-based translation models. In addition, we examine
the eﬀects of three diﬀerent heuristics for handling one-to-
many word alignments. We show that a combination-of-
evidence approach consistently outperforms a strong token-
based baseline as well as a one-best translation baseline for
three diﬀerent languages, Arabic (Ar), Chinese (Zh) and
French (Fr), using either ﬂat or hierarchical translation gram-
mars. Second, we discuss diﬀerences between the two MT
models and provide insights on the tradeoﬀs each represent.
Experiments show that a hierarchical translation model yields
higher eﬀectiveness, which suggests that there is value in
more sophisticated modeling of linguistic phenomena.

2. BACKGROUND AND RELATED WORK
Although word-by-word translation provides the starting
point for query translation approaches to CLIR, there has
been much work on using term co-occurrence statistics to
select the most appropriate translations [10, 15, 1, 21]. Ex-
plicitly expressing term dependency relations has produced
good results in monolingual retrieval [9, 18], but extend-
ing that idea to CLIR has not proven to be straightforward.
Another thread of research has focused on translating multi-
word expressions in order to deal with ambiguity [2, 28].

Borrowing ideas from MT for IR dates back to at least
Ponte and Croft’s work on retrieval using language model-
ing [20]. That work was later extended to translation models
for retrieval [3], followed by a series of successful adaptations
to the cross-language case [26, 14, 8].

As MT systems have evolved away from the token-based
translation approach, researchers have started exploring ways
to integrate various components of modern MT systems for
better CLIR eﬀectiveness. Magdy et al. [17] showed that
preprocessing text consistently for MT and IR systems is
beneﬁcial. Nikoulina et al. [19] built MT models tailored
to query translation by tuning model weights with queries
and reranking the top n translations to maximize eﬀective-
ness on a held-out query set. While improvements were
more substantial using the latter method, another interest-
ing ﬁnding was the low correlation between translation and

813retrieval quality. This indicates that better translation may
not necessarily help retrieval.
2.1 Context-Independent Baseline

As a baseline, we consider the technique presented by
Darwish and Oard [6]. Given a source-language query s,
we represent each token sj by its translations in the tar-
get language, weighted by the bilingual translation proba-
bility. These token-to-token translation probabilities, called
P rtoken, are learned independently from a parallel bilin-
gual corpus using automatic word alignment techniques [4].
In this approach, the score of document d, given source-
language query s, is computed by the following equations:

Score(d|s) =

(cid:2)

tf(sj, d) =

df(sj) =

j

(cid:2)

ti
(cid:2)

ti

Weight(tf(sj , d), df(sj))
tf(ti, d)P rtoken(ti|sj)
df(ti)P rtoken(ti|sj)

(1)

(2)

(3)

In order to reduce noise from incorrect alignments, we im-
pose a lower bound on the token translation probability, and
also a cumulative probability threshold, so that translation
alternatives of sj are added (in decreasing order of probabil-
ity) until the cumulative probability has reached the thresh-
old. Any weighting function can be used in conjunction with
the tf and df values, and we chose the Okapi BM25 term
weighting function (with parameters k1 = 1.2, b = 0.75).

2.2 Flat vs. Hierarchical Phrase-based MT

Machine translation can be divided into three steps: train-
ing the translation model, tuning parameters, and decoding.
We will mostly focus on the ﬁrst step, since that is where
ﬂat and hierarchical MT approaches diﬀer the most.

The output of the ﬁrst step is the translation model (called
TM hereafter). For both ﬂat and hierarchical variants, the
TM consists of a set of rules (i.e., the translation grammar)
in the following format:

α = α0α1. . . || β = β0β1. . . || A || (cid:4)(α → β)

We call the sequence of αi’s the source side of the rule,
and sequence of βj’s the target side of the rule. The above
indicates that the source side translates into the target side
with a likelihood of (cid:4)(α → β).1 A contains token alignments
in the format i-j, indicating that source token αi is aligned
to target token βj .

A hierarchical model [5] diﬀers from a ﬂat model [13] in
terms of rule expressivity: rules are allowed to contain one
or more nonterminals, each acting as a variable that can
be expanded into other expressions using the grammar, car-
ried out in a recursive fashion. These grammars are called
synchronous context-free grammars (SCFG), as each rule
describes a context-free expansion on both sides.

Consider the following two rules from an SCFG:

R1. [X] leave in europe || cong´e de [X] en europe

|| 1-0 2-3 3-4 || 1

R2. maternal || maternit´e || 0-0 || 0.69

1

The likelihood function (cid:2) is not a probability density function
because it is not normalized.

In R1, the non-terminal variable [X] allows an arbitrarily
long part of the sentence to be moved from the left of the
sentence in English to the middle of the sentence in French,
even though it generates a single token (i.e., maternal) using
R2 in this particular example. As a result, an SCFG can
capture distant dependencies in language that may not be
realized in ﬂat models.

Each sequence of rules that covers the entire input is called
a derivation, D, and produces a translation candidate, t,
which is scored by a linear combination of features. One
can use many features to score a candidate, but two fea-
tures are the most important: the product of rule likelihood
values indicates how well the candidate preserves the origi-
nal meaning, TM(t, D|s), whereas the language model score,
LM(t), indicates how well-formed the translation is. Com-
bining the two, the decoder searches for the best translation:

t(1)

= arg max

t

[ max
D∈D(s,t)

TM(t, D|s)LM(t)]

(4)

There is a tradeoﬀ between using either ﬂat or hierarchical
grammars. The latter provides more expressivity in rep-
resenting linguistic phenomena, but at the cost of slower
decoding [16]. On the other hand, ﬂat models are faster
but less expressive. Also, due to the lack of variables, ﬂat
grammars contain more rules, resulting in a more verbose
translation grammar.

3. QUERY TRANSLATION WITH MT

In our previous work [24], we described two ways to con-
struct a context-sensitive term translation probability distri-
bution using internal representations from an MT system.
These distributions can then be used to retrieve ranked doc-
uments using equations (1)–(3).
3.1 Using the Translation Model

With appropriate data structures, it is possible to eﬃ-
ciently extract all rules in a TM (either ﬂat or hierarchical)
that apply to a given source query, s, called TMs. For each
such applicable rule r, we identify each source token sj in r,
ignoring any non-terminal symbols. From the token align-
ment information included in the rule structure, we can ﬁnd
all target tokens aligned to sj . For each such target token ti,
the likelihood value of sj being translated as ti is increased
by the likelihood score of r. At the end of the process, we
have a list of possible translations and associated likelihood
values for each source token that has appeared in any of the
rules. We can then convert each list into a probability distri-
bution, called P rPBMT for ﬂat and P rSCFG for hierarchical
grammars by normalizing the sum of likelihood scores:

P rSCFG/PBMT(ti|sj) =

1
ψ

(cid:4)(r)

(5)

(cid:2)

r∈TMs
sj↔ti in r

where sj ↔ ti represents an alignment between tokens sj
and ti and ψ is the normalization factor.

When a source token sj is aligned to multiple target tokens
in a rule, it is not obvious how to distribute the probability
mass. In our previous implementation [24], each alignment
was treated as an independent event with the same proba-
bility. We call this the one-to-one heuristic, and introduce
two alternatives due to the following drawback: the target
tokens aligned to sj are usually not independent. For exam-
ple, the token brand is aligned to three tokens marque, de,

814fabrique (En. brand, of, factory), which is an appropriate
translation when put together. Even if de is discarded as a
stopword, the one-to-one heuristic will learn the token pair
(brand, fabrique) incorrectly. An alternative heuristic is to
ignore these rules altogether, assuming that good transla-
tion pairs will appear in other rules, thus discarding these
cases would not cause any harm (we call this the one-to-
none technique). A third approach is to combine the target
tokens into a multi-token expression. Thus, in the above
example, we would learn the translation of brand as marque
de fabrique, which is a useful mapping that we might not
learn otherwise. We call the third technique one-to-many,
and compare these three heuristics in our evaluation.
3.2 Using N-best Translations

Given t(1), the most probable translation of query s com-
puted by equation (4), we can score a document d as follows:

Score(d|s) =

(cid:2)

Weight(tf(t(1)

i

, d), df(t(1)

i

))

(6)

i

Since MT systems generate a set of candidate translations
in the process of computing equation (4), we can consider
the n most likely candidates. For each candidate translation
t(k), and for each source token sj, we use token alignments
to determine which tokens in t(k) are associated with sj . If
there are multiple target tokens, we apply one of the three
methods introduced previously: one-to-none, one-to-one, or
one-to-many. By the end of the process, we obtain a prob-
ability distribution of translations for each sj based on the
n best query translations. If source token sj is aligned to
(i.e., translated as) ti in the kth best translation, the value
(cid:4)(t(k)|s) is added to its probability mass, producing the fol-
lowing for P rnbest (where ϕ is the normalization factor):

P rnbest(ti|sj) =

1
ϕ

n(cid:2)

(cid:4)(t(k)|s)

(7)

k=1

sj↔ti in t(k)

3.3 Evidence Combination

For P rtoken, translation probabilities are learned from all
sentence pairs in a parallel corpus, whereas P rSCFG/PBMT
only uses portions that apply to the source query, which
reduces ambiguity in the probability distribution based on
this context. P rnbest uses the same set of rules in addition
to a language model to search for most probable transla-
tions. This process ﬁlters out some irrelevant translations
at the cost of less diversity, even among the top 10 or 100
translations. Since the three approaches have complemen-
tary strengths, we can perform a linear interpolation of the
three probability distributions:
P rc(ti|sj ; λ1, λ2) =λ1P rnbest(ti|sj ) + λ2P rSCFG/PBMT(ti|sj)
(8)

+ (1 − λ1 − λ2)P rtoken(ti|sj )

Replacing any of these probability distributions introduced
above for P rtoken in equations (1)–(3) yields the respective
scoring formula.

4. EVALUATION

We performed experiments on three CLIR test collections:
TREC 2002 En-Ar CLIR, NTCIR-8 En-Zh Advanced Cross-
Lingual Information Access (ACLIA), and CLEF 2006 En-Fr
CLIR, with sizes 383,872, 388,589 and 177,452 documents,

respectively. We used the title text of the 50 topics for the
Arabic and French collections, and we treated the 73 well-
formed questions in NTCIR-8 as queries.

For the ﬂat and hierarchical translation models, we used
Moses [12] and cdec [7], respectively. The training data
consisted of Ar-En GALE 2010 evaluation (3.4m sentence
pairs), Zh-En FBIS corpus (0.3m pairs), and Fr-En Europarl
corpus v7 (2.2m pairs). A 3-gram language model was built
for Arabic and Chinese using the target side of the parallel
corpora. For French, we trained a 5-gram LM from the
monolingual dataset provided for WMT-12. More details of
the experimental setup can be found in [23].

Source code for replicating all the results presented in this

paper is available in the open-source Ivory toolkit.2

4.1 Effectiveness

The baseline token-based model yields a Mean Average
Precision (MAP) of 0.271 for Arabic, 0.150 for Chinese,
and 0.262 for French. These numbers are competitive when
compared to similar techniques applied to these collections.
For each collection, we evaluated the three CLIR techniques
(P rtoken, P rSCFG/PBMT, and P rnbest, with n ∈ {1, 10}), ex-
ploring the eﬀect of the diﬀerent alignment heuristics as well
as ﬂat vs. hierarchical phrase-based translation models. Pa-
rameters of the interpolated model were learned by a grid
search. Experimental results are summarized in Table 1.3

Based on a randomized signiﬁcance test [22], the interpo-
lated model outperforms (with 95% conﬁdence, marked *)
the token-based model for all runs except for Arabic with
Moses, consistently with the one-to-many heuristic and in
some cases with the two other heuristics. Furthermore, in
ﬁve out of the six conditions, the interpolated model with
the one-to-many heuristic is signiﬁcantly better than the
one-best MT approach (marked †). This conﬁrms that com-
bining diﬀerent query translation approaches is beneﬁcial,
and is also robust with respect to the test collection, lan-
guage, and underlying MT model. The one-to-many term
mapping heuristic seems to be the most eﬀective overall.

However, the two MT models display signiﬁcant diﬀer-
ences in the “grammar” column, as the hierarchical model
signiﬁcantly outperforms the ﬂat model. This supports the
argument that the former is better at representing trans-
lation alternatives since it is more expressive. Also as a
result of this diﬀerence, the ﬂat grammar is much larger
than the hierarchical one, which leads to an order of magni-
tude increase in processing time for P rPBMT.4 These diﬀer-
ences become especially important for the Arabic collection,
where P rSCFG/PBMT performs much better than P r10-best,
using either MT system. An additional beneﬁt of using
P rSCFG/PBMT is that we do not need to tune model param-
eters for translation, which is computationally intensive.

It is also interesting that the diﬀerences between the two
MT models are insigniﬁcant for the 10-best approach, where
the decoder ﬁnds similar translations in both cases. There-
fore, it might be better to use ﬂat representations for the
10-best approach for eﬃciency, since the end-to-end trans-
lation process is faster than hierarchical models.

2

3

http://ivory.cc/
For the 1-best model, one-to-one and one-to-many perform very
similarly, so we present only the former for space considerations.
4
On the other hand, decoding with a ﬂat grammar is substantially
faster than decoding with hierarchical MT due to constraints im-
posed by language modeling.

815Language MT

token

Ar

Zh

Fr

cdec
Moses
cdec
Moses
cdec
Moses

0.271

0.150

0.262

grammar

1-best

10-best

many
0.293
0.274
0.182
0.156
0.297
0.264

one
0.282
0.266
0.188
0.167
0.288
0.257

none
0.302
0.273
0.170
0.151
0.292
0.262

one
0.249
0.249
0.155
0.155
0.276
0.297

none many
0.255
0.249
0.232
0.264
0.159
0.155
0.169
0.146
0.307
0.235
0.242
0.289

one
0.249
0.254
0.159
0.163
0.304
0.300

interpolated
none many
∗†
0.293
0.248
†
0.249
0.280
∗†
0.192
0.159
∗†
0.183
0.163
∗†
0.318
0.295
∗
0.282
0.307

one
0.282
0.274
∗
0.193
∗
0.177
∗
0.314
0.301

none
∗
0.302
0.276
∗
0.182
∗
0.188
∗
0.315
0.300

Table 1: A summary of experimental results under diﬀerent conditions, for all three CLIR tasks. Superscripts
* and † indicate the result is signiﬁcantly better than the token-based and one-best approaches, respectively.

5. CONCLUSIONS AND FUTURE WORK

In this paper, we extended an MT-based context-sensitive
CLIR approach [24], comparing ﬂat and hierarchical phrase-
based translation models on three collections in three diﬀer-
ent languages. We make a number of interesting observa-
tions about the tradeoﬀs in incorporating machine transla-
tion techniques for query translation.

A combination-of-evidence approach was found to be ro-
bust and eﬀective, but we have not examined how the in-
terpolation model parameters can be learned using held-out
data—this is the subject of ongoing work. Also, we are
exploring ways of leveraging the translation of multi-token
source-side expressions. Although we demonstrated the ben-
eﬁts of hierarchical grammars, we still do not explicitly take
advantage of non-terminal information in the rules. It might
be beneﬁcial to perform a detailed error analysis to see what
types of topics are improved with the use of SCFGs over ﬂat
grammars. Finally, we brieﬂy discussed interesting trade-
oﬀs between eﬃciency and eﬀectiveness, but more detailed
experiments are required to better understand diﬀerent op-
erating points and the overall design space.

6. ACKNOWLEDGMENTS

This research was supported in part by the BOLT pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-12-C-0015; NSF under awards IIS-
0916043 and IIS-1144034. Any opinions, ﬁndings, conclu-
sions, or recommendations expressed are those of the au-
thors and do not necessarily reﬂect views of the sponsors.
The second author is grateful to Esther and Kiri for their
loving support and dedicates this work to Joshua and Jacob.

7. REFERENCES
[1] M. Adriani and C. Van Rijsbergen. Phrase identiﬁcation in

cross-language information retrieval. RIAO, 2000.

[2] L. Ballesteros and W. Croft. Phrasal translation and query

expansion techniques for cross-language information
retrieval. SIGIR Forum, 31:84–91, 1997.

[3] A. Berger and J. Laﬀerty. Information retrieval as

statistical translation. SIGIR, 1999.

[4] P. Brown, V. Pietra, S. Pietra, and R. Mercer. The

mathematics of statistical machine translation: parameter
estimation. CL, 19(2):263–311, 1993.

[5] D. Chiang. Hierarchical phrase-based translation. CL,

33(2):201–228, 2007.

[6] K. Darwish and D. Oard. Probabilistic structured query

methods. SIGIR, 2003.

information retrieval using n-best query translations.
SIGIR, 2002.

[9] J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence

language model for information retrieval. SIGIR, 2004.

[10] J. Gao, J.-Y. Nie, and M. Zhou. Statistical query

translation models for cross-language information retrieval.
TALIP, 5(4):323–359, 2006.

[11] B. Jones, J. Andreas, D. Bauer, K. Hermann, and

K. Knight. Semantics-based machine translation with
hyperedge replacement grammars. COLING, 2012.
[12] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,

M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
Moses: open source toolkit for statistical machine
translation. ACL Demos, 2007.

[13] P. Koehn, F. Och, and D. Marcu. Statistical phrase-based

translation. NAACL-HLT, 2003.

[14] W. Kraaij, J. Nie, and M. Simard. Embedding web-based

statistical translation models in cross-language information
retrieval. CL, 29(3):381–419, 2003.

[15] Y. Liu, R. Jin, and J. Chai. A maximum coherence model
for dictionary-based cross-language information retrieval.
SIGIR, 2005.

[16] A. Lopez. Statistical machine translation. ACM Computing

Surveys, 40(3):8:1–8:49, 2008.

[17] W. Magdy and G. Jones. Should MT systems be used as

black boxes in CLIR? ECIR, 2011.

[18] D. Metzler and W. Croft. A Markov random ﬁeld model for

term dependencies. SIGIR, 2005.

[19] V. Nikoulina, B. Kovachev, N. Lagos, and C. Monz.

Adaptation of statistical machine translation model for
cross-language information retrieval in a service context.
EACL, 2012.

[20] J. Ponte and W. Croft. A language modeling approach to

information retrieval. SIGIR, 1998.

[21] H.-C. Seo, S.-B. Kim, H.-C. Rim, and S.-H. Myaeng.

Improving query translation in English-Korean
cross-language information retrieval. IP&M, 41(3):507–522,
2005.

[22] M. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval
evaluation. CIKM, 2007.

[23] F. Ture. Searching to Translate and Translating to Search:

When Information Retrieval Meets Machine Translation.
PhD thesis, University of Maryland, College Park, 2013.

[24] F. Ture, J. Lin, and D. Oard. Looking inside the box:

context-sensitive translation for cross-language information
retrieval. SIGIR, 2012.

[25] D. Wu. A polynomial-time algorithm for statistical machine

translation. ACL, 1996.

[26] J. Xu and R. Weischedel. Empirical studies on the impact

of lexical resources on CLIR performance. IP&M,
41(3):475–487, 2005.

[7] C. Dyer, J. Weese, H. Setiawan, A. Lopez, F. Ture,

[27] K. Yamada and K. Knight. A syntax-based statistical

V. Eidelman, J. Ganitkevitch, P. Blunsom, and P. Resnik.
cdec: a decoder, alignment, and learning framework for
ﬁnite-state and context-free translation models. ACL
Demos, 2010.

[8] M. Federico and N. Bertoldi. Statistical cross-language

translation model. ACL, 2001.

[28] W. Zhang, S. Liu, C. Yu, C. Sun, F. Liu, and W. Meng.

Recognition and classiﬁcation of noun phrases in queries for
eﬀective retrieval. CIKM, 2007.

816