Live Nuggets Extractor

A Semi-Automated System for Text Extraction and Test Collection Creation

Matthew Ekstrand-Abueg
Javed A Aslam
College of Computer and Information Science, Northeastern University

Virgil Pavlu

{mattea,vip,jaa}@ccs.neu.edu

1. ABSTRACT

The Live Nugget Extractor system provides users with a
method of eﬃciently and accurately collecting relevant in-
formation for any web query rather than providing a sim-
ple ranked lists of documents. The system utilizes an online
learning procedure to infer relevance of unjudged documents
while extracting and ranking information from judged doc-
uments. This creates a set of judged and inferred relevance
scores for both documents and text fragments, which can
be used for test collections, summarization, and other tasks
where high accuracy and large collections with minimal hu-
man eﬀort are needed.

Categories and Subject Descriptors:
H.3.3 [Information Search and Retrieval]
Keywords: information retrieval; relevance assessment; text
extraction; test collection; nuggets

2.

INTRODUCTION

With Information Retrieval research increasingly moving
from document retrieval to retrieval of text fragments, the
needs is growing for systems which can create new test col-
lections based on smaller text units and give users a way to
guide their search for text answers, rather than ranked lists
of documents. To this end, we have created an interface
that allows a user to input a query and iteratively judge
both documents and text fragments, or nuggets, for rele-
vance 12. These judgments are actively incorporated into a
relevance model and new documents and nuggets are pre-
sented to maximize relevant information returned.

We have previously shown that the nuggets system allows
for test collections to be created with high information re-
call and accuracy of inferred document relevance using a
relatively small number of judgments [3]. Further, it creates

1A live demonstration of the interface can be accessed at
http://fiji.ccs.neu.edu/nuggets/demo/.
2We gratefully acknowledge support provided by NSF IIS-
1256172.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

collections which can be used to evaluate both document-
centric and text-centric systems by extracting text fragments
during the judging process. The system has been used for
the English track of NTCIR-10 1CLICK-2 and adapted to
Japanese to compare to a manual methodology.

The demo system presented here builds upon our previ-
ous work [3] for test collection creation by creating a live
system which accepts a query from the user, presents doc-
uments and nuggets for judgment feedback, then reweights
documents and nuggets according to the feedback. The mo-
tivation for this is to create a collection of documents to
pass into summarizer systems and simultaneously ﬁnd the
text against which to evaluate the output of such systems.
2.1 Related Work

To the best of our knowledge, there exists no similar tool
for collecting relevant information without fully manual fact-
base extraction. Pooling methods using ranker systems fol-
lowed by exhaustive judging or statistical sampling are com-
mon, but neither can scale with modern collections. TREC,
a U.S. government eﬀort, coordinates professional assessors
to judge up to 3,000 documents per query, but even this
eﬀort has fatigue-driven errors [1] and many relevant docu-
ments not even considered for assessment [2].

Although direct relevance feedback at the document level
is too expensive in web search, our active framework for
judging and reﬁning limits the manual work required and
provides inferred relevance of text fragments. This is a
highly expensive operation if performed manually, more com-
parable to TREC’s track on document assessment, which re-
quired considerably more manual work than our system. In
previous work, we showed that our method also outperforms
both Relevance Feedback and Learning to Rank schemes.

Basic query reﬁnement and expansion exist in popular
search engines, providing a query’s common similar searches
and related entities. This, however, is not based on the
information relevant to the query, and does not allow the
user to see ranked text fragments based on this reﬁnement.
As we will show, our system allows the user to guide the
search both for types of relevant documents and information,
while easily viewing the progress of the inference and the
reﬁned set of relevant information.

3. SYSTEM COMPONENTS
3.1 User Interface

The interface for the Nugget Extractor system presents
the user with the ability to enter a query and then perform

1087Figure 1: The Nugget Extractor Interface. Multiple documents are shown on the left, similar to web search
results, expandable to view the documents. Extracted nuggets are shown on the right, ranked in the top
panel by inferred relevance, with judged nuggets visible in lower panels.

iterative judgments on documents and nuggets to grow the
space of relevant information.

The left side of the interface allows the user to read a
document or an expandable ranked list of documents, and
judge them for relevance. The right side of the interface
allows the user to view the information that has been ex-
tracted so far, sorted by inferred relevance, and optionally
to judge individual nuggets.

Additionally, a user may manually create a nugget. This
can be useful to expand the scope of the search, in a similar
fashion to iterative query reformulation. The nugget can
be in any category, to allow for both positive and negative
examples to aid reﬁnement of the relevance space.

Once a user feels they have completed their search, they
may export lists of relevance for nuggets and documents.
They may also see lists of unjudged items sorted by their
inferred relevance scores.

3.2 Backend Procedure

The User Interface provides the interaction for the man-
ual portions of the extraction system, but most of the work
is done behind the scenes. We use an iterative procedure
to incorporate judgments on documents and nuggets into a
belief graph of relevance, incorporating the observations and
modifying the weights based on matches between documents
and judgments. This procedure can be seen in full in our
previous work [3].

Figure 2 illustrates the backend system, where our TEXT
MATCHING system creates the links between documents
and nuggets, allowing assessments on either side to inﬂuence
the inferred relevance scores for all other objects.

When a user enters a query, a set of candidate documents
are retrieved and cleaned for the interface and then the ﬁrst
set of documents is displayed to the user. When the user
judges a document, it is placed in the QREL and the backend
procedure extracts nuggets from the document if it is marked
relevant. Then all nuggets which matched the document
are reweighted according to its judgment. Finally, the new

Figure 2: The overall design of assessment process:
Iteratively, documents are selected and assessed,
and nuggets are extracted and [re]weighted.

nuggets are also matched against all documents, and new
document scores are computed from the new weights on all
nuggets. Judging a nugget reweights the documents which
match it, as does creating a new nugget.

4. CONCLUSION

Our Nugget Extractor system allows users to cover the
relevant information space of a large number of documents
without having to fully read and extract information from
each one. This is useful in creating lists of facts for a query,
creating document or nugget test collections with both ex-
plicit and implicit relevance judgements, and in guided search.

5. REFERENCES
[1] B. Carterette and I. Soboroﬀ. The eﬀect of assessor

error on IR system evaluation. SIGIR ’10.

[2] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR

system evaluation using nugget-based test collections.
WSDM ’12.

[3] S. Rajput, M. Ekstrand-Abueg, V. Pavlu, and J. A.

Aslam. Constructing test collections by inferring
document relevance via extracted relevant information.
CIKM ’12.

DOCS_REL-------DOCS_NRELDOCUMENT SELECTORJUDGENUGGETEXTRACTIONNUGGETWEIGHTUPDATENUGS_REL-------NUGS_NRELQREL(docs)GREL(nuggets)TEXT MATCHING1088