Kernel-based Learning to Rank with Syntactic and

Semantic Structures

Alessandro Moschitti

Qatar Computing Research Institute

Qatar Foundation, Doha, Qatar
DISI, University of Trento, Italy

amoschitti@qf.org.qa

ABSTRACT
In recent years, machine learning (ML) has been more and
more used to solve complex tasks in diﬀerent disciplines,
ranging from Data Mining to Information Retrieval (IR)
or Natural Language Processing (NLP). These tasks often
require the processing of structured input. For example,
NLP applications critically deal with syntactic and seman-
tic structures. Modeling the latter in terms of feature vec-
tors for ML algorithms requires large expertise, intuition
and deep knowledge about the target linguistic phenomenon.
Kernel Methods (KMs) are powerful ML techniques (see e.g.,
[5]), which can alleviate the data representation problem
as they substitute scalar product between feature vectors
with similarity functions (kernels) directly deﬁned between
training/test instances, e.g., syntactic trees, (thus features
are not needed anymore). Additionally, kernel engineering,
i.e., the composition or adaptation of several prototype ker-
nels, facilitates the design of the similarity functions required
for new tasks, e.g., [1, 2]. KMs can be very valuable for
IR research, e.g., KMs allow us to easily exploit syntac-
tic/semantic structures, e.g., dependency, constituency or
shallow semantic structures, in learning to rank algorithms
[3, 4].
In general, KMs can make easier the use of NLP
techniques in IR tasks.

This tutorial aims at introducing essential and simpli-
ﬁed theory of Support Vector Machines (SVMs) and KMs
for the design of practical applications.
It describes ef-
fective kernels for easily engineering automatic classiﬁers
and learning to rank algorithms, also using structured data
and semantic processing. Some examples are drawn from
well-known tasks, i.e., Question Answering and Passage Re-
ranking, Short and Long Text Categorization, Relation Ex-
traction, Named Entity Recognition, Co-Reference Resolu-
tion. Moreover, some practical demonstrations are given
with SVM-Light-TK (tree kernel) toolkit. More in detail,
best practices for successfully using KMs for IR and NLP
are presented according to the following outline:

application viewpoint) and KM theory (the essential content
for understanding practical procedures).

(ii) Presentation of kernel engineering building blocks, such
as linear, polynomial, lexical, sequence and tree kernels, by
focusing on their function, accuracy and eﬃciency rather
than their mathematical characterization, so that they can
be easily understood.

(iii) Illustration of important applications for which ker-
nels achieve the state of the art, i.e., Question Classiﬁca-
tion, Question and Answer (passage) Reranking, Relation
Extraction, coreference resolution and hierarchical text cat-
egorization. In this perspective kernels for reranking will be
presented as an eﬃcient and eﬀective approach to learning
dependencies between structured input and output.

(iv) Practical exercise on quick design of ML systems us-
ing SVM-Light-TK toolkit, which encodes several kernels in
SVMs.

(v) Summary of the key points to engineer innovative and
eﬀective kernels starting from basic kernels and using sys-
tematic data transformations.

(vi) Presentation of the latest KM ﬁndings: kernel-based
learning on large-scale with fast SVMs, generalized struc-
tural and semantic kernels and reverse kernel engineering.
Categories and Subject Descriptors
I.2.7 [Natural Language Processing]: [Language parsing
and understanding, Text analysis]
General Terms
Algorithms, Experimentation
Keywords
Question Answering, Kernel Methods, Large-Scale Learn-
ing, Support Vector Machines, Structural Kernels
REFERENCES
[1] A. Moschitti. Eﬃcient convolution kernels for

dependency and constituent syntactic trees. In
Proceedings of ECML, 2006.

(i) a very brief introduction to SVMs (explained from an

[2] A. Moschitti. Kernel methods, syntax and semantics for

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

relational text categorization. In Proceeding of CIKM,
2008.

[3] A. Moschitti and S. Quarteroni. Linguistic kernels for

answer re-ranking in question answering systems.
Information Processing and Management, 2011.

[4] A. Severyn and A. Moschitti. Structural relationships

for large-scale learning of answer re-ranking. In
Proceedings of SIGIR, 2012.

[5] J. Shawe-Taylor and N. Cristianini. Kernel Methods for

Pattern Analysis. Cambridge University Press, 2004.

1128