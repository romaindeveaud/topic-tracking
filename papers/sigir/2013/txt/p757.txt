Building a Web Test Collection using Social Media

Chia-Jung Lee

Center for Intelligent Information Retrieval

School of Computer Science

University of Massachusetts, Amherst

cjlee@cs.umass.edu

W. Bruce Croft

Center for Intelligent Information Retrieval

School of Computer Science

University of Massachusetts, Amherst

croft@cs.umass.edu

ABSTRACT
Community Question Answering (CQA) platforms contain
a large number of questions and associated answers. An-
swerers sometimes include URLs as part of the answers to
provide further information. This paper describes a novel
way of building a test collection for web search by exploit-
ing the link information from this type of social media data.
We propose to build the test collection by regarding CQA
questions as queries and the associated linked web pages as
relevant documents. To evaluate this approach, we collect
approximately ten thousand CQA queries, whose answers
contained links to ClueWeb09 documents after spam ﬁlter-
ing. Experimental results using this collection show that
the relative eﬀectiveness between diﬀerent retrieval models
on the ClueWeb-CQA query set is consistent with that on
the TREC Web Track query sets, conﬁrming the reliability
of our test collection. Further analysis shows that the large
number of queries generated through this approach com-
pensates for the sparse relevance judgments in determining
signiﬁcant diﬀerences.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval; H.3.m [Information Storage and
Retrieval]: Miscellaneous—Test Collections

General Terms
Experimentation, Performance

Keywords
Test collection, social media, community question answering

1.

INTRODUCTION

The most diﬃcult part of building a test collection is per-
haps creating a set of queries with associated relevance judg-
ments. Click data can be used as a substitute in some cases,

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for prof t or commercial advantage and that copies bear
this notice and the full citation on the f rst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specif c permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Figure 1: An example query with relevance judg-
ments generated from social media data.

but this is not available for general use in academic environ-
ments. In this paper, we explore an approach to generating
a set of queries and relevance judgments for a collection of
web documents by exploiting the information in social me-
dia, and more speciﬁcally Community Question Answering
services. CQA sites such as Yahoo! Answers1 and Baidu
Zhidao2 provide social platforms for users to raise questions,
to obtain useful information and to share potential answers.
The collaborative nature of such platforms motivates inter-
ested users to voluntarily engage in providing useful answers
to many topics. Answerers sometimes provide URLs as part
of their answers. These links are used to provide additional
information, to explain more complicated concepts that can
not be detailed in short paragraphs, or to present reliable
citations, etc. Figure 1 shows an example of an answer that
incorporates several URLs.

The central hypothesis of this paper is that these links can
be regarded as relevant documents for the CQA question (or
query). Although the relevance judgments obtained in this
way are likely to be sparse, our expectation is that the large
number of queries obtained will compensate for this.

In this paper, we collected a large number of question-
answer pairs from existing and newly crawled CQA collec-
tions. We then reduced this set to include only questions
whose answers contained links, where the links pointed to
documents in the ClueWeb09 collection. The ﬁnal ClueWeb-
CQA (CW-CQA) set of queries and relevance judgments was
produced after some additional ﬁltering to deal with issues

1http://answers.yahoo.com/
2http://zhidao.baidu.com/

757such as spam. To test the validity of this new test collection,
the relative eﬀectiveness of some well-known benchmark re-
trieval models is evaluated and compared. Our CW-CQA
results show that a term dependency model signiﬁcantly out-
performs a bag of words model, and pseudo-relevance feed-
back techniques can be helpful in most cases. These ﬁndings
are consistent with the results using standard TREC Web
Track query sets. As expected, the relevance judgments are
sparse and incomplete. Carterette et al. [4] demonstrated
that, up to a point, evaluation over more queries with fewer
judgments is as reliable as fewer queries with more judg-
ments. Similarly, we show that evaluating using a suﬃcient
number of queries shows signiﬁcant diﬀerences between re-
trieval models despite the incomplete relevance judgments.
The rest of paper is laid out as follows. Section 2 summa-
rizes related work and Section 3 describes the methodology
of building the CW-CQA test collection. We discuss the
experimental results in Section 4. Section 5 makes closing
remarks and discusses several future directions.

2. RELATED WORK

There have been several web test collections created for
supporting reproducible experiments at TREC. Examples
include .GOV2, WT2g, and WT10g as well as a recent larger
collection ClueWeb09. For large collections, relevance judg-
ing is mostly done through pooling techniques [7]. Such tech-
niques assemble and judge results from multiple searches and
systems, with the assumption that most relevant documents
will be found.

Even with the use of pooling methods, creating relevance
judgments is often costly and the judged results can be bi-
ased and incomplete [2]. Buckley and Voorhees [1] proposed
the metric bpref that is both highly correlated with exist-
ing measures when complete judgments are available and
more robust to incomplete judgment sets. For the Million
Query Track at TREC 2007, Carterette et al. [4] presented
two document selection algorithms [3] to acquire relevance
judgments. Their results suggested that, up to a point, eval-
uation over more queries with fewer judgments is more cost-
eﬀective and as reliable as fewer queries with more judg-
ments.

3. METHODOLOGY
3.1 Test Collection

We build the CW-CQA test collection using large CQA
datasets and the web collection ClueWeb09 3. We obtain a
large number of question-answer pairs from the CQA cor-
pora and harvest all links provided in answers. We then
reduce this set to include only questions whose answers con-
tained links pointing to ClueWeb09 documents. We obseve
that some of the links contained in CQA answers can be con-
sidered to be spam pages. To ensure a reliable test collec-
tion, we ﬁlter the reduced question sets based on two spam-
controlling parameters SR and SA. Cormack et al [5] pro-
posed a content-based classiﬁer that quantiﬁes the “spam-
miness” of a document based on a scale of 0 to 100, where a
lower score indicates that the page has a higher likelihood to
be spam. Accordingly, SA calculates the average spam score
of all links LQ extracted for a question Q and SR records

3http://lemurproject.org/clueweb09/

4. Varying SR and SA af-
the ratio of spam links among LQ
fects the ﬁnal number of queries and the proportion of spam
links. After ﬁltering, we can establish our ﬁnal CW-CQA
test collection by using the remaining questions and asso-
ciated links as test queries and relevance judgments. We
dicuss the parameter settings in Section 4.
3.2 Retrieval Models

We test four existing retrieval models including query like-
lihood model (QLM), relevance model (RM) [8], sequential
dependency model (SDM) [9] and a query expansion model
using latent concept expansion (LCE) [10]. We choose these
models because they include both common baselines used in
other papers and methods that are state-of-the-art in terms
of eﬀectiveness.

QLM computes the likelihood of generating query texts

based on documents models, and can often be written as:

P (Q|D) rank= X

log(P (qi|D))

qi ∈Q

RM ranks documents according to the odds of their being
observed in the relevant class. P (w|qi . . . qk) can be eﬀec-
tively used to approximate P (w|R) with w as a word in the
collection.

P (D|R)
P (D|N )

≈ Y

w∈D

P (w|R)
P (w|N )

SDM is an eﬀective instantiation of the Markov random
ﬁeld for information retrieval (MRF-IR) that makes the se-
quential dependence assumption. It ranks documents by:

P (D|Q) rank= λT Pqi ∈Q fT (qi, D)

+λO Pqi ∈Q fO(qi, qi+1, D)
+λU Pqi ∈Q fU (qi, qi+1, D)

LCE is a robust query expansion technique based on MRF-
IR. This technique provides a mechanism for modeling term
dependencies during expansion. The central idea is to com-
pute a probability distribution over latent concepts using a
set of pseudo-relevant documents in response to Q. Retrieval
is then done by incorporating the top k latent concepts with
the highest likelihood into original MRF model. Details of
these models can be found in the appropriate papers.

4. EXPERIMENTS
4.1 Building The Collection

CQA datasets. We used two large CQA datasets, Yahoo
Webscope L6 (Y6) 5 and a recently crawled Yahoo! Answers
dataset (YA). Corpus Y6 provides a 10/25/2007 Yahoo! An-
swers dump. We additionally crawled the YA corpus by us-
ing the Yahoo! Answers API. Speciﬁcally, we collected up
to 10,000 questions for each of the 26 Yahoo root categories
as well as their corresponding answers 6. Table 1 shows the
number of questions NQ, the average number of associated
answers per question NAavg, and the average number of links
per question NLavg in ﬁrst row.

4We consider a page with spam score below 60 to be spam.
5http://webscope.sandbox.yahoo.com/
6The collection contains approximately one month Yahoo!
Answers data starting from 7/31/12 to 9/5/12.

758CatA−Y6

CatB−Y6

CatA−YA

CatB−YA

)
l
r
u
Q
n
(
g
o

 

l

0
1

8

6

4

2

0

y =  −3.3307 x +  13.8248

r square =  0.9199

)
l
r
u
Q
n
(
g
o

 

l

0
1

8

6

4

2

0

y =  −3.3787 x +  13.2745

r square =  0.9147

)
l
r
u
Q
n
(
g
o

 

l

8

6

4

2

0

y =  −3.4497 x +  9.5577

r square =  0.9482

)
l
r
u
Q
n
(
g
o

 

l

8

6

4

2

0

y =  −3.5153 x +  8.9705

r square =  0.9808

0

1

2

3

4

0

1

2

3

4

0.0

0.5

1.0

1.5

2.0

2.5

0.0

0.5

1.0

1.5

2.0

2.5

log(n url)

log(n url)

log(n url)

log(n url)

Figure 2: Linear ﬁt of the logarithms of links per question (nurl) and frequency of these questions (nQurl ).

CQA

CW−CQA (CatA)

CW−CQA (CatB)

NQ
NAavg
NLavg
NQ
NLavg
NQ
NLavg

Y6
4,483,032
7.11
1.95
272,619
1.74
186,651
1.64

YA
216,474
3.42
1.92
8,386
1.44
5,567
1.33

Table 1: Dataset statistics.

Figure 3: Query length (x-axis) and the frequency
of queries (y-axis) for query sets 10k, 3k and TREC.

Connecting CQA and ClueWeb. To ﬁnd connec-
tions, we then compared the CQA links with two subsets of
ClueWeb09 pages, namely Category A (CatA) and Category
B (CatB), which contain approximately 500M and 50M En-
glish documents, respectively. The second and third rows in
Table 1 summarize the number of questions whose answers
contained links to the ClueWeb data NQ and their corre-
7. Figure 2 shows the relation between the
sponding NLavg
number of links each question has (nurl) and the frequency
of questions with nurl links (nQurl ). Since this is a log-log
plot, Figure 2 shows that nurl and nQurl follow a power law
distribution; that is, questions with few links occupy a sig-
niﬁcant portion of entire population. R2 statistics show the
goodness of the ﬁt. The connection distributions for CatA
and CatB resemble each other; in the following, we focus
on evaluation using the ClueWeb09 CatB connections and
searching on CatB for computational eﬃciency.

Queries and Relevance Judgments. To build the ﬁ-
nal test collection, we aggregate questions from Y6 and YA
constrained by SR ≤ 0.1, SA ≥ 90 and nurl > 1. The con-
straint nurl > 1 is used to avoid the extremes in the power
law distributions shown in Figure 2. A ﬁnal set of 9988
questions are selected as the CW-CQA test queries and the
associated links are regarded as relevance judgments. This
set is denoted as the 10k set. We construct an additional
query set by selecting queries from 10k that have at least
one relevant document returned by any of the four retrieval
models described in Section 3, resulting in the query set 3k

7Multiple appearances of the same URL for a question is
considered only once.

containing 3440 questions. The maximum nurl in query sets
10k and 3k are respectively 22 and 19, and the minimum for
both is 2. The relation of nurl and nQurl remains as power
law distributions for both query sets. The nature of CQA
questions can make the queries quite long. We apply stop
structure removal techniques [6] to the CW-CQA queries
and Figure 3 compares the query length distributions.

For comparison, we use 148 standard TREC web track
2009, 2010 and 2011 title and description (desc) queries. We
search these TREC queries on ClueWeb09 CatB and evalu-
ate the results using standard TREC relevance judgments.
Figure 3 suggests that the CW-CQA queries are more simi-
lar to TREC description queries in terms of query length.

Retrieval Setup. Indri 8 is used for indexing and search-
ing. We use Dirichlet smoothing with µ = 2500 for all runs
without tuning. We apply spam ﬁltering to all retrieval runs
based on [5]. We evaluate using the top 1000 documents and
report mean average precision (MAP), precision at 10/100
(P@10, P@100), mean reciprocal rank (MRR) and bpref.
4.2 Retrieval Performance

Table 2 shows the retrieval performance of the CW-CQA
query sets 10k and 3k where the top performing runs are
underlined. We perform paired t-tests on pairs of retrieval
models (QLM, SDM), (QLM, RM) and (SDM, LCE). Specif-
ically, RM and SDM are marked † if p-value < 0.05 com-
pared to QLM. LCE is marked ⋆ if p-value < 0.05 compared
to SDM. We observe that SDM signiﬁcantly outperforms
QLM in both query sets for every metric. RM can signiﬁ-
cantly improve QLM for most metrics, showing the utility
of pseudo-relevance feedback. LCE seems to improve SDM,
but the signiﬁcant diﬀerence is only observed for metrics
P@100 and bpref. In general, models SDM and LCE are the
most eﬀective compared to others. The performance of the
10k and 3k query sets show similar trends. For query set 3k,
MRR shows that on average all models rank the ﬁrst known
relevant document above rank 20.

Table 3 shows the retrieval performance of TREC title and
desc queries. Similar to CW-CQA results, SDM signiﬁcantly
outperforms QLM in all cases. For title queries, unlike CW-
CQA results, pseudo-relevance feedback techniques such as
RM and LCE sometimes can hurt performance of QLM and
SDM, respectively. The utility of pseudo-relevance feedback
is more evident for desc queries. The similarity of the query
length for CW-CQA queries and TREC descriptions pro-
vides a possible explanation for the higher level of consis-
tency between their results.

In general, the relative eﬀectiveness between retrieval mod-
els is similar for CW-CQA and TREC queries. The improve-
ments based on term dependency modeling are signiﬁcant

8http://www.lemurproject.org/indri/

759Model MAP
QLM .0107
SDM .0114†
RM .0114†
LCE
.0114
QLM .0312
SDM .0331†
RM .0330†
LCE
.0331

P@10
.0047
.0051†
.0050†
.0051
.0137
.0149†
.0144†
.0149

P@100 MRR
.0195
.0018
.0019†
.0208†
.0019†
.0204
.0203
.0020⋆
.0566
.0051
.0605†
.0054†
.0055†
.0593
.0057⋆
.0590

bpref
.1815
.1866†
.1942†
.2014⋆
.5271
.5417†
.5639†
.5849⋆

10k

3k

l

e
u
a
v
−
p

e
g
a
r
e
v
a

4

.

0

3
0

.

2
0

.

1
0

.

0

.

0

MAP
P@10
P@100
MRR
bpref

p−value = 0.05

Table 2: Retrieval results for CW-CQA query sets
10k and 3k.

0

500

1000

1500

2000

2500

3000

3500

sample size

Model MAP
QLM .1804
SDM .1989†
RM .1810
LCE
.2037⋆
QLM .1309
SDM .1471†
RM .1365
LCE
.1463

P@10 P@100 MRR
.4860
.3628
.5171†
.3831
.4808
.3622
.4910
.3830
.2892
.4559
.4611
.2932
.4482
.2896
.3000
.4537

.1853
.1928†
.1848
.1926
.1147
.1184†
.1168†
.1214

bpref
.2715
.2877†
.2747
.2926
.2953
.3030†
.2975
.3049

title

desc

Table 3: Retrieval results for TREC query sets.

for all query sets.

The absolute retrieval performance in Table 2 is rather
low compared to Table 3. This is to be expected given the
sparseness of relevance judgments. Carterette et al. [4] sug-
gested that evaluation over more queries with fewer judg-
ments can be reliable. Similarly, an interesting question
from our perspective is: how many queries do we need to
conﬁrm the existence of signiﬁcant diﬀerences between re-
trieval models? To this end, we compute the p-value be-
tween QLM and SDM using diﬀerent number of CW-CQA
queries. Speciﬁcally, from the 3k set, we randomly sample
k queries where k ranges from 100 to 3400 in steps of 100.
We perform 20 random samples at each k and report the
average p-value in Figure 4. All metrics share a tendency
that using more queries results in smaller p-values. For met-
rics such as MAP, MRR and P@10, stable signiﬁcance (i.e.,
p-value < 0.05) is reached when the sample size grows be-
yond 2100. For other metrics such as P@100 and bpref, a
sample size of more than 1000 queries is suﬃcient to conﬁrm
a signiﬁcant diﬀerence. These observations support [4] in a
sense that evaluating using a suﬃcient number of CW-CQA
queries distinguishes retrieval model eﬀectiveness despite in-
complete judgments.

5. CONCLUSIONS

We proposed a novel way of building a test collection for
web search by considering CQA questions as queries and
the associated URLs as relevant documents. This approach
has the advantage that a large number of queries and rel-
evance judgments can be gathered automatically and eﬃ-
ciently. We ﬁltered CW-CQA queries based on the spam
scores of their links. Experimental results on the CW-CQA
query sets show that the relative eﬀectiveness between diﬀer-
ent retrieval models is consistent with previous ﬁndings us-
ing the TREC queries, showing the reliability of the test col-
lection. The relevance judgments for the CW-CQA queries
are incomplete and the absolute retrieval performance is rel-
atively low. However, we demonstrated that evaluation us-
ing a suﬃcient number of queries ensures that signiﬁcant

Figure 4: Average p-value of 20 times of sampling
at each sample size.

diﬀerences can be found.

These initial experimental results indicate several direc-
tions for future work. Validating consistency with manual
relevance judgments will be important for our study. We
plan to select a small set of queries for human assessors
to judge, and compare the results with the automated ap-
proach. In addition, we will evaluate the same method using
the newly constructed ClueWeb12 collection. The CW-CQA
query sets for both ClueWeb09 and ClueWeb12 will be dis-
tributed through the Lemur project.

Acknowledgements
We thank Samuel Huston for his professional suggestions.
This work was supported in part by the Center for Intelli-
gent Information Retrieval and in part by NSF IIS-1160894.
Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are those of the authors and do
not necessarily reﬂect those of the sponsor.

6. REFERENCES

[1] C. Buckley and E. M. Voorhees. Retrieval evaluation with

incomplete information. In Proc. of SIGIR, SIGIR ’04, pages
25–32, 2004.

[2] S. B¨uttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroﬀ.
Reliable information retrieval evaluation with incomplete and
biased judgements. In Proc. of SIGIR, SIGIR ’07, pages 63–70,
2007.

[3] B. Carterette, J. Allan, and R. Sitaraman. Minimal test

collections for retrieval evaluation. In Proc. of SIGIR, SIGIR
’06, pages 268–275, 2006.

[4] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and

J. Allan. Evaluation over thousands of queries. In Proc. of
SIGIR, SIGIR ’08, pages 651–658, 2008.

[5] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Eﬃcient

and eﬀective spam ﬁltering and re-ranking for large web
datasets. CoRR, abs/1004.5168, 2010.

[6] S. Huston and W. B. Croft. Evaluating verbose query

processing techniques. In Proc. of SIGIR, SIGIR ’10, pages
291–298, 2010.

[7] K. Jones, C. Van Rijsbergen, B. L. Research, and D. Dept.

Report on the Need for and Provision of an Ideal Information
Retrieval Test Collection. British Library Research and
Development reports. 1975.

[8] V. Lavrenko and W. B. Croft. Relevance based language

models. In Proc. of SIGIR, SIGIR ’01, pages 120–127, 2001.

[9] D. Metzler and W. B. Croft. A markov random ﬁeld model for

term dependencies. In Proc. of SIGIR, SIGIR ’05, pages
472–479, 2005.

[10] D. Metzler and W. B. Croft. Latent concept expansion using
markov random ﬁelds. In Proc. of SIGIR, SIGIR ’07, pages
311–318, 2007.

760