Tagcloud-based Explanation with Feedback for

Recommender Systems

Wei Chen

Wynne Hsu

Mong Li Lee

School of Computing, National University of Singapore, Singapore

fweichen,whsu,leemlg@comp.nus.edu.sg

bought “Mulan" also bought “Toy Story". User-based explanation
of recommendations looks at the rating proﬁle of a user u and ﬁnds
the set of users U with similar rating proﬁles. The items rated pos-
itively by U will be recommended to u. The main drawback of
item-based and user-based explanations of recommendations is that
different users may like an item for different reasons. For example,
some may like “Mulan" because it has good animation whereas
user u bought “Mulan" because of its ﬁlial piety theme.
In this
case, recommending “Toy Story” to u may not be suitable.

ABSTRACT
Personalized recommender systems aim to push only the relevant
items and information directly to the users without requiring them
to browse through millions of web resources. The challenge of
these systems is to achieve a high user acceptance rate on their
recommendations. In this paper, we aim to increase the user accep-
tance of recommendations by providing more intuitive tag-based
explanations of why the items are recommended. Tags are used as
intermediary entities that not only relate target users to the recom-
mended items but also understand users’ intents. Our system also
allows tag-based online relevance feedback. Experiment results on
the Movielens dataset show that the proposed approach is able to
increase the acceptance rate of recommendations and improve user
satisfaction.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Search and Retrieval

Keywords
Tensor factorization, Recommendation, Social tags, Personaliza-
tion, Explanation

1.

INTRODUCTION

Existing recommender systems utilize different approaches such
as content ﬁltering and collaborative ﬁltering to ﬁnd the relevant
items and information directly to the users. However, oftentimes
users do not know why items are recommended to them, and the
acceptance rate of these recommendations is not high. This has
led to low users’ satisfaction and distrust towards the recommen-
dations. Recent works aim to address this problem by explaining
the systems’ recommendations to users. Explanations of recom-
mendations fall into one of three categories: (a) item-based [1], (b)
user-based [1], and (c) feature-based [2].

Item-based explanation of recommendations assumes that users
who have bought similar items have similar interests. For example,
if a user u is interested to buy the DVD “Mulan", Amazon.com
would also recommend item “Toy Story" to u as other users who

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Feature-based explanation utilizes predeﬁned categories and on-
tologies. The work in [2] uses features such as genre, director and
cast to explain the recommended movies. However, good explana-
tion of recommendations remains a challenging task as many items
do not have predeﬁned categories or ontologies and creating them
are domain dependent, laborious and sometimes infeasible. Fur-
thermore, items in recommender systems are very diverse and en-
compasses photos, video and music. These items may not have
readily available textual content to provide the semantics for expla-
nation.

Social tagging has become a common online activity of web
users. This has generated a rich set of tags for products, movies,
videos clips, news articles, blogs. All these tags capture the se-
mantics of items from users’ points of view, using a ubiquitous
vocabulary for heterogeneous domains of objects. In this work, we
propose to use social tags to bridge the gap from the limited text de-
scription and the limited availability of pre-deﬁned categories and
ontologies to the rich semantic feature space of items.

Table 1 shows the quaternary relations among users, tags, ratings
and items. For example, u1 likes the movie \F orrest Gump" and
tags it with the tag \P sychology". We observe that an item is often
associated with tags that provide the semantic features for charac-
terizing the item. As shown in Table 1, the tag \comedy" high-
lights the light-hearted nature of the movie \T oy Story". We can
also infer users’ preference for certain aspects of an item based on
the tags used and the rating information. For example, u1 does not
seem to like comedies since he tags \T oy Story" as \comedy"
and rates them with \dislike" (see Table 1). In contrast, u2 tags the
same movie as \comedy" and rates them with \like". In addition,
tags can serve to highlight the latent associations between an item
and the user. For example, a system may recommend \T oy Story"
to u3 since u3 likes and tags \F orrest Gump" as comedy.

The work in [3] utilizes a 4-order tensor to model users, items,
tags and ratings in a social tagging system. A multi-way latent se-
mantic analysis is applied on the tensor, and dimensionality reduc-
tion is performed using the higher-order analogue of matrix singu-
lar value decomposition. The singular values obtained after dimen-
sion reduction denote the latent features for both users and items.
However, it is difﬁcult to attach semantic meaning to these latent

945Table 1: Quaternary relations among users, tags, ratings and
items

User
u1
u1
u1
u2
u2
u3

1
2
3
4
5
6

Tag

psychology

comedy

psychology

comedy
comedy
comedy

Rating

like
dislike
like
like
like
like

Item

Forrest Gump

Toy Story

Beautiful Mind
Forrest Gump

Toy Story

Forrest Gump

features and therefore providing explanations on why the items are
recommended to users proved to be impossible.

In this work, we overcome this limitation by utilizing the parallel
factor (PARAFAC) model [4] to extract the latent features of users
and items and map them to a common basis in terms of tags. These
tags capture the semantic features of items from users’ point of
views and allow us to generate explanations that are intuitive to
users in the form of tagclouds. We develop a recommender system
that allows users to provide feedback on the recommended items.
Based on the feedback, we incrementally update the users’ proﬁle
to generate a new list of recommendations. Experiment results on
the Movielens dataset show that the proposed approach is able to
increase the acceptance rate of recommendations and improve user
satisfaction.
2. SYSTEM OVERVIEW

Figure 1 shows the framework of the proposed recommender
system. The Repository contains users’ rating and social tagging
activities and user proﬁles. The Watcher monitors log user activ-
ities such as the tags they use, the ratings they give to items, etc.
We provide an interface for users to choose tags that are used by
other users (e.g, Pixar, Disney, animation, TomHanks, cgi.etc) or
add their own tags to the movies. Besides that, user can rate the
item based on his/her opinion.

Figure 1: Recommendation System Overview

Based on the user proﬁles, the Recommender generates person-
alized recommendation and provides explanation for the recom-
mended items. The Advisor shows the top-N items to the user,
and accepts feedback from the user if s/he is not satisﬁed with the
recommendations. Based on the feedback, the Recommender will
compute a new list of recommendations for the user.

Figure 2(a) shows a screenshot of our recommender system. A
red (blue) colored tag indicates that it is often associated with pos-
itively (negatively) rated items. A black colored tag means that
it is neutral. From the user and item tagclouds, a user will real-
ize that \T oy story" is recommended to him/her because these
clouds have tags in common \classic", \disney" \imdb top 250"
and \animation". The context to aid user understanding, e.g.,
\disney" \animation", \classic", \imdb top 250" and \Oscar"

(a) Before feedback

(b) After feedback

Figure 2: Screenshots of recommendation system

are key factors that characterize the user. Note that a user can
choose different levels of summarization by controlling the number
of tags displayed. Figure 2(b) shows the new list of items recom-
mended after the user provides his/her feedback by clicking on the
thumb-up icon for \T om Hanks" and \Adventure".
3. PROPOSED METHOD

We model Table 1 as a 4-order tensor with dimensions 3(cid:2)2(cid:2)2(cid:2)
3. The tuple (u1,pyschology,like,\F orest Gump") corresponds
to the entry A(1,1,1,1). The value of this tensor entry is 1 since
(u1,pyschology,like,\F orest Gump") can be found in Table 1.
On the other hand, the tuple (u1,comedy,like,\F orest Gump")
corresponds to A(1,2,1,1) but the value of this entry is 0 as the
tuple does not exist in Table 1.

The work in [3] uses Higher Order Singular Value Decomposi-
tion (HOSVD) to capture the underlying relationships among user-
tag-item-rating by reducing the rank of the original tensor to lower
rank such that the effect of noise on the underlying population is
minimized. We have the approximate tensor ^A with a lower rank
(c1 (cid:1)(cid:1)(cid:1) c4)

^A = ^S (cid:2)1 ^U (1) (cid:2)2 ^U (2) (cid:1)(cid:1)(cid:1) (cid:2)4 ^U (4)

(1)
where ^U (i) is the latent features representation (e.g.users, etc.)
and ^S is the approximate core tensor such that the frobenius norm
jjA (cid:0) ^Ajj2
F is minimized [3]. User/Item/Tag recommendation can
be generated based on the ^A [3]. However, one drawback of such
model is that reason why we recommend item to user is not clear,
which is suffered by most of the model based recommender algo-
rithms.

In order to provide an intuitive explanation for the recommenda-
tion, our idea is to map the underlying relationships among user-

UserAsk for RecommendationReceive Recommendation and explanationMonitor user activitiesWatcherRespositoryRecommenderAdvisorTop-N Recommendation explanation by Tag cloudTop-N RecommendationUser feedback946tag-item-rating to a common basis in terms of tags so that it is
meaningful and understandable to the users. To achieve this, we
extract the latent features of users, tags, items and map them into
a common space. Figure 3 shows the mapped 2D space of users,
items, and tags for our running example.

From the distribution, we observe that \T oy Story" and u3 are
close together, suggesting that the two are rather similar. In addi-
tion, the tag closest to u3 and \T oy Story" is comedy. In other
words, the dominant feature in u3 and \T oy Story" is comedy.
Hence, the recommender system can explain to u3 that \T oy Story"
is recommended because he/she likes comedy and \T oy Story"
is a comedy.

The extraction of the latent features of users, tags, and items and
mapping them into a common space requires a special decomposi-
tion model that allows a one-to-one mapping of dimension across
each mode. Here, we adopt the PARAFAC model [4] to carry out
further tensor decomposition on the approximate core tensor ^S to
obtain a set of projection matrices ^P (i) (1 (cid:20) i (cid:20) 4).

⃗^P (1)

(cid:10) ⃗^P (2)

^S =
(2)
denotes the j th column of matrix ^P (i) and ^P (i) 2

(cid:10) (cid:1)(cid:1)(cid:1) ⃗^P (4)

j=1

j

j

j

j

where ⃗^P (i)
Rci(cid:2)r (1 (cid:20) i (cid:20) 4).
^U (i) (cid:1) ^P (i) (1 (cid:20) i (cid:20) 4), we have

By substitute the Equation (2) into Equation (1), let ^U′(i)

=

R∑

^A = ^S′ (cid:2)1 ^U′(1) (cid:2)2 ^U′(2) (cid:2)3 ^U′(3) (cid:2)4 ^U′(4)

where ^U′(1) 2 RjUj(cid:2)r, ^U′(2) 2 RjV j(cid:2)r, ^U′(3) 2 RjRj(cid:2)r and
^U′(4) 2 RjTj(cid:2)r
As such, we have the latent features ^U′(i) extracted for users, tags,
and items mapped to a same r dimension space. In this mapped
space, we will characterize both users and items using the tags
based on the cosine similarity between users/items to tags. Once
this is done, the recommender system can automatically generate
the explanation for the "why" question using tags.

To provide further insights on the recommendation, we catego-
rize tags into three groups depending on how often they are asso-
ciated with positively rated items, negatively rated items, or mixed
rating by the user. The categorization of a tag t for a user u is ob-
tained by computing the total tensor values for each rating over all
the items.

∑

^A(u, i, t, \like") (cid:0)

^A(u, i, t, \dislike")

pref (u, t) =

i2V

∑

i2V

We say that t is a positive tag for u if pref (u, t) > 0, negative if
pref (u, t) < 0, and neutral if pref (u, t) = 0.
4. ADAPTIVE FEEDBACK RECOMMENDA-

TION

After receiving the recommendations and the corresponding ex-
planations, sometimes the users may ﬁnd the recommendations un-
suitable due to inaccurate proﬁle descriptions. A novel feature
of our recommendation framework is its ability to allow users to
provide feedback and dynamically adjust the recommendation list
based on the feedback.

In our running example, suppose u3 is not happy with the rec-
ommendation of \T oy Story". S/he is able to rate the recommen-
dation \T oy Story" with the rating \dislike". This is equivalent
to changing the weight of the tensor element A (u3,\Comedy",
\dislike",\T oy Story") to 1.

Figure 3: Distribution of users, tags, and items in r = 2 dimen-
sional space.

Alternatively, the user may choose to adjust his/her proﬁle de-
scription to more accurately reﬂect his/her current interests. Sup-
pose a user u likes the artist \T im Allen", he can click on the
thumb-up icon (see Figure 2(a)). If he does not like \T im Allen",
he will click on the thumb-down icon. For each thumb-up on the tag
t, we search for movie m that has been tagged using t by users other
than u and replace weight of the tensor element A(u, t, \like", m)
by a small constant c = 1/q where q is the number of users who
tagged movie m with tag t. Similarly, for each thumb-down ac-
tion, the weight of tensor element A(u, t, \dislike", m) will be
replaced by a small constant c = 1/q. . With each update of the
tensor elements, we can adopt the online PCA technique [5] to up-
date the latent feature matrice ^U (i), 1 (cid:20) i (cid:20) 4.

5. EXPERIMENTAL RESULTS

We implemented our algorithms in MATLAB and developed our
recommender system as a web application using ASP.NET with
MYSQL server Database. In our experiments, we set r =50.

We obtain 4 years of data (Dec 2005 to Jan 2009) from the pub-
licly available MovieLens dataset 1. This data comprises of two
ﬁles. The ﬁrst ﬁle contains users’ tags on different movies and the
second ﬁle contains users’ ratings on different movies. The ratings
are based on a scale of 1 to 5, with 1 being bad and 5 being excel-
lent. By joining these two ﬁles over users and movies, we obtain
the quadruple < user, movie, tag, rating >. We have a total of
24563 tuples with 2,026 users, 5,088 movies, and 9,078 tags.

We pre-process these tuples to obtain a subset such that each
user, movie and tag occur at least 10 times in the dataset. We also
ﬁlter out those tags that have been used on only 1 item by less than
5 users. We further standardize the tags by stemming and removing
stop words. The resulting dataset has 11122 tuples with 201 users,
501 movies, and 404 tags.

We carried out two sets of experiments to evaluate our proposed
approach. The ﬁrst set of experiments is a user study to demon-
strate the effectiveness of using tag-based recommendations with
explanations in increasing user satisfaction and acceptance. The
second set of experiments shows that updating the recommenda-
tions through user feedback is able to increase user acceptance.
5.1 Evaluation of Explanation Styles

In order to evaluate the user acceptance of the items recom-

1http : //www.grouplens.org/node/73

0.51Forrest Gump1U10.5Beautiful MindPsychologyU2ComedyToy StoryU3947mended, we conduct an extensive user study to compare 3 styles
of explanations.

1. Item-based Explanation[6]. This approach computes the
top (cid:0) k most similar items users rated and tagged before.
It has the format "Item X is recommended, because you have
tagged and rated items Y." The similarity (inﬂuence) between
X and Y is computed based on the cosine similarity of the
items’ ratings and is provided along with the recommenda-
tion.

2. Feature-based Explanation[2]. This approach shows the
features of items recommended to the user. It has the format
"Item X is recommended, because it contains features a,b....".

3. TagCloud Explanation. This approach uses a tagcloud to
summarize the tags used for characterizing the user proﬁle
and the recommended item.

Table 2 shows the different explanations for the recommended

item “Jurassic Park".

Table 2: Example explanations for recommended movie.
Method

Recommen-

Explanation

dation

Item-based

Jurassic Park

Feature-based

Jurassic Park

TagCloud

Jurassic Park

We have a total of 30 participants in our online user study. This

study has a data collection phase and an evaluation phase.

In the data collection phase, the goal is to construct users’ proﬁle
based on their ratings and tagging activities. A user is asked to rate
(on a scale of 1 to 5) and tag two sets of movies. The ﬁrst set
of movies contains the top-40 most popular movies in MovieLens.
The second set of movies are selected from the top-200 movies in
MovieLens. These movies have the highest variance in their user
ratings. In order to ensure that there is a reasonable overlap in the
tags used, users have to choose from a pre-determined list of tags.
In the evaluation phase, the system will show each user the 3
different styles of explanation that corresponds to a list of movies.
We remove the movie title and ask the user to rate solely based
on the information provided in the explanation. We call these rat-
ings explanation ratings and the ratings obtained during the data
collection phase the actual ratings.

The explanation style that minimizes the difference between the
explanation ratings and actual ratings indicates that the style is ef-
fective. Table 3 shows the results of the user study. We use µd
and σd to denote the mean and standard deviation of the differ-
ences between explanation ratings and actual ratings.
It is clear
that tagcloud-based explanation has the lowest mean and standard
deviation compared to item-based and feature-based explanation,
indicating that it is the most intuitive and easily understood by the
users.

We also ask the participants to explicitly express their prefer-
ences for each explanation style. The rating is performed on a scale
of 1 to 5 with 1 being the least preferred and 5 being the most pre-
ferred. Table 3 shows the survey results with µq and σq denote
the mean and standard deviation of the survey ratings respectively.
We observe that the participants strongly preferred tagcloud-based
explanation style.

Explanation style
Item-based
Feature-based
Tagcloud-based

Table 3: Evaluation of Explanation Styles
σq
0.91
0.86
0.74

σd
0.82
1.22
0.73
5.2 Evaluation of Feedback

µd
0.56
0.47
0.03

µq
2.56
2.17
4.06

∑

In this set of experiments, we evaluate the effectiveness of users’
feedback in improving user acceptance of our recommended items.
Users are given a list of top 10 recommendations by our system,
and asked to provide ratings for these items. They are allowed to
give feedback on the recommendations that they are not satisﬁed
with. After each feedback, the system will generate a new list of
top 10 recommendation list to the user. We repeat these rounds of
feedback up to 5 times.

i2Items 5(cid:0)ru;i

We use the following evaluation metrics. Mean Absolute Error
where ru;i
(MAE) which is given by M AE =
is the rating given by user u for item i, ^ru;i is the predicted rating
and N is the number of items recommended; Precision measures
the proportion of the correctly recommended items for top-N items
and is deﬁned as P recision(N ) = N umber of items with rating 5
;
Average precision (AP) is the average of the precision for top-k
items, 1 (cid:20) k (cid:20) N and is deﬁned as AP (k) =
N
k=1 P recision(k)
Table 4 shows the results for each round of feedback. We observe
a 9% improvement in MAE, 13.8% improvement in Precision(10)
and 41 % improvement in AP between round 1 and round 5. Note
that 50 % of the users stop giving feedback after round 2, implying
that they are satisﬁed with the recommended items. The results
obtained in round 3 is comparable to round 5, indicating that we are
able to achieve near optimal values in just a few rounds of feedback.

∑

N

N

N

Table 4: Results of User Feedback

round 1
1.04
0.36
0.2522

round 2
1.06
0.37
0.3132

M AE
P (10)

AP

round 3
0.91
0.41
0.3525

round 4
1
0.41
0.3485

round 5
0.94
0.41
0.3582

6. REFERENCES
[1] J. Herlocker, J. A. Konstan, and J. Riedl, “Explaining

collaborative ﬁltering recommendations,” in CSCW, 2000, pp.
241–250.

[2] N. Tintarev, “Explanations of recommendations,” in RecSys,

2007, pp. 203–206.

[3] C. Wei, W. Hsu, and M. L. Lee, “A uniﬁed framework for

recommendations based on quaternary semantic analysis,” in
SIGIR, 2011.

[4] R. A. Harshman, “Foundations of the PARAFAC procedure:

Models and conditions for an" explanatory" multi-modal
factor analysis,” UCLA Working Papers in Phonetics, 1970.

[5] S. Papadimitriou, J. Sun, and C. Faloutsos, “Streaming pattern

discovery in multiple time-series,” ser. VLDB, 2005.

[6] M. Bilgic and R. J. Mooney, “Explaining recommendations:

Satisfaction vs. promotion,” in IUI, 2005.

948