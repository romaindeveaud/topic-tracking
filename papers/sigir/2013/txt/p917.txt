Composition of TF Normalizations: New Insights on

Scoring Functions for Ad Hoc IR

François Rousseau*, Michalis Vazirgiannis*†‡

*LIX, École Polytechnique, France

†Department of Informatics, AUEB, Greece

‡Institut Mines-Télécom, Télécom ParisTech, France

rousseau@lix.polytechnique.fr, mvazirg@aueb.gr

ABSTRACT
Previous papers in ad hoc IR reported that scoring functions
should satisfy a set of heuristic retrieval constraints, provid-
ing a mathematical justiﬁcation for the normalizations his-
torically applied to the term frequency (TF). In this paper,
we propose a further level of abstraction, claiming that the
successive normalizations are carried out through composi-
tion. Thus we introduce a principled framework that fully
explains BM25 as a variant of TF-IDF with an inverse or-
der of function composition. Our experiments over standard
datasets indicate that the respective orders of composition
chosen in the original papers for both TF-IDF and BM25
are the most eﬀective ones. Moreover, since the order is dif-
ferent between the two models, they also demonstrated that
the order is instrumental in the design of weighting models.
In fact, while considering more complex scoring functions
such as BM25+, we discovered a novel weighting model in
terms of order of composition that consistently outperforms
all the rest. Our contribution here is twofold: we provide a
unifying mathematical framework for IR and a novel scoring
function discovered using this framework.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
models

General Terms
Theory, Algorithms, Experimentation

Keywords
IR theory; scoring functions; TF normalizations; heuristic
retrieval constraints; function composition

1. MOTIVATION

Fang et al.

introduced in [3] a set of heuristic retrieval
constraints that any scoring function used for ad hoc infor-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

mation retrieval (IR) should satisfy. In particular, these con-
straints involve term frequency, term discrimination, docu-
ment length and the interactions between them. For in-
stance, they stated that a scoring function should favor
document matching more distinct query terms.
It is one
of the earliest works that formally deﬁned the properties
that both the TF and the IDF components of any weighting
model should possess. It is a unifying theory in IR that ap-
plies to the vector space model (TF-IDF [13]), probabilistic
(BM25 [10]), language modeling (Dirichlet prior [15]) and
information-based (SPL [2]) approaches and the divergence
from randomness framework (PL2 [1]).

The deﬁnition of these constraints contributed to the im-
provement of the overall eﬀectiveness of most modern scor-
ing functions. Constraints on the term frequency result in
successive normalizations on the raw TF, each one satisfy-
ing one or more properties. In our work, we intended to go
one step further and we propose the use of composition to
explain how the normalizations are applied successively in
the general TF×IDF weighting scheme. In section 2, we de-
scribe in details the mathematical framework we designed.
In section 3, we present the experiments we conducted over
standard datasets and the results obtained that indicate
how important the order of composition is, along with a
novel and eﬀective weighting model, namely TFl◦δ◦p×IDF.
Finally, in section 4, we conclude and mention future work.

2. MATHEMATICAL FRAMEWORK

In ad hoc IR, a scoring function associates a score to a
term appearing both in a query and a document. This func-
tion consists of three components supposedly independent of
one another: one at the query level (QF), one at the docu-
ment level (TF) and one at the collection level (IDF). These
components are aggregated through multiplication to obtain
a ﬁnal score for the term denoted hereinafter by TF×IDF.
We omit voluntarily to mention QF in the name since it is
a function of the term frequency in the query and it has
usually a smaller impact on the score, in particular for Web
queries that tend to be short. We make here a diﬀerence be-
tween the TF×IDF general weighting scheme and TF-IDF,
the pivoted normalization weighting deﬁned in [13]. Note
that because we rank documents, these term scores will be
aggregated through sum to obtain a document score but this
is beyond the scope of the current paper.
2.1 A set of TF normalizations

Since the early work of Luhn [4], term frequency (TF)
has been claimed to play an important role in information

917retrieval and is at the center of all the weighting models.
Intuitively, the more times a document contains a term of
the query, the more relevant this document is for the query.
Hence, it is commonly accepted that the scoring function
must be an increasing function of the term frequency and
the simplest TF component can be deﬁned as follows:

T F (t, d) = tf(t, d)

(1)
where tf(t, d) is the term frequency of the term t in the
document d. However, as the use of the raw term frequency
proved to be non-optimal in ad hoc IR, the research com-
munity started normalizing it considering multiple criteria,
mainly concavity and document length normalization. Later,
these normalizations were explained as functions satisfying
some heuristic retrieval constraints as aforementioned [3].

Concave normalization.

The marginal gain of seeing an additional occurrence of a
term inside a document is not constant but rather decreas-
ing. Indeed, the change in the score caused by increasing
TF from 1 to 2 should be much larger than the one caused
by increasing TF from 100 to 101. Mathematically, this cor-
responds to applying a concave function on the raw TF. We
prefer the term concave like in [2] to sublinear like in [5]
since the positive homogeneity property is rarely respected
(and actually not welcomed) and the subadditivity one, even
though desirable, not suﬃcient enough to ensure a decreas-
ing marginal gain.

There are mainly two concave functions used in practice:
the one in TF-IDF [13] and the one in BM25 [10] that we re-
spectively called log-concavity (TFl) and k-concavity (TFk):
(2)

T Fl(t, d) = 1 + ln[1 + ln[tf(t, d)]]
T Fk(t, d) = (k1 + 1) · tf(t, d)
k1 + tf(t, d)

(3)

where k1 is a constant set by default to 1.2 corresponding
to the asymptotical maximal gain achievable by multiple
occurrences compared to a single occurrence.

Document length normalization.

When collections consist of documents of varying lengths
(like web pages for the Web), longer documents will – as
a result of containing more terms – have higher TF val-
ues without necessary containing more information. For in-
stance, a document twice as long and containing twice as
more times a term should not get a score twice as large but
rather a very similar score. As a consequence, it is com-
monly accepted that the scoring function should be an in-
verse function of the document length to compensate that
eﬀect. Early works in vector space model suggested to nor-
malize the score by the norm of the vector, be it the L1
norm (document length), the L2 norm (Euclidian length) or
∞ norm (maximum TF value in the document) [11].
the L
These norms still mask some subtleties about longer docu-
ments – since they contain more terms, they tend to score
higher anyway. Instead, the research community has been
using a more complex normalization function known as piv-
oted document length normalization and deﬁned as follows:

T Fp(t, d) =

(4)
where b ∈ [0, 1] is the slope parameter, |d| the document

|d|
avdl

tf(t, d)
1 − b + b ·

length and avdl the average document length across the col-
lection of documents as deﬁned in [12].
2.2 Composition of TF normalizations

Based on this set of properties and their associated func-
tions, it seems natural to apply them to the raw TF succes-
sively by composing them. In the literature, the document
length normalization has usually been applied to either the
overall term score or the document score like one would nor-
mally normalize a vector. However, it was then hard to fully
ﬁt BM25 in the TF×IDF weighting scheme. With composi-
tion, it is just a matter of ordering the functions. We present
here the two compositions behind TF-IDF and BM25, re-
spectively TFp◦l (= TFp ◦ TFl) and TFk◦p (= TFk ◦ TFp):
(5)

T Fp◦l(t, d) = 1 + ln[1 + ln[tf(t, d)]]

T Fk◦p(t, d) =

1 − b + b ·

|d|
avdl
tf(t,d)
1−b+b· |d|

avdl

(k1 + 1) ·

k1 + tf(t,d)
1−b+b· |d|
(k1 + 1) · tf(t, d)

avdl

=
k1 · [1 − b + b ·
= (k1 + 1) · tf(t, d)
K + tf(t, d)

|d|
avdl] + tf(t, d)

(6)

|d|
avdl) as deﬁned in [10].

where K = k1 · (1 − b + b ·
Note that under this form, it is not obvious that TFk◦p is
really a composition of two functions with the same proper-
ties as the one in TFp◦l. We think this is the main reason
why composition has never been considered before. By do-
ing so, we provide not only a way to fully explain BM25 as a
TF×IDF weighting scheme but also a way to easily consider
variants of a weighting model by simply changing the order
of composition. As we will see in section 3, this led us to a
new TF×IDF weighting scheme that outperforms BM25.
2.3 Inverse Document Frequency

While the higher the frequency of a term in a document is,
the more salient this term is supposed to be, this is no longer
true at the collection level. This is actually quite the inverse
since these terms have a presumably lower discrimination
power. Hence, the use of a function of the term speciﬁcity,
namely the Inverse Document Frequency (IDF) as deﬁned
in [14] and expressed as follows:

IDF (t) = log N + 1
df(t)

(7)

where N is the number of documents in the collection and
df(t) the document frequency of the term t.
2.4 TF-IDF versus BM25

By TF-IDF, we refer to the TF×IDF weighting model
deﬁned in [13], often called pivoted normalization weighting.
The weighting model corresponds to TFp◦l×IDF:

T F -IDF (t, d) = 1 + ln[1 + ln[tf(t, d)]]

1 − b + b ·

|d|
avdl

× log N + 1
df(t)

(8)

By BM25, we refer to the scoring function deﬁned in [10],
often called Okapi weighting. It corresponds to TFk◦p×IDF
when we omit QF (k-concavity of parameter k3 for tf(t, q)).
Thus, it has an inverse order of composition between the

918concavity and the document length normalization compared
to TF-IDF. The within-document scoring function of BM25
is written as follows when using the IDF formula deﬁned in
subsection 2.3 to avoid negative values, following [3]:
× log N + 1
df(t)
2.5 Lower-bounding TF normalization

BM25(t, d) = (k1 + 1) · tf(t, d)
K + tf(t, d)

(9)

Through composition, we also allow additional constraints
for the TF component to be satisﬁed easily. Subadditiv-
ity is for instance a desirable property – if two documents
have the same total occurrences of all query terms, a higher
score should be given to the document covering more dis-
tinct query terms. Here, it just happens that TFl and TFk
already satisfy it as noted in [3].

Recently, Lv and Zhai introduced in [6] two new con-
straints to the work of Fang et al. to lower-bound the TF
component. In particular, there should be a suﬃciently large
gap in the score between the presence and absence of a query
term even for very long documents where TFp tends to 0 and
a fortiori the overall score too. Mathematically, this corre-
sponds to be composing with a third function TFδ that is
always composed after TFp since it compensates the poten-
tial null limit introduced by TFp and it is deﬁned as follows:

{

T Fδ(t, d) =

tf(t, d) + δ
0

if tf(t, d) > 0
otherwise

(10)

where δ is the gap, set to 0.5 if TFδ is composed immediately
after TFp and 1 if concavity is applied in-between. These
are the two values deﬁned in the original papers [6, 7] and
we just interpreted their context of use in terms of order of
composition. We did not change nor tune the values.
The weighting models Piv+ and BM25+ deﬁned in [6]
correspond respectively to TFδ◦p◦l×IDF and TFδ◦k◦p×IDF
while BM25L deﬁned in [7] to TFk◦δ◦p×IDF. We clearly see
that the only diﬀerence between BM25+ and BM25L is the
order of composition: this is one of the advantages of our
framework – easily represent and compute multiple variants
of a same general weighting model. In the experiments, we
considered all the possible orders of composition between
TFk or TFl, TFp and TFδ with the condition that TFp
always precedes TFδ as explained before.
For instance, we will consider a novel model TFl◦δ◦p×IDF

deﬁned as follows:
T Fl◦δ◦p×IDF (t, d) = 1+ln[1+ln[

tf(t, d)
1 − b + b ·

|d|
avdl

+δ]] (11)

where b is set to 0.20 and δ to 0.5.
3. EXPERIMENTS

Following our mathematical framework that relies on com-
position, we wondered why the order of composition was dif-
ferent between two widely used scoring functions – TF-IDF
and BM25. In the original papers [11, 9], there was no men-
tion of the diﬀerence in the order and this motivated us to
investigate the matter. Our initial thought was that using
an inverse order of composition in BM25 could improve it
or vice-versa for TF-IDF. As a consequence, we tried ex-
haustively the combinations among TFk, TFl and TFp and
report the results. Thereafter, as mentioned in subsection
2.5, we followed the same procedure considering a third func-
tion to compose with: TFδ. Indeed, we wanted to explore

the extensions considered by the research community [6, 7]
in terms of composition. This led us to a novel weighting
model that outperforms them (see subsection 3.4).
3.1 Datasets and evaluation

We used two TREC collections to carry out our exper-
iments: Disks 4&5 (minus the Congressional Record) and
WT10G. Disks 4&5 contains 528,155 news releases while
WT10G consists of 1,692,096 crawled pages from a snap-
shot of the Web in 1997. For each collection, we used a
set of TREC topics (title only to mimic Web queries) and
their associated relevance judgments: 301-450 and 601-700
for Disks 4&5 (TREC 2004 Robust Track) and 451-550 for
WT10G (TREC9-10 Web Tracks).

We evaluated the scoring functions in terms of Mean Av-
erage Precision (MAP) and Precision at 10 (P@10) con-
sidering only the top-ranked 1000 documents for each run.
Our goal is to compare weighting models that use the same
functions but with a diﬀerent order of composition and se-
lect the best ones on both metrics. For example, in Table 1,
TFp◦l×IDF is compared with TFl◦p×IDF and TFk◦p×IDF
with TFp◦k×IDF. The statistical signiﬁcance of improve-
ment was assessed using the Student’s paired t-test consid-
ering p-values less than 0.01 to reject the null hypothesis.
3.2 Platform and models

We have been using Terrier version 3.5 [8] to index, re-
trieve and evaluate over the TREC collections. For both
datasets, the preprocessing steps involved Terrier’s built-in
stopword removal and Porter’s stemming. We did not tune
the slope parameter b of the pivoted document length nor-
malization on each dataset. We set it to the default value
suggested in the original papers: 0.20 when used with log-
concavity [13] and 0.75 when used with k-concavity [10].
3.3 Results for TF-IDF versus BM25

We report in Table 1 the results we obtained on the afore-
mentioned datasets when considering concavity and pivoted
document length normalization. To the best of our knowl-
edge, experiments regarding the same functions (TFk, TFl
and TFp) with a diﬀerent order of composition have never
been reported before. They indeed show that the original or-
der chosen for both TF-IDF and BM25 is the most eﬀective
one: TFp◦l×IDF outperforms TFl◦p×IDF and TFk◦p×IDF
outperforms TFp◦k×IDF. But since the order is diﬀerent be-
tween the two, this also indicates that the order does matter
depending on which function is chosen for each property.

For these two models (TF-IDF and BM25), the use of
a diﬀerent concave function to meet the exact same con-
straints requires the pivoted document length normalization
to be applied before or after the function. The impact is even
more signiﬁcant on the Web dataset (WT10G) that corre-
sponds the most to contemporary collections of documents.
3.4 Results for lower-bounding normalization
In Table 2, we considered in addition the lower-bounding
normalization function TFδ deﬁned in subsection 2.5. The
best-performing weighting model on both datasets is a novel
one – TFl◦δ◦p×IDF – and it even outperforms BM25+ and
BM25L (signiﬁcantly using the t-test and p < 0.01). This
model has never been considered before in the literature to
the best of our knowledge. In fact, the results from Table 1
establish that TFl should apparently be applied before TFp

919Table 1: TF-IDF vs. BM25: an inverse order of
composition; bold indicates signiﬁcant performances

Weighting model
IDF
TF
TFp [b=0.20]
TFp [b=0.75]
TFl
TFk
TFp◦k
TFl◦p
TFp◦l
TFk◦p
TFp◦k×IDF
TFl◦p×IDF
TFp◦l×IDF [TF-IDF]
TFk◦p×IDF [BM25]

TREC 2004 Robust
MAP
0.1396
0.0480
0.0596
0.0640
0.1591
0.1768
0.0767
0.1645
0.1797
0.2045
0.1034
0.1939
0.2132
0.2368

P@10
0.2040
0.0867
0.1193
0.1289
0.3141
0.3269
0.1932
0.3651
0.3647
0.3863
0.2293
0.3964
0.4064
0.4161

TREC9-10 Web
P@10
MAP
0.0729
0.0539
0.0833
0.0376
0.0531
0.1021
0.1000
0.0473
0.2063
0.1329
0.2104
0.1522
0.0604
0.0465
0.0622
0.1854
0.1875
0.1260
0.2208
0.1702
0.0833
0.0507
0.2125
0.0750
0.1430
0.2271
0.2479
0.1870

like in TF-IDF. With lower-bounding normalization, it no
longer holds. The formula for TFl◦δ◦p×IDF was given in
equation 11. Without the use of our formal framework and
composition, it would have been harder to detect and test
these variants that can outperform state-of-the-art scoring
functions when the order of composition is chosen carefully.

Table 2: TFl◦δ◦p×IDF vs. BM25+ and BM25L; bold
indicates signiﬁcant performances.
TREC 2004 Robust
MAP
0.1056
0.1807
0.2130
0.2002
0.2155
0.2165
0.1466
0.2096
0.2495
0.2368
0.2472
0.2466

Weighting model
TFδ◦p◦k
TFδ◦l◦p
TFl◦δ◦p
TFδ◦p◦l
TFk◦δ◦p
TFδ◦k◦p
TFδ◦p◦k×IDF
TFδ◦l◦p×IDF
TFl◦δ◦p×IDF
TFδ◦p◦l×IDF [Piv+]
TFk◦δ◦p×IDF [BM25L]
TFδ◦k◦p×IDF [BM25+]

TREC9-10 Web
MAP
P@10
0.0771
0.0556
0.2021
0.0668
0.2625
0.1907
0.2021
0.1436
0.1806
0.2292
0.2354
0.1835
0.1000
0.0715
0.2292
0.0806
0.2771
0.2084
0.1643
0.2438
0.2563
0.2000
0.2026
0.2521

P@10
0.2349
0.3751
0.4064
0.3876
0.3936
0.3956
0.2723
0.4048
0.4305
0.4157
0.4217
0.4145

4. CONCLUSIONS AND FUTURE WORK
Scoring function design is a cornerstone issue in informa-
tion retrieval. In this short paper, we intended to provide
new insights on scoring functions for ad hoc IR. In partic-
ular, we proposed a unifying mathematical framework that
explains how weighting models articulate around a set of
heuristic retrieval constraints introduced in related work.

Using composition to combine the successive normaliza-
tions historically applied to the term frequency, we were
able to fully explain BM25 as a TF×IDF weighting scheme
with just an inverse order of composition between the con-
cavity and the document length normalization compared to
TF-IDF. Besides, the framework also allowed us to discover
and report a novel weighting model – TFl◦δ◦p×IDF – that
consistently and signiﬁcantly outperformed BM25 and its
extensions on two standard datasets in MAP and P@10.

Future work might involve the design of novel retrieval
constraints and their compositions with existing ones. We

are conﬁdent that reﬁning the mathematical properties be-
hind scoring functions will continue to improve the eﬀective-
ness of these models in ad hoc IR.

5. ACKNOWLEDGMENTS

We thank the anonymous reviewers for their useful feed-
backs. This material is based upon work supported by the
French DIGITEO Chair grant LEVETONE.

6. REFERENCES
[1] G. Amati and C. J. Van Rijsbergen. Probabilistic

models of information retrieval based on measuring
the divergence from randomness. ACM Transactions
on Information Systems, 20(4):357–389, Oct. 2002.

[2] S. Clinchant and E. Gaussier. Information-based

models for ad hoc IR. In Proceedings of SIGIR’10,
pages 234–241, 2010.

[3] H. Fang, T. Tao, and C. Zhai. A formal study of
information retrieval heuristics. In Proceedings of
SIGIR’04, pages 49–56, 2004.

[4] H. P. Luhn. A statistical approach to mechanized

encoding and searching of literary information. IBM
Journal of Research and Development, 1(4):309–317,
Oct. 1957.

[5] Y. Lv and C. Zhai. Adaptive term frequency

normalization for BM25. In Proceedings of CIKM’11,
pages 1985–1988, 2011.

[6] Y. Lv and C. Zhai. Lower-bounding term frequency

normalization. In Proceedings of CIKM’11, pages
7–16, 2011.

[7] Y. Lv and C. Zhai. When documents are very long,

BM25 fails! In Proceedings of SIGIR’11, pages
1103–1104, 2011.

[8] I. Ounis, G. Amati, V. Plachouras, B. He,

C. Macdonald, and C. Lioma. Terrier: A high
performance and scalable information retrieval
platform. In Proceedings of SIGIR’06, 2006.

[9] S. E. Robertson and S. Walker. Some simple eﬀective

approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
SIGIR’94, pages 232–241, 1994.

[10] S. E. Robertson, S. Walker, K. Spärck Jones,

M. Hancock-Beaulieu, and M. Gatford. Okapi at
TREC-3. In Proceedings of the TREC-3, pages
109–126, 1994.

[11] G. Salton and C. Buckley. Term-weighting approaches

in automatic text retrieval. Information Processing
and Management, 24(5):513–523, 1988.

[12] A. Singhal, C. Buckley, and M. Mitra. Pivoted

document length normalization. In Proceedings of
SIGIR’96, pages 21–29, 1996.

[13] A. Singhal, J. Choi, D. Hindle, D. Lewis, and

F. Pereira. AT&T at TREC-7. In Proceedings of
TREC-7, pages 239–252, 1999.

[14] K. Spärck Jones. A statistical interpretation of term
speciﬁcity and its application in retrieval. Journal of
Documentation, 28(1):11–20, 1972.

[15] C. Zhai and J. Laﬀerty. A study of smoothing
methods for language models applied to ad hoc
information retrieval. In Proceedings of SIGIR’01,
pages 334–342, 2001.

920