Modeling the Uniqueness of the User Preferences for

Recommendation Systems

Haggai Roitman, Yosi Mass, Iris Eiron

IBM Research Haifa
Haifa 31905, Israel

{haggai,yosimass,irise}@il.ibm.com

David Carmel∗
Yahoo! Research
Haifa 31905, Israel

david.carmel@ymail.com

ABSTRACT
In this paper we propose a novel framework for modeling
the uniqueness of the user preferences for recommendation
systems. User uniqueness is determined by learning to what
extent the user’s item preferences deviate from those of an
“average user” in the system. Based on this framework, we
suggest three diﬀerent recommendation strategies that trade
between uniqueness and conformity. Using two real item
datasets, we demonstrate the eﬀectiveness of our uniqueness
based recommendation framework.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval] Information Filtering

General Terms: Algorithms, Experimentation

Keywords: Recommender Systems, User uniqueness, Popularity

tures “encoded” in user rating data can be derived. Based
on derived user preferences, we further show how the user’s
global uniqueness level can be estimated. The second strat-
egy, termed POP (for popularity), assumes that the user has
no unique preferences, i.e., she actually “toes the line”, and
therefore, recommends poplar items that are likely to satisfy
the preferences of an average user of the system.

Finally, recognizing that every user preferences may ac-
tually lie in between the two extremes, we further propose
a hybrid recommendation strategy, termed POPERS, which
utilizes the user’s uniqueness level to trade between the two
pure strategies. As a proof of concept, using two real item
datasets, we evaluate the eﬀectiveness of the various recom-
mendation strategies and demonstrate the important role
that user uniqueness may play for recommender systems.

1.

INTRODUCTION

In recent years, recommender systems have gained im-
mense popularity for information ﬁltering and personaliza-
tion purposes. Existing recommendation methods can be
roughly classiﬁed into three main types: collaborative ﬁlter-
ing (CF), content-based (CB), and hybrids of the two [1].
While remarkable research has been done and many instan-
tiations of various CB and CF methods have been proposed,
much less eﬀort has been made to take into consideration the
uniqueness of the user preferences for recommendation.

In this work we propose a novel framework for model-
ing the uniqueness of the user preferences for recommenda-
tion systems. User uniqueness is determined by learning to
what extent the user’s item preferences deviate from those
of an “average user” in the system. Using this framework,
we study three alternative recommendation strategies that
are based on dual user uniqueness assumptions.

The ﬁrst strategy, termed PERS (for personalization), rec-
ommends items that satisfy the user unique preferences. For
that, we extend the user model we previously suggested
in [7] and show how user preferences for various item fea-
∗

This work was done while the author worked in IBM

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

2. RELATED WORK

We now shortly review several related works. A survey on
state-of-the-art recommendation methods is available at [1].
This work is based on an earlier work we did in the domain
of user proﬁling [7].
In [7], user content preferences were
implicitly estimated from web browsing logs by utilizing the
collection of users of the system (termed user community
in [7]) as an auxiliary reference. Though, while the basic
model in [7] considers only textual data, this work considers
both diverse item features and explicit user rating data. In
addition,
[7] neither focused on user uniqueness modeling
nor its usage for recommender systems.

In this work we combine a recommendation method based
on user’s proﬁle with a recommendation method based on
item popularity. Several previous works have studied the us-
age of popular items for recommendation, and more specif-
ically, its combination with more traditional recommenda-
tion methods [6, 2, 3, 4]. Recommendation of popular items
has been studied in the context of the cold-start problem of
recommender systems [4]. Ahn [2] has studied various popu-
larity characteristics for recommendation and demonstrated
that recommendation of popular items can be superior to
other methods (for example, to CF) under data sparsity and
cold-starting situations. Clema and Cano [3] have further
shown that collaborative ﬁltering methods are more prone
to popularity bias. Rashid et al. [6] has combined CF with
popularity by biasing CF recommendations towards popu-
lar items. Our work complements these works by further
demonstrating how user uniqueness can be estimated and
utilized to automatically trade between personalization and
popularity based recommendations.

7773. FRAMEWORK

(cid:3)

(cid:2)

3.1 Preliminaries
Let I denote a collection of items and F a set ofn fea-
tures. Let Vf be the vocabulary of feature f ∈ F , i.e., an
ordered set of all possible feature values. For example, a
movie may have the features title, director, actors, genre,
and Vgenre = {comedy, drama, romance}. We further denote
by f.v a speciﬁc feature value in Vf (e.g., genre.comedy).

Let (cid:2)if =

if.v1 , if.v2 , . . . , if.v|Vf |

be a binary vector over
the feature vocabulary; i.e., if.v = 1 if item i has the feature
value f.v ∈ Vf , else, if.v = 0. Each item i is represented by
the list of its feature vectors, i = ((cid:2)if1 , . . . ,(cid:2)ifn ).
Let U be a set of users of the system (also termed user
community hereinafter). A user u ∈ U is modeled by a
user proﬁle deﬁned by the pair pu = (cid:3)W u, βu(cid:4). W u =
( (cid:2)wu
f models
u’s intra-feature preferences over the feature’s vocabulary
Vf ,

fn ) is a list of weight vectors, where (cid:2)wu

f1 , . . . , (cid:2)wu

(cid:4)

(cid:5)

(cid:2)wu

f =

wu

f.v1

, wu

f.v2

, . . . , wu

f.v|Vf |

(1)

wu
f.v is a real number that corresponds to the relative im-
portance of feature value f.v for user u. For simplicity, in
this work, we assume that all features are equally important
to all users of the system.
Finally, the scalar βu ∈ [0, 1] models the uniqueness level
of user u within her community U . The lower the value of
βu, the higher the uniqueness of u.

3.2 Learning the user preferences

In this work, we derive a given user u’s preferences from
u’s ratings to various items in I. Let ru
i be the rating given
by user u ∈ U to item i ∈ I. Furthermore, for a given item
i ∈ I, let Ui be the set of users who rated item i.
Given a feature f ∈ F , let If.v denote the subset of items
such that if.v = 1. We now derive Pf , the probability dis-
tribution of the community’s preferences over the feature
vocabulary Vf , by:

(cid:6)

(cid:2)(cid:6)

(cid:3)

Pf.v (cid:2)

(cid:6)

(cid:6)
i∈If.v log

f.v(cid:2)∈Vf

i∈If.v(cid:2) log

(cid:2)(cid:6)
u∈Ui

ru
i
u∈Ui

(cid:3)

ru
i

(2)

Similarly, for a given user u, we deﬁne I u

f.v to be the set
of items in If.v rated by u. The probability distribution of
u’s preferences over the feature vocabulary Vf , P u
f , is then
estimated by:

(cid:6)

(cid:2) (1 − λ)

P u

f.v

(cid:6)

(cid:6)
i∈Iu

f.v

ru
i
i∈Iu

f.v(cid:2)∈Vf

f.v(cid:2) ru

i

+ λPf.v

(3)

where λ = 0.01 is a (ﬁxed) smoothing coeﬃcient parameter1.
Finally, user preferences over feature values in Vf are de-

rived according to their marginal contribution to the Kullback-
Leibler (KL) divergence [5] between the two distributions:

DKL(P u
f

(cid:2)Pf ) =

P u

f.v log

P u
f.v
Pf.v

(4)

(cid:7)
f.v∈Vf

1

We smooth P u
to the limited amount of training data for many of the users.

f.v with the community’s preferences distribution due

Hence, for a given user u ∈ U , the marginal contribution
of each feature value f.v to the divergence of the user pref-
erences from that of the average user of U derives its ﬁnal
weight:

wu

f.v

(cid:2) P u

f.v log

P u
f.v
Pf.v

(5)

f.v

Consequentially, if wu
f.v

< 0, it follows that P u
f.v

> 0, it immediately implies that
P u
> Pf.v which can be interpreted as “user u prefers fea-
f.v
ture value f.v more than an average user ”. Similarly, if
wu
< Pf.v, hence this feature
value is less preferred by the user in relation to an average
user. The special case of wu
f.v = 0 implies that both the user
and the average user exhibit similar preferential pattern for
this feature value.
3.3 Learning the user uniqueness level

Using the last observation from Section 3.2 we now shortly
describe how the user’s uniqueness level (or non-conformity)
can be estimated using the same framework.

For a given feature f ∈ F , let F u

f (w) denote the probabil-
ity density function of the user preferences over the feature
value weights. The user level of uniqueness, therefore, may
be estimated by measuring the conﬁdence interval around
zero of the user preferences density function. Formally, let
βu
f denote the probability that user u is a conformist up to
some conﬁdence level  ≥ 0, given by βu
f (w)dw
F u
which can be estimated as follows:

f =

(cid:8) +−
(cid:10)
| ≤ 

(cid:9)

#

βu
f =

f.v | f.v ∈ Vf ∧ |wu

f.v

|Vf|

(6)

f denotes the standard deviation of F u

The conﬁdence level parameter, , is determined by  (cid:2)
θ · σu
f , where θ is a free parameter for the entire user com-
munity and σu
f (w). A
large θ value would imply that we relax the criterion for con-
formity within the user community, while a smaller θ value
would imply that we require higher conﬁdence interval for
a user’s conformity pattern. It is further important to note
that, while θ is set based on the entire user community, the
uniqueness threshold for each user is actually personalized
and based on the properties of her private F u
f (w) preference
distribution. Hence, the uniqueness level is personally set
for each user.

βu

f∈F

Finally, the global uniqueness level of the user is set by
βu = 1
f , which models the average uniqueness level
n
of user u over the feature set F . Next we describe how a
recommendation strategy can trade between recommending
popular items to items that “satisfy” the user proﬁle, based
on the user uniqueness level.
3.4

Item recommendation

(cid:6)

Built on top of our new user modeling framework, we
now describe three diﬀerent item recommendation strate-
gies. The ﬁrst one personalizes recommended items based
on the user proﬁle. The second recommends popular items
to the user. The third one utilizes the uniqueness level of
the user to trade between the two basic strategies.

Personalized Item Scoring. Our ﬁrst item recommenda-
tion strategy, termed PERS, aims at providing personalized
recommendations to the user based on her preferences by
scoring the items as follows:

778scorePERS(i, u) =

(cid:7)
f∈F

1
n

(cid:2)if · (cid:2)wu
(cid:2)(cid:2)if(cid:2)

f

(7)

According to Equation 7, positive feature values in the
user’s intra-preferences vector contribute to the item score,
while those with a negative value reduce its score. We fur-
ther linearly normalize scorePERS(i, u) over all items in I into
the range of [0, 1], where the lowest scored item is assigned
with 0 score and the highest with 1. The top scored items
are recommended.

Popularity Item Scoring. Our second item recommenda-
tion strategy, termed POP, recommends items that would
probably satisfy the preferences of an average user of U . A
straight forward strategy is to recommend the most popular
items to the user. In this work we recommend popular items
to user u, derived from the ratings of her K nearest neigh-
bors (KNN). The K nearest neighbors are determined based
on their rating similarity to u’s ratings (e.g., using Pearson
Correlation [1]).
k denote the top-k most similar users in U to user
u; the popularity score for each item i ∈ I is calculated by
the relative rating volume given by the community U u
k to
that item, measured as follows:

Let U u

(cid:6)

(cid:6)
u(cid:2)∈Ui∩U u

k

(cid:2)

ru
i

scorePOP(i, u) =

(cid:6)

i(cid:2)∈I

u(cid:2)∈Ui(cid:2)∩U u

k

ru(cid:2)
i(cid:2)

(8)

Combined Item Scoring. Our ﬁnal item recommendation
strategy, termed POPERS, is a hybrid strategy which utilizes
the user uniqueness level to trade between the two basic
strategies. The hybrid item score of POPERS strategy for
user u is then given by:
scorePOPERS(i, u) = βuscorePOP(i, u) + (1− βu)scorePERS(i, u) (9)
The lower the uniqueness level of the user (the higher the
user conformity), the higher the eﬀect of POP on the ﬁnal
score of the item and vice versa. Therefore, unique users
will be recommended by items that satisfy their preferences,
while conformist users will be recommended mostly by pop-
ular items.

4. EXPERIMENTS
4.1 Data Sets

We evaluated our item recommendation strategies using
two real-world datasets, namely: MovieLens and BookCross-
ing. We now shortly describe each dataset.

MovieLens is a movie rating dataset2 from GroupLens
Research. This dataset contains about 1 million anonymous
ratings of approximately 3,900 movies made by 6,040 users
during 2000. Ratings in this dataset were made on a 5-
star scale, with 5 stars being the highest rating. The set of
movie features in this dataset is limited to genre, title and
release year. Each movie’s metadata was enhanced with ad-
ditional features (list of actors, director, writer, screenplay,
and world certiﬁcate) obtained from the movie’s record in
IMDB3.

BookCrossing is a book rating dataset4, collected by

2

3

4

http://www.movielens.org

http://www.imdb.com/

http://www.informatik.uni-freiburg.de/~cziegler/BX/

[8] in a 4-week crawl during 2004 from the

Ziegler et al.
BookCrossing community5. It contains about 1 million anony-
mous ratings of 271,379 books made by 278,858 users. Rat-
ings in this dataset were made on a 10-star scale, with 10
stars being the highest rating. Ziegler et al. provide for
each book several features such as the book’s title, author
and publisher. Google Books6 API was further used to en-
rich the books metadata with two more features: categories
and keywords.
4.2 Setup & Evaluation

The three diﬀerent item recommendation strategies were
evaluated on each dataset. As a baseline, the TASTE open
source package7 was used to compare the three recommenders
with several state-of-the-art collaborative ﬁltering (CF) meth-
ods and with an SVD-based matrix-factorization (MF) method.
We shall use the label “CF” and “MF” to refer to TASTE’s best
performing CF method and the MF-based method, respec-
tively. The various recommenders (ours and TASTE’s) were
implemented in Java SDK 6, and were run on a dual core
Windows XP machine with 4GB RAM.

The popularity based recommenders (i.e., POP and POP-
ERS) and the CF and MF methods were experimented with
various sizes of U u
k , the subset of the u’s nearest neighbors,
varying k over the values {20, 50, 100, 200, 500,|U|}.

Additionally, the POPERS recommender depends on the
global parameter θ for deriving personalized βu values for
each user. The θ parameter was tuned for each dataset us-
ing a random sample of 100 users from U with at least 50
ratings; the values obtained were θ = 1.0 and θ = 0.5 for
the movies and the books datasets respectively.

The various recommenders were evaluated on each dataset
using 30 user subsets, each contains 100 users with at least
50 ratings that were randomly drawn from U . For each rec-
ommended user, we took out the user’s top-20 rated items
(while further using the item ids for breaking possible rat-
ing ties) and used them for testing, while the remaining
user’s rated items were used for training. The various rec-
ommenders were evaluated based on their ability to identify
the testing items (rank them higher) out of all items in I
that were not used for training.

Recommendation quality was measured using nDCG@20
which measures the ranks of the testing items at the top-20
recommended results, while considering their rating score as
a relevance level. Items recommended by CF and MF methods
were ranked according to their predicted user ratings.
In
the followings, we report on the average performance of all
recommenders over the 30 user datasets.
4.3 Results

The results of our evaluation are depicted in Figure 1. The
ﬁrst observation we make is that the relative performance
of the various recommenders among the two datasets sig-
niﬁcantly diﬀers. We mainly attribute this diﬀerence to the
fact that the BookCrossing dataset is much sparser than the
MovieLens dataset.

As we can observe, for both datasets, popularity based
recommendation (i.e., POP) dominates personalized recom-
mendation based on the user’s proﬁle (i.e., PERS), CF (sup-
porting [2]) and MF. Next, we observe that overall, for both
5

http://www.bookcrossing.com/

6

7

http://books.google.com/

http://mahout.apache.org/taste.html

779(a) MovieLens

(b) BookCrossing

Figure 1: Evaluation results of the various recommenders for the two datasets

This also demonstrates the eﬀectiveness of our model for
learning user uniqueness for recommendation.

5. SUMMARY

In this work we have shown how the user’s unique pref-
erences over item features can be estimated and used for
recommendation. Using an experimental evaluation with
two real datasets we have studied various recommendation
strategies and demonstrated the eﬀectiveness of a recom-
mendation strategy that trades between personalization and
popularity according to the user’s uniqueness level.

For future work, we wish to explore more usages of user
uniqueness with other recommendation strategies (e.g., CF
methods) and further extend our basic model to consider
the possibility of diversity in item feature signiﬁcance among
users.

6. REFERENCES
[1] Gediminas Adomavicius and Alexander Tuzhilin. Toward the

next generation of recommender systems: A survey of the
state-of-the-art and possible extensions. IEEE Trans. on
Knowl. and Data Eng., 17:734–749, June 2005.

[2] Hyung Ahn. Utilizing popularity characteristics for product

recommendation. Int. J. Electron. Commerce, 11:59–80,
December 2006.

[3] `Oscar Celma and Pedro Cano. From hits to niches?: or how

popular artists can bias music recommendation and discovery. In
KDD Workshops, NETFLIX ’08, pages 5:1–5:8, New York, NY,
USA, 2008. ACM.

[4] Nadav Golbandi, Yehuda Koren, and Ronny Lempel. On

bootstrapping recommender systems. In Proceedings of
CIKM’10.

[5] Solomon Kullback and Richard A. Leibler. On information and

suﬃciency. The Annals of Mathematical Statistics, 22(1):79–86,
1951.

[6] Al Mamunur Rashid, Istvan Albert, Dan Cosley, Shyong K.
Lam, Sean M. McNee, Joseph A. Konstan, and John Riedl.
Getting to know you: learning new user preferences in
recommender systems. In IUI’02.

[7] Michal Shmueli-Scheuer, Haggai Roitman, David Carmel, Yosi

Mass, and David Konopnicki. Extracting user proﬁles from large
scale data. In MDAC ’10.

[8] Cai-Nicolas Ziegler, Sean M. McNee, Joseph A. Konstan, and
Georg Lausen. Improving recommendation lists through topic
diversiﬁcation. In WWW 2005, pages 22–32, 2005.

Figure 2: Comparing the user uniqueness distribu-
tions in the MovieLens and BookCrossing datasets

−10) and 13% (p-value<10

datasets and all conﬁgurations, the POPERS recommender
signiﬁcantly outperformed the other alternatives up to 4%
−10) improvement over
(p-value<10
the second best recommender, using the best conﬁguration
for the movies (k = 50) and the books datasets (k = 100),
respectively. Recall that POPERS utilizes the user uniqueness
level for “optimal” trading between POP and PERS strategies.
An important observation that we make is that PERS rel-
ative performance, compared to the other strategies, is sig-
niﬁcantly higher for BookCrossing. This diﬀerence can be
explained by analyzing the distribution of user uniqueness
level within the two datasets.

Figure 2 provides a box-plot comparison between the user
uniqueness distributions in the two datasets. As we can ob-
serve, user uniqueness level varies among users with a me-
dian value of 0.213 (IQR = 0.0489) and 0.444 (IQR = 0.08)
for the movies and books datasets, respectively. Therefore,
the relative better performance of PERS for the books dataset
can be directly explained by the higher user uniqueness level
among recommended users in that dataset. This diﬀerenti-
ation is exploited by the POPERS strategy which (relatively)
utilizes more personalization for users in the books dataset.

780