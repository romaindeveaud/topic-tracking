Pseudo Test Collections for

Training and Tuning Microblog Rankers

Richard Berendsen

r.w.berendsen@uva.nl

Manos Tsagkias
e.tsagkias@uva.nl
Maarten de Rijke
derijke@uva.nl
ISLA, University of Amsterdam, Amsterdam, The Netherlands

Wouter Weerkamp
w.weerkamp@uva.nl

ABSTRACT
Recent years have witnessed a persistent interest in generating pseu-
do test collections, both for training and evaluation purposes. We
describe a method for generating queries and relevance judgments
for microblog search in an unsupervised way. Our starting point is
this intuition: tweets with a hashtag are relevant to the topic cov-
ered by the hashtag and hence to a suitable query derived from the
hashtag. Our baseline method selects all commonly used hashtags,
and all associated tweets as relevance judgments; we then gener-
ate a query from these tweets. Next, we generate a timestamp for
each query, allowing us to use temporal information in the training
process. We then enrich the generation process with knowledge
derived from an editorial test collection for microblog search.

We use our pseudo test collections in two ways. First, we tune
parameters of a variety of well known retrieval methods on them.
Correlations with parameter sweeps on an editorial test collection
are high on average, with a large variance over retrieval algorithms.
Second, we use the pseudo test collections as training sets in a
learning to rank scenario. Performance close to training on an edi-
torial test collection is achieved in many cases. Our results demon-
strate the utility of tuning and training microblog search algorithms
on automatically generated training material.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval Models
Keywords
Simulation, pseudo test collections, learning to rank, microblog re-
trieval

1.

INTRODUCTION

Modern information retrieval (IR) systems have evolved from
single model based systems to intelligent systems that learn to com-
bine uncertain evidence from multiple individual models [10, 22].
The effectiveness and ﬂexibility of such systems has led to wide
adoptation in IR research. A key contributor to the success of such
systems is the learning phase, i.e., the training set they are given for

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

learning. Training sets have to be tailored to the task at hand and,
in contrast to the systems themselves, do not generalize to other
tasks. This characteristic requires compiling task-speciﬁc training
sets, which is a time consuming and resource intensive process, as
it usually involves human labor. Automating the process of com-
piling training sets has obvious advantages in reducing costs, while
it simultaneously increases the size of the training set. This obser-
vation has led to a persistent interest in ﬁnding ways for generating
so-called pseudo test collections, which consist of a set of queries,
and for each query a set of relevant documents (given some doc-
ument set). In this paper, we consider the problem of generating
pseudo test collections for microblog search.

Microblog search is the task of ﬁnding information in microblogs,
such as Facebook status updates, Twitter posts, etc. The task be-
came popular with the advent of social media and is distinct from
web search and from blog search due mainly to its real-time nature,
the very limited length of microblog posts and the use of “micro-
blog language,” e.g., hashtags, mentions, which can provide useful
information for retrieval purposes. In 2011 the Text REtrieval Con-
ference (TREC) launched the Microblog track aimed at developing
a test collection from Twitter data and evaluating systems’ perfor-
mance on retrieving—given a query and time-stamp—relevant and
interesting tweets in a simulated real-time scenario. Several par-
ticipants approached the task using learning to rank methods for
combining evidence from multiple rankers [21]. This approach to
microblog search comes natural because of the many dimensions
available for ranking microblog posts, e.g., recency, user authority,
content, existence of hyperlinks, hashtags, retweets. For training a
learning to rank-based system at the TREC 2011 Microblog track,
participants used a traditional supervised method: many manually
labeled data for compiling a training set. What if we could generate
the required training sets automatically? In 2012 a second edition
of the Microblog track was organized. This gives us the opportunity
to compare what yields better learning to rank performance: train-
ing on the 2011 relevance assessments, or training on automatically
generated ground truth?

Our starting point is the following intuition, based upon the ob-
servation that hashtags tend to represent a topic in the Twitter do-
main: From tweets Th associated with a hashtag h, select a subset
of tweets Rh ⊆ Th that are relevant to an unknown query qh re-
lated to h. We build on this intuition for creating a training set for
microblog rankers. To this end, we take several steps, each raising
research questions. First, we select hashtags h and associated rel-
evant tweets Rh. Can we just select all hashtags and use all their
associated tweets? In microblog search, time is important: what
is considered relevant to a query may change rapidly over time. A

53microblog query, then, has a timestamp, and relevant tweets must
occur prior to this timestamp. As for a query, the topic a hashtag is
associated with may change over time. Can we exploit this analogy,
and label hashtags with a timestamp, regarding tweets prior to this
timestamp as relevant? Another well-known aspect of microblog
posts is that they often contain casual conversation that is unlikely
to be relevant to a query. Can we improve generated training sets
by selecting interesting tweets and hashtags associated with such
tweets? Once we have selected a hashtag h and a set of tweets Rh,
how do we generate a query qh related to h?

The main contribution of this paper is a set of methods for creat-
ing pseudo test collections for microblog search. These collections
are shown to be useful as training material for tuning well-known
retrieval methods from the literature, and for optimizing a learn-
ing to rank method. In particular, we contribute: (1) unsupervised
pseudo test collection generation methods; (2) a supervised pseudo
test collection generation method, where we learn what are inter-
esting tweets from TREC Microblog track assessments; (3) insights
into the sensitivity of our methods to parameter settings.

2. PROBLEM DEFINITION

Below, we consider a number of instantiations of our pseudo
test collection generator. For the purposes of the TREC Micro-
blog track, a test collection for microblog search consists of queries
with timestamps and a set of relevant documents for these queries.
A pseudo test collection for microblog search consists of a set of
queries Q, in which each query q ∈ Q is associated with a times-
tamp qt and a set of relevant documents Rq. Given this deﬁnition,
there are three main steps for generating a pseudo test collection
for microblog search: generating (a) the query; (b) the query times-
tamp; and (c) a set of relevant tweets for the query.

We start from the following intuition: From the tweets Th that
contain a hashtag h, we can select tweets Rh that are relevant to
an unknown query qh related to h. In the next section, we present
three methods to generate a pseudo test collection. Each method
selects hashtags and for every hashtag h it selects tweets Rh from
Th that will act as relevant tweets to a suitable query related to h.
In §4, we present a technique for generating queries from Rh.

3. SELECTING HASHTAGS AND TWEETS
We propose four solutions to selecting hashtags and tweets for
inclusion in a pseudo test collection. (A) Random: A sanity check
baseline against our hypothesis that hashtags are good sources for
generating pseudo test collections. Collections are created by ran-
domly sampling a set of relevant tweets for each topic, without
replacement. All these random collections are of a ﬁxed size, equal
to our largest hashtag-based pseudo test collection. (B) Hashtags:
A naive method that serves as baseline in our experiments and that
considers all hashtags and tweets to be equally important (§3.1).
(C) Hashtags-T: A method that creates a test collection in the
microblog retrieval sense, in which queries have timestamps (§3.2).
(D) Hashtags-TI: A method that aims at capturing interestingness
in tweets. Interesting tweets should contain good candidate terms
for a query. We present a method with which we can estimate from
example queries and relevant tweets the probability of interesting-
ness of a tweet (§3.3).
3.1 Hashtags: All hashtags, tweets are equal
We select all hashtags, with a single requirement: that they are
mentioned in a reasonable amount of tweets, m (Algorithm 1).
There are three reasons for this lower bound: (i) it reﬂects a cer-
tain consensus about the meaning of a hashtag; (ii) we generate

Algorithm 1: Generating collection Hashtags
H ←− {h : |Th| >= m};
for h ∈ H do

Rh ←− Th;
Generate query qh from Rh;

end

// See §4

our queries based on word distributions in these tweets: for this
to work reliably, we need a reasonable amount of tweets, see §4;
(iii) we train a learning to rank retrieval algorithm on our pseudo
test collection; we hypothesize that it would beneﬁt from a rela-
tive large set of positive training examples, see §5. We normalize
hashtags by lowercasing them and removing any non-alphanumeric
characters. In all our experiments, we set m = 50. The pseudo test
collection generated by Algorithm 1 is called Hashtags.
3.2 Hashtags-T: Generating timestamps

Microblog search is sensitive to the query issue time because of
the real time nature of tweets. To generate a timestamp for a query
related to a hashtag h, we make an analogy between search volume
over time for a query and publishing volume over time for tweets
with h. Our assumption is that users often issue queries for trending
topics because they want to monitor developments, similar to cer-
tain types of blog search [28]. We generate a timestamp for hashtag
h just after peaks in publishing volume. In this way, our generated
queries will be about trending topics. In addition, we keep a large
amount of tweets from Th, while discarding a limited number, after
h stops trending. In collections that span a considerable period of
time, re-occurring topics, such as Christmas or Super Bowl, may
quite likely be observed. In this case, one may want to assign mul-
tiple issue times for a query, depending on the number of observed
peaks. Our corpus (see §5) covers a relatively short period, and we
assign only one issue time to every query sampled.

In detail, our query issue time generation works as follows. First,
we group the time span of the collection in 8-hours bins. Then, for
each hashtag, we count how many relevant documents belong to
each bin; this results in generating the hashtag’s timeseries. In our
setting, timeseries are short and sparse; our peak detection method
aims at coping with this challenge. We ﬁnd the bin with the most
counts and resolve ties by taking the earliest date. This approach
allows us to return a peak even for very sparse timeseries. We call
the pseudo test collection generated by Algorithm 2: Hashtags-T.
3.3 Hashtags-TI: Selecting interesting tweets
Consider the following tweet: “Hey follow me here #teamfol-
lowback #justinbieber.” We hypothesize that this tweet would not
be useful for sampling terms for topics labeled #teamfollowback or
#justinbieber or as a relevant document for these topics. To avoid
selecting such tweets, we rank tweets by their probability of in-
terestingness and keep the best X percent. We think of a tweet

Algorithm 2: Generating collection Hashtags-T
H(cid:48) ←− {h : |Th| ≥ m};
for h ∈ H(cid:48) do

Generate timestamp t(h);
Rh ←− {τ : τ ∈ Th and t(τ ) ≤ t(h)};

end
H ←− {h : h ∈ H(cid:48)and |Rh| ≥ m};
for h ∈ H do

Generate query qh from Rh;

end

// See §3.2

// See §4

54as interesting if it carries some information and could be relevant
to a query. We use a set of criteria to capture interestingness and
present a method to learn from example queries and relevant docu-
ments from an editorial collection.

Let C1, C2, . . . , Cn be random variables associated with the cri-
teria and let Iτ := I(τ ) = 1 be the event that a tweet is interest-
ing. We estimate P (Iτ|C1 = c1, . . . , Cn = cn), or, shorthand:
P (Iτ|c1, . . . , cn). Following Bayes’ rule, we have

P (Iτ|c1, . . . , cn) =

P (c1, . . . , cn|Iτ )P (Iτ )

P (c1, . . . , cn)

,

(1)

where P (Iτ ) is the a-priori probability that a tweet is interest-
ing, P (c1, . . . , cn|Iτ ) is the likelihood of observing the evidence
given that a tweet is interesting, and P (c1, . . . , cn) is the proba-
bility of observing the evidence. The crucial step is to estimate
P (c1, . . . , cn|Iτ ). We hypothesize that tweets that are known to be
relevant to a query are interesting and estimate P (c1, . . . , cn|Iτ )
with P (c1, . . . , cn|Rτ ), where Rτ is the event that a tweet is rele-
vant to a query in an editorial collection. We use the TREC Micro-
blog 2011 qrels for this estimation. Since we do not have enough
relevant tweets to estimate the full joint probability, we assume con-
ditional independence of ci given that a tweet is relevant:

P (Iτ|c1, . . . , cn) ≈

.

(2)

Since we rank tweets by interestingness we do not have to estimate
P (Iτ ). Instead, we have:
rankτ (P (Iτ|c1, . . . , cn)) = rankτ

i log(P (ci|Rτ ))
P (c1, . . . , cn)

(cid:19)

. (3)

Most of the criteria we use have discrete distributions. For those
that do not, we bin their values in B bins; we set B = 10. To
avoid rejecting a tweet on the basis of one measurement that did
not occur in any of the relevant tweets, we add one observation to
every bin of every P (ci|Rτ ) distribution. For P (c1, . . . , cn) we
use the empirical distribution of all tweets in the collection. After
selecting the best X percent of tweets, we again ﬁlter out hashtags
that have less than 50 interesting tweets. We build this method
on top of our pseudo test collection Hashtags-T, only ranking the
tweets in this collection and keeping the best 50% of them. We call
the pseudo test collection generated by Algorithm 3 Hashtags-TI.

i P (ci|Rτ )(cid:1) P (Iτ )
(cid:0)(cid:81)
(cid:18)(cid:80)

P (c1, . . . cn)

Algorithm 3: Generating collection Hashtags-TI
H(cid:48)(cid:48) ←− {h : |Th| ≥ m};
for h ∈ H(cid:48)(cid:48) do

Generate timestamp t(h);
Th,t ←− {τ : τ ∈ Th and t(τ ) ≤ t(h)};

// See §3.2

end
H(cid:48) ←− {h : h ∈ H(cid:48)(cid:48) and |Th,t| ≥ m};
T ←− ∪h∈H(cid:48) Th,t;
// Rank tweets by probability of being
Rank T by P (Iτ|c1, . . . , cn(τ ));
Let TI be the top X percent of this ranking;
for h ∈ H(cid:48) do

interesting

// See eq. 3

Rh ←− Th,t ∩ TI;

end
H ←− {h : h ∈ H(cid:48) and |Rh| ≥ m};
for h ∈ H do

Generate query q from Rh;

end

// See §4

K−1(cid:88)

k=1

The criteria we use build on textual features (density and capi-
talization) and microblog features (links, mentions, recency). Each
criterion is discussed below. The marginal distributions P (ci|Rτ )
of three criteria are shown in Fig. 1 as white histograms. They over-
lap with black histograms of all tweets in our Hashtags pseudo test
collection. These criteria have different distributions over relevant
tweets and over tweets that have a hashtag, which motivates our
idea to keep tweets with high probability of interestingness.
Links. The existence of a hyperlink is a good indicator of the
content value of a tweet. TREC Microblog 2011 deﬁnes interest-
ingness of a tweet as whether a tweet contains a link [21]. Also,
a large fraction of tweets are pointers to online news [19]. Tweets
with links are likely to include terms that describe the linked web
page, rendering them good surrogates for query terms [5].
Mentions. Tweets with mentions (@username) signify discus-
sions about the hashtag’s topic. This type of tweet is likely to
be noisy because of their personal character. They may, however,
bring in query terms used by a niche of people.
Tweet length. Document length has been shown to matter in re-
trieval scenarios [35]. Short tweets are less likely to contain terms
useful for query simulation, see Fig. 1 (Center) for the distribution
of tweet length.
Density. A direct measure for probing a tweet’s content quality
is the density score [20]. Density is deﬁned as the sum of tf-idf
values of non-stopwords, divided by the number of stopwords they
are apart, squared:

Density(τ ) =

K

K − 1

weight(wk) + weight(wk+1)

distance(wk, wk+1)2

,

where K is the total number of non-stopwords terms in tweet τ, wk
and wk+1 are two adjacent keywords in τ. weight(·) denotes the
term’s tf-idf score, and distance(wk, wk+1) denotes the distance
between wk and wk+1 in number of stopwords. Fig. 1 (Left) shows
the distribution of density scores of tweets.
Capitalization. The textual quality of tweets can partially be cap-
tured through the use of capitalization [41]. Words in all capitals
are considered shouting and an indication of low quality. The ratio
of capitals may indicate the quality of the text. Fig. 1 (Right) shows
the distribution of the fraction of capital letters over tweet length.
Direct. A tweet is direct if it is meant to be a “private” message to
another user (i.e., the tweets starts with @user).

4. GENERATING QUERIES

Sampling query terms is a challenging step in the process of au-
tomatically generating pseudo test collections. Azzopardi et al. [3]
propose several methods for sampling query terms from web docu-
ments for known-item search, while Asadi et al. [2] avoid the prob-
lem by using anchor texts. Neither approach is applicable in the
microblog setting due to a lack of both redundancy in the tweets
and anchor texts. Terms in tweets usually occur at most once, but
if not, this is often a signal for spam [26]. Probabilistic sampling
methods that boost terms occurring with high probability are less
likely to return good candidates for query terms. Tf-idf methods
emphasize rare terms, which are likely to be spelling mistakes or
descriptive of online chatter (e.g., “looooool”) in our setting.

The log-likelihood ratio (LLR) is suitable for our problem of
sampling terms from (tweets associated with) a given hashtag [25].
LLR is deﬁned as the symmetric Kullback-Leibler divergence of
the expected and observed term probabilities in two corpora. Terms
are ranked by how discriminative they are for both corpora. For

55Figure 1: Distribution of (Left) density scores, (Center) tweet length, and (Right) capitalization. Where the histograms for the tweets associated with
hashtags (dark grey) and for the TREC MB 2011 relevant tweets (white) overlap, the color is light grey.

LLR(w) =

(cid:20)

(cid:21)

,

our purposes, we set one corpus to be a set of tweets associated
with hashtag h and the other to consist of the rest of the tweets in
the collection. Stopwords, spam terms, or terms indicative of on-
line chatter will rank lower because they occur in both corpora with
roughly the same frequency.

ods described in §3, and w a term in τ. For every w ∈ (cid:83)

Let T be a collection of tweets τ, Rh a set of relevant tweets as-
sociated with a hashtag h resulting from one of the sampling meth-
τ
we compute LLR(w), given corpora Rh and B = T \ Rh:

τ∈Rh

2 ·

ORh (w) log

ORh (w)
ERh (w)

+ OB(w) log

OB(w)
EB(w)

where ORh (w) = tf (w, Rh) and OB(w) = tf (w, B) are the ob-
served term frequencies of w in Rh and B, respectively. ERh (w)
and EB(w) are the expected values of the term frequency of w in
Rh and B, respectively.

For every hashtag h, terms are ranked in descending order of
their log-likelihood ratio score. We remove terms if one of the
following pertains: (1) The term is equal to the hashtag up to the
‘#’ character.
In this case, all tweets in Th contain the hashtag,
making the query very easy for all rankers, which then leads to a
learning to rank method having a hard time distinguishing between
rankers.
(2) The term occurs in fewer than 10 documents. We
generate queries that consist of the top-K ranked terms. For all
pseudo test collections described in §3 we set K = 10. For our
most promising method, we examine the impact of this parameter
by generating queries of length 1, 2, 3, 5, and 20.

5. EXPERIMENTAL SETUP

The main research question we aim to answer is: What is the
utility of our test collection generation methods for tuning retrieval
approaches and training learning to rank methods?

Parameter tuning. We do parameter sweeps for some retrieval
runs on different (pseudo) test collections, see Table 1 for details.
We ask: (a) What is better in terms of retrieval performance: tuning
on a different editorial test collection or tuning on a pseudo test col-
lection? We answer by calculating how far performance obtained
by tuning on either collection is from optimal performance for each
retrieval model. (b) Do scores between a pseudo test collection and
an editorial test collection correlate better than scores between edi-

torial test collections? We answer by calculating Kendall’s tau and
expected loss in effectiveness.

Learning to rank. We compare the utility of test collections as
training material for a learning to rank algorithm. We ask: (a) What
is better in terms of retrieval performance: training on a different
editorial test collection, or training on a pseudo test collection? We
answer by calculating the difference in retrieval performance be-
tween training on each. (b) Which of our pseudo test collections is
most useful? (c) Do learning to rank algorithms trained on pseudo
test collections outperform the best individual feature?

Parameter sensitivity. We also analyze parameter sensitivity
of our methods, focusing on two parameters. First, in generating
Hashtag-TI (§3), we keep the best X = 50% percent of tweets.
How sensitive are our results to this method to different values for
X? We try these values: 20, 40, 60, and 80. In all our experiments,

Algorithm 4: Training an LTR system on a pseudo test collec-
tion, and testing it on an editorial collection
for i = 1 → N do

// -- Training phase:
Generate the pseudo test collection;
for each ranker do

--

Sweep parameters on the pseudo test collection;
Randomly sample parameter vector to use
from winners;

end
for q ∈ Q do

Merge the ranked lists into Mq;
Let positive training examples ← Rq,h ∩ Mq;
Randomly sample |Rq,h| negative training examples
from Mq \ Rq,h;

end
Learn a LTR model on the training set;
// -- Testing phase:
for each ranker do

--

Run on test topics using the sampled parameter
vector;

end
Run the learned LTR model on the test set;

end

density0100200300∪Th(447957)∪Rq(2388)length020406080100120∪Th(447957)∪Rq(2388)capital0.00.20.40.60.81.0∪Th(447957)∪Rq(2388)56Table 1: Retrieval algorithms tuned.

Ranker
LM
Tf-idf

Description

Language modeling with Dirichlet smoothing
Indri’s implementation of tf-idf

Okapi

Okapi, also known as BM25 [35]

1

.

5

i
r
d
n
I

BOW
BUW
5 Tf-idf

Boolean ordered window
Boolean unordered window
Terrier’s implementation of tf-idf
A Divergence from Randomness (DFR) model [1]

PL2
DFR-FD Terrier’s DFRee model with a document score modiﬁer

that takes into account co-occurence within a window
Terrier’s implementation of query expansion

.

3

r
e
i
r
r
e
T

QE

Parameter

µ
k1
b
k1
b
k3
Window size
Window size
b
c
Window size

No. documents
No. terms

Values

{50, 150, . . . , 10050}
{0.2, 0.4, . . . , 3}
{0, 0.05 . . . , 1}
{0, 0.2, . . . , 3}
{0, 0.05 . . . , 1}
{0}
{1, 2 . . . , 15, inf}
{1, 2 . . . , 15, inf}
{0, 0.05 . . . , 1}
{0.5, 1, 5, 10}
{2, 3 . . . , 15}
{1, 5, 10, 20, 30, 50}
{1, 5, 10, 20, 30, 50}

cf. Literature
[42]
[33]

[33]

[33]
[9]
[31]

[23]

when we generate queries, we keep the top 10 terms of the ranking
produced by LLR (§4). For our best pseudo test collection, we ask
how parameter tuning results and learning to rank performance is
inﬂuenced by different query length. We try query lengths 1, 2, 3,
5, and 20.

5.1 Dataset and preprocessing

We use the publicly available dataset from the TREC 2011 and
2012 Microblog tracks. It covers two weeks of Twitter data, from
January 24, 2011–February 8, 2011, consisting of approximately
16 million tweets. We perform a series of preprocessing steps on
the content of tweets. We discard non-English tweets using a lan-
guage identiﬁcation method for microblogs [6]. Exact duplicates
are removed; among a set of duplicates the oldest tweet is kept.
Retweets are discarded; in ambiguous cases, e.g., where comments
were added to a retweet, we keep the tweet. Punctuation and stop
words are removed using a collection-based stop word list, but we
keep hashtags without the ‘#’ character. After preprocessing we are
left with 4,459,840 tweets, roughly 27% of all tweets. Due to our
aggressive preprocessing, we miss 19% of the relevant tweets per
topic, on average. Our 10 retrieval models avoid using future evi-
dence by using per topic indexes. For completeness, we note that
our stopword list and the idf-weights in the density feature were
computed on the entire collection. Pseudo test collections and both
TREC microblog test collections also contain tweets from the en-
tire collection. For generating queries, we index the collection with
Lucene without stemming or stopword removal.

Table 2 lists statistics of the pseudo test collections generated
with the methods described in §3, as well as statistics of the collec-
Table 2: Statistics for pseudo test collection generated from our methods
and the TREC Microblog 2011 track.

Collection

Topics

# Max Min Avg.

length

Relevant documents

# Max Min Avg.

TREC MB 2011
TREC MB 2012
Random-1
Hashtags
Hashtags-T
Hashtags-TI
H-TI-X20
H-TI-X40
H-TI-X60
H-TI-X80

49
59
1,888
1,888
891
481
175
392
576
740

6
7
10
10
10
10
10
10
10
10

1
1
10
10
10
10
10
10
10
10

3.4
2.9
10
10
10
10
10
10
10
10

2,965
6,286
462560
462,013 16,105
212,377 9,164
98,586 4,949
32,221
3661
4804
75,287
5277
121,639
166,489
6956

1
60.5
178
1 106.5
572
245 245 245.0
50 244.7
50 238.4
50 205.0
50 184.1
50 192.1
50 211.2
50 225.0

tions generated by choosing different values for X. The Hashtags-
TI-QL{1,2,3,5,20} pseudo test collections are of the same propor-
tions as Hashtags-TI, apart from the query length. We have listed
only one of ﬁfteen Random collections, but these are all of the same
proportions: about as large as the Hashtags collection.
5.2 Learning to rank

We follow a two-step approach to learning to rank, outlined in
Algorithm 4. First, we run several retrieval algorithms, then we
rerank all retrieved tweets. For retrieval, we use the retrieval algo-
rithms listed in Table 1, optimized for MAP [24, 34] after tuning on
a training collection. In case of ties among parameter vectors for
a ranker, we randomly sample a parameter vector. We also use a
parameter free retrieval algorithm, DFRee [1]. For re-ranking, we
compute three groups of features.
Query-tweet features. These are features that have different val-
ues for each query-tweet pair. We subdivide these as follows. Rank-
ers:
the raw output of each retrieval algorithm. For LM, BOW
and BUW we transform the raw output X by taking the exponent:
exp(X). Ranker meta features:
the number of rankers that re-
trieved the tweet, the maximal, average, and median reciprocal rank
of the tweet over all rankers. Recency: query-tweet time difference
decay, computed as exp(t(τ )− qt), where t(τ ) is the timestamp of
the tweet and qt the timestamp of the query. We linearly normalize
query-tweet features over all retrieved tweets for the query.
Query features. These are features which have the same value for
every retrieved tweet within the same query. We use Query clarity,
a method for probing the semantic distance between the query and
the collection [11]. We linearly normalize query features over the
set of retrieved tweets for all queries.
Tweet features. These are features that have the same value for
each tweet independent of the query. We use the Quality criteria
listed in §3: link, mentions, tweet length, density, capitalization,
and direct. We linearly normalize tweet features over the set of
retrieved tweets for all queries.

To build a training set, one needs positive and negative training
examples. Let q ∈ Q be a query from the training collection, Rq
the set of relevant tweets for query q, and Mq the set of all retrieved
tweets for q. Then, for each query in the training collection we use
Rq ∩ Mq as positive examples. To have a balanced training set, we
randomly sample |Rq| tweets as negative training examples from
Mq \ Rq.

57Next, we feed the training set to four state of the art learners:
(a) Pegasos SVM1 [36, 37], (b) Coordinate ascent [27], (c) Rank-
SVM [17], and (d) RT-Rank [29]. We set Pegasos SVM to optimize
the area under the ROC curve using indexed sampling of training
examples, with regularization parameter λ = 0.1 for a maximum of
100,000 iterations. We set coordinate ascent to optimize for MAP
with  = 0.0001 and maximum step size of 3. We use line search to
optimize each feature with uniform initialization and consider only
positive feature weights without projecting points on the manifold.
For RankSVM we set the cost parameter to 1 per query. In pre-
liminary experiments, RT-Rank performed poorly and therefore we
choose to leave it out from our report.

Recall that training sets are compiled using tuned rankers and
that in case of ties between different parameter vectors for a ranker,
a random vector is selected. When compiling test sets for TREC
MB 2011 and TREC MB 2012 to evaluate the utility of a training
set, we use the exact same parameter vectors, so that the same set
of features are used for training and testing.

Algorithm 4 has randomness in several stages: (i) when gener-
ating the pseudo test collection (only in the case of the Random
collections), (ii) when sampling a winning parameter setting for
each feature, (iii) when randomly sampling negative training exam-
ples, and (iv) during model learning. To obtain a reliable estimate
of of the performance when training on a pseudo test collection,
this procedure is repeated N = 10 times, each time generating a
new pseudo test collection (in the case of the Random test collec-
tion), selecting random parameter vectors, selecting random nega-
tive training examples, and training an LTR model.
5.3 Evaluation

We report on precision at 30 (P30) on binary relevance judg-
ments. We choose P30 because it has been one of the main metrics
in both the TREC 2011 and 2012 Microblog track. We also report
on MAP, as it is a well understood and commonly used evaluation
metric in information retrieval, allowing us to better understand the
behavior of our pseudo test collections. Note that in the 2011 task,
tweets had to be ordered by their publication date instead of by their
relevance. Many top performing systems treated the task as normal
relevance ranking and cut off their ranked lists at rank 30 [21]. In
the 2012 track organizers decided to focus on ranking by relevance
again, which is what we will focus on.

Testing for statistical signiﬁcance. For each training collection,
we run Algorithm 4 N = 10 times, giving rise to N scores for each
topic, for each collection. We report average performance and sam-
ple standard deviation over these iterations. To also gain insight if
any differences between a pair of training collections would be ob-
served on different microblog topics from the same hypothetical
population of topics, we proceed as follows. We pick for each col-
lection the iteration of Algorithm 4 which had the smallest training
error on that collection. Then, we do a paired t-test over differences
per topic as usual and report the obtained p-values. Statistically sig-
niﬁcant differences are marked as (cid:78)(or (cid:72)) for signiﬁcant differences
for α = .001, or (cid:77)(and (cid:79)) for α = .05.
6. RESULTS AND ANALYSIS

First, we report on our parameter tuning results; then on our
learning to rank results. We also analyze parameter sensitivity with
regard to the percentage of interesting tweets kept and query length.
6.1 Parameter tuning results
The main outcomes in this section will be correlations, to an-

1http://code.google.com/p/sofia-ml/

swer the question whether relative performance of parameter val-
ues on pseudo test collections correlates with relative performance
of the same values on an editorial collection. We begin with two
case studies to gain a better understanding of the behavior of our
pseudo text collections. We sweep (a) the document length normal-
ization parameter b for Terrier’s tf-idf implementation (Fig. 2(a)),
and (b) µ for Indri’s implementation of language modeling with
Dirichlet smoothing (Fig. 2(b)). We only include one of our ten
random pseudo test collections; all random collections behave sim-
ilarly, for all retrieval systems and metrics.

Fig. 2(a) shows that on the TREC MB 2011 collection there is
a general trend to prefer lower values of b, possibly because of the
very small average document length, which, in turn, renders the
deviation from the average length close to one. All pseudo test
collections capture this trend, including the Random-1 pseudo test
collection. The curves of the pseudo test collections are smoother
than the curve obtained when tuning on the TREC MB 2011 top-
ics; this is because the pseudo test collections have far more test
topics. Pseudo test collections show differences in absolute scores,
but most importantly, we are interested in whether pseudo test col-
lection predictions that one parameter vector is better than another
correlate to such predictions of editorial collections. Kendall’s tau
expresses exactly that correlation, see Table 3. In addition, we want
to know the following: if we sample a random parameter vector
from those predicted to yield optimal performance on a pseudo test
collection, what will be the expected loss with regard to optimal
performance on an editorial collection? Table 5 provides these
quantities. Correlations are high across the board, and expected
loss is low. All pseudo test collections can be used to reliably tune
the b parameter of Terrier’s TF-IDF, even the Random-1 collection.
Turning to a second case study, Indri’s language modeling algo-
rithm with Dirichlet smoothing, we see a different picture (Fig. 2(b)).
The TREC MB 2011 topics show a slightly decreasing trend for
larger µ values. We believe this is due to the short document length;
tweets after processing are few terms long, and therefore even small
µ values overshadow the document term probability with the back-
ground probability. This trend is not entirely captured by the pseudo
test collections. All have a short increase for low values of µ which
is much less pronounced in the TREC MB 2011 curve. After that,
all except the Random-1 collection show a decline, if only in the
third digit. Correlations are fair (Table 4), but the Random-1 col-
lection fails here. Expected loss is low across the board (Table 6).
Looking at the big picture, we average the correlations and ex-
pected loss ﬁgures in Tables 7 and 8 over all nine retrieval models
from Table 1. For the hashtag based collections, correlations are
high, with a large variance over systems. Expected loss is low.
This indicates that pseudo test collections can be used to reliably
and proﬁtably tune parameters for a variety of well established re-
trieval algorithms, with more or less success depending on which
model is being tuned. Tuning retrieval models on all hashtag based
pseudo test collections is about as reliable as tuning on editorial
test collections. For most retrieval algorithms, a Random collec-
tion cannot be recommended for tuning. Thus, the idea of grouping
tweets by hashtag has value for creating pseudo test collections for
tuning retrieval algorithms.

6.2 Learning to rank results

In this section we evaluate the usefulness of our pseudo test col-
lections (PTCs) by training several learning to rank algorithms on
them. In Tables 9 and 10, we report P30 and MAP performance
on the TREC 2011 Microblog track topics. We compare training
on our PTCs with training on the TREC 2012 Microblog track top-

58Figure 2: Sweeping the b parameter of Tf-idf (Terrier) (2(a)) and the µ parameter of LM (2(b)). The x-axes have parameter values, the y-axes average P30
over the topics of the respective tuning collection.

Table 3: For Tf-idf (Indri), and P30, Kendall’s tau correlations of parameter
sweeps on several pseudo test collections with sweeps on TREC MB 2011
and 2012 collections. Kendall’s tau between sweeps over TREC MB 2011
and 2012 is 0.85.

Tune on
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
0.18
0.87
0.90
0.90

TREC MB 2012
0.20
0.86
0.86
0.86

Table 4: For language modeling (LM), and P30, Kendall’s tau correla-
tions of parameter sweeps on several pseudo test collections with sweeps
on TREC MB 2011 and 2012 collections. Kendall’s tau between sweeps
over TREC MB 2011 and 2012 is 0.61.

Tune on
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
-0.87
0.65
0.69
0.58

TREC MB 2012
-0.65
0.78
0.77
0.79

ics and indicate signiﬁcant differences. In Tables 11 and 12, we
report P30 and MAP performance on the TREC 2012 Microblog
track topics, and compare training on our PTCs with training on
the TREC 2011 Microblog track topics.

A ﬁrst brief glance at all four tables tells us that training on
an editorial test collection is in most cases (but not all) the best
strategy. Still, if training on a PTC is not substantially and signif-
icantly worse, we may conclude that in the abscence of training
data, pseudo test collections are a viable alternative.

A second glance at all four tables shows us that training on the
Random PTC is always signiﬁcantly outperformed by training on
editorial collections. Still, in most cases, there is a hashtag based
PTC on which training yields performance on par with training on
editorial collections. This shows that there is added value in our
idea of using hashtags to group tweets by topic.

Another phenomenon we can observe in all tables is that Pega-
sos has remarkably stable performance over pseudo test collections,
compared to the other two learning to rank algorithms. With the
exception of Hashtags-TI-QL1 and Random, it seems to be able
to exploit the structure in any PTC to learn a function that yields
performance comparable to a function learned on editorial training
data. RankSVM, on the other hand, has unstable performance. Es-
pecially in Table 12 it refuses to work on anything but manually ob-
tained ground truth. Coordinate Ascent, a remarkably simple learn-
ing to rank algorithm holds the middle ground. When queries are

Table 5: For Tf-idf (Indri), expected loss in P30 performance of parameter
sweeps on several training collections compared to optimal performance on
TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011
TREC MB 2012
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
-
0.006±0.005 (3)
0.029
0.012±0.002 (2)
0.002
0.002

TREC MB 2012
0.003
-
0.024
0.002±0.002 (2)
0.000
0.000

Table 6: For language modeling (LM), expected loss in P30 performance
of parameter sweeps on several training collections compared to optimal
performance on TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011
TREC MB 2012
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
-
0.006
0.039±0.001 (11)
0.010
0.013±0.004 (2)
0.014

TREC MB 2012
0.010
-
0.014±0.001 (11)
0.001
0.001±0.001 (2)
0.002

too short, as in Hashtags-TI-QL1, Hashtags-QL2 and Hashtags-TI-
QL3 performance detoriates. When subsamples of tweets become
too small (Hashtags-TI-X20) the same happens.

So which PTC is the best? All PTCs are signiﬁcantly outper-
formed by training on an editorial collection in at least one of the
conditions. Hashtags-TI-X60 and Hashtags-TI-X80 both yield best
results in one case. Also, like Hashtags, Hashtags-T and Hashtags-
TI are signiﬁcantly outperformed in only a small number of cases.
Learning to rank only makes sense if improvements over indi-
vidual retrieval algorithms can be obtained. Tables 13 and 14 show
performance of the retrieval models we use as features. In the great
majority of cases our hashtag based PTCs outperform the best fea-
ture. The Random collection never achieves this.
7. RELATED WORK

We describe two types of related work: (i) searching microblog

posts and (ii) pseudo test collections.
Searching microblog posts. Microblog search is a growing re-
search area. The dominant microblogging platform that most re-
search focuses on is Twitter. Microblogs have characteristics that
introduce new problems and challenges for retrieval [12, 40]. Mas-
soudi et al. [26] report on an early study of retrieval in microblogs,
and introduce a retrieval and query expansion method to account for

0.00.20.40.60.81.00.00.20.40.60.81.0Tf−idf (Terrier) bP30TREC−MB−2011HashtagsHashtags−THashtags−TIRandom−102000400060008000100000.00.20.40.60.81.0LM muP30TREC−MB−2011HashtagsHashtags−THashtags−TIRandom−159Table 7: For all systems, and P30, Kendall’s tau correlations of parameter
sweeps on several pseudo test collections with sweeps on TREC MB 2011
and 2012 collections. Kendall’s tau between sweeps over TREC MB 2011
and 2012 is 0.80.

Tune on
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
0.27±0.62 (2 NA)
0.78±0.20 (1 NA)
0.80±0.20 (1 NA)
0.75±0.26 (1 NA)

TREC MB 2012
0.32±0.56 (1 NA)
0.70±0.35 (1 NA)
0.78±0.30 (1 NA)
0.81±0.23 (1 NA)

Table 8: Over all retrieval models, average expected loss in P30 perfor-
mance of parameter sweeps on several training collections compared to op-
timal performance on TREC MB 2011 and 2012 collections.

Tune on

TREC MB 2011
TREC MB 2012
Random-1
Hashtags
Hashtags-T
Hashtags-TI

TREC MB 2011
-
0.003±0.003
0.021±0.021
0.005±0.006
0.004±0.005
0.004±0.005

TREC MB 2012
0.003±0.006
-
0.013±0.012
0.004±0.006
0.002±0.005
0.002±0.005

Table 9: P30 performance on TREC MB 2011 topics for various LTR al-
gorithms, trained on different collections. Features are tuned on MAP. Best
run in bold. Indicated statistically signiﬁcant differences are with regard to
training on TREC-MB-2012. The best individual tuned ranker, DFR-FD,
achieved 0.416.

CA

Pegasos
RankSVM
Train on
0.446±0.001
0.447±0.004
0.436±0.004
TREC-MB-2012
0.401±0.004(cid:79) 0.356±0.011(cid:72) 0.378±0.006(cid:72)
Random
0.437±0.002
0.434±0.002
0.430±0.004
Hashtags
0.426±0.002
0.438±0.001
0.434±0.003
Hashtags-T
0.406±0.007(cid:79) 0.429±0.002(cid:79)
0.437±0.001
Hashtags-TI
0.265±0.016(cid:72) 0.383±0.016(cid:72)
0.432±0.001
Hashtags-TI-X20
0.361±0.006(cid:72) 0.427±0.001(cid:79)
0.435±0.001
Hashtags-TI-X40
0.415±0.004(cid:79) 0.431±0.003(cid:79)
0.438±0.001
Hashtags-TI-X60
0.438±0.001
0.435±0.002
0.427±0.003
Hashtags-TI-X80
0.189±0.135(cid:72) 0.034±0.015(cid:72) 0.293±0.107 (cid:72)
Hashtags-TI-QL1
0.244±0.024(cid:72) 0.421±0.047 (cid:72)
0.435±0.002
Hashtags-TI-QL2
0.240±0.012(cid:72) 0.427±0.023 (cid:72)
0.443±0.001
Hashtags-TI-QL3
0.259±0.008(cid:72) 0.428±0.005 (cid:79)
0.443±0.001
Hashtags-TI-QL5
0.320±0.007(cid:72) 0.432±0.004
Hashtags-TI-QL20 0.438±0.001

microblog search challenges. Efron and Golovchinsky [13] inves-
tigate the temporal aspects of documents on query expansion using
pseudo relevance feedback. Naveed et al. [30] develop a retrieval
model that takes into account document length and interestingness
deﬁned over a range of features.

In 2011, TREC launched the microblog search track, where sys-
tems are asked to return relevant and interesting tweets given a
query [21]. The temporal aspect of Twitter and its characteristics,
e.g., hashtags and existence of hyperlinks, were exploited by many
participants, for query expansion, ﬁltering, or learning to rank [21].
Whereas teams depended on self-constructed training data to train
learning to rank systems during TREC 2011, most of these systems
were successfully trained on the 2011 queries during TREC 2012.
Teevan et al. [40] ﬁnd that over 20% of Twitter queries contain a
hashtag, an indication that hashtags may be good topical surrogates,
which can be leveraged for building pseudo test collections.
Pseudo test collections. The issue of creating and using pseudo
test collections is a longstanding and recurring theme in IR, see,
e.g., [38, 39]. Several attempts have been made to either simulate
human queries or generate relevance judgments without the need
of human assessors for a range of tasks. Azzopardi et al. [3] sim-
ulate queries for known-item search and investigate term weight-
ing methods for query generation. Their main concern is not to
develop training material, but to determine whether their pseudo
test collection generation methods ultimately give rise to similar
rankings of retrieval systems as manually created test collections.
Kim and Croft [18] generate a pseudo test collection for desktop
search. Huurnink et al. [15] use click-through data to simulate rel-
evance assessments. Later, they evaluate the performance of query
simulation methods in terms of system rankings [16] and they ﬁnd
that incorporating document structure in the query generation pro-
cess results in more realistic query simulators. Hofmann et al. [14]
try to smooth noise from click-through data from an audio-visual
archive’s transaction log using purchase information of videos. We
extend previous work on pseudo test collection generation [4] with
a principled method to obtain good quality training material, using
knowledge derived from editorial judgements.

Asadi et al. [2] describe a method for generating pseudo test
collections for training learning to rank methods for web retrieval.
Their methods build on the idea that anchor text in web documents
is a good source for sampling queries, and the documents that these
anchors link to are regarded as relevant documents for the anchor
text (query). Our work shares analogies, but in the microblog set-

CA

Table 10: MAP performance on TREC MB 2011 topics for various LTR
algorithms, trained on different collections. Features are tuned on MAP.
Best performance per LTR algorithm in bold. Indicated statistically signif-
icant differences are with regard to training on TREC-MB-2012. The best
individual tuned ranker, Tf-idf (Indri), achieved 0.357.
RankSVM
0.362±0.003
0.360±0.005
0.362±0.005
0.344±0.003

Pegasos
Train on
0.387±0.003
0.388±0.001
TREC-MB-2012
0.348±0.004(cid:72) 0.294±0.009(cid:72) 0.319±0.009(cid:72)
Random
0.385±0.001
0.374±0.001
Hashtags
0.379±0.000
0.386±0.000
Hashtags-T
0.383±0.001
0.377±0.002
Hashtags-TI
0.376±0.001(cid:79) 0.198±0.015(cid:72) 0.332±0.013(cid:72)
Hashtags-TI-X20
0.300±0.007(cid:79) 0.373±0.001(cid:79)
0.380±0.001
Hashtags-TI-X40
0.381±0.003
0.352±0.005
0.383±0.001
Hashtags-TI-X60
0.381±0.000
0.363±0.002
0.384±0.002
Hashtags-TI-X80
0.141±0.125(cid:72) 0.020±0.009(cid:72) 0.202±0.119(cid:72)
Hashtags-TI-QL1
0.370±0.001(cid:79) 0.185±0.019(cid:72) 0.360±0.046(cid:72)
Hashtags-TI-QL2
0.166±0.011(cid:72) 0.366±0.026(cid:72)
0.379±0.000
Hashtags-TI-QL3
0.165±0.007(cid:72) 0.372±0.006(cid:79)
0.383±0.001
Hashtags-TI-QL5
0.231±0.008(cid:72) 0.381±0.002
Hashtags-TI-QL20 0.383±0.001

ting, there is no anchor text to sample queries. Moreover, the tem-
poral aspect of relevance plays a bigger role in microblog search.

Carterette et al. [7] investigate what the minimal judging effort
is that must be done to have conﬁdence in the outcome of an eval-
uation. Rajput et al. [32] present a method for extending the re-
call base in a manually created test collection. Carterette et al. [8]
ﬁnd that test collections with thousands of queries with fewer rel-
evant documents considerably reduce the assessor effort with no
appreciable increase in evaluation errors. This ﬁnding inspired us
to come up with pseudo test collection generators that are able to
produce large numbers of queries: while the signal produced by an
individual query may be noisy, the volume will produce a signal
that is useful for learning and parameter tuning.

8. DISCUSSION AND CONCLUSION

Following the results of our experiments we list three main ob-
servations: (1) The Random pseudo test collection performs signif-
icantly worse than editorial collections in the retrieval experiments
and shows low correlation to these collections in the tuning phase.
(2) The top pseudo test collections are not signiﬁcantly worse than
editorial collections and show high correlation when tuning param-
eters. (3) Differences between various pseudo test collections on
retrieval effectiveness are small.

60Table 11: P30 performance on TREC MB 2012 topics for various LTR
algorithms, trained on different collections. Features are tuned on MAP.
Best performance per LTR algorithm in bold. Indicated statistically signif-
icant differences are with regard to training on TREC-MB-2011. The best
individual tuned ranker, DFR-FD, achieved 0.351.

CA

Train on
Pegasos
RankSVM
0.392±0.001
0.392±0.007
0.391±0.007
TREC-MB-2011
0.341±0.003(cid:72) 0.289±0.008(cid:72) 0.314±0.006(cid:72)
Random
0.330±0.011(cid:79) 0.378±0.001(cid:79)
0.379±0.001
Hashtags
0.372±0.001(cid:79) 0.336±0.005(cid:79) 0.382±0.001
Hashtags-T
0.326±0.007(cid:79) 0.393±0.002
0.381±0.001
Hashtags-TI
0.248±0.013(cid:72) 0.353±0.008
0.377±0.001
Hashtags-TI-X20
0.379±0.001(cid:79) 0.294±0.009(cid:72) 0.389±0.001
Hashtags-TI-X40
0.348±0.003(cid:79) 0.394±0.006
0.379±0.001
Hashtags-TI-X60
0.373±0.001(cid:79) 0.337±0.006(cid:79) 0.388±0.007
Hashtags-TI-X80
Hashtags-TI-QL1 0.198±0.089(cid:72) 0.047±0.016(cid:72) 0.291±0.059(cid:72)
Hashtags-TI-QL2 0.374±0.002(cid:79) 0.231±0.016(cid:72) 0.377±0.035(cid:72)
0.218±0.007(cid:72) 0.382±0.016(cid:72)
Hashtags-TI-QL3 0.381±0.001
0.248±0.005(cid:72) 0.391±0.004
Hashtags-TI-QL5 0.385±0.001
0.302±0.005(cid:72) 0.388±0.002
Hashtags-TI-QL20 0.380±0.001

Table 12: MAP performance on TREC MB 2012 topics for various LTR
algorithms, trained on different collections. Features are tuned on MAP.
Best performance per LTR algorithm in bold. Indicated statistically signif-
icant differences are with regard to training on TREC-MB-2011. The best
individual tuned ranker, DFR-FD, achieved 0.220.

CA

RankSVM
0.245±0.004

Train on
Pegasos
0.246±0.004
0.245±0.000
TREC-MB-2011
0.207±0.001(cid:72) 0.159±0.005(cid:72) 0.176±0.006(cid:72)
Random
0.231±0.000(cid:79) 0.181±0.008(cid:72) 0.224±0.001(cid:79)
Hashtags
0.227±0.001(cid:79) 0.193±0.004(cid:72) 0.227±0.001(cid:79)
Hashtags-T
0.234±0.001(cid:79) 0.172±0.005(cid:72) 0.233±0.001(cid:79)
Hashtags-TI
0.233±0.001(cid:79) 0.143±0.007(cid:72) 0.210±0.005(cid:72)
Hashtags-TI-X20
0.233±0.000(cid:79) 0.158±0.005(cid:72) 0.230±0.001(cid:79)
Hashtags-TI-X40
0.196±0.002(cid:72) 0.234±0.003(cid:79)
0.233±0.000
Hashtags-TI-X60
0.230±0.000(cid:79) 0.189±0.004(cid:72) 0.230±0.004(cid:79)
Hashtags-TI-X80
Hashtags-TI-QL1 0.106±0.060(cid:72) 0.026±0.008(cid:72) 0.165±0.048(cid:72)
Hashtags-TI-QL2 0.229±0.001(cid:79) 0.123±0.011(cid:72) 0.233±0.021(cid:72)
0.106±0.004(cid:72) 0.231±0.015(cid:72)
Hashtags-TI-QL3 0.234±0.000
0.118±0.004(cid:72) 0.233±0.003(cid:79)
Hashtags-TI-QL5 0.235±0.000
Hashtags-TI-QL20 0.231±0.000(cid:79) 0.162±0.003(cid:72) 0.228±0.001(cid:79)

Combining the top two observations leads us to conclude that
our approach to constructing pseudo test collections works. We
can successfully use pseudo test collections, as long as we ﬁnd ap-
propriate surrogate relevance labels. Why are these ﬁndings impor-
tant? To train learning to rank methods on microblog retrieval tasks
we do not have to invest in manual annotations but can use hashtags
for creating training examples. Pseudo test collections can also be
used successfully for tuning parameters of retrieval models.

The third observation is that the differences between our pseudo
test collections are limited. More advanced methods for selecting
tweets and hashtags result in performance that is only sporadically
better than the naive baseline method, which treats all tweets and
hashtags equally. We can look at this from two angles. (i) Col-
lection construction: We can limit time spent on constructing a
smooth and interesting pseudo test collection by substituting more
advanced methods (Hashtags-T and -TI) with the naive Hashtags
method. Using the naive method is faster and results in a larger
collection, with similar results. (ii) Training volume: Investing in
obtaining more interesting tweets and hashtags using our more ad-
vanced methods substantially reduces the number of queries in our
collections. While the naive Hashtags uses over 1,800 queries and
460,000 relevant tweets, the other methods use only 890 (Hashtags-
T) and 480 (Hashtags-TI) queries and equally reduced sets of rele-
vant tweets. Training efﬁciency improves substantially by limiting

Table 13: P30 performance on 2011 and 2012 topics for retrieval models
for which MAP was tuned on 2011 topics, ordered by 2012 performance.
The only parameter free retrieval model, DFRee, achieved 0.416 on the
2011 topics, and 0.346 on the 2012 topics.

Model

DFR-FD (Terrier)
Tf-idf (Terrier)
PL2 (Terrier)
PRF (Terrier)
Tf-idf (Indri)
Okapi (Indri)
LM (Indri)
BUW (Indri)
BOW (Indri)

2011
0.416
0.397
0.406
0.391
0.413
0.409
0.404
0.128
0.094

2012
0.351
0.348
0.336
0.335
0.331
0.329
0.325
0.154
0.134

Table 14: MAP performance on 2011 and 2012 topics for retrieval models
for which MAP was tuned on 2011 topics, ordered by 2012 performance.
The only parameter free retrieval model, DFRee, achieved 0.351 on the
2011 topics, and 0.216 on the 2012 topics.

Model

DFR-FD (Terrier)
Tf-idf (Terrier)
PL2 (Terrier)
PRF (Terrier)
Tf-idf (Indri)
Okapi (Indri)
LM (Indri)
BUW (Indri)
BOW (Indri)

2011
0.352
0.352
0.348
0.335
0.357
0.354
0.346
0.075
0.060

2012
0.220
0.215
0.209
0.201
0.194
0.191
0.188
0.069
0.061

the number of queries in the collections. In other words, we can
choose between spending more time on constructing our collec-
tions, while reducing training time, or take a naive collection con-
struction approach that results in larger collections and thus longer
training times. A similar observation holds for the editorial col-
lections, which are the smallest collections (50–60 queries), but
(supposedly) with the highest quality.
Summarizing, we have studied the use of pseudo test collections for
training and tuning LTR systems for microblog retrieval. We use
hashtags as surrogate relevance labels and generate queries from
tweets that contain the particular hashtag. These pseudo test col-
lections are then used for (1) tuning parameters of various retrieval
models, and (2) training learning to rank methods for microblog
retrieval. We explore three ways of constructing pseudo test collec-
tions, (i) a naive method that treats all tweets and hashtags equally,
(ii) a method that takes timestamps into account, and (iii) a method
that uses timestamps and selects only interesting microblog posts.
We compare their performance to those of a randomly generated
pseudo test collection and two editorial collections.

Our pseudo test collections have high correlation with the edito-
rial collections in the parameter tuning phase, whereas the random
collection has a signiﬁcantly lower correlation. In the LTR phase
we ﬁnd that in most cases our collections do not perform signiﬁ-
cantly worse than the editorial collections, while the random col-
lection does perform signiﬁcantly worse.

Looking forward, we are interested in training on a mixture of
editorial and generated ground truth. Our work is related to creating
ground truth in a semi-supervised way and we also aim to further
explore this relation.
Acknowledgements
This research was partially supported by the European Commu-
nity’s Seventh Framework Programme (FP7/2007-2013) under grant
agreements nr 258191 (PROMISE Network of Excellence) and 288-
024 (LiMoSINe project), the Netherlands Organisation for Scien-
tiﬁc Research (NWO) under project nrs 640.004.802, 727.011.005,
612.001.116, HOR-11-10, the Center for Creation, Content and

61Technology (CCCT), the BILAND project funded by the CLARIN-
nl program, the Dutch national program COMMIT, the ESF Re-
search Network Program ELIAS, the Elite Network Shifts project
funded by the Royal Dutch Academy of Sciences (KNAW), and the
Netherlands eScience Center under project number 027.012.105.

9. REFERENCES

[1] G. Amati and C. van Rijsbergen. Probabilistic models of
information retrieval based on measuring the divergence
from randomness. ACM Trans. Inf. Syst., 20(4):357–389,
2002.

[2] N. Asadi, D. Metzler, T. Elsayed, and J. Lin. Pseudo test
collections for learning web search ranking functions. In
SIGIR ’11, pages 1073–1082, 2011.

[3] L. Azzopardi, M. de Rijke, and K. Balog. Building simulated

queries for known-item topics: an analysis using six
european languages. In SIGIR ’07, pages 455–462, 2007.

[4] R. Berendsen, M. Tsagias, M. de Rijke, and E. Meij.

Generating pseudo test collections for learning to rank
scientiﬁc articles. In CLEF ’12, 2012.

[5] M. Bron, E. Meij, M. Peetz, M. Tsagkias, and M. de Rijke.

Team COMMIT at TREC 2011. In TREC 2011, 2011.
[6] S. Carter, W. Weerkamp, and E. Tsagkias. Microblog

language identiﬁcation. Language Resources and Evaluation
Journal, 47(1), 2013.

[7] B. Carterette, J. Allan, and R. Sitaraman. Minimal test
collections for retrieval evaluation. In SIGIR ’06, pages
268–275, 2006.

[8] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and

J. Allan. Evaluation over thousands of queries. In SIGIR ’08,
pages 651–658, 2008.

[9] S. Clinchant and E. Gaussier. Bridging language modeling

and divergence from randomness models: A log-logistic
model for ir. In ECIR ’09, pages 54–65, 2009.

[10] B. Croft, D. Metzler, and T. Strohman. Search Engines:

Information Retrieval in Practice. Addison Wesley, 2009.
[11] S. Cronen-Townsend and W. B. Croft. Quantifying query

ambiguity. In HLT ’02, pages 104–109, 2002.

[12] M. Efron. Information search and retrieval in microblogs. J.

Am. Soc. Inf. Sci. Technol., 62:996–1008, 2011.

[13] M. Efron and G. Golovchinsky. Estimation methods for

ranking recent information. In SIGIR ’11, pages 495–504,
2011.

[14] K. Hofmann, B. Huurnink, M. Bron, and M. de Rijke.

Comparing click-through data to purchase decisions for
retrieval evaluation. In SIGIR ’10, pages 761–762, 2010.
[15] B. Huurnink, K. Hofmann, and M. de Rijke. Simulating

searches from transaction logs. In SIGIR 2010 Workshop on
the Simulation of Interaction, 2010.

[16] B. Huurnink, K. Hofmann, M. de Rijke, and M. Bron.

Validating query simulators: an experiment using
commercial searches and purchases. In CLEF ’10, pages
40–51, 2010.

[17] T. Joachims. Training linear SVMs in linear time. In KDD

’06, pages 217–226, 2006.

[18] J. Kim and W. B. Croft. Retrieval experiments using

pseudo-desktop collections. In CIKM ’09, pages 1297–1306,
2009.

[19] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a

social network or a news media? In WWW ’10, pages
591–600, 2010.

[28] G. Mishne and M. de Rijke. A study of blog search. In ECIR

Springer, 2011.

’06, pages 289–301, 2006.

[20] G. G. Lee et al. SiteQ: Engineering high performance QA
system using lexico-semantic pattern matching and shallow
NLP. In TREC 2001, pages 442–451, 2001.

[21] J. Lin, C. Macdonald, I. Ounis, and I. Soboroff. Overview of

the TREC 2011 Microblog track. In TREC 2011, 2012.

[22] T.-Y. Liu. Learning to rank for information retrieval. Found.

and Trends in Inf. Retr., 3:225–331, 2009.

[23] C. Lundquist, D. Grossman, and O. Frieder. Improving

relevance feedback in the vector space model. In CIKM ’97,
pages 16–23, 1997.

[24] C. Macdonald, R. Santos, and I. Ounis. The whens and hows

of learning to rank for web search. Inf. Retr., pages 1–45,
2012.

[25] C. D. Manning and H. Schütze. Foundations of statistical

natural language processing. MIT Press, 1999.

[26] K. Massoudi, E. Tsagkias, M. de Rijke, and W. Weerkamp.

Incorporating query expansion and quality indicators in
searching microblog posts. In ECIR ’11, pages 362–367,
2011.

[27] D. Metzler. A Feature-Centric View of Information Retrieval.

[29] A. Mohan, Z. Chen, and K. Q. Weinberger. Web-search

ranking with initialized gradient boosted regression trees. J.
Mach. Learn. Res., Workshop and Conf. Proc., 14:77–89,
2011.

[30] N. Naveed, T. Gottron, J. Kunegis, and A. C. Alhadi.

Searching microblogs: coping with sparsity and document
quality. In CIKM ’11, pages 183–188, 2011.

[31] J. Peng, C. Macdonald, B. He, V. Plachouras, and I. Ounis.
Incorporating term dependency in the DFR framework. In
SIGIR ’07, pages 843–844, 2007.

[32] S. Rajput, V. Pavlu, P. B. Golbus, and J. A. Aslam. A
nugget-based test collection construction paradigm. In
CIKM ’11, pages 1945–1948, 2011.

[33] S. Robertson and K. Jones. Simple, proven approaches to

text retrieval. Technical report, U. Cambridge, 1997.

[34] S. Robertson and H. Zaragoza. On rank-based effectiveness
measures and optimization. Inf. Retr., 10(3):321–339, 2007.

[35] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu,

and M. Gatford. Okapi at trec-3. In TREC ’94, pages
109–109, 1995.

[36] D. Sculley. Large scale learning to rank. In NIPS Workshop

on Advances in Ranking, 2009.

[37] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos:

Primal estimated sub-gradient solver for SVM. In ICML ’12,
pages 807–814, 2012.

[38] J. Tague and M. Nelson. Simulation of user judgments in

bibliographic retrieval systems. In SIGIR ’81, pages 66–71,
1981.

[39] J. Tague, M. Nelson, and H. Wu. Problems in the simulation

of bibliographic retrieval systems. In SIGIR ’80, pages
236–255, 1980.

[40] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a
comparison of microblog search and web search. In WSDM
’11, pages 35–44, 2011.

[41] W. Weerkamp and M. de Rijke. Credibility-based reranking

for blog post retrieval. Inf. Retr., 15(3–4):243–277, 2012.
[42] C. Zhai and J. Lafferty. A study of smoothing methods for
language models applied to ad hoc information retrieval. In
SIGIR ’01, pages 334–342, 2001.

62