Summaries, Ranked Retrieval and Sessions:

A Uniﬁed Framework for Information Access Evaluation

Tetsuya Sakai

Microsoft Research Asia, Beijing, P.R.C.

tetsuyasakai@acm.org

Zhicheng Dou

Microsoft Research Asia, Beijing, P.R.C.

zhichdou@microsoft.com

ABSTRACT
We introduce a general information access evaluation framework
that can potentially handle summaries, ranked document lists and
even multi-query sessions seamlessly. Our framework ﬁrst builds a
trailtext which represents a concatenation of all the texts read by the
user during a search session, and then computes an evaluation met-
ric called U-measure over the trailtext. Instead of discounting the
value of a retrieved piece of information based on ranks, U-measure
discounts it based on its position within the trailtext. U-measure
takes the document length into account just like Time-Biased Gain
(TBG), and has the diminishing return property. It is therefore more
realistic than rank-based metrics. Furthermore, it is arguably more
ﬂexible than TBG, as it is free from the linear traversal assumption
(i.e., that the user scans the ranked list from top to bottom), and can
handle information access tasks other than ad hoc retrieval. This
paper demonstrates the validity and versatility of the U-measure
framework. Our main conclusions are: (a) For ad hoc retrieval, U-
measure is at least as reliable as TBG in terms of rank correlations
with traditional metrics and discriminative power; (b) For diversi-
ﬁed search, our diversity versions of U-measure are highly corre-
lated with state-of-the-art diversity metrics; (c) For multi-query ses-
sions, U-measure is highly correlated with Session nDCG; and (d)
Unlike rank-based metrics such as DCG, U-measure can quantify
the differences between linear and nonlinear traversals in sessions.
We argue that our new framework is useful for understanding the
user’s search behaviour and for comparison across different infor-
mation access styles (e.g. examining a direct answer vs. examining
a ranked list of web pages).

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval
Keywords
diversity, evaluation, metrics, sessions, test collections

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

Evaluation is central to the progress of Information Retrieval
(IR) research. It helps researchers build better systems to achieve
their ultimate goal, namely, to satisfy the user’s information need.
In this era of digital information overload, system-oriented evalua-
tion is a necessity as a complement to user-oriented evaluation, as
the latter is difﬁcult to scale and to generalise. However, system-
oriented evaluation in IR is facing a few serious challenges:
Challenge 1 System-oriented evaluation tends to oversimplify real
search tasks in such a way that high effectiveness values may
not guarantee high user performance or satisfaction. For ex-
ample, whenever IR researchers rely entirely on metrics like
Average Precision and Discounted Cumulative Gain (DCG),
they implicitly assume that the user scans a ranked list of
document IDs (not even documents) from top to bottom, like
a machine; most metrics do not consider the user’s actual
effort for ﬁnding information from snippets and from docu-
ments of various lengths. Hence the criticism from the user-
oriented camp: “Where’s the user?”

Challenge 2 “Search” has become a commodity, and as a result,
IR tasks have diversiﬁed. It is no longer just about a list of
documents. We want to evaluate a system that returns a sin-
gle multi-document summary in response to a query (e.g. [19]),
a diversiﬁed search result page (e.g. [20]), an aggregated
search result page (e.g. [29]), or a system that tries to sat-
isfy the user in a session that accommodates query reformu-
lations (e.g. [14]). For every new task, an appropriate evalu-
ation methodology needs to be designed, and virtually every
such methodology faces Challenge 1. Furthermore, the IR
community lacks a “common language” across these diverse
tasks, which makes it difﬁcult for us to compare across them
and to build a uniﬁed IA system.

In the present study, we tackle the above two challenges: we
believe that our proposed evaluation framework is a small but sig-
niﬁcant step towards that goal.

Figure 1 introduces trailtexts, the central concept in our eval-
uation framework. Part (a) shows a single textual query-biased
summary being shown to the user. Suppose that we have observed
(by means of, say, eyetracking [13]) that the user read only the
ﬁrst and the last sentences of this summary. In this case, we de-
ﬁne the trailtext as a simple concatenation of these two sentences:
“Sentence1 Sentence2.” Part (b) shows an aggregated search
output: the user reads a snippet in the news panel, then reads an ad,
and ﬁnally reads a snippet in the web panel. In this case, the trail-
text is deﬁned as “Snippet1 Ad2 Snippet3.” Part (c) is a
more traditional search engine result page: the user reads the ﬁrst
two snippets, and then visits the second URL to read the full text. In

473Sentence 1 

Summary 

Sentence 2 

(a) Reading a summary: 
Sentence 1 → Sentence 2 

search 

ads 

Ad 2 

query 

news 

Snippet 1 

web 
Snippet 3 

(b) Browsing an aggregated search output: 
Snippet 1 → Ad 2 → Snippet 3 

query 

search 

query1 

search 

query2 

search 

Snippet 1 

Snippet 2 

Fulltext 3 

Snippet 1 

Snippet 2 

Snippet 3 

Fulltext 4 

(c) Scanning a ranked list: 
Snippet 1 → Snippet 2 → Fulltext 3 

(d) Scanning multiple ranked lists in a session: 
Snippet 1 → Snippet 2 → Snippet 3 → Fulltext 4 

Figure 1: Constructing trailtexts for various tasks.

this case, the trailtext is “Snippet1 Snippet2 Fulltext3.”
Note that such trailtexts may possibly be constructed systematically
under a certain user behaviour model instead of actual user obser-
vation. Finally, Part (d) shows a session that involves one query
reformulation: the user reads two snippets in the original ranked
list, reformulates the query, reads one snippet in the new ranked
list, and ﬁnally visits the actual document. The trailtext is then
“Snippet1 Snippet2 Snippet3 Fulltext4.”

Our new evaluation framework comprises two steps:

Step 1 Generate a trailtext, or multiple possible trailtexts, by either

observing the actual user or assuming a user model;

Step 2 Evaluate the trailtext(s), based on relevant information units
(e.g. documents, passages, nuggets) found within it, while
discounting the value of each information unit based on its
position within the trailtext.

Our proposal was inspired by the S-measure framework of Sakai,
Kato and Song [19], and the Time-Biased Gain (TBG) framework
of Smucker and Clarke [23]. The S-measure framework, which
only considered the evaluation of textual summaries (See Figure 1(a)),
assumes that the user reads an entire summary from beginning to
end at a constant speed, and that the value of an information unit
wears out linearly according to its offset position in the summary.
Thus, their key idea was to use position-based discounting, in con-
trast to the rank-based discounting employed by DCG for ranked
retrieval. On the other hand, the TBG framework as described at
SIGIR’12 was designed for ranked retrieval, and uses the time spent
by the user as the basis for discounting the value of relevant infor-
mation instead of the document rank1.

While the work of Smucker and Clarke [23] primarily focused
on estimating the time to reach a document at rank k, and assumed
linear traversal, i.e., that all users scan the ranked list from top to
bottom sequentially, our new framework is potentially more gen-
eral. To be more speciﬁc, Step 1 above says nothing about the user
model: our framework can handle nonlinear traversal, i.e., cases
where the user clicks a document at rank k and then one at rank
j(< k). Our framework is also more general in that it can handle
various tasks such as Parts (a), (b) and (d) of Figure 1, and in that it
provides a common language across these tasks by means of trail-
texts. Potentially, this may be useful for addressing questions such
as: “Given this query, which is a better IA system? One that re-
turns a single multi-document summary, or one that returns a list of
1S-measure’s position-based discounting is in fact a form of time-
based discounting, as it assumes that the user’s reading speed is
constant [19].

URLs with snippets?” although we leave this for future work. The
objective of this paper is to demonstrate the validity and versatility
of the U-measure framework.

2. RELATED WORK
2.1 Evaluating Textual Output

In summarisation and Question Answering evaluation, compar-
ison units for computing precision, recall and the like are created
either manually (e.g. semantic content units [16]) or automatically
(e.g. N-grams). The matching between the system output and the
gold standard may also be performed either manually or automati-
cally: ROUGE [15] is an example of the completely automatic ap-
proach. In contrast, the S-measure framework [19] relies on man-
ual information unit extraction and manual matching, and uses the
positions of the information units for discounting their values. S-
measure encourages systems to present important pieces of infor-
mation ﬁrst and to minimise the amount of text the user has to read.
In the present study, we borrow the idea of position-based dis-
counting from S-measure, but devise a general framework for eval-
uating summaries, ranked retrieval, search sessions and other tex-
tual information seeking activities. In our U-measure framework,
an information unit may be a nugget, a search engine snippet, or
an entire document as in traditional document retrieval. Both au-
tomatic and manual evaluation approaches are possible with this
framework, although we only consider automatic evaluation based
on document relevance and clicks in this paper.
2.2 Evaluating Ranked Document Retrieval

In the evaluation of ranked document retrieval, the common as-
sumption is that the (average) user scans the ranked list from top
to bottom until he stops at a certain rank [9, 12, 13]. This lin-
ear traversal assumption forms the basis of virtually all existing IR
metrics, including those designed for diversiﬁed search, where sys-
tems are required to achieve not only high relevance but also high
diversity across possible search intents [9, 10, 20].

Unlike traditional rank-based metrics, TBG pays attention to the
fact that the time spent at each rank differs, depending especially on
the document length and document novelty [23]. However, TBG as
described in that work relies on estimating the time to reach rank k,
which in turn relies on the linear traversal assumption on a single
ranked list. In contrast, our U-measure is arguably more general
in that it can naturally handle multi-query sessions and nonlinear
traversals.
In fact, we drop the notion of “document ranks” al-
together once we have constructed the trailtext (Recall Figure 1).
Just like TBG, U-measure can take into account not only the doc-
uments visited but also search engine snippets, which has recently
been recognised as important in search engine evaluation (e.g. [24,
28]). On the other hand, one potential advantage of TBG (and other
time-based approaches [5]) over ours is that it may also be useful
for evaluating retrieval of nontextual information2.

Evaluating diversiﬁed search has received attention recently, and

several diversity metrics have been proposed and compared: Chapelle
et al. [9] and Clarke et al. [10] independently showed that α-nDCG

2One SIGIR reviewer commented: “it is better to convert mate-
rial read to a measure of time and stay in the TBG framework [..]
TBG can be nicely adapted to non-ranked list evaluation.” How-
ever, it is not immediately obvious to us exactly how TBG may be
extended beyond “estimating the time to reach rank k” and to han-
dle nonlinear traversal. Moreover, we argue that evaluating textual
information access based on texts rather than time has beneﬁts: for
example, U-measure with information units should be useful for
evaluating the quality of search engine snippets.

474and ERR-IA (intent-aware expected reciprocal rank) have similar
properties; Sakai and Song [20, 21] showed several advantages of
D-nDCG over ERR-IA and α-nDCG. In this study, we experiment
with a “D-” version and an “IA” version of U-measure for diversity
evaluation, as we shall describe later.

For both traditional and diversiﬁed IR, we believe that informa-
tion units that are ﬁner than “relevant documents” are required to
properly handle information novelty and redundancy (e.g. [17]).
As we discussed in Section 2.1, our framework can take any tex-
tual pieces of information as information units. However, in the
present study, we limit ourselves to considering snippets and rele-
vant/clicked documents. Note that our U-measure is a generalisa-
tion of S-measure, and that the latter has already been used success-
fully for evaluating query-biased summaries based on information
units [19].

Recently, user simulation has received attention as a method for
bridging the gap between system-oriented and user-oriented evalu-
ation [8, 22]. Our framework is agnostic to whether the trailtext is
generated based on user observation or simulation. While the eval-
uation methods explored in this paper are deterministic in that one
particular user model is considered to generate exactly one trailtext
for each search scenario, it would be possible to incorporate simu-
lation to generate a population of trailtexts that reﬂect different user
behaviours. But this is beyond the scope of this paper3.
2.3 Evaluating More Complex Tasks

The IR community has considered the evaluation of tasks that
are more complex than returning a single ranked list of documents.
Session-based IR evaluation [5, 14] is one example. Our proposed
framework can evaluate sessions as well, as all user actions are
encoded as a trailtext. However, unlike Baskaya, Keskustalo and
Järvelin [5], we do not explicitly consider the cost of user actions
such as query (re)formulation and clicking on “next page.” We
assume that the text read by the user is an adequate representation
of the user effort. Azzopardi [4] views interactive IR applications
as a stream of documents and proposes evaluation metrics such as
the “frequency of observing a relevant document.” His document
stream is similar to our trailtext, but the latter can potentially handle
arbitrary pieces of text.

Other IR tasks are something of a mix between summarisation
and ranked retrieval: character-based bpref (binary preference) has
been used for evaluating a ranked list of passages [2]; Yang and
Lad [26] proposed a nugget-based evaluation method that mod-
els utility as beneﬁt minus cost of reading for evaluating multiple
ranked lists of passages for a standing information need. Arvola,
Kekäläinen and Junkkari [3] have proposed an evaluation method
for an XML retrieval task where the user ﬁrst sees a list of docu-
ments and then jumps to relevant passages of a document selected
from that list. Their proposal is also similar to ours in that it also
considers the amount of text read by the user as well as the ac-
tual reading order (within each document). The key differences are
that they treat document list scanning and document browsing as
two separate modes, and that they evaluate the former by average-
precision-like metrics, which assume linear traversal.

3. PROPOSED FRAMEWORK

As was mentioned in Section 1, our evaluation framework ﬁrst
generates a trailtext based on user observation (e.g. eyetracking
or click logging) or a user model, and then deﬁnes an evaluation

3In our diversity experiments (Section 5), we do consider an Intent-
Aware version of U-measure, which considers multiple trailtexts
that represent different user intents.

(cid:2)k

metric over the trailtext by applying position-based discounting.
Section 3.1 deﬁnes the general U-measure framework, which com-
putes a score for a given trailtext and relevance information asso-
ciated with it. Then Section 3.2 discusses how we actually derive
trailtexts from document relevance and clicks in this study.
3.1 U-measure
A trailtext tt is a concatenation of n strings: tt = s1s2 . . . sn.
Each string sk(1 ≤ k ≤ n) could be a document title, snippet,
full text, or even some arbitrary part of a text (e.g. nugget). We
assume that the trailtext is exactly what the user actually read, in
the exact order, during an information seeking process. We deﬁne
j=1 |sj|. We measure
the offset position of sk as pos(sk) =
lengths in terms of the number of characters [19]. Each sk in a
trailtext tt is considered either l-relevant, i.e.
relevance level of
l(> 0), or nonrelevant. For example, in the case of summarisation
evaluation based on nuggets, a relevant sk may be a string that has
been found (either manually or automatically) to be a match with
a gold standard nugget [19]. Alternatively, in an evaluation en-
vironment where only document relevance assessments are avail-
able, a relevant sk may be the full text of a relevant document,
where it is assumed that the user actually read the entire docu-
ment, as we shall discuss in Section 3.2. We deﬁne the position-
based gain as g(pos(sk)) = 0 if sk is considered nonrelevant, and
g(pos(sk)) = gv l if it is considered l-relevant, where gv l is a gain
value for relevance level l. Note that a string sk that is considered
nonrelevant may in fact be relevant: for example, if the user reads
duplicate documents, it is possible to count only the ﬁrst one as
relevant [23]; similarly, for information needs that do not require
exhaustive pieces of information, it is possible to treat only the ﬁrst
relevant piece of information as relevant, as in the Reciprocal Rank
metric.

The general form of U-measure is given by:

|tt|(cid:3)

1N

pos=1

U =

g(pos)D(pos)

(1)
where N is a normalisation factor (which we simply set to N = 1
in this study, following recent evaluation studies [5, 23]) and pos is
an offset position within tt and D(pos) is a position-based decay
function. Following the S-measure framework [19], here we as-
sume that the value of a relevant information unit decays with the
amount of text the user has read, and adopt a linear function:

D(pos) = max(0, 1 − pos
L

)) .

(2)

Here, L is the amount of text at which all relevant information units
become worthless. Note that, if the user’s reading speed is constant,
Eq. 2 is also equivalent to linear discounting by time.

In this study, we interpret L as the largest Maximal Trailtext
Length (MTL) across all possible search sessions, where the MTL
of a session is the sum of the lengths of (a) all snippets above the
lowest click and (b) all documents clicked by the user in that ses-
sion. Thus, L represents the largest amount of text that the user may
have had to read in one session. This is the point where we consider
that all information units become worthless. Note that we do not
rely on the linear traversal assumption: the snippets may have been
read in any order, and the documents may have been clicked in any
order. We simply sum up the snippet and document lengths.

To estimate L, we ﬁrst obtained 21,911,694 sessions (partitioned
based on 30-minute inactivities) from Microsoft’s Bing (September
7, 2012, US market), under the constraint that every query in the
session received at least one click. This constraint is convenient for
evaluating multi-query sessions, as we shall discuss in Section 6.

475where gi(pos(sk)) = gv l if sk is l-relevant to the i-th intent. Thus,
this is the overall value of a document obtained by combining the
“local” (i.e. per-intent) gain values. We plug Eq. 3 into Eq. 1 to ob-
tain D-U, an extension of D-measure for diversity evaluation [20].
Another natural way to handle diversity is to compute a U-measure
value (Ui) for each intent i separately, and ﬁnally combine them us-
ing the intent probabilities P (i|q). This follows the Intent-Aware
(IA) approach to diversity evaluation [1, 9]:
P (i|q)Ui .

U-IA =

(cid:3)

(4)

i

3.2 Deriving Trailtexts from Document Rele-

vance and Clicks

In Section 3.1, we discussed the general case where each string
sk could be any piece of text that the user has read. Without eye-
tracking studies, however, it is difﬁcult to construct a trailtext based
on user observation. Therefore, in this paper, we consider special
cases where we assume that sk is either a search engine snippet or
(a part of) the full text of the document. For each query (or a ses-
sion), we construct a trailtext or several trailtexts automatically, by
leveraging either document relevance assessments of existing test
collections or document clicks that we obtained from a commercial
search engine. In the latter case where click order information is
available, we can drop the linear traversal assumption.

Figure 3 illustrates one way to automatically construct trailtexts
based on relevance assessments and document rankings. Part (a)
could be a ranked list from a TREC run: its second and fourth doc-
uments are relevant. (For simplicity we consider binary relevance
here, although we actually leverage graded relevance.) Based on
the linear traversal assumption, we can build a trailtext as shown in
Part (b). Then, U can easily be computed using the relevant posi-
tions pos(s3) and pos(s6).

An important point to note here is that the U-measure computed
based on trailtexts such as those described above satisﬁes the di-
minishing return property, similar to ERR [9]. Suppose that, in
Figure 3 Part (a), the nonrelevant document at rank 3 is replaced by
a relevant document. Then, since we now assume that the third doc-
ument is also read, the trailtext will be longer than the one shown
in Part (b), and the full text of the fourth document is pushed back
towards the end of the new trailtext. As a result, the value of the
fourth document diminishes according to the decay function shown
in Figure 2. Whereas, many rank-based metrics such as nDCG lack
this property: the value of the relevant document at rank 4 is deter-
mined absolutely by its gain value and its rank, no matter what the
ranked list has above that rank.

Figure 3(c) shows a diversiﬁed ranked list of documents for a
query that is known to have two intents. The second document
is relevant to Intent 1 but not to Intent 2, while the fourth one is
relevant to Intent 2 but not to Intent 1. In the D-U methodology
discussed in Section 3.1, a single “global gain” value is computed
for every document using Eq. 3, and a single trailtext is created
as shown in Part (b). It is assumed that both of the two relevant
documents are read. On the other hand, in the aforementioned U-
IA methodology, a trailtext is created for each intent, as shown in
Part (d). For example, the trailtext for Intent 1 is created by assum-
ing that only the document at rank 2 is read.

Figure 4 illustrates one way to automatically construct trailtexts
based on click data, which is in line with the way we compute
MTLs (Section 3.1). Using click data, we can conduct session
evaluation involving query reformulations and nonlinear traversals.
Given the lack of eyetracking evidence, we assume that, in every
session, the user reads every document he clicks, and that he reads

Figure 2: Proportion of sessions whose MTLs are no more
than pos (after removing 0.5% of the sessions with the largest
MTLs), and a linear decay function derived from it.

By assuming that every snippet is 200-character long (which on
average is a valid assumption for Bing) and automatically counting
the number of characters for every clicked document, we computed
the MTL for each session, and discarded 0.5% sessions with the
largest MTL values (extremely long sessions for some unknown
reasons). As the largest MTL among the remaining 21,802,136
sessions was 131,579, we chose to set L = 132, 000. While it is
difﬁcult to imagine a user who actually reads over 100,000 charac-
ters within a session, it is important that L is sufﬁciently large for
evaluation purposes, as discussed below.

For the 21,802,136 sessions, the curve in Figure 2 shows the pro-
portion of sessions whose MTLs are no more than x. There is a sud-
den drop near the 70% line, which is caused by extremely popular
“navigational” sessions whose MTLs were identical (computed as
the length of the ﬁrst snippet plus that of the top page of the popular
website). The curve seems to be in line with existing click-based
studies, and that an exponential function may be a good approx-
imation to it (e.g. [6, 23]). We leave the curve ﬁtting for future
work, and use the simple linear decay function shown in the ﬁgure:
perhaps what really should be done is to segment queries into sev-
eral user behaviour types (e.g. informational, navigational or even
more ﬁne-grained types), and to devise different decay functions
that match these types. The raw top-heavy curve is just an aver-
age of various search behaviours, and not necessarily appropriate
for evaluation purposes where, for example, we are also interested
in handling informational queries. For example, the average doc-
ument length computed based on our full session data is 5,445.0
characters (averged over 39,716,443 clicked web pages); that com-
puted based on the relevant news articles of the TREC 2005 robust
track test collection (which we use in Section 4 to replicate the TBG
experiment by Smucker and Clarke [23]) is 3,672.0 characters (av-
eraged over 334,079 articles). Whereas, according to the curve, the
decay value is 50% when x = 4, 717 characters. Thus, if we use
the curve directly as a decay function, the value of a relevant docu-
ment is halved after only one or two relevant documents have been
found. In the present instantiation of U, we use the linear function
to pay attention to more relevant documents. Also, since L deter-
mines the gradient of the linear function, we set it to a large value
for the same reason.

For evaluating summarisation and traditional ranked retrieval,
the gain value for an l-relevant information unit could be set, for
example, as gv l = (2l − 1)/2H , where H is the highest rele-
vance level [9, 20]. Whereas, in a diversity evaluation environment
where multiple possible intents i are known for each query q and
per-intent relevance assessments as well as the intent probabilities
P (i|q) are available [20, 21], we can deﬁne the “global gain” for
each relevant sk as:

g(pos(sk)) =

i

(cid:3)

P (i|q)gi(pos(sk))

(3)

476(a) Ranked list 
nonrelevant 

relevant 

nonrelevant 

relevant 

1 
2 
3 
4 

(b) Trailtext for U (based on (a)) and D-U (based on (c)) 

Rank 1 snippet 

Rank 3 snippet 

Rank 2 snippet + full text 

Rank 4 snippet + full text 

s1 

s2 

s3 

s4 

s5 

s6 

pos(s3) 

pos(s6) 

(c) Diversified  
      ranked list 

Rank 1 snippet 

Ranks 3-4 snippets 

Rank 2 snippet + full text 

Intent1 
nonrel 

rel 

1 
2 
nonrel 
3 
4  nonrel 

Intent2 
nonrel 
nonrel 
nonrel 

rel 

s1 

s2 

s3 

s4 

s5 

for Intent1 

Ranks 1-3 snippets 

Rank 4 snippet + full text 

pos(s3) 

(d) Trailtexts for U-IA 

s1 

s2 

s3 

s4 

s5 

for Intent2 

pos(s5) 

Figure 3: Automatically constructing trailtexts from relevance
assessments of traditional and diversiﬁed IR test collections.
(e) Ranked lists  
      in a session 

(f) Session-based trailtext 

Rank 1 snippet 

Query  
refromulation 

click1 

1 
2 
3 
4 

click2 

1’ 
2’ 
3’ 
4’ 

Rank 2 snippet + full text  Rank 1’ snippet + full text 

s1  s2 

s3 

s4 

s5 

pos(s3) 

pos(s5) 

(g) Nonlinear traversal 

(h) Trailtext for nonlinear traversal 

Ranks 1-4 snippets 

Rank 4 full text 

Rank 2 full text 

click2 

1 
2 
3 
4 
Figure 4: Automatically constructing trailtexts from clicks for
nonlinear traversals and sessions.

pos(s6) 

pos(s5) 

click1 

s1 

s2 

s3 

s4 

s5 

s6 

every snippet ranked above the lowest clicked rank in every ranked
list shown during the session4. Part (e) illustrates a session involv-
ing one query reformuation and therefore two ranked lists: the user
clicks the second document in the ﬁrst list and then the ﬁrst docu-
ment in the second list. Part (f) shows a possible trailtext for this
behaviour. Part (g) shows a nonlinear traversal in which the user
clicks the document at rank 4 and then one at rank 2; Part (h) shows
a possible trailtext for this user, by assuming that snippets between
ranks 1 and 4 are read before the full texts of the two documents
are read. In this paper, we only consider sessions in which every
ranked list contains at least one click: without eyetracking we can-
not tell how the user examined a ranked list that did not result in
any clicks [14].

So far, we have assumed that the user reads (I) all snippets above
the lowest relevant or clicked document; and (II) entire full text
of every relevant/clicked document. In practice, we make a more
(cid:2)
realistic assumption than (II), namely, that (II
) the user reads only
F % of each relevant/clicked document. In the present study, we
assume that the user reads only 20% of each clicked document (i.e.
F = 0.2) by default. In our comparative experiments with TBG
using the TREC 2005 robust data (Section 4), we show that the
choice of F has a direct impact on the correlation with ranked-
based metrics and on discriminative power, and that F = 0.2 is a
reasonable choice. In Section 4.3, we show that our initial attempt
at estimating F from click data also supports this choice. On the
other hand, we stick to the choice of L = 132, 000, based on the
largest MTL discussed in Section 3.1. This is because, even if an
average user reads only F % of each clicked document, we want to
accommodate users who read the entire documents as well.

Figure 5 shows our instantiation of U-measure based on click
data. The pseudocode reads a ﬁle where each line is a triple con-

4An eyetracking study by Joachims et al. supports this assump-
tion [13]. For example, they report that when rank 5 is clicked,
then snippets between ranks 1 and 4 are read 54.5-81.8% of the
time; that at rank 5 is read 100% of the time, and that at rank 6 is
read only 18.2% of the time.

snippetlen = 200;
g = 0.5; // gain of a clicked document: (2l − 1)/2H = (21 − 1)/21.
pos = 0; U = 0;
while read < querynumber , clickedrank , doclen > sorted by time

if querynumber is new then initialise array snippetdone [];
// stores whether or not snippet at rank r has already been read.
for( r = 1; r ≤ clickedrank ; r++)
if snippetdone [r] == 0 then

pos += snippetlen; //reads all snippets above a click.
snippetdone [r] = 1;

end if

pos += F ∗ doclen; // reads F % of clicked document.
U += g ∗ max(0, 1 − pos/L);

end while
return U;

Figure 5: Algorithm for computing U-measure by reading a
session data ﬁle, which consists of querynumber, clickedrank and
doclen sorted by time.
sisting of querynumber (e.g. 1 for the ﬁrst query in a session),
clickedrank and doclen (length of clicked document), and the lines
are chronologically ordered. It handles multi-query sessions and
nonlinear traversals as we have illustrated in Figure 4. The use of
the array snippetdone reﬂects our assumption that the user does
not read the same snippet twice, although alternatives are possible.

4. EVALUATING TRADITIONAL IR
4.1 Experimental Setting

To demonstrate that U-measure is a useful alternative to TBG, we
ﬁrst compare them in an experimental setting very similar to that
of Smucker and Clarke [23]: we evaluate 74 TREC 2005 Robust
Track runs with 50 topics [25], using the document length statistics
from the AQUAINT corpus5. Smucker and Clarke estimated sev-
eral parameters required to instantiate TBG, based on a user study
involving eight TREC 2005 Robust Track topics and 48 partici-
pants, where each search session ran up to 10 minutes. We copy all
parameter values from their study: hence our instantiation of TBG
can be described as follows.
∞(cid:3)

T BG =

r=1

g(r) exp(−T (r)

ln 2
224

)

(5)

where the exponential factor is the time-based decay function6 and
T (r) is the estimated time to reach rank r, computed as the time to
read snippets plus the time to read clicked documents:

r−1(cid:3)

T (r) =

4.4 + (0.018lm + 7.8)pclick (m) .

m=1

(6)

Eq. 6 relies on two important assumptions, namely, the linear
traversal assumption (as the summation over previous ranks sug-
gests), and that the user’s reading speed is constant (the time re-
quired to read a full text is linear with respect to its length lm as
measured by the number of words). Also, according to Smucker
and Clarke’s calibration, pclick (m) = 0.64 if the document at rank
m is relevant and pclick (m) = 0.39 otherwise; the gain value was
estimated to be g(r) = 0.4928 for a relevant document and other-
wise zero [23]. Thus this TBG is binary-relevance-based.

5http://www.ldc.upenn.edu/Catalog/docs/
LDC2002T31/
6Smucker and Clarke [23] estimated this half-life of h = 224 from
an MSN search engine query log that contains about ﬁve million
searches.

477While U and TBG are similar, we note that diminishing return
is not guaranteed with TBG. Consider what happens to a relevant
document at rank 2 when a 1000-word nonrelevant document at
rank 1 is replaced by a 10-word relevant document. According to
Eq. 6, the gain for rank 2 actually increases7. But this is not a
problem if the document lengths do not vary wildly.

We note that while Smucker and Clarke used automatic dupli-
cate document detection to account for the fact that users tend to
read “redundant” documents quickly, our experiments do not em-
ploy any special treatment of such documents. As we mentioned
earlier, we plan to address novelty and redundancy issues based on
information units smaller than documents in our future work.

As was discussed in Section 3, we use L = 132, 000 and F =
0.2 for computing U-measure by default. We compare U with
TBG, AP, nDCG@10 and nDCG@1000. As U and nDCG can
handle graded relevance, we used the standard gain value setting
of gv l = (2l − 1)/2H , where the highest relevance level H is 2 for
the Robust test collection. In addition, we also experimented with
their binary versions, which we denote by Ubin and nDCGbin, as
TBG and AP do not utilise graded relevance. AP and nDCG were
computed using NTCIREVAL8; the document lengths (in words
for TBG and in characters for U) were computed using the HEAD-
LINE and TEXT ﬁelds of the AQUAINT collection.
4.2 Main Traditional IR Results

Table 1 compares the run rankings produced by different metrics
in terms of Kendall’s τ and symmetric τap. The latter is similar
to τ but is more sensitive to top ranks [27].
It can be observed
that U is highly correlated with nDCG@1000, AP and TBG, and
that it is more highly correlated with nDCG@1000 and with AP
than TBG is. For example, the τ between U and nDCG@1000 is
.819, while that between TBG and nDCG@1000 is .780. Whereas,
TBG is more highly correlated with nDCG@10 than U is (.734
vs. .653 in τ ), which is probably because the exponential decay of
TBG is more top heavy than the linear decay of U. Table 2 shows
the correlation results for Ubin and nDCGbin, which suggest that
reducing graded relevance to binary relevance has little impact on
the rankings.
Indeed, although not shown in these tables, the τ
(τap) between U and Ubin is .920 (.803).

We also compare the metrics in terms of discriminative power [18],

which has been used in a number of recent studies for comparing
the stability of metrics (e.g. [10, 20, 23]). While discriminative
power does not say anything about whether the metric is right or
wrong, low discriminative power means that the metric is not use-
ful for drawing conclusions from an experiment. Table 3 compares
the metrics in terms of discriminative power at the 0.05 signiﬁ-
cance level, based on a randomised version of two-sided Tukey’s
Honestly Signiﬁcant Differences test. (This test is more conserva-
tive than standard pairwise tests, as it considers the entire set of
runs [7].) Here, “required Δ” is the minimal performance differ-
ence that is usually statistically signiﬁcant [18]. For example, Ta-
ble 3 shows that nDCG@1000 detected a signiﬁcant difference for
26.9% of the (74∗73/2 =)2, 701 run pairs, and that a difference of
0.15 in terms of Mean nDCG@1000 over 50 topics is usually sta-
tistically signiﬁcant. Figure 6 provides a more general picture for
some of the metrics in Table 3, in the form of ASL (Achieved Signif-
icance Level) curves for 0 < ASL ≤ 0.1 [18]. These results show
that while U is substantially less discriminative than nDCG@1000
and AP, it is at least as discriminative as TBG, and much more dis-
criminative than nDCG@10. Also, the results are consistent with
7T (2) before and after the replacement are 14.5 and 9.5.
8http://research.nii.ac.jp/ntcir/tools/
ntcireval-en.html

Table 1: TREC 2005 Robust τ/τap rank correlation (74 runs;
50 topics): AP, TBG and graded-relevance metrics.

nDCG@10
nDCG@1000
AP
TBG

nDCG@1000
.730/.591
-
-
-

AP
.726/.592
.913/.843
-
-

TBG
.734/.638
.780/.635
.792/.644
-

U
.653/.531
.819/.680
.816/.685
.834/.692

Table 2: TREC 2005 Robust τ/τap rank correlation (74 runs;
50 topics): all metrics use binary relevance information.

nDCGbin@10
nDCGbin@1000
AP
TBG

nDCGbin@1000
.720/.565
-
-
-

AP
.715/.561
.903/.817
-
-

TBG
.732/.663
.781/.635
.792/.644
-

Ubin
.648/.494
.825/.742
.814/.736
.846/.717

Table 3: TREC 2005 Robust discriminative power at α = 0.05
(74 runs; 50 topics).

metric
nDCG@1000
nDCGbin@1000
AP
U
Ubin
TBG
nDCGbin@10
nDCG@10

disc. power
26.9%
26.7%
26.0%
20.0%
19.6%
18.3%
16.4%
14.6%

TBG 

AP 

required Δ
0.15
0.16
0.12
5.28
11.00
1.66
0.21
0.17

TREC’05robust 
50 topics; 74 runs  
(2,701 run pairs) 

U 

nDCG@10 

nDCG@1000 

0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
1801

2001

1901

2101

2701
Figure 6: ASL curves for TREC 2005 Robust runs. The y axis
represents the ASL and the x axis represents run pairs sorted
by the ASL. nDCG and U utilise graded relevance.

2201

2301

2401

2501

2601

the discriminative power experiments of Smucker and Clarke [23],
who reported that TBG lies between AP and nDCG@10. The di-
minishing return property of U and TBG is probably one reason
why they are not as discriminative as AP and nDCG@1000: once a
relevant document is found, they tend to ignore additional relevant
documents, which means observing fewer data points.

To summarise our experiments with the TREC Robust Track data
(following Smucker and Clarke [23]), we have shown that U is
highly correlated with nDCG@1000, AP and TBG, and that it is
at least as discriminative as TBG. Hence we believe that it is fair
to regard U as a good alternative to TBG for the purpose of tra-
ditional ad hoc IR evaluation. In the remainder of this paper, we
demonstrate the usefulness of U in other IR settings. We shall not
discuss TBG any further as the instantiation of TBG we used was
calibrated based on a particular user study [23], and its appropri-
ateness for other IR settings is unknown.
4.3 Effect of F

While we have demonstrated that our default version of U per-
forms at least as well as TBG in terms of rank correlation and dis-
criminative power, these results are in fact dependent on the choice
of F , the percentage of text that the user is expected to read for
each relevant document. The main reason U and TBG are highly
correlated with rank-based metrics is that they retain the rank-based
penalty mechanism in the form of snippet length or snippet reading
time; the main reason U and TBG differ from the ranked-based
metrics is that they more realistically reﬂect the effort spent on

478reading each relevant document, and have the diminishing return
property (on average for TBG). Thus, if we emphasize the snippet
reading feature and suppress the document reading features of U
or TBG, we get a metric that is more like a ranked-based one. As
for discriminative power, a highly discriminative metric is required
to (a) examine a wide range of ranks; and (b) emphasize the top
ranks9. In the case of U, it is relatively easy to boost rank corre-
lation with existing metrics and discriminative power at the same
time, by lowering the value of F . Recall that lowering F means
accumulating shorter strings sk for trailtexts: this results in exami-
nation of deeper ranks, as illustrated below.

Given a ranked list to be evaluated, let the Effective Measure-
ment Depth (EMD) be the rank at which the decay D(pos) falls
below 0.0001, so that the ranks further down will actually be ig-
nored. Furthermore, let the Average EMD (AEMD) be the EMD
averaged across all runs and all topics. In our TREC Robust ex-
periments, the AEMD of TBD is 198.2. That is, TBD examines
about top 200 documents on average. Whereas, the AEMD of U is
shown in Table 4 as a function of F , together with the τ and τap
between U and other metrics: it can be observed that, as F is in-
creased, the AEMD of U goes down monotonically. Moreover, as
F is increased, the impact of the snippet reading feature (i.e. rank-
based penalty) is reduced and the correlations with nDCG and AP
go down monotonically as well. For example, with nDCG@1000,
the τ is .876 when F = 0.1 but .471 when F = 1.0. On the other
hand, F = 0.2, our default version of U, appears to be the most
similar to TBG: the τ values with TBG for F = 0.1 and F = 0.4
are lower than that for F = 0.2, namely, τ = .834.

Figure 7 shows the effect of F on the ASL curve of U: it can
be observed that the discriminative power of U can be enhanced by
setting F to a value even smaller than 0.2 (which also enhances the
correlations with nDCG and AP as shown in Table 4). Interestingly,
however, the relationship between F and discriminative power is
not monotonic: for example, U with F = 1.0 appears to be a little
more highly discriminative than U with F = 0.6.

We have also explored ways to estimate F from actual click data,
but decided not to use these estimates directly in the present study
for two reasons. First, we found that F is heavily dependent on
clicked ranks: users spend a lot of time on top ranked documents.
While the U-measure framework is open to incorporating a vari-
able F for constructing the trailtext, this does make the metric more
complex. Second, we found that the estimate of F varies consid-
erably depending on how it is estimated. One problem we encoun-
tered is that users apparently do other things besides reading text
when they dwell on a clicked page or a search engine result page.
One of the more successful methods we tried was as follows.
First, to estimate the user’s reading speed (SP), which we assume
to be constant (as does TBG), we computed the Snippet Reading
Time SRT = t/c for every query q, where c is the rank of the docu-
ment that received the ﬁrst click and t is the time spent after issuing
the query and until this ﬁrst click. By averaging over 46,526,324
queries from Bing’s query log (September 7, 2012, US market), we
obtained SRT = 14.002 seconds10. Hence, the reading speed can
be estimated as SP = 200/14.002 = 14.283 chars/sec. Second,
to estimate F , we obtained 1,789,636 instances from the same ses-
sion data where ranks k and (k + 1) in the ﬁrst result page were
(cid:2)
clicked consecutively, in this exact order. For each instance, let t
be the time spent between these clicks (i.e. time to read the doc-
ument at rank k and then the snippet at rank (k + 1)), and let dl
be the length of the document at rank k. Then the amount of text
9For example, Precision@1000 satisﬁes (a), but not (b).
10This is considerably longer than 4.4 seconds in Eq. 6, the “time to
evaluate a summary” in the TBG framework [23].

F
AEMD of U
nDCG@10

Table 4: τ (top row) and τap (bottom row) with the TREC 2005
Robust data (74 runs and 50 topics): effect of F on U vs. other
metrics. The F = 0.2 column has been copied from Table 1.
Highest values across columns are shown in bold.
0.8
182.0
.413
.282
.491
.302
.485
.289
.634
.428

0.1
462.9
.686
.525
.876
.776
.876
.795
.811
.682

0.6
221.8
.476
.332
.571
.371
.562
.355
.697
.489

1.0
155.7
.348
.235
.471
.241
.411
.228
.562
.364

0.4
281.3
.590
.450
.698
.500
.690
.506
.801
.608

0.2
384.0
.653
.531
.819
.680
.816
.685
.834
.692

nDCG@1000

TBG

AP

0.2 (default) 

0.1 

0.4 
1.0 

0.8 

0.6 

TREC’05robust 
50 topics; 74 runs  
(2,701 run pairs) 

0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0
1801

2501

2601

2201

1901

2401

2101

2001

2301

2701
Figure 7: Effect of F on the ASL curves of U for TREC 2005
Robust runs.
read at rank k can be estimated as SP ∗ t
F = (SP ∗ t
0.282.

(cid:2) − 200, and therefore
(cid:2) − 200)/dl . The average F over all instances was

In summary, U with F = 0.2 is a reasonable choice, as it is
highly correlated with TBG as well as with nDCG and AP, achieves
relatively high discriminative power, and is supported by real click
data. Hereafter, we use L = 132, 000 and F = 0.2 in all of our
experiments.

5. EVALUATING DIVERSIFIED IR

In this section, we compare our trailtext-based diversity met-
rics D-U and U-IA with existing rank-based diversity metrics. For
this purpose, we use the TREC 2011 Web Track Diversity Task
data [11] as it contains per-intent graded relevance assessments
(with H = 3) unlike its predessessors. We used its 50 topics and
17 Category A runs. The rank-based metrics we consider are I-rec
(a.k.a. subtopic recall), D-nDCG and D(cid:4)-nDCG (i.e. simply an
average of I-rec and D-nDCG), and per-intent-normalised version
of ERR-IA [9] which we call nERR-IA [20, 21]. The ﬁrst three
are the ofﬁcial metrics at the NTCIR INTENT task; A version of
ERR-IA was used as the primary metric at the TREC diversity task.
Again, we use NTCIREVAL to compute these rank-based metrics,
using the exponential gain value setting (See Section 4). Following
TREC, the intent probabilities P (i|q) are assumed to be uniform.
As diversiﬁcation concerns the very top of the ranked list, we eval-
uate the top ten documents (D-nDCG@10, etc.).

Table 5 compares the TREC 2011 Diversity rankings according
to different metrics in terms of τ and symmetric τap.
It can be
observed that D-U and U-IA are highly correlated with existing
ranked-based diversity metrics (e.g.
the τ between D-U and D-
nDCG is .809), and that D-U and U-IA are extremely highly cor-
related with each other (τ = .985). Below, we explain the latter
observation.

As diversity metrics are generally more complex than traditional
metrics, Figure 8 provides an example ranked list from an actual
TREC 2011 Diversity run (Topic=137; Run=uwBA), with how D-
U and U-IA are actually computed. Understanding this example
should also help the reader see why D-U and U-IA are very similar.

479Table 5: TREC 2011 Diveristy τ/τap rank correlation (17 Cate-
gory A runs; 50 topics).

Table 6: TREC 2011 Diveristy discriminative power at α =
0.05 (17 Category A runs; 50 topics).

I-rec
D-nDCG
D(cid:2)-nDCG
D-U
nERR-IA

D-nDCG
.809/.761
-
-
-
-

D(cid:2)-nDCG
.897/.890
.912/.859
-
-
-

D-U
.706/.560
.809/.668
.750/.610
-
-

nERR-IA
.676/.626
.779/.689
.750/.696
.647/.528
-

U-IA
.691/.551
.794/.659
.735/.601
.985/.991
.632/.519

Topic=137; Run=uwBA 
Intent1 

Intent2 

Intent3 

1 
2 
3 
4 
5 
6 
7 
8 

3 
0 
0 
1 
0 
0 
0 
0 

0 
0 
0 
0 
0 
0 
0 
0 

3 
0 
0 
0 
0 
0 
0 
3 

Global 
gain 
2*(7/8)/3 

Global 
decay 
.9890 

Local 
gain1 
7/8 

Local 
decay1 
.9890 

Local 
gain3 
7/8 

Local 
decay3 
.9890 

.9875 

.9859 

.9831 

.9816 

.9801 

.9785 

.9705 

(1/8)/3 

(7/8)/3 

1/8 

.9875 

.9859 

.9831 

.9816 

.9801 

.9785 

.9770 

7/8 

.9875 

.9859 

.9844 

.9829 

.9814 

.9799 

.9718 

Local relevance levels 

D-U=((14*.9890 
           +1*.9831 
           +7*.9705)/8)/3= .9009 

U-IA=(((7*.9890+1*.9831)+ 
           (7*.9890+7*.9718))/8)/3 
       = .9013 

Figure 8: Examples of how D-U and U-IA are computed for the
TREC 2011 Diversity runs.

This topic has three intents (i.e. subtopics), and the run returned
three relevant documents: the one at rank 1 is 3-relevant to Intents 1
and 3; the one at rank 4 is 1-relevant to Intent 1 only, and so on. D-U
assumes that all of the relevant documents are read, and computes
“global gains” using Eq. 3 as shown in the ﬁgure. For example,
for the document at rank 1, since gv 3 = (23 − 1)/23 = 7/8 for
both Intents 1 and 3, the global gain is 2 ∗ (7/8)/3. On the other
hand, the D(pos) values are shown in the “global decay” column:
for example, as the length of the document at rank 1 was found
to be 6,279 characters, the estimated trailtext length after reading
(a part of) this document is 200 + 0.2 ∗ 6279 = 1455.8. Thus
D(pos ) = 1−1455.8/132000 = .9890. Whereas, at rank 2, since
we assume that only the snippet is read, D(pos) = 1 − (1455.8 +
200)/132000 = .9875. The ﬁnal value of D-U is .9009. On the
other hand, U-IA is computed as shown on the right of Figure 8.
For Intent 1, it is assumed that only the documents at ranks 1 and
4 are read; for Intent 3, it is assumed that only the documents at
ranks 1 and 8 are read. Then a U value is computed separately for
Intents 1 and 3, and the ﬁnal value of U-IA is .9013. Note, for
example, that the local decay for Intent 3 starts deviating from the
global decay at rank 4 (.9844 vs. .9831), as the document at rank 4
is not relevant to this intent.

(cid:2)

(cid:2)

(cid:2)

More generally, let I be the set of known intents for a topic, and
(⊆ I) be the set of intents covered by a ranked list (so that
let I
I-rec = |I
(cid:2)|/|I|). We say that a document in the list is strictly
locally relevant if it is relevant to at least one intent from I
and
nonrelevant to at least one intent from I
. In Figure 8, the docu-
ments at ranks 4 and 8 are strictly locally relevant with respect to
the ranked list. It is easy to see that if there is no strictly locally rele-
vant document in the ranked list, then Di(pos) = D(pos) holds for
any (i, x). That is, any local decay value would be identical to the
i P (i|q)gi(pos))D(pos) =
global one. Whereas, D-U =
(cid:2)
pos gi(pos)D(pos )) from Eqs. 1 and 3, and U-IA =
(cid:2)
pos gi(pos)Di(pos)) from Eq. 4. Hence it is clear
that if there is no strictly locally relevant document in the ranked
list, then D-U = U-IA holds. A corollary is that if the ranked list
covers only one intent (i.e. |I

(cid:2)| = 1), then D-U = U-IA holds.

i P (i|q)(
i P (i|q)(

(cid:2)
(cid:2)

pos (

(cid:2)

(cid:2)

metric
D-nDCG
D(cid:2)-nDCG
I-rec
nERR-IA
D-U
U-IA

TREC’11diversity 
50 topics; 17 runs  
(136 run pairs) 

disc. power
27.2%
23.5%
19.1%
17.6%
14.7%
14.7%

D#- 
nDCG 

required Δ
0.12
0.15
0.19
0.14
0.35
0.35

D-U 

U-IA 

D-nDCG 

I-rec 

nERR-IA 

0.1
0.09
0.08
0.07
0.06
0.05
0.04
0.03
0.02
0.01
0

81

71

91

101

111

121

131

Figure 9: ASL curves for TREC 2011 Diversity runs. The y axis
represents the ASL and the x axis represents run pairs sorted
by the ASL.

Table 6 compares the discriminative power of the diversity met-
rics at α = 0.05. Figure 9 visualises the discriminative power for
0 < ASL ≤ 0.1. From these results, it can be observed that D-
U and U-IA are substantially less discriminative than D((cid:4))-nDCG.
On the other hand, while they are less discriminative than nERR-IA
at α = 0.05, they are actually more discriminative at α = 0.01.
Again, the diminishing return property of D-U, U-IA and nERR-IA
is one reason why they are not as discriminative as D((cid:4))-nDCG11.
To sum up: D-U and U-IA are highly correlated with existing
ranked-based diversity metrics, and are very highly correlated with
each other. While they are not as discriminative as D((cid:4))-nDCG,
they may be useful for evaluating web search result diversiﬁcation
from the user’s perspective, as they are the only ones that take the
document lengths into account.

6. EVALUATING MULTI-QUERY SESSIONS
Sections 4 and 5 discussed TREC-style evaluations using rele-
vance assessments. In this section and in Section 7, we utilise the
session data mentioned in Section 3.1 to compute U-measure based
on clicks, using the algorithm shown in Figure 5.

To test the validity of U for multi-query session evaluation, we
ﬁrst constructed records of the form sessionID, querynum, clicke-
drank, doclen for the aforementioned 21,911,694 sessions. For ex-
ample, a record (S1,2,3,500) indicates that in Session S1, the third
URL for the second query (i.e. after one query reformulation) was
clicked and that the document length is 500 characters. Moreover,
within each session, the records are sorted by time. 5,178,327 of
these sessions (23.6%) contained multiple queries. From these ses-
sions, we obtained a random sample containing 50,000 sessions.
The average and the maximum number of queries per session for
this sample are 2.649 and 50, respectively; the average and the max-
imum number of clicks per session are 3.566 and 124, respectively.
Sessions with many queries and clicks are often to do with pornog-
raphy, as we shall discuss later.

For comparison, we also computed a version of Session DCG
(sDCG), similar to that instantiated by Kanoulas et al. [14]. Al-
though they also proposed session evaluation metrics that consider
many possible browsing paths over multiple ranked lists, we do
not consider them, since in our experimental setting, we can deter-
ministically construct a trailtext based on the lowest click in each
11As for I-rec, its discriminative power is known to vary widely
across test collections [21].

480ranked list in a session. Note that sDCG and other rank-based met-
rics rely on the linear traversal assumption.

We compute sDCG for a given session as follows. First, con-
struct a single ranked list by (i) truncating each ranked list at the
lowest clicked rank; (ii) concatenating the ranked lists. (We do not
employ any special treatment for duplicate documents [14].) Then,
sDCG is computed over this concatenated ranked list:

isrel (r)

(7)

(cid:3)

sDCG =

log4(querynum (r) + 3) log2(r + 1)

r

where isrel (r) is 1 if the document at rank r in the concatenated
list was clicked and 0 otherwise, and querynum(r) returns j when
the document at r comes from the ranked list for the j-th query
in the session. In short, sDCG discounts relevant (or clicked in our
case) documents not only by ranks, but also by the number of query
reformulations that the user had to go through.

Figure 10 visualises the correlation between U and sDCG. As
indicated in the ﬁgure, the Pearson’s correlation (which takes into
account the absolute scores) is .820, while kendall’s τ (which com-
pares the session rankings) is .600. Example A in this ﬁgure repre-
sents a session with extremely high U and sDCG values. This ses-
sion actually contains two unrelated queries (“2000 Ranger 175”
– a ﬁshing boat, and “kfc in janesville wi”), but all but one of the
clicks are with the former query: the user clicked 92 times to exam-
ine 39 unique documents. It is possible that (s)he was conducting a
survey of the ﬁshing boat. Example B is a session with 124 clicks
for 4 porn queries. Example C represents a navigational informa-
tion need: the queries are “check yahoo mail” and “check yahoo
mail account,” and the user clicked on the same yahoo mail page
12 times. With the ﬁrst query, the user clicked the top ranked doc-
ument (i.e. checked email) 11 times, which adds 11 to sDCG12;
then, with the second query, the user clicked the top ranked doc-
ument (i.e. the document at rank 2 in the concatenated list) once,
which further adds 1/ log4(2 + 3) log2(2 + 1) = .5435 to sDCG.
Thus sDCG = 11.5435. Whereas, owing to the diminishing re-
turn property, the value of a click on the yahoo mail page decays
gradually with U: as the length of this page was estimated to be
539 characters, according to Figure 5, the decay value for the ﬁrst
click is 1 − (200 + 0.2 ∗ 539)/132000 = .9977, while that for the
eleventh click is 1 − (200 + 11 ∗ 0.2 ∗ 539)/132000 = .9895.

To sum up, click-based U is highly correlated with sDCG when
used for multi-query session evaluation, but unlike sDCG, it has the
diminishing return property similar to ERR [9], and can take doc-
ument lengths into account. It is also possible to design trailtext-
based metrics based on different browsing paths in a way similar to
Kanoulas et al. [14], but this is beyond our current scope.

7. EVALUATING NONLINEAR TRAVERSAL

In this section, we demonstrate the potential of evaluating non-
linear traversals using click-based U. For this experiment, we ﬁrst
extracted a total of 2,015,311 sessions from our sample date (Septem-
ber 7, 2012, US market) that contained at least one nonlinear traver-
sal somewhere in the session. From this set, we randomly sampled
50,000 sessions that contained at least one nonlinear traversal be-
fore the ﬁrst query reformulation. Furthermore, to isolate the prob-
lem of evaluating nonlinear traversals from that of evaluating multi-
query sessions, we truncated all records where querynum > 1, i.e.
all clicks after the ﬁrst query reformulation. The average and the
12Duplicate clicks can of course be ﬁltered out, but we decided to
reward every click in our experiment. In the case of Example C,
note that the user may obtain new information every time he clicks
on the yahoo mail page.

Figure 10: Correlation between U and sDCG for the 50,000
multi-query sessions.

Figure 11: Correlation between U and DCG for the 50,000
truncated nonlinear traversal sessions.

maximum number of clicks per truncated session for this sample
are 3.612 and 431, respectively. As we are now dealing with sin-
gle ranked lists, sDCG reduces to the standard, binary-relevance
version of DCG (Substitute querynum (r) = 1 to Eq. 7).

Figure 11 visualises the correlation between U and DCG. As
shown in the ﬁgure, Pearson’s correlation is .807, while Kendall’s
τ is .608: the values are remarkably similar to the multi-query case
even though we are using a different sample here. In addition, we
also computed U after sorting each session data ﬁle by the clicked
rank, thereby obtaining artiﬁcial linear traversal sessions. Note that
DCG cannot reﬂect this change as it simply discounts every clicked
document based on the rank. As indicated in the ﬁgure, the effect of
sorting on the correlation values is negligible, but in fact only 368
of the 50,000 sessions were unaffected by the sort in terms of U.
Below, we discuss a few speciﬁc examples indicated in the ﬁgure.
Example D and Example E are porn queries with 431 and 38
clicks, respectively. For the latter, the value of U increases slightly
after the sort. Example F is also a porn query, with 79 clicks, but
we examine this closely as the value of U increased considerably
from 4.1711 to 7.9019 after the sort. This user clicked the doc-
ument at rank 426 (which was his ninth click), and then clicked
one at rank 58 (his tenth click). Moreover, after clicking on the
document at rank 402 (which was his 40th click), he clicked one
at rank 399. Thus there were two nonlinear traversals within this
truncated session. The document at rank 58 was the highest-ranked
clicked document, so the artiﬁcial linear traversal ﬁle regards this
as the ﬁrst clicked document. This document happened to be very
long: 20,044 characters. Thus, while the value of U for the original
nonlinear traversal session reﬂects the user who examined nine rel-

481atively short documents before reaching the long document at rank
58, the U for the artiﬁcial linear traversal session represents a user
who had to read this long document ﬁrst. This property of encour-
aging systems to return relevant and concise information ﬁrst has
been inherited from S-measure for summary evaluation [19]. While
other variants of U are possible, we believe that handling nonliner
traversals and different document lengths as we do is a useful step
towards understanding the real users’ search behaviour.
8. CONCLUSIONS AND FUTURE WORK
We introduced a general information access evaluation frame-
work that can potentially handle summaries, ranked document lists
and even multi-query sessions seamlessly. Our framework ﬁrst
builds a trailtext which represents a concatenation of all the texts
read by the user during a search session, and then computes U-
measure over the trailtext, based on position-based discounting.
U-measure takes the document length into account just like TBG,
and has the diminishing return property. It is therefore more re-
alistic than rank-based metrics. Furthermore, it is arguably more
ﬂexible than TBG, as it is free from the linear traversal assump-
tion, and can handle information access tasks other than ad hoc
retrieval. Our main conclusions are: (a) For ad hoc retrieval, U-
measure is at least as reliable as TBG in terms of rank correlations
with traditional metrics and discriminative power; (b) For diver-
siﬁed search, our diversity versions of U-measure are highly cor-
related with state-of-the-art diversity metrics; (c) For multi-query
sessions, U-measure is highly correlated with Session nDCG; and
(d) Unlike rank-based metrics like DCG, U-measure can quantify
the differences between linear and nonlinear traversals in sessions.
We argue that our new framework is useful for understanding the
user’s search behaviour and for comparison across different infor-
mation access styles (e.g. examining a direct answer vs. examining
a ranked list of web pages).

Our future work includes the following: (1) Exploring setting
and varying F based on real search logs, while maintaining the sim-
plicity of U; (2) Devising alternative decay functions for accommo-
dating various types of information needs; (3) Comparing different
information access styles, as was discussed above; (4) Combining
the U-measure framework with eyetracking and/or information unit
approaches, so that, for example, the difference between two search
engines with similar DCG values but different snippet qualities can
be quantiﬁed.

To make our work as reproducible as possible, we have made the

following publicly available at http://research.microsoft.
com/u/ : (I) Length statistics for all relevant documents in the
TREC 2005 Robust and 2011 Diversity data; (II) Multi-query and
Nonlinear session records of the form sessionID, querynum, clicke-
drank, doclen (50,000 sessions each); and (III) Raw evaluation met-
ric values from all experiments reported in this paper.
9. REFERENCES
[1] R. Agrawal, G. Sreenivas, A. Halverson, and S. Leong. Diversifying

search results. In Proceedings of ACM WSDM 2009, pages 5–14,
2009.

[2] J. Allan, B. Carterette, and J. Lewis. When will information retrieval

be “good enough”? In Proceedings of ACM SIGIR 2005, pages
433–440, 2005.

[3] P. Arvola, J. Kekäläinen, and M. Junkkari. Expected reading effort in

focused retrieval evaluation. Information Retrieval, 13(5):460–484,
2010.

[4] L. Azzopardi. Usage based effectiveness measures. In Proceedings of

ACM CIKM 2009, pages 631–640, 2009.

[5] F. Baskaya, H. Keskustalo, and K. Järvelin. Time drives interaction:

Simulating sessions in diverse searching environments. In
Proceedings of ACM SIGIR 2012, pages 105–114, 2012.

[6] B. Carterette. System effectiveness, user models, and user utility: A

conceptual framework for investigation. In Proceedings of ACM
SIGIR 2011, pages 903–912, 2011.

[7] B. Carterette. Multiple testing in statistical analysis of systems-based

information retrieval experiments. ACM TOIS, 30(1), 2012.

[8] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user

behavior for system effectiveness evaluation. In Proceedings of ACM
CIKM 2011, pages 611–620, 2011.

[9] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu.

Intent-based diversiﬁcation of web search results: Metrics and
algorithms. Information Retrieval, 14(6):572–592, 2011.

[10] C. L. A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A

comparative analysis of cascade measures for novelty and diversity.
In Proceedings of ACM WSDM 2011, pages 75–84, 2011.

[11] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees.

Overview of the TREC 2011 web track. In Proceedings of TREC
2011, 2012.

[12] G. Dupret and B. Piwowarski. A user behavior model for average

precision and its generalization to graded judgments. In Proceedings
of ACM SIGIR 2010, pages 531–538, 2010.

[13] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski, and

G. Gay. Evaluating the accuracy of implicit feedback from clicks and
query reformulations in web search. ACM TOIS, 25(2), 2007.
[14] E. Kanoulas, B. Carterette, P. D. Clough, and M. Sanderson.

Evaluating multi-query sessions. In Proceedings of ACM SIGIR
2011, pages 1053–1062, 2011.

[15] C.-Y. Lin. ROUGE: A package for automatic evaluation of

summaries. In Proceedings of the ACL 2004 Workshop on Text
Summarization Branches Out, 2004.

[16] A. Nenkova, R. Passonneau, and K. McKeown. The pyramid method:

Incorporating human content selection variation in summarization
evaluation. ACM Transactions on Speech and Language Processing,
4(2):Article 4, 2007.

[17] S. Rajput, M. Ekstrand-Abueg, V. Pavlu, and J. Aslam. Constructing

test collections by inferring document relevance via extracted
relevant information. In Proceedings of ACM CIKM 2012, pages
145–154, 2012.

[18] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In

Proceedings of ACM SIGIR 2006, pages 525–532, 2006.

[19] T. Sakai, M. P. Kato, and Y.-I. Song. Click the search button and be

happy: Evaluating direct and immediate information access. In
Proceedings of ACM CIKM 2011, pages 621–630, 2011.

[20] T. Sakai and R. Song. Evaluating diversiﬁed search results using
per-intent graded relevance. In Proceedings of ACM SIGIR 2011,
pages 1043–1042, 2011.

[21] T. Sakai and R. Song. Diversiﬁed search evaluation: Lessons from

the NTCIR-9 INTENT task. Information Retrieval, 2013.

[22] M. D. Smucker and C. L. A. Clarke. Modeling user variance in

time-biased gain. In Proceedings of HCIR 2012, 2012.

[23] M. D. Smucker and C. L. A. Clarke. Time-based calibration of

effectiveness measures. In Proceedings of ACM SIGIR 2012, pages
95–104, 2012.

[24] A. Turpin, F. Scholer, K. Järvelin, M. Wu, and J. S. Culpepper.

Including summaries in system evaluation. In Proceedings of ACM
SIGIR 2009, pages 508–515, 2009.

[25] E. M. Voorhees. Overview of the TREC 2005 robust retrieval track.

In Proceeings of TREC 2005, 2006.

[26] Y. Yang and A. Lad. Modeling expected utility of multi-session

information distillation. In Proceedings of ICTIR 2009 (LNCS 5766),
pages 164–175, 2009.

[27] E. Yilmaz, J. Aslam, and S. Robertson. A new rank correlation

coefﬁcient for information retrieval. In Proceedings of ACM SIGIR
2008, pages 587–594, 2008.

[28] E. Yilmaz, M. Shokouhi, N. Craswell, and S. Robertson. Expected
browsing utility for web search evaluation. In Proceedings of ACM
CIKM 2010, pages 1561–1564, 2010.

[29] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose. Evaluating

aggregated search pages. In Proceedings of ACM SIGIR 2012, pages
115–124, 2012.

482