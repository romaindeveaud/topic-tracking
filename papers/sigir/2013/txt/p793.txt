Hybrid Retrieval Approaches to

Geospatial Music Recommendation

Markus Schedl

Department of Computational Perception
Johannes Kepler University, Linz, Austria

markus.schedl@jku.at

Dominik Schnitzer

Austrian Research Institute for Artiﬁcial

Intelligence, Vienna, Austria
dominik.schnitzer@ofai.at

ABSTRACT
Recent advances in music retrieval and recommendation al-
gorithms highlight the necessity to follow multimodal ap-
proaches in order to transcend limits imposed by methods
that solely use audio, web, or collaborative ﬁltering data. In
this paper, we propose hybrid music recommendation algo-
rithms that combine information on the music content, the
music context, and the user context, in particular, integrat-
ing location-aware weighting of similarities. Using state-of-
the-art techniques to extract audio features and contextual
web features, and a novel standardized data set of music lis-
tening activities inferred from microblogs (MusicMicro), we
propose several multimodal retrieval functions.

The main contributions of this paper are (i) a systematic
evaluation of mixture coeﬃcients between state-of-the-art
audio features and web features, using the ﬁrst standard-
ized microblog data set of music listening events for retrieval
purposes and (ii) novel geospatial music recommendation
approaches using location information of microblog users,
and a comprehensive evaluation thereof.

Categories and Subject Descriptors
Information systems [Information search and retrieval]:
Music retrieval; Human-centered computing [Collaborative
and social computing]: Social media mining

Keywords
Hybrid Music Recommendation, Evaluation

1.

INTRODUCTION

The ﬁeld of Music Information Retrieval (MIR) is seeing
a paradigm shift, away from system-centric perspectives to-
wards user-centric approaches [3]. In this vein, incorporating
user models and addressing user-speciﬁc demands in music
retrieval and music recommendation systems is becoming
more and more important.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

We present several approaches that combine music con-
tent, music context, and user context aspects to build a hy-
brid music retrieval system [12]. Music content and music
context are incorporated using state-of-the-art feature ex-
tractors and corresponding similarity estimators. The user
context is addressed by taking into account musical prefer-
ence and geospatial data, using a standardized collection of
listening behavior mined from microblog data [11].
We make use of the best feature extraction and similarity
computation algorithms currently available to model music
content and music context. We then integrate these similar-
ity models as well as a user context model into a novel user-
aware music recommendation approach that encompasses all
three modalities important to human music perception [12].
The main contributions of this paper are: (i) a systematic
evaluation of combining audio- and web-based state-of-the-
art approaches to music similarity measurement and (ii) two
approaches to incorporate geospatial information into music
recommendation algorithms.

The remainder of the paper is organized as follows. Sec-
tion 2 details the acquisition of the raw music (meta-)data,
which serves as input to the feature extraction and data rep-
resentation techniques presented in Section 3. In Section 4,
we construct diﬀerent hybrid (music content and music con-
text) models and systematically evaluate their mixture co-
eﬃcients. Section 5 proposes two methods to incorporate
geospatial information into music recommendation models.
These extended models are evaluated and compared to the
models without geospatial data and to a random baseline.
Section 6 brieﬂy reviews related literature. Eventually, Sec-
tion 7 draws conclusions and points to further directions.

2. DATA ACQUISITION

The only standardized public data set of microblogs, as far
as we are aware of, is the one used in the TREC 2011 and
2012 Microblog tracks1 [4]. Although this set contains ap-
proximately 16 million tweets, it is not suited for our task as
it is not tailored to music-related activities, i.e. the amount
of music-related posts is marginal.

We hence have to acquire multimodal data sets of mu-
sic items and listeners, reﬂecting the three broad aspects
of human music perception (music content, music context,
and user context) [12]. Whereas the music content refers
to all information that is derived from the audio signal it-
self (such as ryhthm, timbre, or melody), the music context
covers contextual information that cannot be derived from

1http://trec.nist.gov/data/tweets

793the actual audio with current technology (e.g., meaning of
song lyrics, background of a performer, or co-listening re-
lationships between artists). The user context encompasses
all information that are intrinsic to the listener. Examples
range from musical education to spatiotemporal properties
to physiological measures to current activities.
User Context.
Only very recently a data set of music listening activities
inferred from microblogs has been released [11].
It is en-
titled MusicMicro and is freely available2, fostering repro-
ducibility of social media-related MIR research. This data
set contains about 600,000 listening events posted on Twit-
ter3. Each event is represented by a tuple <twitter-id, user-
id, month, weekday, longitude, latitude, country-id, city-id,
artist-id, track-id>, which allows for spatiotemporal identi-
ﬁcation of listening behavior.
Music Content.
Based on the lists of artist and song names in the MusicMi-
cro collection, we gather snippets of the songs from 7dig-
ital4. These serve as input to the music content feature
extractors (cf. Section 3).
Music Context.
To capture aspects of human music perception which are not
encoded in the audio signal, we extract music-related web
pages that represent such contextual information. Following
the approach suggested in [13], we retrieve the top 50 web
pages returned by Bing5 for queries comprising the artist
name6 and the additional keyword “music”, to disambiguate
the query for artists such as “Bush” or “Kiss”.

In summary, we gathered raw data covering each of the
three categories of perceptual music aspects [12]: music con-
tent (audio snippets), music context (related web pages),
and user context (user-speciﬁc music listening events with
spatiotemporal labels).

3. DATA REPRESENTATION

To represent the music content, we use state-of-the-art
audio features proposed in [7]. These algorithms won three
times in a row (since 2010) the annually run benchmarking
activity Music Information Retrieval Evaluation eXchange
(MIREX): “Audio Music Similarity and Retrieval” task7.
They hence constitute the reference in music feature extrac-
tion for similarity-based retrieval tasks. More precisely, we
extract the auditory music features proposed in [7], which
combine various rhythmic features derived from the audio
signal, e.g., “onset patterns” and “onset coeﬃcients” (note
onsets), with timbral features, e.g., “Mel Frequency Cepstral
Coeﬃcients” (coarse description of the amplitude envelop).
The eventual output is pairwise similarity estimates between
songs, which are later aggregated to the artist level.

We again employ a state-of-the-art technique to obtain
features reﬂecting the music context. To describe the mu-

2http://www.cp.jku.at/musicmicro
3http://www.twitter.com
4http://www.7digital.com
5http://www.bing.com
6Please note that issuing queries at the song level is not
reasonable, as doing so typically yields only very few results.
7http://www.music-ir.org/mirex/wiki/2012:Audio_
Music_Similarity_and_Retrieval

tfd,t = 1 + log2 fd,t

(cid:18)

(cid:88)

d∈Dt

(cid:19)

− fd,t
Ft

log2

fd,t
Ft

(1)

(2)

(3)

sic items at the artist level, we follow the approach pro-
posed in [13]. In particular, we model each artist by creat-
ing a “virtual artist documents”, i.e. we concatenate all web
pages retrieved for the artist. In accordance with ﬁndings of
[10], we then use a dictionary of music-related terms (genres,
styles, instruments, and moods) to index the resulting doc-
uments. From the index, we compute term weights accord-
ing to the best feature combination found in the large-scale
experiments of [13]: TF_C3.IDF_I.SIM_COS, i.e. computing
term weight vectors and artist similarity estimates according
to Equations 1, 2, and 3, respectively for tf , idf , and cosine
similarity; fd,t represents the number of occurrences of term
t in document d, N is the total number of documents, Dt
is the set of documents containing term t, Ft is the total
number of occurrences of term t in the document collection,
Td is the set of distinct terms in document d, and Wd is the
length of document d.

wt = 1 − nt

log2 N

, nt =

(cid:80)

Sd1,d2 =

(wd1,t · wd2,t)

t∈Td1 ,d2

Wd1 · Wd2

3.1 Availability of the Data Sets

All components of the data set used in this paper are
publicly available. The sole exception is the actual audio
content of the songs under consideration. We cannot share
them due to copyright restrictions. However, we provide
identiﬁers by means of which corresponding 30-second-clips
can be downloaded from 7digital. If you are interested in
the data sets, please contact the ﬁrst author.
4. HYBRID MUSIC RETRIEVAL

A main research question is how to ideally combine audio
and web features for music retrieval. Although a few MIR
researchers suggest such a combination [2, 1, 3, 12, 5], a sys-
tematic evaluation of combining state-of-the-art similarity
estimators is still missing, hence provided here.
4.1 Experimental Setup

In a preprocessing step, we aggregate the audio features
on the artist level, as they are computed on single tracks.
To obtain audio similarities asim(i, j) between two artists i
and j, we compute the minimum of the distances between
all pairs of tracks by i and j as the minimum yielded the
best results in preliminary experiments similar to the ones
described later in this section. Web similarities wsim(i, j)
are already deﬁned on the artist level. Both, audio and web
similarites, are normalized using the global distance scaling
method Mutual Proximity [14].

Linear combinations of web similarities and audio simi-
larities yield a hybrid similarity function sim(i, j) between
artists i and j. It is given in Equation 4, where ξ is the mix-
ture coeﬃcient, i.e., the weight of the audio part, diﬀerent
values of which we systematically evaluate.

sim(i, j) = ξ · asim(i, j) + (1 − ξ) · wsim(i, j)
As gold standard we use genre information and assess re-
trieval performance via the overlap between the genres as-
signed to the query artist and those assigned to his K nearest

(4)

794web only – 0.00
.05
.15
.25
.35
.45
.55
.65
.75
.85
.95
audio only – 1.00

ξ K = 1 K = 3 K = 5
.5774
.6257
.6261
.6258
.6257
.6252
.6244
.6232
.6221
.6188
.6059
.5247

.5753
.6280
.6286
.6275
.6275
.6266
.6259
.6255
.6234
.6202
.6083
.5302

.5829
.6421
.6432
.6433
.6430
.6408
.6394
.6379
.6368
.6330
.6215
.5436

Table 1: Overlap scores for diﬀerent mixture coeﬃ-
cients ξ between web and audio features.

neighbors according to the similarity function under investi-
gation. This is a standard evaluation approach in MIR. We
gather genre information by (i) retrieving the top tags for
each artist via the Last.fm API8 and (ii) using the top 20
main genres from allmusic9 to index the sets of tags.

To evaluate retrieval performance, we use a Jaccard-like
overlap measure, shown in Equations 5 and 6, where i is the
query artist, Genresi is the set of genres assigned to i, K
is the number of i’s nearest neighbors to consider, and A is
the number of all artists in the data set. The range of the
performance measures is [0, 1], i.e., they are 1.0 if the genres
of the seed artist i’s K nearest neighbors perfectly overlap
with those of i.

overlapi =

1
K

|Genresi ∩ Genresj|

|Genresi|

· (cid:88)

j=1...K

· (cid:88)

i=1...A

overlapi

(5)

(6)

overlap =

1
A

4.2 Results

Performance scores for the hybrid retrieval function for
diﬀerent mixture coeﬃcients ξ are shown in Table 1, to-
gether with results for a random baseline. Although using
only web features (ξ = 0.0) yields better results than us-
ing audio only (ξ = 1.0), adding a small amount of content
features to web features (or vice versa) boosts performance
considerably. Adding a small amount of a complementary
similarity component thus proves highly beneﬁcial. Overall,
values of ξ around 0.15 perform best. We hence use Equa-
tion 7 as hybrid (audio and web features) music model (MU)
for subsequent experiments.

sim(i, j) = 0.15 · asim(i, j) + 0.85 · wsim(i, j)

(7)

5. MUSIC RECOMMENDATION MODELS
Building recommendation systems requires a user model.
In our case, each user u is modeled by the set of artists
UM (u) he listened to. Based on this simple model, we im-
plement the following recommendation strategies: (i) the
hybrid music retrieval model (MU) elaborated in the previ-
ous section and (ii) a standard collaborative ﬁltering (CF)
8http://www.last.fm/api
9http://www.allmusic.com

Abbreviation Description
BL
MU
CF
CF-GEO-Lin

random baseline
hybrid music model (Equation 7)
collaborative ﬁltering model
CF model: geospatial user weighting
using linear spatial distances

CF-GEO-Gauss CF model: geospatial user weighting

weighting using a Gauss kernel

Table 2: Overview of recommendation models.

model. In the MU model, the hybrid music similarity func-
tion (Equation 7) is used to determine the artists closest to
UM (u), which are then recommended. In the CF model, the
users closest to u are determined (using the Jaccard index
between the user models), and the artists listened to by these
nearest users are recommended. For comparison, we further
implemented a random baseline model (BL) that randomly
picks K users from the ﬁltered user set (via the parameter τ ,
see below) and recommends the artists they listened to. To
integrate geospatial information into the CF model, we ﬁrst
compute a centroid of each user u’s geospatial listening dis-
tribution µu[λ, ϕ]10. We then use the normalized geodesic
distance gdist(u, v) (Equation 8) between the seed user u
and each other user v to weight the distance based on the
user models. To this end, we propose two diﬀerent weight-
ing schemes:
linear weighting and weighting according to
a Gaussian kernel around µu[λ, ϕ]. We eventually obtain a
geospatially modiﬁed user similarity sim(u, v) by adapting
the Jaccard index between UM (u) and UM (v) via geospatial,
linear or Gauss weighting, according to Equation 9 (GEO-
Lin) or Equation 10 (GEO-Gauss), respectively. We recom-
mend the artists listened to by u’s nearest users v. Table 2
summarizes all investigated recommendation algorithms.
gdist(u, v) = arccos ( sin(µu[ϕ]) · sin(µv[ϕ]) + cos(µu[ϕ])·

cos(µv[ϕ]) · cos(µu[λ] − µv[λ]) ) ·

max(gdist)

−1

sim(u, v) = J(UM (u), UM (v)) · gdist(u, v)

−1

sim(u, v) = J(UM (u), UM (v)) · exp(−gdist(u, v))

(10)

5.1 Experimental Setup

In order to ensure suﬃcient artist coverage of users, we
evaluate our models using diﬀerent thresholds τ for the min-
imum number of unique artists a user must have listened to
in order to include him in the experiments. We vary τ be-
tween 50 and 150 using a step size of 10. Denoting as Uτ the
number of users in the MusicMicro data set with equal or
more than τ unique artists, we perform Uτ -fold leave-one-
out cross-validation for each value of τ .
5.2 Results

Figure 1 shows accuracies for K = [3, 5] nearest neighbors
and τ = [50 . . . 150]. We can see that all approaches sig-
niﬁcantly outperform the random baseline. Comparing the
MU approach with the CF approaches, it is evident that CF
generally works better for data sets with high numbers of
users (smaller τ ), while content-based MU outperforms CF

10It is common to denote longitude by λ and latitude by ϕ.

(8)

(9)

795when the number of users is restricted. This ﬁnding suggests
a combination of MU and CF, which will be addressed as
part of future work. As for geospatial weighting, a similar
observation comparing the linear weighting with the Gauss
weighting can be made. The more active the users (higher
τ ), the better the performance of the linear weighting ap-
proach, and the worse the Gauss kernel approach. An expla-
nation for this may be that very frequent users of Twitter
typically live in agglomerations, whereas occasional twitter-
ers live in less densely populated areas. For these users in
rural areas, a Gauss weighting is seemingly beneﬁcial as very
nearby users frequently know each other and share common
music tastes (which is not true for highly populated areas).
The models that integrate geospatial information outper-
form the standard CF model for high τ values, indicating
again that this kind of information is beneﬁcial for “power
users”, who typically live in densely populated areas.

6. RELATED WORK

Speciﬁc related work on geospatial music retrieval is very
sparse, probably due to the fact that geospatially annotated
music listening data is hardly available. Among the few
works, Park et al. [6] use geospatial positions and suggest
music that matches a selected environment, based on aspects
such as ambient noise, surrounding, or traﬃc. Raimond et
al. [8] combine information from diﬀerent sources to derive
geospatial information on artists, aiming at locating them
on a map. Zangerle et al. [15] use a co-occurrence-based
approach to map tweets to artists and songs and eventually
construct a music recommendation system. However, they
do not take location into account.

On a more general level, this work relates to context-
based and hybrid recommendation systems, a detailed re-
view of which is unfortunately beyond the scope of the pa-
per. A comprehensive elaboration, including a decent liter-
ature overview, can be found in [9].

7. CONCLUSIONS AND OUTLOOK

We presented the ﬁrst systematic evaluation of hybrid mu-
sic retrieval approaches (combining the currently best per-
forming audio/music content and web/music context fea-
tures), using a recently published, standardized data set of
music listening activities mined from microblogs. Experi-
ments showed that a linear mixture coeﬃcient of 0.15 for

Figure 1: Accuracy plots for diﬀerent values of K
and τ .

the audio part and 0.85 for the web component performed
best, overall. Interestingly, adding only a very small amount
of audio-based information to web features (or vice versa)
considerably improves results.
To the best of our knowledge, this is also the ﬁrst work that
integrates geospatial information into music recommenda-
tion algorithms. Experiments indicate that including geospa-
tial information is particularly beneﬁcial for music recom-
mendation when users listen to many diﬀerent artists. The
collaborative ﬁltering approach (CF) outperforms the hybrid
music retrieval model (MU) when the data set comprises a
high number of users who listen to less artists, overall.

Future work will include considering more diverse data
about the user context, such as demographics, listening time
(hour of day, working day versus weekend), or gender. In
addition, we plan to combine the MU and the CF models,
including geospatial weighting. As a further usage scenario,
we target users frequently traveling around the world and
wanting to listen to music tailored to their current location,
but also complying to their music taste. We will look into
adapting our approaches accordingly.

Acknowledgments
This research is supported by the Austrian Science Fund
(FWF): P22856, P24095, P25655, and the EU FP7: 601166.

8. REFERENCES

[1] D. Bogdanov, J. Serr`a, N. Wack, P. Herrera, and X. Serra.

Unifying Low-Level and High-Level Music Similarity Measures.
IEEE Transactions on Multimedia, 13(4):687–701, Aug 2011.
[2] E. Coviello, A. B. Chan, and G. Lanckriet. Time Series Models
for Semantic Music Annotation. IEEE Transactions on Audio,
Speech, and Language Processing, 19(5):1343–1359, Jul 2011.

[3] C. Liem, M. M¨uller, D. Eck, G. Tzanetakis, and A. Hanjalic.
The Need for Music Information Retrieval with User-centered
and Multimodal Strategies. In Proc. MIRUM, Scottsdale, AZ,
USA, 2011.

[4] R. McCreadie, I. Soboroﬀ, J. Lin, C. Macdonald, I. Ounis, and

D. McCullough. On Building a Reusable Twitter Corpus. In
Proc. SIGIR, Portland, OR, USA, 2012.

[5] B. McFee and G. Lanckriet. Heterogeneous Embedding for

Subjective Artist Similarity. In Proc. ISMIR, Kobe, Japan,
2009.

[6] S. Park, S. Kim, S. Lee, and W. S. Yeo. Online Map Interface

for Creative and Interactive MusicMaking. In Proc. NIME,
Sydney, Australia, 2010.

[7] T. Pohle, D. Schnitzer, M. Schedl, P. Knees, and G. Widmer.

On Rhythm and General Music Similarity. In Proc. ISMIR,
Kobe, Japan, 2009.

[8] Y. Raimond, C. Sutton, and M. Sandler. Automatic

Interlinking of Music Datasets on the Semantic Web. In
Proc. WWW: LDOW Workshop, Beijing, China, 2008.

[9] F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, editors.

Recommender Systems Handbook. Springer, 2011.

[10] M. Schedl. #nowplaying Madonna: A Large-Scale Evaluation

on Estimating Similarities Between Music Artists and Between
Movies from Microblogs. Information Retrieval, 15:183–217,
June 2012.

[11] M. Schedl. Leveraging Microblogs for Spatiotemporal Music

Information Retrieval. In Proc. ECIR, Moscow, Russia, 2013.

[12] M. Schedl and A. Flexer. Putting the User in the Center of

Music Information Retrieval. In Proc. ISMIR, Porto, Portugal,
2012.

[13] M. Schedl, T. Pohle, P. Knees, and G. Widmer. Exploring the

Music Similarity Space on the Web. ACM Transactions on
Information Systems, 29(3), Jul 2011.

[14] D. Schnitzer, A. Flexer, M. Schedl, and G. Widmer. Local and

global scaling reduce hubs in space. Journal of Machine
Learning Research, 13:2871–2902, October 2012.

[15] E. Zangerle, W. Gassler, and G. Specht. Exploiting Twitter’s

Collective Knowledge for Music Recommendations. In
Proc. WWW: #MSM Workshop, Lyon, France, 2012.

50607080901001101201301401500.20.250.30.350.40.450.5τaccuracyBL (K=3)CF−GEO−Gauss (K=3)CF−GEO−Lin (K=3)CF (K=3)MU (K=3)BL (K=5)CF−GEO−Gauss (K=5)CF−GEO−Lin (K=3)CF (K=5)MU (K=5)796