A Comparison of the Optimality of Statistical

Signiﬁcance Tests for Information Retrieval Evaluation

Julián Urbano∗

jurbano@inf.uc3m.es

Mónica Marrero∗

mmarrero@inf.uc3m.es

Diego Martín†

dmartin@dit.upm.es

∗
University Carlos III of Madrid

Department of Computer Science

Leganés, Spain

†
Technical University of Madrid

Department of Telematics Engineering

Madrid, Spain

ABSTRACT
Previous research has suggested the permutation test as
the theoretically optimal statistical signiﬁcance test for IR
evaluation, and advocated for the discontinuation of the
Wilcoxon and sign tests. We present a large-scale study
comprising nearly 60 million system comparisons showing
that in practice the bootstrap, t-test and Wilcoxon test out-
perform the permutation test under diﬀerent optimality cri-
teria. We also show that actual error rates seem to be lower
than the theoretically expected 5%, further conﬁrming that
we may actually be underestimating signiﬁcance.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance evaluation.

Keywords
Evaluation, Statistical signiﬁcance, Randomization, Permu-
tation, Bootstrap, Wilcoxon test, Student’s t-test, Sign test.

1.

INTRODUCTION

An Information Retrieval (IR) researcher is often faced
with the question of which of two IR systems, A and B, per-
forms better. She conducts an experiment with a test col-
lection, and chooses an eﬀectiveness measure such as Aver-
age Precision or nDCG. Based on the eﬀectiveness diﬀerence
she concludes that, for instance, system A is better. But we
know there is inherent noise in the evaluation for a wealth
of reasons concerning document collections, topic sets, rel-
evance assessors, etc. Therefore the researcher needs the
conclusion to be reliable, that is, the observed diﬀerence un-
likely to have happened just by random chance. She employs
a statistical signiﬁcance test to compute this probability (the
p-value). If p ≤ α (the signiﬁcance level, usually α = 0.05 or
α = 0.01) the diﬀerence is considered statistically signiﬁcant
(A(cid:3)(cid:3) B). In practice this means that she can be conﬁdent
that the diﬀerence measured with a similar test collection

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

will be (at least) as large as currently observed. If p > α
the diﬀerence is not signiﬁcant (A(cid:3) B), and she can not be
conﬁdent that the observed diﬀerence is indeed real.

Unfortunately, there has been a debate regarding statisti-
cal signiﬁcance testing in IR evaluation. Classical tests such
as the paired t-test, the Wilcoxon test and the sign test
make diﬀerent assumptions about the distributions, and ef-
fectiveness scores from IR evaluations are known to violate
these assumptions. The bootstrap test is an alternative that
makes fewer assumptions and has other advantages over clas-
sical tests, and the permutation or randomization test is an
even less stringent test in terms of assumptions that theoreti-
cally provides exact p-values. Because IR evaluations violate
most of the assumptions, it is very important to know how
robust these tests are in practice and which one is optimal.
Previous work [4, 5] compared these ﬁve tests with TREC
Ad Hoc data, reaching the following conclusions: a) the
bootstrap, t-test and permutation test largely agree with
each other, so there is hardly any practical diﬀerence in us-
ing one or another; b) the permutation test should be the
test of choice, though the t-test seems suitable as well; the
bootstrap test shows a bias towards small p-values; c) the
Wilcoxon and sign tests are unreliable and should be dis-
continued for IR evaluation. However, all these conclusions
were based on the assumption that the permutation test is
optimal. For example, authors showed that the Wilcoxon
and sign tests fail to detect signiﬁcance when the permuta-
tion test does and vice versa. That is, they are unreliable
according to the permutation test.

But we may follow diﬀerent criteria to chose an optimal
test. We may want the test to be powerful, that is, to pro-
duce signiﬁcant results as often as possible. Additionally, we
may want it to be safe and yield low error rates so that it
is unlikely that we draw wrong conclusions. But power and
safety are inversely related; diﬀerent tests show diﬀerent re-
lations depending on the signiﬁcance level. The lower α the
lower the power, because we need p ≤ α for the result to be
signiﬁcant. Error rates are expected to be at the nominal
α level, so the higher the signiﬁcance level the higher the
expected error rate. The test is exact if we can trust that
the actual error rate is as dictated by the signiﬁcance level.
If it is below it means we are being too conservative and we
are missing signiﬁcant results; if it is above it means we are
deeming as signiﬁcant results that probably are not.

This paper presents a large-scale empirical study that
compares all ﬁve tests according to these optimality criteria,
providing signiﬁcance and error rates at various signiﬁcance
levels for 50-topic sets. Our main ﬁndings are:

925• In practice the bootstrap test is optimal in terms of
power, the t-test is optimal in terms of safety, and the
Wilcoxon test is optimal in terms of exactness.
• For all tests the actual error rate seems to be lower
than the nominal 0.05 level, meaning that we are ac-
tually being too conservative.
• In practice the permutation test is not found to be

optimal under any criterion.

2. DATA AND METHODS

To compare the ﬁve statistical signiﬁcance tests at hand,
we employed data from the TREC 2004 Robust Track. A
total of 249 topics were used, 100 of which were originally de-
veloped in the TREC 7 and 8 Ad Hoc tracks (50 and 50). A
total of 110 runs were submitted by 14 diﬀerent groups. This
dataset is unusually large both in terms of topics and runs,
given that TREC tracks usually employ 50 topics. The sub-
set with the 100 Ad Hoc topics is especially interesting: all
100 topics were developed and judged by the same assessors
for the most part, and they were developed using the same
methodology and pooling protocol with roughly the same
number of runs contributing to the pools [6]. Additionally,
all three tracks used disks 4 and 5 as document collection.
Therefore, we can consider these two sets of 50 topics as two
diﬀerent samples drawn from the same universe of topics.
We randomly split these 100 topics into two disjoint sub-
sets of 50 topics each: T and T (cid:2)
. For each of these two
subsets we evaluated all 110 runs as per Average Precision.
This provides us with 5,995 system pairwise comparisons
with T and another 5,995 with T (cid:2)
. We ran all ﬁve statis-
tical signiﬁcance tests between each of these system pairs1.
This gives us a total of 5,995 pairs of p-values per test, which
can be regarded as the two p-values observed with two dif-
ferent test collections for any two systems. We performed
1,000 random trials of this experiment, so we have a total of
5,995,000 system pairwise comparisons and the correspond-
ing 5,995,000 with another topic subset. Thus, this paper
reports results on nearly 12 million p-values for each of the
ﬁve tests, for a grand total of nearly 60 million p-values. To
our knowledge, this is to date the largest study of this type.
Given an arbitrary topic set split, the 5,995 pairs of p-
values provided by a test can be used to study its optimality.
Consider a researcher that used topic subset T and ran a
test to compute a p-value; under the signiﬁcance level α he
draws a conclusion. What can he expect with a diﬀerent
topic subset T (cid:2)

?. One of these situations can occur:

• Success. The result with both T and T (cid:2)

can really expect any result with T (cid:2)
statistical power in the experiment.

• Non-signiﬁcance. The result with T is A(cid:3) B. We
; there is a lack of
is A(cid:3)(cid:3) B.
Both experiments show evidence of one system outper-
forming the other.
• Lack of power. The diﬀerence is A(cid:3)(cid:3) B with T but
it is A(cid:3) B with T (cid:2)
. There is evidence of a lack of power
in the second experiment.
• Minor error. The result with T is A(cid:3)(cid:3) B, but with
T (cid:2)
it is A≺ B. The second experiment shows some
evidence of a wrong conclusion in the ﬁrst one.
• Major error. The result with T is A(cid:3)(cid:3) B, but with
T (cid:2)
it is A≺≺ B. The two experiments conﬂict.

1As in [4, 5], we calculated 100,000 samples in the permu-
tation and bootstrap tests.

A powerful test minimizes the non-signiﬁcance rate, a safe
test minimizes the minor and major error rates, and an exact
test maintains the global error rate at the nominal α level.

3. RESULTS

For every statistical signiﬁcance test we computed the
non-signiﬁcance, success, lack of power and error rates at
32 signiﬁcance levels α ∈ {0.0001,
...,
0.009, ..., 0.1, ..., 0.5}. Tables 1 and 2 report the results for
a selection of signiﬁcance levels, and Figures 1 and 2 plot
detailed views in the arguably most interesting [0.001, 0.1]
range. Please note that all plots are log-scaled.

..., 0.0009, 0.001,

Non-signiﬁcance rate. The bootstrap test consistently
produces smaller p-values, and it is therefore the most pow-
erful of all tests across signiﬁcance levels. Next are the per-
mutation test for α < 0.01 and the Wilcoxon test for the
usual α ≥ 0.01. The t-test is consistently less powerful,
though the diﬀerence is as small as roughly 1% fewer signiﬁ-
cant results at the usual α = 0.05. The sign test is by far the
least powerful of all ﬁve. Its stair-like behavior is explained
by its resolution: p-values depend only on the sign of the
score diﬀerences, not on the magnitude (see Figure 5 in [4]).
Success rate. The bootstrap and Wilcoxon tests are
the most successful overall. For small signiﬁcance levels
α ≤ 0.001 the bootstrap test shows the highest success rate,
but for the more usual levels 0.001 < α≤ 0.05 the Wilcoxon
test performs better. Next are again the permutation test
and the t-test, with very similar success rates about 0.3%
lower than the Wilcoxon and bootstrap tests at the usual α
levels. The sign test is clearly the worst of all.

Lack of power rate. Most of the unsuccessful compar-
isons are due to a lack of power with the second topic subset
T (cid:2)
. Relative results are comparable to results above: the
bootstrap test dominates at small signiﬁcance levels and the
Wilcoxon tests dominates at the usual levels, again followed
by the permutation test and the t-test.

Minor error rate. Except for rare occasions where the
sign test’s step-like behavior results in the smallest minor
error rate, the t-test is generally the safest of all ﬁve across
signiﬁcance levels. The permutation test follows next with
rates about 0.03% higher. The bootstrap test is consis-
tently outperformed by the t-test and the permutation test;
it yields 0.13% more minor errors at α = 0.05. The Wilcoxon
test performs even better than the permutation test for low
signiﬁcance levels, but it performs worse at the usual levels.
As mentioned, the sign test wiggles between the other tests.
Major error rate. Similarly the t-test consistently per-
forms best in terms of major errors, followed by the permu-
tation and bootstrap tests. It is noticeable that for small
signiﬁcance levels neither of these three tests show any ma-
jor error at all. For instance, at α = 0.005 the t-test provides
as many as 3,006,441 (50.2%) signiﬁcant comparisons, and
yet none of them results in a major error with the second
topic subset. The Wilcoxon test outperforms the permu-
tation test sporadically, but it performs worse overall.
In
general though, it is important to the bear in mind the mag-
nitudes of the major error rates. For instance, at α = 0.05
the t-test produced 1,082 major errors and the bootstrap
test produced 1,523. While the diﬀerence may seem small
compared to the total of signiﬁcants (0.0277% vs 0.0383%),
this is actually a large +41% relative increase. The sign test
is clearly the worst of all, having an extremely large major
error rate at small signiﬁcance levels.

926perm. boot. Wilcox. sign

t-test

perm. boot. Wilcox. sign

t-test

perm. boot. Wilcox. sign

Success rate

Lack of power rate

Non-signiﬁcance rate

t-test

.56367 .5532

α
.0001 .67698 .65006 .6402
.67189
.0005 .61184 .59202 .58186 .60471
.5807
.5722
.001
.49842 .48755 .47647 .48911
.005
.45752 .44937 .43847 .4485
.01
.34779 .34539 .33613 .34215
.05
.29264 .29235 .28412 .28725
.1
.12398 .12581 .12153
.5

.8123

.80788 .8107

.72367 .78749 .79451 .79757 .78859
.6782
.80765
.63438 .81328 .81491 .81547 .8147
.58347 .82051 .82145 .82365
.53308 .82777 .82893 .83233
.42762 .85579 .85565 .85856
.37308 .86905 .86899 .87086 .86941

.73691 .21222 .20503 .20191 .21107
.75479 .19138 .18827 .18653 .19146
.76847 .18556 .18359 .18285 .18392

.82598 .78018 .1764
.17503 .17243
.83338 .79225 .16753 .16595 .16205
.85935 .81999 .13157 .1314
.12743

.83013 .1107

.11072 .10736 .10805

.26264
.24431
.22998
.17039 .2169
.16115 .20276
.12624 .16709
.15031
.04985 .07511

.11957 .14934 .8836

.88369 .8836

.88429 .85641 .05175 .05232 .05088

Table 1: Non-signiﬁcance rates over total of pairs (lower is better), success rates over total of signiﬁcants
(higher is better), and lack of power rates over total of signiﬁcants (lower is better). Best per α in bold face.

5
6
.
0

5
5
.
0

5
.
0

5
4
.
0

4
.
0

5
3

.

0

3

.

0

l

a
t
o
T

 
/
 
s
t
n
a
c
i
f
i
n
g
s
−
n
o
N

i

Non−significance rate

Success rate

Lack of power rate

t−test
permutation
bootstrap
Wilcoxon
sign

6
8
.
0

4
8
.
0

2
8
.
0

0
8
.
0

8
7
0

.

s
t
n
a
c
i
f
i
n
g
s
 
l
a
t
o
T

i

 
/
 
s
e
s
s
e
c
c
u
S

2
2
.
0

0
2
.
0

8
1
.
0

6
1
.
0

4
1
0

.

2
1
0

.

s
t
n
a
c
i
f
i
n
g
s
 
l
a
t
o
T

i

 
/
 
r
e
w
o
p

 
f

o

 
s
k
c
a
L

.001

.005
Significance level α

.01

.05

.1

6
7
0

.

.001

.005
Significance level α

.01

.05

.1

.001

.005
Significance level α

.01

.05

.1

Figure 1: Non-signiﬁcance rates over total of pairs (lower is better), success rates over total of signiﬁcants
(higher is better), and lack of power rates over total of signiﬁcants (lower is better).

Minor error rate

perm. boot. Wilcox. sign

t-test

α
.0001 .00029 .00046 .00051
.00034
.0005 .00074 .00104 .00117
.00089
.00116 .00149 .00168
.00138
.001
.00362
.00309
.005
.00352 .00392
.00469 .00511 .0056
.00546
.01
.01236 .01264 .01363
.014
.05
.01906 .02027
.01903
.1
.02123
.03409 .03389 .03645
.5
.03403

t-test
0
.00045
0
.00089
0
.00155
.00282 0
.00484
.01251
.01862 .00122
.03518
.03062

Major error rate

Global error rate

perm. boot. Wilcox. sign
0
0
0
0

0
0
0
6.37e-7 1.96e-6 .0001

5.08e-7 6.04e-7 .00029 .00046 .00051
4.22e-7 5.18e-7 .00074 .00104 .00117
4.56e-7 .00116 .00149 .00168
3.9e-7

t-test

perm. boot. Wilcox. sign

.00034
.00089
.00138
.00352 .00392 .00362
.00512 .00562 .00547
.01294 .01402
.02029 .02178
.06399 .06552

.00045
.00089
.00155
.00292
.00499
.01441 .01292
.02254 .01956
.06849
.06586

.00001 .00001 .00002 .00001
.00028 .0003
.00038 .0004

.00123 .00152 .00131
.0299

.03163 .02941 .0333

.00309
.0047
.00016
.00041
.01264
.00095 .02025
.06465

Table 2: Minor error rates over total of signiﬁcants (lower is better), major error rates over total of signiﬁcants
(lower is better), and global error rates over total of signiﬁcants (errors = α is better). Best per α in bold face.

t−test
permutation
bootstrap
Wilcoxon
sign

y=x

0
2
0
0

.

0
1
0
0

.

5
0
0
0

.

2
0
0
0

.

s
t
n
a
c
i
f
i

n
g
s
 
l

i

t

a
o
T

 
/
 
s
r
o
r
r
e

 
r
o
n
M

i

1
0
0
0

.

.001

Minor error rate

Major error rate

s
t
n
a
c
i
f
i

n
g
s
 
l

i

t

a
o
T

 
/
 
s
r
o
r
r
e

j

 
r
o
a
M

4
0
−
e
5

5
0
−
e
5

6
0
−
e
5

7
0
−
e
5

Global error rate

2
0
−
e
5

2
0
−
e
2

3
0
−
e
5

3
0
−
e
2

4
0
−
e
5

s
t
n
a
c
i
f
i

n
g
s
 
l

i

t

a
o
T

 
/
 
s
r
o
r
r
e

j

 
r
o
a
M
d
n
a

 

 
r
o
n
M

i

.005
Significance level α

.01

.05

.1

.001

.005
Significance level α

.01

.05

.1

.0001

.0005.001

.005 .01

Significance level α

.05 .1

.5

Figure 2: Minor error rates over total of signiﬁcants (lower is better), major error rates over total of signiﬁ-
cants (lower is better), and global error rates over total of signiﬁcants (errors = α is better).

927Global error rate. Aggregating minor and major errors
we have a global error rate that can be used as an overall
indicator of test safety and exactness. Given the relative size
of minor and major error rates, the trends are here nearly
the same as fwith minor errors, but for the sake of complete-
ness we plot the full range of signiﬁcance levels. The t-test
approximates best the nominal error rate for low signiﬁcance
levels, but the Wilcoxon test does better for the usual levels
and best overall. Surprisingly the permutation test does not
seem to be the most exact at any signiﬁcance level.

4. DISCUSSION

Zobel [7] compared the t-test, Wilcoxon test and ANOVA
at α = 0.05, though with only one random split in 25-25
topics. He found lower error rates with the t-test than with
the Wilcoxon test, and generally lower than the nominal
0.05 level. Given that the latter showed higher power and
has more relaxed assumptions, he recommended it over the
t-test. Sanderson and Zobel [3] ran a larger study also with
splits of up to 25-25 topics. They found that the sign test has
higher error rates than the Wilcoxon test, which has itself
higher error rates than the t-test. They also suggested that
the actual error rate is below the nominal 0.05 level when
using 50 topic sets. Voorhees [6] also observed error rates
below the nominal 0.05 level for the t-test, but more unstable
eﬀectiveness measures resulted in higher rates. Cormack and
Lynam [1] used 124-124 topic splits and various signiﬁcance
levels. They found the Wilcoxon test more powerful than the
t-test and sign test; and the t-test safer than the Wilcoxon
and sign test. Sakai [2] proposed the bootstrap method for
IR evaluation, but did not compare it with other tests.

Smucker et al. [4] compared the same ﬁve tests we study
in this paper, arguing that the t-test, permutation and boot-
strap tests largely agree with each other. Nonetheless, they
report RMS Errors among their p-values of roughly 0.01,
which is a large 20% for p-values of 0.05. Based on the ar-
gument that the permutation test is theoretically exact, they
concluded that the Wilcoxon and sign tests are unreliable,
suggesting that they should be discontinued for IR evalua-
tion. They ﬁnd the bootstrap test to be overly powerful,
and given the appealing theoretical optimality of the per-
mutation test they propose its use over the others, though
the t-test admittedly performed very similarly.
In a later
paper [5] they found that the tests tended to disagree with
smaller topic sets, though the t-test still showed acceptable
agreement with the permutation test, again assumed to be
optimal. The bootstrap test tended again to produce smaller
p-values, so authors recommend caution if using it.

In this paper we ran a large-scale study to revisit these
issues under diﬀerent optimality criteria. In terms of safety,
the t-test produced the smallest error rates across signiﬁ-
cance levels, followed by the Wilcoxon test for low levels
and the permutation test for usual levels.
In general, all
tests yielded error rates higher than expected for low sig-
niﬁcance levels, but much lower for the usual levels. This
suggests that we are being too conservative when assessing
statistical signiﬁcance at α = 0.05; we expect 5% of our
signiﬁcant results to be wrong, but in practice only about
1.3% do indeed seem wrong. We must note though that this
global error rate, as the sum of minor and major errors, is
just an approximation of the true Type I error rates [1].
Table 3 shows the agreement of the ﬁve tests with them-
selves: p-values with topic subset T compared to those with

perm.
.04348
.11635
.12999 .1623

t-test
p≤.0001 .03603
.0001< p≤.0005 .10124
.0005< p≤.001
.13059
.001< p≤.005
.16716 .17044
.005< p≤.01
.20724 .21624
.01< p≤.05
.25275 .26685
.05< p≤.1
.29734 .31101
.1< p≤.5
.31624 .31855

boot. Wilcox. sign
.04475 .03514 .0556
.11923 .09976 .13014
.14619
.18024
.21737
.26114
.30344
.31802

.14516
.20032 .17841
.2387
.21454
.29801 .25779
.33996 .30015
.33816 .31804

Table 3: RMS Error of all ﬁve tests with themselves
(lower is better). Best per bin in bold face.

subset T (cid:2)
. The Wilcoxon test turns out to be the most sta-
ble of all for very small p-values, and generally more so than
the permutation test. The t-test is the most stable overall.
Indeed, if we compute the diﬀerence between the actual and
nominal error rates we ﬁnd that the Wilcoxon test is the one
that best tracks the signiﬁcance level and therefore seems to
be the most exact (RMSE 0.1146), followed by the boot-
strap, t-test, sign and permutation tests (RMSEs 0.1148,
0.1153, 0.1153 and 0.1155). This is particularly interesting
for the bootstrap test:
it provides the most signiﬁcant re-
sults and the actual error rate is still lower than expected.
In summary, a researcher that wants to maximize the
number of signiﬁcant results may use the more powerful
bootstrap test and still be safe in the usual scenario. Re-
searchers that want to maximize safety may use the t-test,
and researchers that want to be able to trust the signiﬁ-
cance level may proceed with the Wilcoxon test. For large
meta-analysis studies we encourage the use of the t-test and
Wilcoxon test because they are far less computationally ex-
pensive and show near-optimal behavior. Unlike previous
work concluded, our results suggest that in practice the per-
mutation test is not optimal under any criterion. Further
analysis with varied test collections and eﬀectiveness mea-
sures should be conducted to clarify this matter, besides
devising methods to better approximate what actual Type I
error rates we have in IR evaluation. We further support
the argument of discontinuing the sign test.

5. REFERENCES
[1] G. V. Cormack and T. R. Lynam. Validity and Power

of t-test for Comparing MAP and GMAP. In ACM
SIGIR, pages 753–754, 2007.

[2] T. Sakai. Evaluating Evaluation Metrics Based on the

Bootstrap. In ACM SIGIR, pages 525–532, 2006.
[3] M. Sanderson and J. Zobel. Information Retrieval

System Evaluation: Eﬀort, Sensitivity, and Reliability.
In ACM SIGIR, pages 162–169, 2005.

[4] M. D. Smucker, J. Allan, and B. Carterette. A
Comparison of Statistical Signiﬁcance Tests for
Information Retrieval Evaluation. In ACM CIKM,
pages 623–632, 2007.

[5] M. D. Smucker, J. Allan, and B. Carterette. Agreement

Among Statistical Signiﬁcance Tests for Information
Retrieval Evaluation at Varying Sample Sizes. In ACM
SIGIR, pages 630–631, 2009.

[6] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR,

pages 806–807, 2009.

[7] J. Zobel. How Reliable are the Results of Large-Scale
Information Retrieval Experiments? In ACM SIGIR,
pages 307–314, 1998.

928