Extracting Query Facets from Search Results

Weize Kong and James Allan

Center for Intelligent Information Retrieval

School of Computer Science

University of Massachusetts Amherst
{wkong, allan}@cs.umass.edu

Amherst, MA 01003

ABSTRACT
Web search queries are often ambiguous or multi-faceted,
which makes a simple ranked list of results inadequate. To
assist information ﬁnding for such faceted queries, we ex-
plore a technique that explicitly represents interesting facets
of a query using groups of semantically related terms ex-
tracted from search results. As an example, for the query
“baggage allowance”, these groups might be diﬀerent air-
lines, diﬀerent ﬂight types (domestic, international), or dif-
ferent travel classes (ﬁrst, business, economy). We name
these groups query facets and the terms in these groups
facet terms. We develop a supervised approach based on
a graphical model to recognize query facets from the noisy
candidates found. The graphical model learns how likely a
candidate term is to be a facet term as well as how likely
two terms are to be grouped together in a query facet, and
captures the dependencies between the two factors. We pro-
pose two algorithms for approximate inference on the graph-
ical model since exact inference is intractable. Our evalu-
ation combines recall and precision of the facet terms with
the grouping quality. Experimental results on a sample of
web queries show that the supervised method signiﬁcantly
outperforms existing approaches, which are mostly unsuper-
vised, suggesting that query facet extraction can be eﬀec-
tively learned.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Clustering, Query formulation

General Terms
Algorithms, Experimentation

Keywords
Query Facet, Semantic Class Extraction, Multi-faceted Query

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

Web search queries are often ambiguous or multi-faceted [27].

Current popular approaches try to diversify the result list to
account for diﬀerent search intents or query subtopics [24].
A weakness of this approach is that the query subtopics are
hidden from the user, leaving him or her to guess at how the
results are organized.

In this work, we attempt to extract query facets from web
search results to assist information ﬁnding for these queries.
We deﬁne a query facet as a set of coordinate terms – i.e.,
terms that share a semantic relationship by being grouped
under a more general hypernym (“is a” relationship). For
example, for the query mars landing, three possible query
facets are shown in Table 1.

Table 1: Example query facets

Query: mars landing
1. Curiosity, Opportunity, Spirit
2. USA, UK, Soviet Union
3. video, pictures, news
Query: baggage allowance
1. Delta, Jetblue, AA, Continental, ...
2. domestic, international
3. ﬁrst class, business class, economy class
4. weight, size, quantity
Query: mr bean
1. comics, movies, tv, books
2. the curse of mr bean, mr bean goes to town, ...
3. rowan atkinson, richard wilson, jean rochefort, ...
4. mr bean, irma gobb, rupert, hubert, ...

The ﬁrst query facet, {Curiosity, Opportunity, Spirit}, in-
cludes diﬀerent Mars rovers. The second query facet, {USA,
UK, Soviet Union}, includes countries relevant to Mars land-
ings. These are both facets where the terms are instances
of the same semantic class. Somewhat diﬀerently, the last
facet, {video, pictures, news}, includes labels for diﬀerent
query subtopics. These labels can be viewed as instances
of a special semantic class, the subtopics of the query mars
landing.

Query facets can be used to help improve search experi-
ence in many ways. Like in faceted search [6], query facets
can help users to navigate through diﬀerent topics of the
search results by applying multiple ﬁlters. Using the exam-
ples in Table 1, for query baggage allowance, a user can select
Delta, international, business class, quantity from each of its
facets, to ﬁnd pages discussing the number of bags allowed
on Delta’s international business class ﬂights. Query facets

93can also be used as query suggestions or clariﬁcation ques-
tions to help users specify search intent. For example, for the
query mars landing, a system might suggest the query facet
{video, pictures, news} or generate clariﬁcation questions
like “Which Mars rover are you looking for? a) Curiosity,
b) Opportunity, c) Spirit”. Query facets are also useful for
exploratory search since they succinctly summarize interest-
ing facts for the issued query. For example, facets for the
query mr bean list episodes titles, characters and casts for
the Mr. Bean television series.

In this paper we develop a supervised method based on
a graphical model for query facet extraction. The graphical
model learns how likely it is that a term should be selected
and how likely it is that two terms should be grouped to-
gether in a query facet. Further, the model captures the
dependencies between the two factors. We propose two al-
gorithms for approximate inference on the graphical model
since exact inference is intractable. Also, we design an eval-
uation metric for query facet extraction, which combines
recall and precision of the facet term, with the grouping
quality.

The rest of this paper is organized as follows. We discuss
related work in Section 2, and then present the problem
formulation in Section 3. Section 4 describes the general
framework we use for query facet extraction. Section 5 de-
scribes our graphical model based approach in detail. Sec-
tion 6 brieﬂy describes two alternate approaches that we use
as baselines. We describe the dataset as well as the metrics
we used for evaluation in Section 7, and report experimen-
tal results in Section 8. Finally, we conclude the work in
Section 9.

2. RELATED WORK

Related work of query facet extraction can be divided into

the following topics.
2.1 Search Results Diversiﬁcation

Search result diversiﬁcation has been studied as a method
of tackling ambiguous or multi-faceted queries while a ranked
list of documents remains the primary output feature of Web
search engine today[24]. It tries to diversify the ranked list
to account for diﬀerent search intents or query subtopics.
A weakness of search result diversiﬁcation is that the query
subtopics are hidden from the user, leaving him or her to
guess at how the results are organized. Query facet extrac-
tion addresses this problem by explicitly presenting diﬀerent
facets of a queries using groups of coordinate terms.
2.2 Search Results Clustering/Organization

Search results clustering is a technique that tries to orga-
nize search results by grouping them into, usually labeled,
clusters by query subtopics [4]. It oﬀers a complementary
view to the ﬂat ranked list of search results. Most previous
work exploited diﬀerent textual features extracted from the
input texts and applied diﬀerent clustering algorithms with
them. Instead of organizing search results in groups, there is
also some work [14, 15, 16] that summarizes search results or
a collection of documents in a topic hierarchy. For example,
Lawrie et al. [14, 15] used a probabilistic model for creating
topical hierarchies, in which a graph is constructed based on
conditional probabilities of words, and the topic words are
found by approximately maximizing the predictive power
and coverage of the vocabulary. Our work is diﬀerent from

these work in that our target is to extract diﬀerent facets of
a query from search results, instead of organizing the search
results.
2.3 Query Subtopic/Aspect Mining

To address multi-faceted queries, much previous work stud-
ied mining query subtopics (or aspects). A query subtopic is
often deﬁned as a distinct information need relevant to the
original query. It can be represented as a set of terms that to-
gether describe the distinct information need [29, 31, 5] or as
a single keyword that succinctly describes the topic [28]. Dif-
ferent resources have been used for mining query subtopics,
including query logs [30, 11, 32, 29, 31, 33], document cor-
pus [2] and anchor texts [5].

A query subtopic is diﬀerent from a query facet in that
the terms in a query subtopic are not restricted to be coor-
dinate terms, or have peer relationships. Query facets, how-
ever, organize terms by grouping “sibling” terms together.
For example, {news, cnn, latest news, mars curiosity news}
is a valid query subtopic for the query mars landing, which
describes the search intent of Mars landing news, but it is
not a valid query facet, given our deﬁnition, since the terms
in it are not coordinate terms. A valid query facet that de-
scribes Mars landing news could be {cnn, abc, fox}, which
includes diﬀerent news channels. In a recent work [7], Dou
et al. developed a system to extract query facets from web
search results and showed the potential of doing so. How-
ever, the unsupervised method they proposed is far from
optimal, and it does not improve by having human labels
available. Also, to the best of our knowledge, their evalu-
ation can be problematic in some cases, which will be dis-
cussed in Section 7.2.3.
2.4 Semantic Class Mining

Semantic class mining can be used to help query facet
extraction. Class attribute extraction [17, 18] aims to
extract attributes for a target semantic class usually spec-
iﬁed by as a set of representative instances. For example,
given a semantic class country, together with some instances
like USA, UK, China, some class attributes can be capital
city, president, population. Those extracted class attributes
can be used as query subtopics. However, class attribute ex-
traction targets semantic classes, not general search queries.
Semantic class extraction aims to automatically mine
semantic classes represented as their class instances from
certain data corpus. Existing approaches can be roughly
divided into two categories: distributional similarity and
pattern-based [25]. The distributional similarity approach is
based on the distributional hypothesis [8], that terms occur-
ring in analogous contexts tend to be similar. Diﬀerent types
contexts has been studied for this problem, including syntac-
tic context [20] and lexical context [21, 1, 19]. The pattern-
based approach applied textual patterns [9, 22], HTML pat-
terns [26] or both [34, 25] to extract instances of a semantic
class from some corpus. The raw semantic class extracted
can be noisy. To address this problem, Zhang et al. [34]
used topic modeling to reﬁne the extracted semantic classes.
Their assumption is that, like documents in the conventional
setting, raw semantic classes are generated by a mixture of
hidden semantic class. In this paper, we apply pattern-based
semantic class extraction on the top-ranked Web documents
to extract candidates for ﬁnding query facets.

942.5 Faceted Search

Faceted search is a technique for accessing information or-
ganized according to a faceted classiﬁcation system, allowing
users to digest, analyze and navigate through multidimen-
sional data.
It is widely used in e-commerce and digital
libraries [6]. Faceted search is similar to query facet extrac-
tion in that both of them use sets of coordinate terms to
represent diﬀerent facets of a query. However, most existing
works for faceted search are build on as speciﬁc domain or
predeﬁned categories [7], while query facet extraction does
not restrict queries in a speciﬁc domain, like products, peo-
ple, etc.

3. PROBLEM FORMULATION

Query facet extraction is the problem of ﬁnding query
facets for a given query q from available resources, such as
web search results. A query facet F = {t} is a set of
coordinate terms, terms that are part of a semantic set,
which we call facet terms. These facet terms can be in-
stances of a semantic class, for example Curiosity, Opportu-
nity, Spirit are all Mars rovers. They can be labels for query
subtopics, such as video, pictures, news for the query mars
landing. We use F = {F} to denote the set of query facets.
TF = {t|t ∈ F, F ∈ F} is the set of all the facets terms that
appear in F.

Query facets can be extracted from a variety of diﬀer-
ent resources, such as a query log, anchor text, taxonomy
and social folksonomy. In this work, we only focus on ex-
tracting query facets from the top k web search results D =
{D1, D2, . . . , Dk}. We intend to explore the use of other
information sources for this problem in future work.

4. GENERAL FRAMEWORK

In this section, we describe the general framework we use
for extracting query facet from web search results. Given
a query q, we retrieve the top k search results, D, as input
to our system. Then query facets F are extracted by, ﬁrst,
extracting candidates from search results D and then ﬁnding
query facets from the candidates.
4.1 Extracting candidate lists

Similar to Dou et al. [7], we use pattern-based seman-
tic class extraction approach [25] to extract lists of coordi-
nate terms from search results as candidates for query facets.
In pattern-based semantic class extraction, patterns are ap-
plied on the corpus to discover speciﬁc relationships between
terms. For example, the pattern “NP such as NP, NP, ...,
and NP ” can be used to extract coordinate terms and their
hypernyms from text. Besides lexical patterns, HTML pat-
terns are often used on HTML documents to extract co-
ordinate terms from some HTML structures, like <UL>,
<SELECT> and <TABLE>.

Table 2: Semantic class extraction patterns

Type
Lexical

HTML

Pattern
item, {,item}∗, (and|or) {other} item
<select><option>item</option>...</select>
<ul><li>item</li>...</ul>
<ol><li>item</li>...</ol>
<table><tr><td>item<td>...</table>

We use both of the two types of patterns, summarized in

Table 2. In the table, all items in each pattern are extracted
as a candidate list. For example, from the text sentence
“... Mars rovers such as Curiosity, Opportunity and Spirit”,
according to the lexical pattern, we will extract a candidate
list {Curiosity, Opportunity, Spirit}. For the lexical pattern,
we also restrict those items to be siblings in the parse tree
of that sentence. We use the PCFG parser [12] implemented
in Stanford CoreNLP1 for parsing documents. For HTML
tables, following Dou et al. [7], lists from each column and
each row are extracted.
After extracting the lists from the top ranked results D,
we further process them as follows. First, all the list items
are normalized by converting text to lowercase and removing
non-alphanumeric characters. Then, we remove stopwords
and duplicate items in each lists. Finally, we discard all lists
that contain fewer than two item or more than 200 items.
After this process, we have a set of candidate lists L = {L},
where each list L = {t} is a set of list items.
4.2 Finding query facets from candidate lists
The candidate lists extracted are usually noisy [34], and
could be non-relevant to the issued query, therefore they
cannot be used directly as query facets. Table 3 shows four
candidate lists extracted for the query mars landing. L1
contains list items that are relevant to mars landing, but
they are not coordinate terms. L2 is a valid query facet,
but it is incomplete – another Mars rover Spirit appears in
L3. L3 is extracted from the sentence, “It is bigger than the
400-pound Mars Exploration rovers, Spirit and Opportunity,
which landed in 2004 ”. The list item “the 400 pound mars
exploration rovers” is an extraction error.

Table 3: Four candidate lists for query mars landing

L1: curiosity rover, mars, nasa, space
L2: curiosity, opportunity
L3: the 400 pound mars exploration rovers, spirit, opportunity
L4: politics, religion, science technology, sports, ...

Since the candidate lists are frequently noisy, we need an
eﬀective way to ﬁnd query facets from extracted candidate
lists. More formally, given a set of candidate lists L = {l},
the task is to ﬁnd a set of query facets F, where TF ⊆ TL.
Similar to TF , TL = {t|t ∈ L, L ∈ L} is the set of all list
items in L. To address this problem, we develop a graphical
model, which learns how likely a list item is a facet term,
how likely two list items should be grouped in a query facet,
and capture the dependencies between the two factors.

5. A GRAPHICAL MODEL FOR FINDING

QUERY FACETS

In this section, we describe the directed graphical model
we use to ﬁnd query facts form noisy candidate lists. A di-
rected graphical model (or Bayesian network) is a graphical
model that compactly represents a probability distribution
over a set of variables
It consists of two parts: 1)
a directed acyclic graph in which each vertex represents a
variable, and 2) a set of conditional probability distribu-
tions that describe the conditional probabilities of each ver-
tex given its parents in the graph.

[23].

We treat the task of ﬁnding query facets from candidate
lists as a labeling problem, in which we are trying to predict
1http://nlp.stanford.edu/software/corenlp.shtml

951) whether a list item is a facet term, and 2) whether a pair
of list items is in one query facet. Then, we used a directed
graphical model to exploit the dependences that exist be-
tween those labels. Similar to conditional random ﬁelds [13],
we directly model the conditional probability P (y|x), where
y is the label we are trying to predict and x is the observed
data – list items and item pairs. Thus, it avoids model-
ing the dependencies among the input variables x, and can
handle a rich set of features. For our graph model, exact
maximum a posteriori inference is intractable; therefore, we
approximate the results using two algorithms.
5.1 The Graphical Model
5.1.1 Graph
First we deﬁne all the variables in our graphical model.
Let Y = {yi}, where yi = 1{ti ∈ TF} is a label indicating
whether a list item ti is a facet term. Here 1{·} is an indi-
cator function which takes on a value of 1 if its argument
is true, and 0 otherwise. pi,j denotes the list items pair
(ti, tj), and PL = {pi,j|pi,j = (ti, tj), ti, tj ∈ TL, ti (cid:54)= tj}
denotes all the items pairs in TL. Let Z = {zi,j}, where
zi,j = 1{∃F ∈ F , ti ∈ F ∧ tj ∈ F} is a label indicates
whether the corresponding item pair pi,j should be grouped
together in a query facet. The vertices in our graphical
model are V = TL ∪ PL ∪ Y ∪ Z. Note that the list items
TL, and item pairs PL are always observed.

As shown in Figure 1, there are three types of edges in the
graph: 1) edges from each list item ti to its corresponding
labels yi; 2) edges that point to each item pair label zi,j
from the two corresponding list items yi and yj; 3) edges
from each item pair pi,j to its corresponding label zi,j.

Figure 1: A graphical model for candidate list data

5.1.2 Conditional Probability Distribution
We use logistic-based conditional probability distributions
(CPDs) for variable yi and zi,j, deﬁned as in Equation 1 and
Equation 2.

P (yi = 1|ti) =

P (zi,j = 1|pi,j, yi, yj) =

1

1 + exp{−(cid:80)
1 + exp{−(cid:80)

k λkfk(ti)}
yiyj

k µkgk(pi,j)}

(1)

(2)

fk and gk are features that characterize a list item and a
item pair respectively. λ and µ are the weights associated
with fk and gk respectively. Compared to a conventional
logistic function, Equation 2 has an extra term, yiyj, in the
numerator. When yi = 0 or yj = 0, we have P (zi,j =
1|pi,j, yi, yj) = 0. This means when either of the two list

items is not a facet term, the two items can never appear
in a query facet together. When both of the ti and tj are
facet terms, P (zi,j = 1|pi,j, yi, yj) becomes a conventional
logistic function, which models the probability of ti and tj
being grouped together in a query facet, given the condition
that both ti and tj are facet term.

The joint conditional probability for the graphical model

is calculated as
P (Y, Z|TL, PL) =

(cid:89)

yi∈Y

(cid:89)

zi,j∈Z

P (yi|ti)

P (zi,j|pi,j, yi, yj) (3)

where the CPDs are deﬁned in Equation 1 and Equation 2.
5.1.3 Parameter Estimation
The training set for the graphical model can be denoted as
{TL, PL, Y ∗, Z∗}, where Y ∗, Z∗ are the ground truth labels
for the list items TL and item pairs PL. The conditional
probability of the training set can be calculated according
to Equation 4.

P (λ, µ) =

∗

∗|TL, PL)

, Z

P (Y

The log-likelihood l(λ, µ), can be calculated as follows,

TL,PL

(cid:89)

(cid:80)

l(λ, µ) = lt(λ) + lp(µ)

(cid:88)

(cid:88)
(cid:88)

TL

lt(λ) =

(cid:88)

lp(µ) =

TL

zi,j∈Z

log P (yi|ti) −

yi∈Y
log P (zi,j|pi,j, yi, yj) −

k λ2
2σ2

k

(cid:80)

k

k µ2
2γ2

(4)

(5)

(6)

(7)

l(λ, µ) is separated into two parts, lt(λ) and lp(µ). The last
terms of Equation 6 and Equation 7 are served as regular-
izers which penalize large values of λ, µ. σ and γ are reg-
ularization parameters that control the strength of penalty.
Notice that, in the train set, for those item pairs pi,j with
any of its list item not being a facet term, their labels
zi,j = 0. According to Equation 2, for those item pairs,
log P (zi,j|pi,j, yi, yj) = 0, which makes no contribution to
lp(µ), and thus lp(µ) can be simpliﬁed as

lp(µ) =

log P (zi,j|pi,j, yi, yj) −

(8)

(cid:80)

k

k µ2
2γ2

(cid:88)

(cid:88)

TL

zi,j∈Z(cid:48)

where Z(cid:48) is a subset of Z, which contains only the labels for
item pairs with both of its list items being facet terms.

We can see that Equations 6 and 8 are exactly the same as
log-likelihoods for two separated logistic regressions. In fact,
Equation 6 learns a logistic regression model for whether
a list item is a facet term, and Equation 8 learns a logis-
tic regression model for whether two facet terms should be
grouped together. The parameter λ and µ can be learned
by maximizing the log-likelihood using gradient descent, ex-
actly same as in logistic regression.
5.2 Inference

When given a new labeling task, we could perform max-
imum a posteriori inference - compute the most likely la-
bels Y ∗, Z∗ by maximizing the joint conditional probability
P (Y, Z|TL, PL). After that, the query facet set F can be
easily induced from the labeling Y ∗, Z∗. (Collect list items
with yi = 1 as facet terms, and group any two of them into

tiyizi,jpi,j...............tjyj...z1,2t2t1y1y2z1,ip1,2p1,iz1,jp1,jz2,ip2,iz2,jp2,j........................96a query facet if the corresponding zi,j = 1.) Note that the
graphical model we designed does not enforce the labeling
to produce strict partitioning for facet terms. For example,
when Z1,2 = 1, Z2,3 = 1, we may have Z1,3 = 0. There-
fore, an optimal labeling results may induce an overlapping
clustering. To simplify the problem, we add the strict par-
titioning constraint that each facet term belongs to exactly
one query facet. Also, to directly produce the query facets,
instead of inducing them after predicting labels, we rephrase
the optimization problem as follows. First, we use the fol-
lowing notations for log-likelihoods,

st(ti) = log P (yi = 1|ti)
st(ti) = log (1 − P (yi = 1|ti))

sp(ti, tj) = log P (pi,j = 1|pi,j, yi = 1, yj = 1)
sp(ti, tj) = log (1 − P (pi,j = 1|pi,j, yi = 1, yj = 1))

Using the notations above, the log-likelihood l(F) for a par-
ticular query facet set F formed from L can be written as

l(F) = lt(F) + lp(F)
lt(F) =
st(ti) +

(cid:88)

t(cid:54)∈TF

(cid:88)
(cid:88)

t∈TF

(cid:88)

F∈F

ti,tj∈F

lp(F) =

sp(ti, tj) +

sp(ti, tj) (9)

st(ti)

(cid:88)

(cid:88)

F,F (cid:48)
∈F

ti∈F,
tj∈F (cid:48)

In the right hand side of Equation 9, the ﬁrst term is the
intra-facet score, which sums up sp(·,·) for all the item pairs
in each query facet. The second term is the inter-facet score,
which sums up the sp(·,·) for each item pair that appears
in diﬀerent query facets. Then the optimization target be-
comes F = arg maxF∈F l(F), where F is the set of all possi-
ble query facet sets that can be generated from L with the
strict partitioning constraint.

This optimization problem is NP-hard, which can be proved

by a reduction from the Multiway Cut problem [3]. There-
fore, we propose two algorithms, QF-I and QF-J, to ap-
proximate the results.

5.2.1 QF-I
QF-I approximates the results by predicting whether a
list item is a facet term and whether two list items should
be grouped in a query facet independently, which is ac-
complished two phases.
In the ﬁrst phase, QF-I selects a
set of list items as facet terms according to P (yi|ti).
In
this way, the algorithm predicts whether a list item ti is
a facet term independently, ignoring the dependences be-
tween yi and its connected variables in Z.
In our imple-
mentation, we simply select list items ti with P (ti) > wmin
as facet terms.
(For convenience, we use P (ti) to denote
P (yi = 1|ti).) In the second phase, the algorithm clusters
the facet terms TF selected in the ﬁrst phase into query
facets, according to P (ti, tj).
(P (ti, tj) is used to denote
P (zi,j = 1|pi,j, yi = 1, yj = 1)). Many clustering algorithm
can be applied here, using P (ti, tj) as the distance mea-
sure. For our implementation, we use a cluster algorithm
based on WQT [7], because it considers the importance of
nodes while clustering. We use P (ti) as the measure for
facet term importance, and dt(ti, tj) = 1 − P (ti, tj) as the
distance measure for facet terms. The distance between a
cluster and a facet term is computed using complete linkage
distance, df (F, t) = maxt(cid:48)∈F d(t, t(cid:48)), and the diameter of a

cluster can be calculated as dia(F ) = maxti,tj∈F dt(ti, tj).
The algorithm is summarized in Algorithm 1. It processes
the facet terms in decreasing order of P (t). For each facet
term remaining in the pool, it builds a cluster by iteratively
including the facet term that is closest to the cluster, until
the diameter of the cluster surpasses the threshold dmax.

Algorithm 1 WQT for clustering facet term used in QF-I
Input: TF , P (t), df (F, t), dia(F ), dmax
Output: F = {F}
1: Tpool ← F
2: repeat
3:
4:
5:

t ← arg maxt∈Tpool P (t)
F ← {t}
iteratively include facet term t(cid:48) ∈ Tpool that is closest
to F , according to df (F, t(cid:48)), until the diameter of the
cluster, dia(F ), surpasses the threshold dmax.

F ← F ∪ {F}, Tpool ← Tpool − F

6:
7: until Tpool is empty
8: return F

5.2.2 QF-J
QF-I ﬁnds query facets based on the graphical model by
performing inference of yi and zi,j independently. The sec-
ond algorithm, QF-J, instead tries to perform joint inference
by approximately maximizing our target l(F) with respect
to yi and zi,j iteratively. The algorithm ﬁrst guesses a set
of list items as facet terms. Then it clusters those facet
terms by approximately maximizing lp(F), using a greedy
approach. After clustering, the algorithm checks whether
each facet term “ﬁts” in its cluster, and removes those that
do not ﬁt. Using the remaining facet terms, the algorithm
repeats the process (clustering and removing outliers) until
convergence.

QF-J is outlined in Algorithm 2. The input to the al-
gorithm are the candidate list item set TL, and the log-
likelihoods l(F), lp(F). In the ﬁrst step, we select top n list
items according to st(t) as the initial facet terms, because
it is less sensitive to the absolute value of the log-likelihood.
In our experiment, n is set to 1000 to make sure most of
the correct facet terms are included. Then, the algorithm
improves l(F) by iteratively performing functions Cluster
and RemoveOutliers. Cluster performs clustering over
a given set of facet terms. In step 10 to 12, it puts each facet
terms into a query facet by greedily choosing the best facet,
or creates a singleton for the list item, according to the re-
sulting log-likelihood, lp(F). We choose to process these list
items in decreasing order of st(t), because it is more likely to
form a good query facet in the beginning by doing so. Re-
moveOutliers removes facet terms according to the joint
log-likelihood l(F).
In step 20 to 22, it checks each facet
term to see if it ﬁts in the facet, and removes outliers. F (cid:48) is
the set of facet terms the algorithm selected when processing
each facet F .
5.2.3 Ranking Query Facets
The output of QF-I and QF-J is a query facet set F. To
produce ranking results, we deﬁned a score for a query facet
t∈F P (t), and rank the query facets ac-
cording to this scoring, in order to present more facet terms
in the top. Facet terms within a query facet are ranked
according to scoret(t) = P (t).

as scoreF (F ) = (cid:80)

97F ← Cluster(TF , lp)
TF ← RemoveOutliers(F, l)

Algorithm 2 QF-J
Input: TL = {t}, l, lp
Output: F = {F}
1: TF ← top n list items from TL according to st(·)
2: repeat
3:
4:
5: until converge
6: return F
7:
8: function cluster(TF , lp)
9:
10:
11:

F ← ∅
for each t ∈ TF in decreasing order of st(t) do

Choose to put t into the best facet in F or add
t as a singleton into F, whichever that has the highest
resulting lp(F).

12:
end for
return F
13:
14: end function
15:
16: function RemoveOutliers(F, l)
17:
18:
19:
20:
21:

TF ← all facet terms in F
for each F ∈ F do

F (cid:48) = ∅
for each t ∈ F in decreasing order of st(·) do

the highest resulting l({F (cid:48)})

choose to add t into F (cid:48) or not, whichever has
if not, TF ← F − {t}

end for

22:
23:
end for
24:
return TF
25:
26: end function
27: return F

Table 4: Two types of features

Item Features for list item t

SF

docRank

length
clueIDF
TF
DF
wDF

titleTF
titleDF
titleSF
snippetTF
snippetDF
snippetSF
listTF

Number of words in t
IDF of t in ClueWeb09 collection
Term frequency of t in D
Document frequency of t in D
√
Weighted DF. Each document count
weighted by 1/
Site frequency. Number of unique
websites in D that contain t
TF of t for the titles of D
DF of t for the titles of D
SF of t for the titles of D
TF of t for the snippets of D
DF of t for the snippets of D
SF of t for the snippets of D
Frequency of t in candidate lists
extracted from D
Number of documents that contain t
in their candidate lists
Number of unique websites that
contain t in their candidate lists
IDF of t in a general candidate list
collection
TF × clueIDF
listTF × lisIDF
Length diﬀerence, |len(ti) − len(tj)|
Number of candidate lists extracted
from D, in which ti, tj co-occur
textContextSim Similarity between text contexts
listContextSim Similarity between list contexts

Item Pair Features for item pair pi,j = (ti, tj)

TF.clueIDF
listTF.listIDF

lengthDiﬀ
listCooccur

listDF

listSF

listIDF

5.3 Features

There are two types of features used in our graphical

model, summarized in Table 4.

Item features, fk(t) in the graphical model, character-
ize a single list item. To capture the relevance of item t
to the query, we use some TF/IDF-based features extracted
from the top k search results, D. For example, snippetDF
is the number of snippets in top k search results that con-
tain item t. snippetDF and other frequency-based features
are normalized using log(f requency + 1). To capture how
likely item t is to be an instance of a semantic class, we
use features extracted from candidate lists. For example,
listTF is the frequency of t in the candidate lists extracted
from D. Some list items occur frequently in candidate lists
across diﬀerent queries, such as home, contact us and pri-
vacy policy. They are treated as stopwords, and removed
from the candidate lists. We also use listIDF to cope with
this problem. listIDF is the IDF of a list item in a general
collection of candidate lists we extracted (see Section 7.1).
It is calculated as listIDF (t) = log N−Nt+0.5
Nt+0.5 , where N is
the total number of lists in the collection, Nt is the number
of lists contain t. The same form is used for clueIDF, IDF
in ClueWeb092 collection.

Item Pair Features, g(pi,j) in the graphical model, are
used to capture how likely a pair of list items should be
grouped into a query facet, given that the two list item both

2http://lemurproject.org/clueweb09

are facet terms. This can be measured by context similar-
ity [25]. For textContextSim, we use window size 25, and
represent text context as a vector of TF weights. Cosine
similarity is used as the similarity measure. Similarly, we
use the candidate lists that contain the list item as its list
context, and calculate listContextSim in the same way as
textContextSim.

6. OTHER APPROACHES

In this section, we describe two alternative approaches for
ﬁnding query facets from candidate lists. They are used as
baselines in our experiments.
6.1 QDMinder

Dou et al. [7] developed QDMiner/QDM for query facet
extraction, which appears to be the ﬁrst work that addressed
the problem of query facet extraction. To solve the problem
of ﬁnding query facets from the noisy candidate lists ex-
tracted, they used an unsupervised clustering approach. It
ﬁrst scores each candidate list by combining some TF/IDF-
based scores. The candidate lists are then clustered with
bias toward important candidate lists, using a variation of
the Quality Threshold clustering algorithm [10]. After clus-
tering, clusters are ranked and list items in each clusters
are ranked/selected based on some heuristics. Finally, the
top k clusters are returned as results. This unsupervised
approach does not gain by having human labels available.
Also, by clustering lists, they lose the ﬂexibility of breaking
a candidate list into diﬀerent query facets.

986.2 Topic modeling

In semantic class extraction, Zhang et al. [34] proposed
to use topic models to ﬁnd high-quality semantic classes
from a large collection of extracted candidate lists. Their
assumption is, like documents in the conventional setting,
candidate lists are generated by a mixture of hidden topics,
which are the query facets in our case. pLSA and LDA are
used in their experiments. We ﬁnd this approach can be di-
rectly used for ﬁnding query facets from candidate lists. The
major change we need to make is that: in semantic class ex-
traction, topic modeling is applied globally on the candidate
lists (or a sample of them) from the entire corpus; in query
facet extraction, we apply topic modeling only on the top
k search results D, assuming the coordinate terms in D are
relevant to the query. Then, the topics are returned as query
facets, by using the top n list items in each topic (accord-
ing to the list item’s probability in the topic). Though this
topic modeling approach is more theoretically motivated, it
does not have the ﬂexibility of adding diﬀerent features to
capture diﬀerent aspects such as query relevance.

7. EVALUATION
7.1 Data

Queries. We constructed a pool of 232 queries from dif-
ferent sources, including random samples from a query log,
TREC 2009 Web Track queries 3, example queries appear-
ing in related publications [32, 29] and queries generated by
our annotators. Annotators were asked to select queries that
they are familiar with from the pool for annotating. Overall,
we collect annotations for 100 queries (see Table 5).

Table 5: Query statistics

query log
related publications
TREC 2009 Web Track
annotators generated
sum

Source #queries
collected
100
20
50
62
232

#queries
annotated
30
10
20
40
100

Search results. For each query, we acquire the top 100
search results from a commercial Web search engine. A few
search results are skipped due to crawl errors, or if they are
not HTML Web pages. For the 232-query set, we crawled
22,909 Web pages, which are used for extracting feature
listIDF described in Section 5.3. For the 100 annotated
queries, the average number of crawled Web pages is 98.7,
the minimum is 79, both the maximum and the median are
100.

Query facet annotations. We asked human annotators
to construct query facets as ground truth. For each query, we
ﬁrst constructed a pool of terms by aggregating facet terms
in the top 10 query facets generated by diﬀerent models,
including two runs from QDM, one run from each of pLSA
and LDA using top 10 list items in each query facets, and
one run for our graphical model based approach. Then,
annotators were asked to group terms in the pool into query
facets for each query they selected. Finally, the annotator
was asked to give a rating for each constructed query facet,

3http://trec.nist.gov/data/web/09/wt09.topics.queries-
only

regarding how useful and important the query facet is. The
rating scale of good=2/fair=1 is used. Annotation statistics
are given in Table 6. There are 50 query facets pooled per
query, with 224.8 distinct facet terms per query.

Table 6: Annotation statistics

good
55.8
4.8
11.6

pooled
224.8
50.0
8.8

#terms per query
#facets per query
#terms per facet

fair
26.6
3.1
8.6

7.2 Evaluation Metrics

Query facet extraction can be evaluated from diﬀerent as-
pects. We use standard clustering and classiﬁcation evalua-
tion metrics, as well as metrics designed for this particular
task to combine diﬀerent evaluation aspects.
Notation: we use “∗” to distinguish between system gen-
erated results and human labeled results, which we used as
ground truth. For example, F denotes the system generated
query facet set, and F∗ denotes the human labeled query
facet set. For convenience, we use T to denote TF in this
section, omitting subscript F. T ∗ denotes all the facet terms
in human labeled query facet set. We use rF ∗ to denote the
rating score for a human labeled facet F ∗.
7.2.1 Effectiveness in ﬁnding facet terms
One aspect of query facet extraction evaluation is how
well a system ﬁnds facet terms. This can be evaluated using
standard classiﬁcation metrics as follows,

• facet term precision: P (T, T ∗) =
|T∩T ∗|
• facet term recall: R(T, T ∗) =
|T ∗|
• facet term F1: F T (T, T ∗) = 2|T∩T ∗|
|T|+|T ∗|

|T∩T ∗|

|T|

where facet term F1 is denoted as FT (the T stands for facet
term) to avoid confusion with a query facet F and cluster-
ing F1 deﬁned below. These metrics do not take clustering
quality into account.
7.2.2 Clustering quality
To evaluate how well a system groups facet terms cor-
rectly, similar to Dou et al. [7], we use several existing cluster
metrics, namely, Purity, NMI/Normalized Mutual Informa-
tion and F1 for clustering. To avoid confusion with facet
term F1, FT, we call F1 for facet term clustering facet clus-
tering F1, and denote it as FP (with P standing for term
pair ).
In our task, we usually have T (cid:54)= T ∗. The facet terms
in the system generated and human labeled clustering re-
sults might be diﬀerent: the system might fail to include
some human identiﬁed facet terms, or it might mistakenly
include some “incorrect” facet terms. These standard clus-
tering metrics cannot handle these cases properly. To solve
this problem, we adjust F as if only facet terms in T ∗ were
clustered by the system, since we are only interested in how
well the “correct” facet terms are clustered from these met-
rics. The adjusting is done by removing “incorrect” facet
terms (t ∈ T − T ∗) from F, and adding each missing facet
term (t∗ ∈ T ∗ − T ) to F as singletons. By this adjusting, we
do not take into account the eﬀectiveness of ﬁnding correct
facet terms.

997.2.3 Overall quality
To evaluate the overall quality of query facet extraction,
Dou et al. [7] proposed variations of nDCG (Normalized Dis-
counted Cumulative Gain), namely fp-nDCG and rp-nDCG.
It ﬁrst maps each system generated facet F to a human la-
beled facet F ∗ that covers the maximum number of terms
in F . Then, it assigns the rating rF ∗ to F , and evalu-
ates F as a ranked list of query facets using nDCG. The
discounted gains are weighted by precision and/or recall of
facet terms in F , against its mapped human labeled facet
|F ∗∩F|
F ∗. For fp-nDCG, only precision are used as weight,
.
For rp-nDCG, precision and recall are multiplied as weight,
|F ∗∩F|2
|F ∗||F| . However, to the best of our understanding, this
metric can be problematic in some cases. When two facets
F1 and F2 are mapped to a same human labeled facet F ∗,
only the ﬁrst facet F1 is credited and F2 is simply ignored,
even if it is more appropriate to map F2 to F ∗ (e.g. F2 is
exactly same as F ∗, while F1 contain only one facet term in
F ∗).

|F|

The quality of query facet extraction is intrinsically multi-
faceted. Diﬀerent applications might have diﬀerent empha-
sis in the three factors mentioned above - precision of facet
terms, recall of facet terms and clustering quality of facet
terms. We propose a metric P RFα,β to combine the three
factors together, using weighted harmonic mean. Let p =
P (T, T ∗), r = R(T, T ∗), f = F P (F,F∗), then P RFα,β can
be expressed as follows,

P RFα,β(F,F∗

) =

(α2 + β2 + 1)prf
α2rf + β2pf + pr

(10)

where α and β are used to adjust the emphasis between the
three factors. When α = β = 1, we omit the subscript part
for simplicity, i.e. P RF ≡ P RF1,1.

While P RFα,β has the ﬂexibility to adjust emphasis be-
tween the three factors, it does not take into account the dif-
ferent ratings associated with query facets. To incorporate
ratings, we use a weighted version of P (T, T ∗), R(T, T ∗) and
F P (F,F∗) in P RFα,β. We call the new metric wP RFα,β.
The weighted facet term precision, recall and FT are deﬁned
as follows

(cid:80)
(cid:80)
• weighted facet term precision: wP (T, T ∗) =
(cid:80)
(cid:80)
t∈T w(t)
• weighted facet term recall: wR(T, T ∗) =
t∈T ∩T ∗ w(t)
t∗∈T ∗ w(t∗)
• weighted facet term F1: wF T (T, T ∗) = 2wP (T,T ∗)wR(T,T ∗)
wP (T,T ∗)+wR(T,T ∗)

t∈T ∩T ∗ w(t)

where w(t) is the weight for facet term t, and assigned as
follows

(cid:26) rF ∗

1

w(t) =

if t ∈ T ∗
otherwise

Similarly, wF P (F,F∗) is computed by weighting its pair-
wise precision and recall in the same fashion as the weighted
facet term precision and recall above. Instead of w(t), we
need weight for a pair of facet terms w(t1, t2) in this calcu-
lation. We assign weight for facet term pair w(t1, t2) using
their sum, w(t1) + w(t2).
8. EXPERIMENT RESULTS
8.1 Experiment settings

We compare eﬀectiveness of the ﬁve models, QDM, pLSA,
LDA and QF-I, QF-J, on the 100-query data set. All the

models take the same candidate lists extracted/cleaned (see
Section 4.1) as input. We perform 10-fold cross validation
for training/testing and parameter tuning in all experiments
and for all models (if applicable). When training the graph-
ical model, we standardize features by removing the mean
and scaling to unit variance. We set both of the two reg-
ularizers σ and γ in Equation 5 to be 1. There are too
many negative instances (yi = 0, zi,j = 0) in the training
data, so we stratify samples by labels with the ratio of posi-
tive:negative to be 1:3. For QDM, we tune the two parame-
ters used in the clustering algorithm Diamax (the diameter
threshold for a cluster) and Wmin (the weight threshold for
a valid cluster), as well as two parameters used for selecting
facet terms in each facet (St|F > α|Sites(F )| and St|F > β).
For pLSA and LDA, we tune the number of facet terms in a
query facet. For QF-I, we tune the weight threshold for facet
terms, wmin, and the diameter threshold, dmax. For QF-J,
there are no parameter need to be tuned. We returned top
10 query facets from all the ﬁve models in all evaluation.
8.2 Finding Facet terms

To evaluate eﬀectiveness in ﬁnding facet terms, we tune
all the models on wFT, which combines both precision and
recall, and takes into account facet term weighting.

Table 7: Facet term precision, recall and F1 tuned
on wFT

P

Model
pLSA 0.284
LDA
0.292
QDM 0.407
0.347
QF-I
QF-J
0.426

wP
0.385
0.394
0.523
0.458
0.534

R

0.562
0.595
0.378
0.644
0.525

wR
0.561
0.593
0.388
0.652
0.526

FT
0.351
0.364
0.360
0.427
0.449

wFT
0.430
0.446
0.420
0.514
0.511

Table 8: Average number of facet terms in output
per query for diﬀerent models

Model #terms/query
153.1
pLSA
154.8
LDA
68.0
QDM
153.7
QF-I
QF-J
97.8

Table 7 shows facet term precision, recall and F1 and
their weighted version described in Section 7.2. QF-I and
QF-J perform relatively well for both precision and recall.
Their improvements over the other three models shown are
all signiﬁcant (p < 0.05, using paired t-test), except the im-
provements of QF-J over QDM for P and wP. The two topic
model based approaches, pLSA and LDA, have relatively
high recall and low precision. Contrarily, QDM has high
precision and low recall. This diﬀerence can be explain by
Table 8, which gives the number of facet terms output per
query from each models. QDM only outputs 68 facet terms
per query, while pLSA and LDA both output over twice that
number. One possible reason for the low precision of pLSA
and LDA is that they select facet terms solely according
to term probabilities in the learned topics (query facets in
our case) and do not explicitly incorporate query relevance.
We ﬁnd most of their facet terms are frequently-occurring
list items, which are not necessary relevant to the query.

100While the number of facet terms QF-I outputs is similar
to pLSA and LDA, QF-I obtain much higher precision and
recall, likely due to the rich set of features used. Table 9
shows the ﬁve most important item features according to
the absolute values of learned weights. Not surprisingly, list
TF/IDF features which are used to capture the likelihood of
being a coordinate term have relatively high weights, as well
as some features that are used to capture query relevance,
e.g. T F.clueIDF .

Table 9: Top 5 item features, ranked by absolute
weights

Feature
listTF.listIDF
listSF
wDF
TF.clueIDF
SF

Weight
2.6424
2.1374
-1.0754
1.0115
0.6873

From Table 7, we also ﬁnd the the weighted metrics are
usually consistent with their corresponding unweighted met-
ric. One exception is that QF-J performs better than QF-I
in FT, but it does slightly worse than QF-J in wFT. This is
likely to be caused by the high recall for QF-I, which may
include more highly rated facet terms.
8.3 Clustering Facet terms

Table 10 shows clustering performance of the ﬁve models,
which are tuned on wFP. The improvements of QF-I and
QF-J over the other three models shown are all signiﬁcant (p
< 0.05, using paired t-test). pLSA and LDA do not perform
well in clustering, which could be caused by data sparsity.
There are on average 5159 candidate lists per query, but
only 3.9 items per list.

Table 10: Facet clustering performance tuned on
wFP

Model Purity NMI
0.524
pLSA
0.793
0.511
0.773
LDA
0.565
QDM 0.871
QF-I
0.606
0.843
0.922
0.631
QF-J

FP
0.230
0.227
0.367
0.408
0.352

wFP
0.229
0.226
0.380
0.410
0.346

Table 11: Weights learned for item pair features

Weight
Feature
listContextSim
1.4944
textContextSim 0.7186
0.0817
listCooccur
lengthDiﬀ
0.0563

The better performance in clustering for QF-I and QF-J
can be explained by their incorporating factors other than
list item co-occurrence information.
In Table 11, we list
the weights learned for item pair features. Besides one item
co-occurrence related feature, listContextSim, we also ﬁnd
that textContextSim has a relatively high weight. textCon-
textSim is used to capture the similarity of the two list items
using their surrounding text, so it can help to group two
facet terms together even if they might not co-occur a lot

in candidate lists. As an example, for the query baggage
allowance, we ﬁnd diﬀerent airlines do not co-occur a lot
in candidate lists, (e.g.
delta and jetblue only co-occur
twice), but they tend to have high textContextSim (e.g.
textContextSim(delta, jetblue) = 0.81), and are therefore
grouped together by QF-I and QF-J.
8.4 Overall Evaluation

To compare overall eﬀectiveness of the ﬁve models, we
tune all the models on wPRF, and the results are reported
in Table 12.

Table 12: Overall performance tuned on wPRF

Model wP
pLSA 0.353
LDA
0.358
QDM 0.523
0.450
QF-I
0.534
QF-J

wR
0.630
0.670
0.388
0.667
0.526

wFP
0.229
0.225
0.253
0.399
0.346

wPRF
0.309
0.311
0.319
0.444
0.417

Unweighted metrics are very similar to their correspond-
ing weighted metrics in terms of conclusions, and are omit-
ted due to space limitation. Results here are consistent with
the results that were tuned on wFT or wFP. pLSA and LDA
have high recall, but low precision and FP. QDM has rela-
tively high precision, but low recall and FP. It has on average
68 facet terms per query as output, and fails to improve the
overall eﬀectiveness when including more facet terms in its
output. QF-I and QF-J are among the best two models
according to both PRF and wPRF.

Since wPRF does not account for facet ranking eﬀective-
ness, we also report fp-NDCG and rp-NDCG tuned on them-
selves in Table 13. QF-J gives the best performance for both
fp-NDCG and rp-NDCG. The improvements of QF-I and
QF-J over the other three models shown in the Table 12
and 13 are all signiﬁcant (p < 0.05, using paired t-test), ex-
cept the improvements of QF-J over QDM for wP and QF-I
over QDM for fp-nDCG.

Table 13: fp-nDCG and rp-nDCG tuned on them-
selves

fp-nDCG rp-nDCG

Model
pLSA 0.250
0.238
LDA
QDM 0.257
0.290
QF-I
QF-J
0.336

0.071
0.063
0.093
0.157
0.193

9. CONCLUSIONS

In this paper, we studied the problem of extracting query
facets from search results. We developed a supervised method
based on a graphical model to recognize query facets from
the noisy facet candidate lists extracted from the top ranked
search results. We proposed two algorithms for approxi-
mate inference on the graphical model. We designed a new
evaluation metric for this task to combine recall and pre-
cision of facet terms with grouping quality. Experimental
results showed that the supervised method signiﬁcantly out-
performs other unsupervised methods, suggesting that query
facet extraction can be eﬀectively learned.

10110. ACKNOWLEDGMENTS

This work was supported in part by the Center for In-
telligent Information Retrieval and in part under subcon-
tract #19-000208 from SRI International, prime contractor
to DARPA contract #HR0011-12-C-0016. Any opinions,
ﬁndings and conclusions or recommendations expressed in
this material are those of the authors and do not necessarily
reﬂect those of the sponsor.

11. REFERENCES
[1] E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,

M. Pa¸sca, and A. Soroa. A study on similarity and
relatedness using distributional and wordnet-based
approaches. In Proceedings of NAACL-HLT’09, pages
19–27, 2009.

[2] J. Allan and H. Raghavan. Using part-of-speech

patterns to reduce query ambiguity. In Proceedings of
SIGIR’02, pages 307–314, 2002.

[3] N. Bansal, A. Blum, and S. Chawla. Correlation

clustering. In MACHINE LEARNING, pages 238–247,
2002.

[4] C. Carpineto, S. Osi´nski, G. Romano, and D. Weiss. A
survey of web clustering engines. ACM Comput. Surv.,
41(3):17:1–17:38, July 2009.

[5] V. Dang, X. Xue, and W. B. Croft. Inferring query

aspects from reformulations using clustering. In
Proceedings of CIKM ’11, pages 2117–2120, 2011.

[6] D. Dash, J. Rao, N. Megiddo, A. Ailamaki, and

G. Lohman. Dynamic faceted search for
discovery-driven analysis. In Proceedings of CIKM ’08,
pages 3–12, 2008.

[7] Z. Dou, S. Hu, Y. Luo, R. Song, and J.-R. Wen.
Finding dimensions for queries. In Proceedings of
CIKM ’11, pages 1311–1320, 2011.

[8] Z. Harris. Distributional structure. The Philosophy of

Linguistics, 1985.

[9] M. A. Hearst. Automatic acquisition of hyponyms

from large text corpora. In Proceedings of COLING
’92, pages 539–545, 1992.

[10] L. Heyer, S. Kruglyak, and S. Yooseph. Exploring

expression data: identiﬁcation and analysis of
coexpressed genes. Genome research, 9(11):1106–1115,
1999.

[11] Y. Hu, Y. Qian, H. Li, D. Jiang, J. Pei, and Q. Zheng.

Mining query subtopics from search log data. In
Proceedings of SIGIR ’12, pages 305–314, 2012.

[12] D. Klein and C. D. Manning. Accurate unlexicalized

parsing. In Proceedings of ACL ’03, pages 423–430,
2003.

[13] J. D. Laﬀerty, A. McCallum, and F. C. N. Pereira.
Conditional random ﬁelds: Probabilistic models for
segmenting and labeling sequence data. In Proceedings
of ICML ’01, pages 282–289, 2001.

[14] D. Lawrie, W. B. Croft, and A. Rosenberg. Finding

topic words for hierarchical summarization. In
Proceedings of SIGIR ’01, pages 349–357, 2001.

[15] D. J. Lawrie and W. B. Croft. Generating hierarchical

summaries for web searches. In Proceedings of SIGIR
’03, pages 457–458, 2003.

[16] C. G. Nevill-manning, I. H. Witten, and G. W.

Paynter. Lexically-generated subject hierarchies for

browsing large collections. International Journal on
Digital Libraries, 2:111–123, 1999.

[17] M. Pa¸sca. Organizing and searching the world wide

web of facts – step two: harnessing the wisdom of the
crowds. In Proceedings of WWW ’07, pages 101–110,
2007.

[18] M. Pa¸sca and E. Alfonseca. Web-derived resources for
web information retrieval: from conceptual hierarchies
to attribute hierarchies. In Proceedings of SIGIR ’09,
pages 596–603, 2009.

[19] P. Pantel, E. Crestan, A. Borkovsky, A.-M. Popescu,
and V. Vyas. Web-scale distributional similarity and
entity set expansion. In Proceedings of EMNLP ’09,
pages 938–947, 2009.

[20] P. Pantel and D. Lin. Discovering word senses from

text. In Proceedings of KDD ’02, pages 613–619, 2002.

[21] P. Pantel, D. Ravichandran, and E. Hovy. Towards

terascale knowledge acquisition. In Proceedings of
COLING ’04, 2004.

[22] M. Pasca. Acquisition of categorized named entities

for web search. In Proceedings of CIKM ’04, pages
137–145, 2004.

[23] J. Pearl. Probabilistic reasoning in intelligent systems:

networks of plausible inference. 1988.

[24] T. Sakai and R. Song. Evaluating diversiﬁed search

results using per-intent graded relevance. In
Proceedings of SIGIR ’11, pages 1043–1052. ACM,
2011.

[25] S. Shi, H. Zhang, X. Yuan, and J.-R. Wen.

Corpus-based semantic class mining: distributional vs.
pattern-based approaches. In Proceedings of COLING
’10, pages 993–1001, 2010.

[26] K. Shinzato and K. Torisawa. Acquisition of
categorized named entities for web search. In
Proceedings of RANLP ’05.

[27] C. Silverstein, H. Marais, M. Henzinger, and

M. Moricz. Analysis of a very large web search engine
query log. SIGIR Forum, 33(1):6–12, Sept. 1999.

[28] R. Song, M. Zhang, T. Sakai, M. Kato, Y. Liu,

M. Sugimoto, Q. Wang, and N. Orii. Overview of the
ntcir-9 intent task. In Proceedings of NTCIR-9
Workshop Meeting, pages 82–105, 2011.

[29] X. Wang, D. Chakrabarti, and K. Punera. Mining
broad latent query aspects from search sessions. In
Proceedings of KDD ’09, pages 867–876, 2009.

[30] X. Wang and C. Zhai. Learn from web search logs to
organize search results. In Proceedings of SIGIR ’07,
pages 87–94, 2007.

[31] F. Wu, J. Madhavan, and A. Halevy. Identifying

aspects for web-search queries. J. Artif. Int. Res.,
40(1):677–700, Jan. 2011.

[32] X. Xue and X. Yin. Topic modeling for named entity

queries. In Proceedings of CIKM ’11, pages 2009–2012,
New York, NY, USA, 2011. ACM.

[33] X. Yin and S. Shah. Building taxonomy of web search

intents for name entity queries. In Proceedings of
WWW ’10, pages 1001–1010, 2010.

[34] H. Zhang, M. Zhu, S. Shi, and J.-R. Wen. Employing

topic models for pattern-based semantic class
discovery. In Proceedings of ACL ’09, pages 459–467,
2009.

102