Leveraging Viewer Comments

for Mood Classiﬁcation of Music Video Clips

Takehiro Yamamoto
Kyoto University, Japan

tyamamot@dl.kuis.kyoto-u.ac.jp

ABSTRACT
This short paper proposes a method to classify music video clips
uploaded to a video sharing service into music mood categories
such as “cheerful,” “wistful,” and “aggressive.” The method lever-
ages viewer comments posted to the music video clips for the mu-
sic mood classiﬁcation. It extracts speciﬁc features from the com-
ments: (1) adjectives in comments, (2) lengthened words in com-
ments, and (3) comments in chorus sections. Our experimental re-
sults classifying 695 video clips into six mood categories showed
that our method outperformed the baseline in terms of macro and
micro averaged F -measures. In addition, our method outperformed
the existing approaches that utilize lyrics and audio signals of songs.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

Keywords
Music Mood Recognition, Music Information Retrieval, User Gen-
erated Media

1.

INTRODUCTION

Music is a fundamental form of entertainment in our lives, and
the expansion of the Web has drastically increased the amount of
the digital music contents available for us. Particularly, in Japan,
a singing synthesizer software called Vocaloid [8] has made it sur-
prisingly much easier for those who have never done so before to
create original songs. As a result, such user-generated songs are
now uploaded to the Web every day. For example, as of the end of
August 2012, over 100,000 music video clips of the user-generated
Vocaloid songs (typically with one or more picture images) had
been uploaded to NicoNico Douga1, one of the most popular video
sharing services in Japan. Furthermore, over 1,000 music video
clips of Vocaloid songs are being uploaded to it every month.

1http://nicovideo.jp, http://en.wikipedia.org/wiki/Nico_Nico_Douga

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Satoshi Nakamura
Meiji University, Japan

JST CREST

satoshi@snakamura.org

While the amout of digital music contents available for us has
been rapidly increasing, the ways to search for desired songs are
still limited. Usually, users have to retrieve songs on the basis of
their meta-data such as artist name, song name, play count, or social
tags annotated by users. To offer more ways to search for desired
songs, some researchers in the ﬁeld of Music Information Retrieval
(MIR) have recently been working on the music mood recogni-
tion [7, 9]. If we can offer a mood-based music retrieval system
to users, they can search for their desired songs with their sub-
jective information needs: “I feel depressed after quarreling with
my friend, so I want to listen a cheerful song to clear my mind,” or
“I’m unfamiliar with Vocaloid songs, but I love wistful songs, so I’d
like to listen to popular wistful Vocaloid songs.” In this way, such
mood-based retrieval can offer users another way to ﬁnd songs. In
particular, it would be beneﬁcial to novices who know little about
the target music domain.

In MIR, most work on the music mood recognition basically
relies on the features extracted from the audio signals. Some re-
cently combined the lyrics of the songs with the audio signals [6,
10]. However, mood recognition is much harder than genre or artist
recognition [9]. One alternative way for the mood-based retrieval
is to use social tags related to the music moods annotated by users.
However, according to our preliminary study with 186,987 music
video clips on NicoNico Douga, only 5% of them contained the
music mood related tags. Similarly, on Last.fm2, which is a popu-
lar music social networking service, only 14% of songs had mood
related tags [6]. These results suggest that the simple exploitation
of social tags is also insufﬁcient. The goal of our work is to achieve
a more effective mood-based music retrieval system to help users
search for many songs.

As the ﬁrst step to this goal, in this paper, we propose a method to
classify music video clips uploaded to NicoNico Douga into mood
categories by leveraging viewer comments. On NicoNico Douga,
viewers can post comments at arbitrary temporal playback posi-
tions in the video clip while they are watching it (See Section 2).
These comments can be seen as viewers’ direct responses to music
video clips, thus they should contain useful information to estimate
the music mood of the music video clips.

This short paper tackles the following two issues. First, to ex-
tract useful features for the mood recognition from viewer com-
ments, we propose a method that combines three feature extraction
approaches: (1) adjectives in comments, (2) lengthened words in
comments, and (3) comments in chorus sections. In this paper, we
examined the effectiveness of the proposed method with a dataset
consisting of 695 music video clips in six mood categories (See
Section 4.3). Second, to investigate the effectiveness of viewer
comments for the music mood recognition, we also compared our

2http://www.last.fm

797(cid:53)(cid:66)(cid:72)(cid:1)

(cid:53)(cid:66)(cid:72)(cid:1)

(cid:53)(cid:66)(cid:72)(cid:1)

(cid:53)(cid:66)(cid:72)(cid:1)

(cid:53)(cid:66)(cid:72)(cid:1)

(cid:45)(cid:74)(cid:84)(cid:85)(cid:1)(cid:80)(cid:71)(cid:1)(cid:66)(cid:79)(cid:79)(cid:80)(cid:85)(cid:66)(cid:85)(cid:70)(cid:69)(cid:1)(cid:85)(cid:66)(cid:72)(cid:84)(cid:1)

(cid:53)(cid:66)(cid:83)(cid:72)(cid:70)(cid:85)(cid:1)(cid:78)(cid:86)(cid:84)(cid:74)(cid:68)(cid:1)(cid:87)(cid:74)(cid:69)(cid:70)(cid:80)(cid:1)(cid:68)(cid:77)(cid:74)(cid:81)(cid:1)

(cid:53)(cid:73)(cid:74)(cid:84)(cid:1)(cid:84)(cid:80)(cid:79)(cid:72)(cid:1)(cid:78)(cid:66)(cid:76)(cid:70)(cid:84)(cid:1)(cid:78)(cid:70)(cid:1)(cid:68)(cid:83)(cid:90)(cid:1)

(cid:72)(cid:83)(cid:70)(cid:70)(cid:70)(cid:70)(cid:66)(cid:85)(cid:1)(cid:84)(cid:80)(cid:79)(cid:72)(cid:2)(cid:2)(cid:1)

(cid:42)(cid:1)(cid:71)(cid:70)(cid:70)(cid:77)(cid:1)(cid:84)(cid:66)(cid:69)(cid:1)

(cid:42)(cid:1)(cid:77)(cid:74)(cid:76)(cid:70)(cid:1)(cid:85)(cid:73)(cid:74)(cid:84)(cid:1)(cid:81)(cid:66)(cid:83)(cid:85)(cid:1)

(cid:34)(cid:73)(cid:73)(cid:73)(cid:73)(cid:73)(cid:73)(cid:73)(cid:73)(cid:73)(cid:1)

(cid:49)(cid:80)(cid:84)(cid:85)(cid:70)(cid:69)(cid:1)(cid:68)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)(cid:66)(cid:83)(cid:70)(cid:1)(cid:80)(cid:87)(cid:70)(cid:83)(cid:77)(cid:66)(cid:74)(cid:69)(cid:1)(cid:1)
(cid:80)(cid:79)(cid:85)(cid:80)(cid:1)(cid:87)(cid:74)(cid:69)(cid:70)(cid:80)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:84)(cid:90)(cid:79)(cid:68)(cid:73)(cid:83)(cid:80)(cid:79)(cid:74)(cid:91)(cid:70)(cid:69)(cid:1)(cid:88)(cid:74)(cid:85)(cid:73)(cid:1)
(cid:81)(cid:77)(cid:66)(cid:90)(cid:67)(cid:66)(cid:68)(cid:76)(cid:1)(cid:81)(cid:80)(cid:84)(cid:74)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)(cid:80)(cid:71)(cid:1)(cid:87)(cid:74)(cid:69)(cid:70)(cid:80)(cid:1)(cid:68)(cid:77)(cid:74)(cid:81)(cid:1)

(cid:55)(cid:74)(cid:70)(cid:88)(cid:70)(cid:83)(cid:1)(cid:68)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)
(cid:81)(cid:80)(cid:84)(cid:85)(cid:70)(cid:69)(cid:1)(cid:85)(cid:80)(cid:1)(cid:87)(cid:74)(cid:69)(cid:70)(cid:80)(cid:1)(cid:68)(cid:77)(cid:74)(cid:81)(cid:1)

(cid:55)(cid:74)(cid:70)(cid:88)(cid:70)(cid:83)(cid:1)(cid:68)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)
(cid:74)(cid:79)(cid:1)(cid:68)(cid:73)(cid:80)(cid:83)(cid:86)(cid:84)(cid:1)(cid:84)(cid:70)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:84)(cid:1)

(cid:46)(cid:42)(cid:44)(cid:54)(cid:1)(cid:74)(cid:84)(cid:1)(cid:72)(cid:83)(cid:70)(cid:66)(cid:85)(cid:1)

(cid:68)(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)(cid:80)(cid:77)(cid:77)(cid:77)(cid:77)(cid:77)(cid:1)

(cid:84)(cid:80)(cid:1)(cid:68)(cid:86)(cid:86)(cid:86)(cid:86)(cid:85)(cid:70)(cid:2)(cid:2)(cid:1)

(cid:77)(cid:80)(cid:87)(cid:70)(cid:77)(cid:90)(cid:1)(cid:84)(cid:80)(cid:79)(cid:72)(cid:1)

(cid:36)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:1)(cid:74)(cid:79)(cid:81)(cid:86)(cid:85)(cid:1)(cid:71)(cid:80)(cid:83)(cid:78)(cid:1)

(cid:81)(cid:77)(cid:66)(cid:90)(cid:67)(cid:66)(cid:68)(cid:76)(cid:1)(cid:81)(cid:80)(cid:84)(cid:74)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)

(cid:36)(cid:73)(cid:80)(cid:83)(cid:86)(cid:84)(cid:1)
(cid:84)(cid:70)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)

(cid:36)(cid:73)(cid:80)(cid:83)(cid:86)(cid:84)(cid:1)
(cid:84)(cid:70)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)

(cid:85)(cid:85)(cid:1)

(cid:36)(cid:86)(cid:83)(cid:83)(cid:70)(cid:79)(cid:85)(cid:1)(cid:81)(cid:77)(cid:66)(cid:90)(cid:67)(cid:66)(cid:68)(cid:76)(cid:1)(cid:81)(cid:80)(cid:84)(cid:74)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)

Figure 1: Overview of viewer interface in NicoNico Douga.

method with the existing techniques that use lyric and audio signal
features (See Section 4.4).
2. OVERVIEW OF NICONICO DOUGA

NicoNico Douga is one of the most popular video sharing ser-
vices in Japan. It had about 29.6 million signed-up users as of the
end of June 2012, and over 5,000 video clips are uploaded to it ev-
ery day. Figure 1 shows a viewer interface in NicoNico Douga. One
unique feature of this service is that viewers can post comments
at arbitrary playback positions to the video clip, and such com-
ments of many other viewers are overlaid directly onto the video
and synchronized to a speciﬁc playback position. This gives view-
ers a sense of sharing the viewing experiences of the video with
others, who originally watched the video at different times.

In addition to its unique comment feature, like other content
sharing services such as YouTube and Flickr, viewers can anno-
tate tags to the video. In this work we employ such tags to build
the data for the music mood recognition (See Section 4.1).

Recently some researchers have been working on leveraging viewer

comments on the video sharing services like YouTube to improve
video retrieval [2, 3]. While comment analyses of NicoNico Douga
have also attracted the attension of some researchers [13], to our
knowledge, our method is the ﬁrst to use viewer comments for mu-
sic mood classiﬁcation.
3. FEATURE EXTRACTION FROM VIEWER

COMMENTS

In this work, we treat music mood recognition of a music video
clip as a multiclass classiﬁcation problem. That is, given a mu-
sic video clip, we classify it into one of the predeﬁned mood cat-
egories. Figure 2 shows the overview of our feature extraction
methods from viewer comments. To extract effective features from
comments, our proposed method combines three feature extraction
approaches: (1) adjectives in comments, (2) lengthened words in
comments, and (3) comments in chorus sections. The rest of this
section describes how each method extracts features from com-
ments.
3.1 Adjectives

One simple approach to obtaining features is to extract Bag-of-
Words from viewer comments. However, comments posted to a
music video clip may contain various pieces of information, but
most have nothing to do with the mood of the song. If we extract
unnecessary features, they may deteriorate the classiﬁcation perfor-
mance.

In this work, we focused on adjectives, rather than all Bag-of-
Words, as features. It is natural to think that the viewers who are
listening a wistful song may write comment about the clip such as
“I feel sad,” or “This song is tear-jerking.” In this way, adjectives
have strong relationship with music moods.

The method works as follows. The method ﬁrst splits all the

comments into words by using a morphological analyzer. The method

(cid:39)(cid:70)(cid:66)(cid:85)(cid:86)(cid:83)(cid:70)(cid:1)(cid:70)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)(cid:78)(cid:70)(cid:85)(cid:73)(cid:80)(cid:69)(cid:84)(cid:1)

(cid:674)(cid:781)(cid:675)(cid:1)

(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:74)(cid:79)(cid:72)(cid:1)
(cid:66)(cid:69)(cid:75)(cid:70)(cid:68)(cid:85)(cid:74)(cid:87)(cid:70)(cid:84)(cid:1)

(cid:674)(cid:782)(cid:675)(cid:1)

(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:74)(cid:79)(cid:72)(cid:1)

(cid:77)(cid:70)(cid:79)(cid:72)(cid:85)(cid:73)(cid:70)(cid:79)(cid:70)(cid:69)(cid:1)(cid:88)(cid:80)(cid:83)(cid:69)(cid:84)(cid:1)

(cid:674)(cid:783)(cid:675)(cid:1)

(cid:36)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)(cid:74)(cid:79)(cid:1)
(cid:68)(cid:73)(cid:80)(cid:83)(cid:86)(cid:84)(cid:1)(cid:84)(cid:70)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:84)(cid:1)

(cid:52)(cid:81)(cid:77)(cid:74)(cid:85)(cid:1)(cid:68)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)

(cid:74)(cid:79)(cid:85)(cid:80)(cid:1)(cid:88)(cid:80)(cid:83)(cid:69)(cid:84)(cid:1)

(cid:37)(cid:70)(cid:85)(cid:70)(cid:68)(cid:85)(cid:1)(cid:1)

(cid:77)(cid:70)(cid:79)(cid:72)(cid:85)(cid:73)(cid:70)(cid:79)(cid:70)(cid:69)(cid:1)(cid:88)(cid:80)(cid:83)(cid:69)(cid:84)(cid:1)

(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:1)(cid:68)(cid:80)(cid:78)(cid:78)(cid:70)(cid:79)(cid:85)(cid:84)(cid:1)
(cid:74)(cid:79)(cid:1)(cid:68)(cid:73)(cid:80)(cid:83)(cid:86)(cid:84)(cid:1)(cid:84)(cid:70)(cid:68)(cid:85)(cid:74)(cid:80)(cid:79)(cid:1)

(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:1)(cid:1)
(cid:66)(cid:69)(cid:75)(cid:70)(cid:68)(cid:85)(cid:74)(cid:87)(cid:70)(cid:84)(cid:1)

(cid:47)(cid:80)(cid:83)(cid:78)(cid:66)(cid:77)(cid:74)(cid:91)(cid:70)(cid:1)

(cid:77)(cid:70)(cid:79)(cid:72)(cid:85)(cid:73)(cid:70)(cid:79)(cid:70)(cid:69)(cid:1)(cid:88)(cid:80)(cid:83)(cid:69)(cid:84)(cid:1)

(cid:34)(cid:81)(cid:81)(cid:77)(cid:90)(cid:1)(cid:78)(cid:70)(cid:85)(cid:73)(cid:80)(cid:69)(cid:84)(cid:1)

(cid:1)(cid:9)(cid:18)(cid:10)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:9)(cid:19)(cid:10)(cid:1)

(cid:38)(cid:89)(cid:85)(cid:83)(cid:66)(cid:68)(cid:85)(cid:70)(cid:69)(cid:1)(cid:71)(cid:70)(cid:66)(cid:85)(cid:86)(cid:83)(cid:70)(cid:84)(cid:1)

(cid:77)(cid:80)(cid:87)(cid:70)(cid:77)(cid:90)(cid:1)
(cid:72)(cid:83)(cid:70)(cid:66)(cid:85)(cid:1)

(cid:68)(cid:80)(cid:80)(cid:77)(cid:1)
(cid:68)(cid:86)(cid:85)(cid:70)(cid:1)

(cid:77)(cid:80)(cid:87)(cid:70)(cid:77)(cid:90)(cid:1)
(cid:68)(cid:86)(cid:85)(cid:70)(cid:1)

(cid:46)(cid:86)(cid:77)(cid:85)(cid:74)(cid:68)(cid:77)(cid:66)(cid:84)(cid:84)(cid:1)(cid:68)(cid:77)(cid:66)(cid:84)(cid:84)(cid:74)(cid:112)(cid:70)(cid:83)(cid:1)

(cid:9)(cid:52)(cid:55)(cid:46)(cid:10)(cid:1)

Figure 2: Overview of our feature extraction method.

(cid:68)(cid:86)(cid:85)(cid:70)(cid:1)(cid:674)(cid:38)(cid:84)(cid:85)(cid:74)(cid:78)(cid:66)(cid:85)(cid:70)(cid:69)(cid:1)(cid:78)(cid:80)(cid:80)(cid:69)(cid:1)(cid:68)(cid:66)(cid:85)(cid:70)(cid:72)(cid:80)(cid:83)(cid:90)(cid:675)(cid:1)

then extracts adjectives from the words and uses them as the fea-
tures for the classiﬁcation.
3.2 Lengthened Words

The method described in Section 3.1 applies a morphological
analysis to comments and extracts adjectives as features. For viewer
comments in NicoNico Douga, however, a morphological analyzer
often fails to split the comments into the correct words, since the
form of comments are far from standard Japanese. One good En-
glish equivalent is tweets on Twitter3, as reported by Brody and
Diakopoulos [1]. Brody and Diakopoulos pointed out that some
words in tweets are often lengthened by repeating some charac-
ters. For example, word “cool” is often lengthened as “cooolll”
or “coooooollllllll” on Twitter. Brody and Diakopoulos further
pointed out that these lengthened words are strongly related to the
sentiments (positive or negative) of the tweets and proposed a method
to leverage such lengthened words to analyze these sentiment. On
NicoNico Douga, similar to the above case on Twitter, viewers of-
ten write comments with such lengthened words. We hypothesized
that such lengthened words in comments are also good indicators of
the music moods and can compensate for a morphological analysis
approach described in Section 3.1.

The method works as follows. The method ﬁrst detects length-
ened words from the comments by using the method proposed by
Brody and Diakopoulos [1]. It then obtains the normalized forms
of these lengthened words and uses them as features. For example,
both “coooolllll” and “cooool” are normalized to “cool” and treated
as the same feature.
3.3 Comments in Chorus Sections

Neither approach described in Sections 3.1 and 3.2 considers as-
sociated playback positions of viewer comments. However, the
likelihood that a comment is related to the mood of the music video
clips might depend on its associated playback position. For exam-

3http://twitter.com/

798Table 1: Dataset used in experiments.

Mood category

# of tags

# of clips

wistful
cute
fresh
cool

cheerful
aggressive

Total

4
9
7
9
5
4
38

160
153
145
109
84
44
695

ple, comments posted before the music actually starts might have
nothing to do with the mood of the music. In this paper, we hypoth-
esized that comments posted in the chorus4 section of the music are
more likely to relate to the mood of the music, as the chorus sec-
tions are a song’s most representative and memorable portions.

The method works as follows. The method ﬁrst detects the cho-
rus sections of the song of the target music video clip. To detect
chorus sections of songs, in this work we use the method pro-
posed by Goto [4]. After detecting the chorus sections by using
Goto’s method, our method then collects comments whose associ-
ated playback positions are within the detected chorus sections and
extracts the features from them by using the methods described in
Sections 3.1 and 3.2.

Note that the three methods described above might extract the
same feature in terms of its characters (e.g., all methods might ex-
tract “cool” as features). We thus assign different feature IDs to
them in order to distinguish them.
4. EVALUATION
4.1 Dataset

To evaluate our method, we built a dataset from music video clips
in NicoNico Douga. As for the music moods, many researchers
have proposed ways to model moods of music [5, 11]. In this work,
we took an approach similar to Hu et al.’s work [6]. Hu et al.
have proposed a method of mood recognition that uses both audio
signal and lyric features. To evaluate their method, they created a
dataset from the social tags annotated to the songs on Last.fm. They
picked up 135 music mood related tags and manually merged them
into 18 mood categories. The created mood categories are public
to researchers as MIREX [7] Mood Tag Dataset5. Their approach,
which creates a dataset from social tags, has two merits: (1) ﬂexible
mood categories that suit the target domain can be obtained, and
(2) ground truth (i.e., which song belongs to which mood category)
can be almost automatically created from the relationships between
tags and songs.

In our work, we ﬁrst manually collected 50 tags related to the
music moods from NicoNico Douga. Then we carefully merged
similar tags into the same group so that the resulting mood groups
would be similar to the Mood Tag Dataset. After this operation, we
obtained 11 mood categories. Then, for each mood category, we
retrieved music video clips that had (1) more than 200 viewer com-
ments and (2) one of the associated tags in the mood category. Fi-
nally, we dropped mood categories that had fewer than 40 retrieved
video clips, resulting in six mood categories with 695 associated
music video clips. Table 1 shows the statistics of the dataset used
in the experiments. In the table, “# of tag” represents the associ-
ated tags with the mood, and “# of clips” represents the number of
music video clips in the mood category.

4.2 Settings

We used Support Vector Machine (SVM) to construct a multi-
class classiﬁer. We used LIBSVM6 to implement the SVM and
chose the linear kernel, since the preliminarily experiment showed
that the linear kernel outperformed the other kernels. We used
MeCab7 as a Japanese morphological analyzer. Following Hu et
al.’s work [6], each feature was weighted by the tf-idf weighting.

For each mood category, F -measure was calculated over a ﬁve-
fold cross validation. We also computed the macro and micro av-
erages over the six mood categories to evaluate the classiﬁcation
performance.
4.3 Experiment #1: Comparison of Feature

Extraction Methods from Comments

The purpose of this experiment is to examine the effectiveness
of our methods for feature extraction from viewer comments. First,
we explain the baseline and proposed methods we used and then
discuss their results.

Methods. For the experiment, we prepared one baseline and
ﬁve proposed methods. For the baseline, BoW uses all the nouns,
verbs, and adjectives obtained from viewer comments as features.
As for the proposed methods, Adj, Len, and Cho correspond to
the methods described in Section 3.1, 3.2, and 3.3, respectively. We
also prepared the combination of these methods: Adj+Len uses
features extracted by both Adj and Len, and Adj+Len+Cho uses
all the features extracted by Adj, Len, and Cho.

Results. Table 2 shows the result of classiﬁcation performances
obtained from the methods described above. First, when we com-
pare the results of Adj, Len, and Cho, we ﬁnd that Adj achieved
the highest macro (.632) and micro (.676) F -measures. In addi-
tion, when we compare Adj with BoW, we can see that Adj also
achieved higher performance than the baseline in terms of macro
and micro F -measures. These results indicate that adjectives in
viewer comments play an important role in recognizing the mu-
sic moods. When we see the results of Len, we found that Len
could recognize the “aggressive” category better than the other cat-
egories. Note that Len uses lengthened words like “coooolll.” Such
words might be likely to be posted to express viewers’ highly in-
tense emotions, rather than calmness or relaxation. Thus, Len
might be useful to recognize the mood categories that are related to
highly intense moods (high arousal space in Russel’s model [11]).
From the table, we can see that Adj+Len+Cho outperformed

the other methods in terms of macro and micro averaged F -measures.
This result suggests that combining the different approaches can
improve the music mood recognition. In particular, note that Cho
considers associated playback positions of viewer comments. This
indicates that considering the playback positions of comments can
improve the music mood recognition. Although in this work we
just combined the features extracted from different methods with
the linear kernel, we plan to explore a more sophisticated method
to integrate features extracted from these three methods.
4.4 Experiment #2: Comparison of Comments,

Lyrics, and Audio Signals

The purpose of this experiment is to examine the effectiveness
of our method, which uses viewer comments for the mood recog-
nition, compared with the existing approaches that uses lyric and
audio signal features [6].

Methods. Following Hu et al.’s work, we prepared two methods
named Lyric and Audio, which extract features from the lyrics

4a.k.a. refrain
5http://music-ir.org/mirex/wiki/2010:Audio_Tag_Classiﬁcation

6http://www.csie.ntu.edu.tw/˜cjlin/libsvm/
7https://code.google.com/p/mecab/

799Table 2: Comparison of F -measure for methods.

Table 3: Comparison of F -measure for methods.

baseline

BoW

Adj

Len

proposed methods
Adj+Len

Cho

Adj+Len
+Cho

.636
.627
549
.638
.228
.792
.578
.586

.758
.731
.691
.659
.480
.469
.632
.676

.610
.708
.492
.635
.246
.739
.572
.578

.678
.727
.568
.577
.334
.528
.568
.607

.708
.780
.621
.771
.432
.744
.659
.674

.722
.782
.638
.672
.432
.742
.665
.683

wistful
cute
fresh
cool

s
e
i
r
o
g
e
t
a
c

cheerful
aggressive
Macro average
Micro average

Comment Lyric

Audio

Lyric
+Audio

.722
.782
.638
.672
.432
.742
.665
.683

.490
.498
.473
.424
.203
.080
.362
.434

.470
.505
.311
.333
.072
.119
.302
.379

.561
.518
.457
.466
.227
.080
.385
.465

Comment
+Lyric
+Audio
.736
.772
.678
.663
.443
.723
.670
.693

wistful
cute
fresh
cool

s
e
i
r
o
g
e
t
a
c

cheerful
aggressive
Macro average
Micro average

and audio signals, respectively. For the lyric features, we collected
the lyrics for the music video clips in the dataset from the lyric
database8. We then split the lyrics into words and extracted nouns,
verbs, and adjectives from them for the lyric features, weighting
with the tf-idf weighting. As for the audio signal features, also
following Hu et al.’s work, we used MARSYAS [12], the best per-
forming audio system evaluated in the MIREX 2007 audio mood
classiﬁcation task [7]. MARSYAS extracts spectral features such
as Spectral Centroid, Rolloff, and MFCCs. We extracted the audio
signals from the music video clips in the dataset and then applied
MARSYAS to them to extract 63 dimensional audio features.

In this experiment, we compare ﬁve methods. Comment repre-
sents the method that extracts features from viewer comments and
exactly equals to Adj+Len+Cho described in Section 4.3. Lyric
+Audio use both features extracted by Lyric and Audio, and
Comment+Lyric+Audio uses all features extracted from Co-
mment, Lyric, and Audio.

Results. Table 3 shows the classiﬁcation results of each method.
From the table, when we compare the results of Comment, Lyric,
and Audio, we can see that Comment outperformed the others.
This suggests that the viewer comments are useful resources for
the mood recognition. Hu et al. reported that the classiﬁcation ac-
curacy is slightly improved when combining lyric and audio signal
features are combined. The similar results can be seen from the
results of Lyric, Audio, and Lyric+Audio: the classiﬁcation
performance of Lyric+Audio improved on these of Lyric and
Audio.

Finally, we can see that Comment+Lyric+Audio outperformed

the other methods in terms of the macro (.670) and micro (.693)
averaged F -measures. Although our dataset is not large, this result
indicates that combining features extracted from different sources
can enhance the performance of the music mood recognition.

One primary limitation of our method is that it cannot be ap-
plied to music video clips that have few comments, as they have
just been uploaded a few days previously. In contrast, a classiﬁ-
cation method based on audio signals can be applied to such new
videos. To achieve a more ﬂexible classiﬁcation method, we plan
to integrate comments, lyrics, and audio signals differently, rather
than just combing features from them. For example, for new video
clips, we ﬁrst apply an audio-based classiﬁcation method, and a
few days later when the video clips receive enough comments, a
comment-based classiﬁcation will be applied to it. We believe such
a ﬂexible integration of different methods might be an interesting
research topic for music mood classiﬁcation.
5. CONCLUSIONS

In this study, we examined the effectiveness of viewer comments
for the music mood classiﬁcation. In the future, we plan to continue

8http://www5.atwiki.jp/hmiku/

this research and develop a more sophisticated method for extract-
ing features from viewer comments. Also, we would like to explore
a method to integrate comments, lyrics, and audio signals for more
ﬂexible and reliable music mood recognition.

6. ACKNOWLEDGMENTS

This work was supported in part by CREST, JST and KAKENHI

(#23680006, #24240013).
7. REFERENCES
[1] S. Brody and N. Diakopoulos.

Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using word
lengthening to detect sentiment in microblogs. In Proc. of
EMNLP, pages 562–570, 2011.

[2] C. Eickhoff, W. Li, and A. de Vries. Exploiting user

comments for audio-visual content indexing and retrieval. In
Proc. of ECIR, to appear, 2013.

[3] K. Filippova and K. Hall. Improved video categorization

from text metadata and user comments. In Proc. of SIGIR,
pages 835–842, 2011.

[4] M. Goto. A chorus section detection method for musical

audio signals and its application to a music listening station.
IEEE Transactions on Audio, Speech and Language
Processing, 14(5), pages 1783–1794, 2006.

[5] K. Hevner. Experimental studies of the elements of

expression in music. The American Journal of Psychology,
48(2), pages 246–268, 1936.

[6] X. Hu, J. Downie, and A. Ehmann. Lyric text mining in

music mood classiﬁcation. In Proc. of ISMIR, pages
411–416, 2009.

[7] X. Hu, J. Downie, C. Laurier, M. Bay, and A. Ehmann. The

2007 MIREX audio mood classiﬁcation task: Lessons
learned. In Proc. of ISMIR, pages 462–467, 2008.

[8] H. Kenmochi and H. Ohshita. Vocaloid–commercial singing

synthesizer based on sample concatenation. In Proc. of
INTERSPEECH, pages 4009–4010, 2007.

[9] Y. Kim, E. Schmidt, R. Migneco, B. Morton, P. Richardson,

J. Scott, J. Speck, and D. Turnbull. Music emotion
recognition: A state of the art review. In Proc. of ISMIR,
pages 255–266, 2010.

[10] C. Laurier, J. Grivolla, and P. Herrera. Multimodal music

mood classiﬁcation using audio and lyrics. In Proc. of
ICMLA, pages 688–693, 2008.

[11] J. Russell. A circumplex model of affect. Journal of

Personality and Social Psychology, 39(6), pages 1161–1178,
1980.

[12] G. Tzanetakis and P. Cook. MARSYAS: A framework for

audio analysis. Organised sound, 4(3), pages 169–175, 1999.

[13] K. Yoshii and M. Goto. MusicCommentator: Generating
comments synchronized with musical audio signals by a
joint probabilistic model of acoustic and textual features. In
Proc. of EC, pages 85–97, 2009.

800