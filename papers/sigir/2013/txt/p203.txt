An Unsupervised Topic Segmentation Model Incorporating

Word Order∗

Shoaib Jameel and Wai Lam

Department of Systems Engineering and Engineering Management,

The Chinese University of Hong Kong,

Hong Kong.

{msjameel, wlam}@se.cuhk.edu.hk

ABSTRACT
We present a new unsupervised topic discovery model for
a collection of text documents.
In contrast to the major-
ity of the state-of-the-art topic models, our model does not
break the document’s structure such as paragraphs and sen-
tences. In addition, it preserves word order in the document.
As a result, it can generate two levels of topics of diﬀer-
ent granularity, namely, segment-topics and word-topics. In
addition, it can generate n-gram words in each topic. We
also develop an approximate inference scheme using Gibbs
sampling method. We conduct extensive experiments using
publicly available data from diﬀerent collections and show
that our model improves the quality of several text mining
tasks such as the ability to support ﬁne grained topics with
n-gram words in the correlation graph, the ability to seg-
ment a document into topically coherent sections, document
classiﬁcation, and document likelihood estimation.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval - Clustering
Keywords
Topic Modeling, Topic Segmentation, N-gram words, Gibbs
Sampling, Document Classiﬁcation
1.

INTRODUCTION

Simplicity may not always lead to greatness! Topic mod-
els such as Latent Dirichlet Allocation (LDA) [5] have been
∗
The work described in this paper is substantially supported by
grants from the Research Grant Council of the Hong Kong Special
Administrative Region, China (Project Code: CUHK413510) and
the Direct Grant of the Faculty of Engineering, CUHK (Project
Codes: 2050476 and 2050522). This work is also aﬃliated with
the CUHK MoE-Microsoft Key Laboratory of Human-Centric
Computing and Interface Technologies. The authors would like to
thank Xiaojun Qian for his help with the experiments and some
discussions related to the technical content in the paper.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2034-4/13/07 ...$15.00.

widely used to ﬁnd topics in a document collection. Typi-
cally, a topic is a probability distribution over words. But
the LDA model has been criticized for its bag-of-words as-
sumption [32] as the model does not consider the structural
information inherent in the text which could help tap extra
knowledge from the text.
It is well known that the bag-
of-words assumption is mainly a simplifying assumption to
reduce the complexity of the model [24].

Some recent topic models have demonstrated better qual-
itative and quantitative performance when the bag-of-words
assumption is relaxed [18], [20], [1], and [23]. Maintain-
ing the word order during the processing of documents in-
troduces some computational overhead, but it allows us to
achieve what the bag-of-words models cannot do in general
[15]. In order to address the shortcoming inherent in the LDA
model, the authors in [39] introduced the Topical N-gram
model (TNG) to ﬁnd n-gram words in topics. By n-gram we
mean a word can be a unigram, a bigram, a trigram word,
etc. The TNG model has the ability to decide whether to
form a unigram or a bigram during the topic discovery pro-
cess. The TNG model mainly extends the LDA Collocation
model [14] (LDACOL) and the Bigram Topic Model [35] (BTM).
All these models advocate that the word order in a docu-
ment is essential. But one shortcoming of these models is
that they lack the ability to consider the document’s struc-
ture such as paragraphs and sentences. Thus they cannot
segment a document into coherent topics. This sometimes
becomes essential in tasks such as tackling the word sense
disambiguation problem as shown in [15], segmenting news
articles and ﬁnding topics in each segment [30], topic detec-
tion and tracking [40], and a plethora of other tasks which
motivate us to explore deeper into the topic segmentation
model with n-gram topic word discovery.

We propose a new unsupervised topic discovery model,
called NTSeg, for a collection of text documents. NTSeg main-
tains the segment structure of the document such as para-
graphs and sentences. In addition, it preserves the word or-
der in the document. NTSeg can help capture topical changes
in the document from one segment to another. As a result,
it can generate two levels of topics of diﬀerent granularity,
namely, segment-topics and word-topics. In addition, it can
generate n-gram words in each word-topic. We also con-
duct extensive experiments on publicly available datasets to
demonstrate the superiority of NTSeg in comparison to the
state-of-the-art models in solving several text mining tasks
such as the ability to support ﬁne grained topics with n-
gram words in the correlation graph, the ability to segment

203a document into topically coherent sections, document clas-
siﬁcation, and document modeling.

2. RELATED WORK

Topic models: Some previous works have considered the
order of the words in a document during the topic discov-
ery process. For example, in [35], the author described the
Bigram Topic Model (BTM) which incorporates the hierarchi-
cal Dirichlet language model into the unigram based topic
model in order to capture the dependencies between the
words in sequence. One drawback of this model is that it
only generates bigram words in a topic. This limitation was
addressed in the LDA Collocation model (LDACOL) [14] which
introduces a new set of status variables in the model called
the bigram status variable. This variable indicates whether
two consecutive words form a bigram or not. One limita-
tion of the LDACOL model is that it cannot decide whether
to form a unigram or a bigram for the same two consec-
utive words depending on their nearby context. Another
issue with the model is that only the ﬁrst term in a bigram
has a topic assignment. One needs to make some assump-
tions in order to give the topic assignment to every term
in a bigram [38], and [37]. The limitations inherent in the
LDACOL model have been addressed in the Topical N-gram
model, TNG [39]. The TNG model allows for consecutive words
in a topic model to depend on each other in that they can
be selected either to come from a unigram term distribu-
tion or from a bigram distribution. However, one limitation
of the Topical N-gram model is that it cannot segment a
document into topically coherent units, known as topic seg-
mentation task. There are also other limitations and they
have been addressed recently in [23] where the authors gave
the same topic assignment to every term in a phrase and the
words share the same probability mass in the phrase by in-
troducing the hierarchical Pitman-Yor processes (HPYP) [34]
in their model named PDLDA. The PDLDA model also cannot
perform topic segmentation. Incorporating the HPYP model
in our NTSeg model to capture phrasal terms would make our
model NTSeg overly complex leading to ineﬃcient processing
of large text corpora.

Recently in [26] the authors proposed a topic model where
n-gram words are viewed as random variables. The authors
manually generated n-gram words from the dataset and con-
sidered those n-grams as part of the vocabulary. This model
focuses on handling discussions or debates. In [19], the au-
thor proposed another topic n-gram model which mines sen-
timents from text corpora. Wang et al.
[36] proposed an
n-gram topic model for academic retrieval. They apply an
online inference algorithm and ﬁnd unigrams and bigrams
in a topic. In [42], the authors presented an n-gram based
news thread extraction model that uses the TNG model with
a background distribution. A method which has adopted a
diﬀerent approach to n-gram topic modeling is [20]. It com-
bines the paradigms of frequent pattern mining and topic
modeling. All the topic models proposed above do not em-
ploy topic segmentation. In [41], the authors proposed an
Auto Topic Number LDA (ATNLDA) model for topic segmen-
tation and apply the model on stem cell research litera-
ture. This model can automatically calculate the optimal
topic number. A diﬀerence between ATNLDA and ours is that
ATNLDA does not consider the word order.

Topic Correlations: Shaﬁei et al. in [31] described a latent
Dirichlet co-clustering method, known as LDCC, which cap-

tures correlation between word-topics and document-topics
(or super-topics). The LDCC model is a hierarchical topic
model where unigram words are assigned to the word-topics
and paragraphs are assigned to the document-topics. The
model can also ﬁnd correlations between the word-topics and
the document-topics. In [3], the authors proposed correlated
topic model (CTM) that can capture the evolution of topics
over time without considering the word order. In [22], the
authors proposed Pachinko Allocation Model (PAM) where
the concept of topic is extended to not only including distri-
butions over words, but also distributions over topics. This
model assumes the structure of an arbitrary DAG in which
each leaf is associated with a word and each non-leaf node
is a distribution over its children. The interior nodes are
distributions over topics called super-topics. Recently, in
[6], the authors presented a new model to ﬁnd correlation
among topics in a corpus using the Generalized Dirichlet
distribution model instead of the Dirichlet distribution.

One similarity between our work and the topic correla-
tion models is that our work also introduces two levels of
topic assignments. For example, word-topics and document-
topics as described in Shaﬁei et al. [31] share the same no-
tion as the word-topics and segment-topics described in our
proposed approach. We adopt the name segment-topics be-
cause known text segments such as paragraphs or sentences
are assigned to the segment-topics. However, all the correla-
tion topic models mentioned above assume exchangeability
among the words in a document. The importance of cap-
turing n-gram words is that it reduces the ambiguity in the
mind of the reader as to what the word is referring to in the
correlation graph. For example, presenting the word “net-
works” in a topic is ambiguous especially for a person who
is not a domain expert. In contrast, showing the word “neu-
ral networks” in a topic signiﬁcantly reduces ambiguities. In
addition, all the topic correlation models mentioned above
cannot conduct topic segmentation.

Topic segmentation:

In [16], the author presented the
TextTiling algorithm for segmenting a textual discourse
into coherent segments. TextTiling is a domain-independent
text segmentation technique that assigns a score to each
segment boundary candidate based on a cosine similarity
measure between chunks of words appearing to the left and
right of the candidate. Then the segment boundaries are
placed at the locations of valleys under this measure, and
are then adjusted to coincide with known paragraph bound-
aries. TextTiling algorithm does not ﬁnd topics. In [12],
the authors presented a topic segmentation based approach
which takes into account the lexical cohesion. The authors
modeled lexical cohesion in a Bayesian context and obtained
signiﬁcant improvement against the existing models. They
assumed that words are drawn from a multinomial language
model. Our work is signiﬁcantly diﬀerent from the above two
models in that ours is a domain-independent topic segmen-
tation method based on a topic model. Apart from ﬁnding a
low-dimensional representation of the original vector space,
our model also segments a document and ﬁnds n-gram words
in each topic.

In [25], the authors presented a method for topic segmen-
tation based on topic modeling where the authors used the
LDA model to segment texts into coherent topics that assume
exchangeability among the words in a document. In [4], the
authors described another topic segmentation method by
unifying the segmenting hidden Markov model in [27] and

204Ω

ρ

α

K

π(d)

τ (d)

y(d)
s−1

y(d)
s

c(d)
s−1

θ(d)
s−1

c(d)
s

z(d)
s−1,i−1

z(d)
s−1,i

z(d)
s−1,i+1

x(d)
s−1,i

x(d)
s−1,i+1

w(d)
s−1,i−1

w(d)
s−1,i

w(d)
s−1,i+1

θ(d)
s

z(d)
s,i

w(d)
s,i

x(d)
s,i+1

z(d)
s,i+1

w(d)

s,i+1

z(d)
s,i−1

w(d)
s,i−1

x(d)
s,i

β

φ

Z

ψ

ZV

γ

σ

ZV

Figure 1: Our proposed model NTSeg in plate notation.

M

δ

the aspect model in [17]. Recently, in [30], the authors pre-
sented TopicTiling based on LDA. Their algorithm is very
similar to the TextTiling algorithm, and segments docu-
ments using the LDA topic model. Also, in [29], the authors
presented methods in which topic models can help segmen-
tation based methods by extending their own TopicTiling
model. Similarly, in [32] the authors proposed a topic seg-
mentation based topic model, known as LDSEG, where they
assumed that the word order is not important. The au-
thors introduced the notion of topic hierarchy where sen-
tences are assigned to the document-topics and unigrams
are assigned to the word-topics. The LDSEG model repre-
sents documents as a distribution over document-topics or
super-topics in such a way that each segment is assigned a
super-topic or document-topic, which is then used to choose
the parameters of a document independent Dirichlet dis-
tribution from which the word-topics for the segment is
drawn.
In order for consecutive segments to have similar
word-topic distributions, an additional binary variable per
segment encodes whether the document-topic is forced to
be the same as that of the previous segment.
In [9], the
authors proposed a topic model based hierarchical segmen-
tation approach where they assumed that the word order
within the segment is not important and apply variational
Bayesian Expectation-Maximization procedure for comput-
ing the posterior inference. This model has been designed for
segmenting the speech data. In [11], the authors proposed a
collapsed Gibbs sampler for the topic segmentation problem

for a faster posterior inference. They employ a hierarchi-
cal Pitman-Yor processes to handle hierarchical modeling,
which our model does not incorporate. In [33], the authors
presented a topic segmentation model which does not ﬁnd
topics in a segment. In contrast to the above models, our
model does not assume exchangeability among the words
in a document. In [8], the authors proposed a subsequence
based topic segmentation approach which uses a suﬃx tree
model for representing text and measures coherence between
sentences based on subsequence. Their model maintains the
order of the words in each segment, but the model does not
ﬁnd collocations in that text segment.

3. OUR PROPOSED MODEL (NTSeg)

We depict our proposed NTSeg model in Figure 1 using a
graphical model in plate notation where shaded circles rep-
resent observed variables and unshaded ones are the latent
variables. Each document, in general, is organized as atomic
segments such as paragraphs or sentences. Our model pre-
serves this structure. One characteristic of our model is
that a document comprises of several topically coherent seg-
ments. Another characteristic of our model is due to the
preservation of the ordering of the words it is able to capture
word-collocations. Thus our proposed model is no longer in-
variant to the reshuﬄing of the words in each segment.

For a properly written discourse comprehension, docu-
ments are generally composed of coherent segments which

205are semantically linked to one another so that a reader could
relate the storyline as one moves forward in the discourse
[21]. Our model comprises of two levels of topic of diﬀer-
ent granularity. One is the segment-topic to which atomic
segments in a document are assigned and the ordering of
segments as they appear in the document deﬁnes the topic
change-points in the document. The other is the word-
topic to which n-gram words in the segment are assigned.
Segment-topics come from a predeﬁned number of segment-
topics K. Each segment-topic comprises of a mixture of
several word-topics where the mixture coeﬃcients uniquely
specify the segment-topic. Word-topics come from a prede-
ﬁned number of word-topics Z. In general, the number of
segment-topics will be less than the number of word-topics.
The reason is that the number of segments in a document is
less than compared to the number of words [31].

In the graphical model shown in Figure 1, M denotes the
number of documents in the collection and V denotes the
In each atomic seg-
number of words in the vocabulary.
ment s, NTSeg ﬁnds n-gram words in a word-topic z.
It
can also ﬁnd correlations between both kinds of topics i.e.
word-topics z and segment-topics y. The segments of each
document are assumed to follow a Markov structure on the
topic distributions of each segment. We assume that there
will be a high probability that the topic for the segment s in
the document will be the same as that of the segment s− 1.
A segment binary switching variable c(d)
for the segment-
topic in the document d indicates whether there is a change
of topic between the segments. The states of the switching
variable correspond to the segmentation of the document
into coherent topical units. Apart from the segment switch-
ing binary variable, NTSeg also incorporates another random
variable known as the bigram status variable x which indi-
cates the bigram status i.e. whether a word w at position i
in the segment s in the document d, denoted as w(d)
si , forms
a bigram with the previous word w(d)
s,i−1. The mechanism is
that if x(d)
s,i−1 and w(d)
form a bigram else
they do not.

si = 1, then w(d)

si

s

si and x(d)

si → z(d)

s,i−1 → z(d)

It can be observed that the existing topical n-gram model
(TNG) [39], LDSEG, [32], and LDCC, [31] are special cases de-
rived from our model. For example, consider only a seg-
ment, for instance segment s, in Figure 1. Removing the
segmentation scheme along with a set of arrows pointing
from z(d)
si reduces to the TNG model.
One can observe that our model, NTSeg, has the capability
of deciding whether to generate a unigram or a bigram in
a topic and the topic assignment for the words in a bigram
are the same. This aspect diﬀerentiates NTSeg from TNG.
Similar to TNG, NTSeg assumes a ﬁrst order Markov assump-
tion i.e. it is mainly a bigram model, but the basic genera-
tion process produces unigram or bigram words. However,
NTSeg has the ability to produce higher order n-grams (i.e.
n > 2) by concatenating consecutive n-grams (unigram or
bigram words) having the same topic and the bigram status
variable between them is 1. In this way, the words in the
n-gram share the same topic. This again contrasts NTSeg
from TNG where TNG analyzes each n-gram post hoc as if the
topic of the ﬁnal word in the n-gram was the topic assign-
ment of the entire n-gram. But it violates the principle of
non-compositionality [23]. Removing the bag-of-words as-
sumption in each segment of our proposed NTSeg model re-
duces to the LDSEG model. Relaxing both bag-of-words and

removing the segmentation switch variable of NTSeg reduces
to the LDCC model. NTSeg has the ability to decide whether
to form a unigram or bigram based on context which the
LDSEG model cannot achieve.

The following generative process of our model, NTSeg,
helps better understand the graphical model shown in Fig-
ure 1:

1. Draw φz from Dirichlet(β) for each word-topic z,
where φz is the word (unigrams only) distribution for
the word-topic z; β is the parameter of the Dirichlet
prior on the per-word-topic word (unigrams only) dis-
tribution

2. Draw ψzw from Beta(γ) for each word-topic z and
each word w, where ψzw is the Bernoulli distribution
for the bigram status variables for the word-topic z
and the word w; γ is the parameter of the Beta prior

3. Draw σzw from Dirichlet(δ) for each word-topic z,
and each word w, where σzw is the bigram word distri-
bution for bigrams; δ is the parameter of the Dirichlet
prior on word-topic bigram word distribution

4. For each document d in the collection

(a) Draw τ (d) from Dirichlet(ρ), where τ (d) is the
mixing proportion of the segment-topics in the
document d; ρ is the parameter of the Dirichlet
prior on the segment-topics

(b) Draw π(d) from Beta(Ω), where π(d) deﬁnes the
parameter of the Bernoulli distribution for the
segment switch variable in the document d; Ω is
the parameter of the Beta prior

(c) For each segment s in the document d

i. Draw c(d)
ii. Draw the segment-topic y(d)

from Bernoulli(π(d))

s

s

s

s = 0 else y(d)

for s from Multi-
s = y(d)
s−1,
is the segment-topic that is as-

nomial(τ (d)) ifc (d)
where y(d)
signed to the segment s in the document d
for s in the document d from Dirich-
s z); α is a K × Z matrix where each

let(α
row represents the mixing proportion of the
word-topics in a segment-topic; θ(d)
is the
mixing proportion of the word-topics in the
segment s in the document d

(d)

y

s

iii. Draw θ(d)

s

iv. For each of N (d)

si = 1 else draw w(d)

si

if x(d)
mial(φ
z

)

(d)
si

s

s words in the segment s in the
document d, where N (d)
denotes the number
of words in the segment s in the document d
A. Draw x(d)
),
where x(d)
between words w(d)
ment s of the document d

is the bigram status variable
in the seg-

from Bernoulli(ψ
z

s,i−1 and w(d)

(d)
s,i−1w

(d)
s,i−1

si

si

si

si

from Multinomial(θ(d)
s,i−1, where z(d)

s ) if x(d)
B. Draw z(d)
0 else z(d)
is the
word-topic assignment for the word w(d)
in the segment s in the document d.

si = z(d)

si

si

C. Draw w(d)

si

from Multinomial(σ
z

si =

(d)
si w

(d)
s,i−1
from Multino-

)

206s = y(d)

c(d)
indicates whether there is a change in the segment-topic
between the segments s − 1 and s in the document d.
s
If
c(d)
s = 1 then it means that y(d)
s−1 i.e. segment-topic
does not change between the segments in the document d.
However, when c(d)
is drawn from a Multino-
mial distribution parameterized by τ (d). The computation
of the probability P (y(d)
s−1) is done based on
two conditions i.e. ρ(y(d)
s = 1 or sampling
from Multinomial (τ (d)) when c(d)
s |c(d)

s , τ (d), y(d)
s−1) when c(d)
s = 0.

The segment distribution P (y(d)

s |c(d)
s , y(d)

s = 0, then y(d)

s , τ (d), y(d)

s

s−1) is not prop-

erly deﬁned for the ﬁrst segment of every document. There-
fore, c(d)
s = 1 is deﬁned for the ﬁrst segment which is drawn
from Multinomial(τ (d)). Similarly we assume that x(d)
s1 is
observed and only unigram is allowed at the beginning of
every segment.

4. POSTERIOR INFERENCE

The inference problem is related to computing the poste-
rior probability of the hidden variables when the input pa-
rameters β, γ, δ, ρ, Ω and the observed variable w are given.
Also, an estimate of the α hyperparameter has to be made.
It can be shown that computing the exact inference in our
model is intractable. Hence, we need to resort to approxi-
mation techniques such as Gibbs sampling [7]. Adoption of
Bayesian methods results in some hidden parameters being
integrated out instead of being explicitly estimated. Assum-
ing conjugate priors on the model parameters also eases the
inference algorithm signiﬁcantly. Algorithm 1 depicts the
Gibbs sampling used in our approximate inference for NTSeg

We need to compute the two conditional distributions:

P (z(d)

si , x(d)

si |z(d)¬si, x(d)¬si, w, c, y, x, α, β, γ, δ, ρ, Ω)

(1)

si

P (y(d)

s , c(d)

s |z, y(d)¬s , c(d)¬s , w, x, α, β, γ, δ, ρ, Ω)

(2)
Note that w(d)¬si deﬁnes all the words in the segment except
the current word w(d)
in segment s in the document d. z(d)¬si
is the word-topic assignment for all other words except the
current word w(d)
si . In Equations 3 and 4, nzw is the number
of times the word w is assigned to the word-topic z as a
unigram. mwvz is the number of times the word w appears
as a second word of a bigram with a previous word v and
both words in the bigram are assigned to the same word-
topic z. pzwt denotes the number of times the status variable
x = t (0 or 1) given the previous word w and the previous
word’s word-topic z. h(d)
sz is the number of times a word in
segment s of document d is assigned to word-topic z. κ(d)
cs,0
and κ(d)
cs,1 is the number of times the switching variable c(d)
is set to 0 and 1 in the document d, respectively. ρ
y
is the corresponding Dirichlet parameter for the segment-
topic y(d)
is the number of times a segment in the
s .
document d has been assigned to the segment-topic k. y(d)¬s
is the segment-topic assignments for all the segments except
the current segment s in the document d.

b(d)
k

(d)
s

s

Beginning with the joint probability of a dataset, and us-
ing the chain rule, we obtain the conditional probabilities
conveniently. We obtain the following equations:

×

8>>>><
>>>>:
8>>>>>>><
“
>>>>>>>:

si , x(d)

P (z(d)
(α
y

(d)
s z

(d)
si

sz

(d)
si

si |w, z(d)¬si, x(d)¬si, y, c, α, β, γ, δ, ρ, Ω) ∝
+ h(d)
`
`

− 1) × (γ
´
−1

(d)
si
βv +n
z
(d)
si

v
(d)
s,i−1 z

if x(d)

(d)
s,i−1w

si = 0

+ p
z

´

(d)
si

(d)
si

(d)
si

(d)
si

−1

−1

+m

+n

w

w

w

x

z

(d)
s,i−1x

δv +m

w

(d)
s,i−1 vz

(d)
si

−1

(d)
si

β

w

PV

v=1

(d)
si

δ

w

PV

v=1

if x(d)

si = 1 & z(d)

si = z(d)

s,i−1

− 1)

(d)
si

(3)

P (y(d)

s , c(d)
(ρ
y

(d)
s

s |z, y(d)¬s , c(d)¬s , w, x, α, β, γ, δ, ρ, Ω) ∝
+ b(d)

+ h(d)

y

(d)
s z

(d)
si

sz

(d)
si

”
− 1) × (α

(d)
s

y
(d)
κ
cs,0+Ω0
(d)
cs ,x+Ω0+Ω1

P1

x=0 κ

(α
y

(d)
s z

(d)
si

+ h(d)

sz

(d)
si

“

− 1) ×
if c(d)

P1

(d)
cs ,1+Ω1
κ
(d)
cs,x+Ω0+Ω1

x=0 κ

s = 1 & s >1 & y(d)

s = y(d)

(s−1)

− 1)×
”

if c(d)

s = 0

(4)
Note that in our model the hyperparameter α captures the
relationships between the segment-topics and word-topics.
This hyperparameter must be estimated from the data. Al-
though there are many ways to estimate this hyperparam-
eter [31], we adopt the moment matching method which is
computationally less expensive [31], and [22]. Therefore at
each iteration of the Gibbs sampling (Line 37 of Algorithm
1), we update:

X

s∈Sk

λkz =

1
qk

h(d)
sz
N (d)

s

νkz =

(5)

1
qk

αkz ∝ λkz
ZX

αkz = exp

z=1

 PZ

(7) λkz =

z=1 log(λkz)
Z − 1

!2

 

X

s∈Sk

−λkz

h(d)
sz
N (d)

s

(6)
− 1 (8)

λkz(1 − λkz)
!

νkz

(9)

where Sk is the set of segments assigned to the segment-topic
k. qk is the number of segments assigned to the segment-
topic k. λkz and νkz are the sample mean and sample vari-
ance, respectively, of the number of times the word-topic z
is assigned to the segment-topic k.

The posterior estimates for θ, φ, ψ, π, τ , σ are:

PZ
P1
P1

ˆθ(d)
s,yz =

αyz + h(d)
z=1(αyz + h(d)
sz )

sz

ˆψzw,x =

γx + pzwx
t=0(γt + pzwt)

ˆπ(d)
r =

Ωr + κcs,r
r=0(Ωr + κcs,r)

(10)

(12)

PV
PV
PK

ˆφz,w =

ˆσzw,v =

ˆτ (d)
y =

βw + nzw
v=1(βv + nzv)

δw + mwvz
v=1(δv + mwvz)

(11)

(13)

y

ρy + b(d)
k=1(ρk + b(d)
k )

(14)

(15)
The target distribution is the posterior distribution of the
word-topics, the segment-topics, the topic switching vari-
ables of the segments, and the bigram status variables. When
we use the Gibbs sampling technique, at each iteration, we

207Input : γ, δ, Z, K, ρ, Ω, β, Corpus, M axIteration
Output: N-gram words derived from the word-topic

assignments; assignments of segment-topics to
segments in documents; an estimate of α

cs,0 and κ(d)

cs,1 for all values of

k , κ(d)
sz for all values of z ∈ {1,··· , Z} in all

1 Initialize count variables in Equations 3 and 4 to 0;
2 Initialize b(d)
k ∈ {1,··· , K} in all documents;
3 Initialize h(d)
documents and their segments;
4 Initialize nzw and pzwt for all values of z ∈ {1,··· , Z}
and for all words in the collection;
5 Initialize mwvz for all values of z ∈ {1,··· , Z} and for
all bigrams in the collection;

6 Randomly initialize word-topic assignments,

segment-topic assignments, segment-topic switch
variables, and bigram status variables;

Initialize α using Equations 5, 6, 7, 8, 9;

7 if performing parameter value estimation then
8
9 end
10 for iter ← 1 to M axIteration do

foreach segment s in the document d do

foreach document d ∈ [1, M ] in the collection do
Exclude segment s and its assigned topic k
from the count variables;
(newk, newc) ← sample new segment-topic
and segment switching variable for segment s
using Equation 4;
if (newc == 0) then

Assign newk as the new segment-topic
for segment s;
Update variables b(d)
new segment-topic newk for segment s;

k , and κ(d)

cs,0 using the

end
if (newc = 1) then

Update variable κ(d)

cs,1;

end
foreach word i in segment s in the
document d, according to order do
Exclude word i and its assigned
word-topic z from the count variables;
(newz, newx)← sample new word-topic
for word i and bigram status variable
using Equation 3;
if (newx==0) then

Assign newz as the new word-topic;
Update nzw, h(d)
sz , pzw0 using
word-topic newz for word i;

end
if (newx==1) then

Update h(d)

sz , pzw1, mwvz for word i;

end

end
Update the posterior estimate for θ(d)
each segment using Equation 10;

s

for

end
Update the posterior estimates for π(d) and τ (d)
for each document using Equations 14, and 15;

sample from the conditional distribution of the word-topics
in a document conditioned on the word-topic assignments
for all other words except the current word (Line 23 in Al-
gorithm 1). In addition, we also sample the bigram status
variable (Line 24). We sample from the conditional distribu-
tion of a segment-topic for a segment (Line 14) and also the
corresponding switching variable given the segment-topic as-
signments (Line 14).

At each iteration of the Gibbs sampling procedure, we
only sample a subset of the variables which are directly re-
lated to the conditional probability. We perform this step
repeatedly until we arrive at some approximation. A vari-
able is sampled from the conditional distribution given that
the assignments for all other variables are known which is a
standard procedure in a Gibbs sampler. As the list of words
is being scanned along with the bigram status variables, the
sampler keeps track of any new segment being encountered.
For each new segment, the sampler decides about the topic
assignment of the segment i.e., whether it should assign the
current segment to the same topic as the previous segment
or a new segment-topic. If the segment has to be assigned to
a new segment-topic, the sampler estimates the probability
of assigning the segment to the segment-topic. These proba-
bilities are computed from the conditional distribution for a
segment given all other topic assignments to every other seg-
ment and all words in the segment as depicted in Algorithm
1.

5. EXPERIMENTS AND RESULTS

Evaluation of topic models is a challenging task. Simply
showing the highly probable n-gram words obtained from
each topic may not be able to portray the underlying strengths
or weaknesses of a topic model. Therefore, we evaluate our
model on several text mining tasks including the ability to
support ﬁne grained topics with n-gram words in the corre-
lation graph, the ability to segment a document into topi-
cally coherent sections, document classiﬁcation, and docu-
ment likelihood estimation.

In each experiment, we chose several existing closely re-
lated comparative methods for comparison purpose. We will
describe those comparative methods in the subsections that
follow. For our proposed framework, NTSeg, the segment
granularity is basically a paragraph because topical changes
typically occur at paragraph boundary and this strategy is
also used in [31]. Note that NTSeg can also work at the
granularity of a sentence which has also been used in one of
our experiments (refer Section 5.2). In our experiments, the
number of iterations for the Gibbs sampler is 1000 which
is the value of the M axIteration used in Algorithm 1. We
have chosen the following hyperparameter values β = 0.01,
γ = 0.1, δ = 0.1, Ω = 0.1, and ρ = 0.1. Other topic models
such as TNG, LDSEG etc, also assume ﬁxed hyperparameter
values. We did not perform any stemming, but removed
stopwords1 from the collection.
5.1 Correlation Graph

NTSeg produces two levels of topics, namely, segment-
topics and word-topics. A word-topic is comprised of n-
grams. We show the correlation graph for the purpose of
depicting how our model ﬁnds correlations among various

1http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11-
smart-stop-list/english.stop

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

end
if performing parameter value estimation then
Update α using Equations 5, 6, 7, 8, and 9;

end

39
40 end
41 Update the posterior estimates for φzw, ψzwt and σzw

using Equations 11,12, and 13,;

Algorithm 1: Inference algorithm for NTSeg.

208weeks
fetal

umbilical artery

fetal heart

adult

human immunodeﬁciency virus

hiv
aids
infected
infection

vaccine
protection

antibody response
immunization
protective

children
child
adults

sexual abuse
young children

results obtained
clinical laboratory

method
system
methods

infants
minor

gestational age
pregnant woman
preterm infants

medical
health

family physicians

primary care
family practice

conﬁdence interval

women
risk
men

risk factors

Figure 2: Correlation identiﬁed by NTSeg between the word-
topics and the segment-topics on OHSUMED collection consid-
ering Z = 200 and K = 100. Each circle shows a segment-topic
and each box corresponds to a word-topic. We can notice that
a segment-topic can capture correlations between several word-
topics.

segment-topics and word-topics. Details regarding construct-
ing and interpreting such correlation graphs can be found in
[22], and [31].

We have used the OHSUMED2 collection to show the cor-
relation graph. The collection is composed of 348,566 doc-
uments with 154,711 words in the vocabulary without stop-
words.

We have experimented by varying both number of the
word-topics Z and the number of the segment-topics K. Z
was varied from 50 to 200 in steps of 50 whereas K was
varied from 50 to 150 in steps of 50. However, we did not
observe signiﬁcant diﬀerence in the quality of the results.
The resulting correlation graph is shown in Figure 2 which
is obtained by setting Z = 200 and K = 100. Due to space
constraint, we only show the graph obtained from our NTSeg
model. Note that other models such as PAM, LDCC, LDSEG, GD-
LDA [6] and CTM, only form unigrams in a topic leading to
ambiguous interpretation. For example, presenting the uni-
gram “conﬁdence” will not be that insightful in a correlation
graph. In contrast, presenting the term “conﬁdence interval”
is more meaningful as shown in Figure 2.
5.2 Topic Segmentation Experiment

The purpose of this experiment is to show how well NTSeg
generates segmentation of documents corresponding to co-
herent topical units. The segmentation information is ob-
tained via the segmentation switch variable c(d)
s which gives
the segment topic change-points in the document.
In our
problem setting we know the segment boundaries in ad-
vance such as paragraphs or sentences, but we do not know
the word and segment topics. Our purpose is thus to learn
the segment and word topics from the document collection.
The prediction output of the segment status variable will
deﬁne the segmentation of a document. To evaluate the
performance, we make use of the annotated segmentation
information. We use two standard metrics, namely, Pk and
WinDiﬀ which are widely used in the topic segmentation
literature [32]. As described in [32], Pk is deﬁned as the
probability that two segments drawn randomly from a doc-
ument are incorrectly identiﬁed as belonging to the same
topic [2]. WinDiﬀ [28] moves a sliding window across the
text and counts the number of times the hypothesized and
reference segment boundaries are diﬀerent from within the

2http://ir.ohsu.edu/ohsumed/ohsumed.html

0.330

0.325

k
P

0.320

0.315

0.310

k
P

0.330

0.325

0.320

0.315

Books Dataset

10 Segment-Topics (K)

30 Segment-Topics (K)

k
P

0.330
0.325
0.320
0.315
0.310

100

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

50 Segment-Topics (K)

10 Segment-Topics (K)

0.350

0.340

0.330

i

ﬀ
D
n
W

i

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

100

30 Segment-Topics (K)

50 Segment-Topics (K)

i

ﬀ
D
n
W

i

0.350

0.345

0.340

0.335

0.330

k
P

0.380
0.375
0.370
0.365
0.360
0.355

k
P

0.380
0.375
0.370
0.365
0.360

i

ﬀ
D
n
W

i

0.460
0.455
0.450
0.445
0.440
0.435
0.430

i

ﬀ
D
n
W

i

0.350
0.345
0.340
0.335
0.330

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

100

Lectures Dataset

10 Segment-Topics (K)

30 Segment-Topics (K)

k
P

0.380

0.370

0.360

0.350

100

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

50 Segment-Topics (K)

10 Segment-Topics (K)

i

ﬀ
D
n
W

i

0.460

0.455

0.450

0.445

0.440

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

100

30 Segment-Topics (K)

50 Segment-Topics (K)

0.461
0.458
0.455
0.452
0.449
0.446
0.443

i

ﬀ
D
n
W

i

20

40

80
Word-Topics (Z)

60

100

20

40

80
Word-Topics (Z)

60

100

Figure 3: Comparison of NTSeg (depicted by
against TopicTiling (depicted by
tation task.

marker)
marker) on topic segmen-

209Precision Recall F-Measure

LDSEG

PAM

LDACOL

TNG

PDLDA
NTSeg

0.580
0.550
0.400
0.490
0.580
0.640

0.420
0.450
0.300
0.420
0.500
0.520

0.487
0.495
0.343
0.452
0.537
0.574

Precision Recall F-Measure

LDSEG

PAM

LDACOL

TNG

PDLDA
NTSeg

0.390
0.540
0.550
0.550
0.590
0.620

0.320
0.490
0.410
0.450
0.410
0.570

0.352
0.514
0.470
0.495
0.484
0.594

Table 1: Document classiﬁcation results for the Computer
Dataset of the 20 Newsgroups corpus.

Table 3: Document classiﬁcation results for the Politics Dataset
of the 20 Newsgroups corpus.

Precision Recall F-Measure

LDSEG

PAM

LDACOL

TNG

PDLDA
NTSeg

0.440
0.500
0.420
0.560
0.580
0.620

0.400
0.330
0.370
0.470
0.510
0.560

0.419
0.398
0.393
0.511
0.543
0.588

Precision Recall F-Measure

LDSEG

PAM

LDACOL

TNG

PDLDA
NTSeg

0.330
0.368
0.200
0.340
0.380
0.420

0.320
0.360
0.180
0.290
0.210
0.380

0.325
0.363
0.189
0.313
0.271
0.399

Table 2: Document classiﬁcation results for the Science Dataset
of the 20 Newsgroups corpus.

Table 4: Document classiﬁcation results for the Sports Dataset
of the 20 Newsgroups corpus.

window. The lower the values obtained for these two met-
rics, the better is the segmentation result.

We use two publicly available datasets that contain seg-
ment boundaries corresponding to the topic changes. The
ﬁrst dataset, called Lectures in our experiment, consists of
spoken lecture transcripts from an undergraduate physics
class and a graduate artiﬁcial intelligence class. The tran-
scripts consist of a 90 minute lecture recording and have
500 to 700 sentences with about 9000 words. Note that here
the segment granularity is a sentence. More details about
this dataset can be obtained from [32]. Our second dataset,
called Books in our experiment, is the books3 dataset in
which each document is a chapter extracted from a medical
textbook.

We chose a recently proposed topic segmentation method
TopicTiling [30] which has outperformed many state-of-
the-art text segmentation models proposed in the literature
and chose the best performing variant of TopicTiling from
[30]. Note that TopicTiling only has the notion of word-
topics. For each of the segment and word-topics, we run the
Gibbs sampler ﬁve times and take the average of the Pk and
WinDiﬀ values at the end of the ﬁfth run.

We illustrate the segmentation results in Figure 3. From
the results, we note that our model performs extremely well
in both datasets compared to the state-of-the-art topic seg-
mentation model. Using a two-tailed signiﬁcance test, our
results are statistically signiﬁcant with p < 0.05 against Top-
icTiling. In the Books dataset, NTSeg performs reasonably
better, but the improvement obtained is not very high con-
sidering both Pk and WinDiﬀ metrics. However, good im-
provement is obtained in the Lectures dataset using both
metrics.
5.3 Document Classiﬁcation Experiment

We conduct document classiﬁcation experiment using topic
models. In the training phase, a topic model is learned for
each class using the set of training documents in that class.
In testing, to conduct document classiﬁcation for a testing
document, we compute the likelihood of the testing docu-
ment against each trained topic model for each class. The
testing document is classiﬁed to the model that produces

the highest likelihood. Note that this procedure is also used
in [22].

We measure the classiﬁcation performance using precision,
recall and F-measure. The meaning of precision for a class
is the number of true positives divided by the total number
of documents predicted to that class. Recall is deﬁned as
the number of true positives divided by the total number
of elements that actually belong to that class in the gold
standard. F-measure is the harmonic mean of precision and
recall.

We use the 20 Newsgroups corpus4 and generated four
datasets. The ﬁrst dataset comprises of documents related
to computer technology (the “comp”directory in the dataset).
It is composed of several classes such as “graphics”, “win-
dows”, “hardware”, etc. Each of these classes consists of
1000 documents. We split the documents in each of these
classes into 75% training and 25% test documents. For each
class, we trained and tested the model by varying the num-
ber of word-topics from 10 to 100 in steps of 10 and the
number of segment-topics from 10 to 50 in steps 20. We com-
pute precision and recall using the test set for each class for
each word-topic and segment-topic values and then we com-
pute the average result for one class across all word-topics
and segment-topics. Similarly, we follow the same precision
and recall computation for all classes. Finally we compute
the average over all precision and recall values for all the
classes. We then compute F-measure from the obtained pre-
cision and recall values. The experimental setup is similar
for the other three datasets, namely, “sci” (called Science
Dataset), “politics” (called Politics Dataset), and “sports”
(called Sports Dataset).

The comparative methods include LDSEG, PAM, LDACOL,
TNG, andPDLDA. All these models are described in Section 2.
Note that some of the comparative methods such as TNG,
PDLDA, and LDACOL have no notion of segment-topics.

The classiﬁcation performance results are presented in Ta-
bles 1, 2, 3 and 4. We can observe that in all the datasets
our model, NTSeg, has outperformed all the comparative
methods. Compared to all the comparative methods, our
results are also statistically signiﬁcant using the sign test
with p < 0.05. Gain obtained in the Computer and Science
datasets is more when compared to the gain in Sports and

3http://groups.csail.mit.edu/rbg/code/bayesseg/

4http://qwone.com/ejason/20Newsgroups/

210−8.600

−8.700

−8.800

d
o
o
h

i
l
e
k
i
L
-
g
o
L

−3.150

−3.200

−3.250

−3.300

d
o
o
h

i
l
e
k
i
L
-
g
o
L

·106

10 Segment-Topics (K)

d
o
o
h

i
l
e
k
i
L
-
g
o
L

−8.600

−8.700

−8.800

200

100

50
Word-Topics (Z)

150

NIPS Dataset

·106

30 Segment-Topics (K)

100

50
Word-Topics (Z)

150

200

·107

50 Segment-Topics (K)

OHSUMED Dataset

100 Segment-Topics (K)

·107

−3.150

−3.200

−3.250

d
o
o
h

i
l
e
k
i
L
-
g
o
L

200

300

400
Word-Topics (Z)

500

200

300

400
Word-Topics (Z)

500

−8.600
−8.700
−8.800

d
o
o
h

i
l
e
k
i
L
-
g
o
L

d
o
o
h

i
l
e
k
i
L
-
g
o
L

−3.150

−3.200

−3.250

−3.300

·106

50 Segment-Topics (K)

100

50
Word-Topics (Z)

150

200

·107

150 Segment-Topics (K)

200

300

400
Word-Topics (Z)

500

Figure 4: Performance of NTSeg (depicted by
generalizes better than other comparative methods which are: LDSEG (depicted by

) in terms of generalizing on the new data. We can see that our model NTSeg
), LDACOL (depicted by

), PAM (depicted by

), TNG (depicted by

), and PDLDA (depicted by

).

Politics datasets. PDLDA also proved to be a better model in
comparison to the other comparative methods.

5.4 Document Likelihood Experiment

Another evaluation scheme to compare the relative per-
formance of topic models is to study how the models gener-
alize on an unseen data. The entire corpus in this method
is ﬁrst split into training and testing set. The training set
generally contains more number of documents as compared
to the testing set. A model is ﬁrst learned on the training
data, and the testing set is used to measure the generaliza-
tion performance of the topic models.
In the topic mod-
eling literature, metrics such as perplexity computation or
log-likelihood have often been used. For example, PAM uses
empirical log-likelihood [10] as an evaluation metric and so
does a recently proposed method GD-LDA [6]. Log-likelihood
has also been widely used as one of the evaluation metrics,
for example in [3]. We chose log-likelihood metric for com-
paring the topic models. The comparative methods here are
LDSEG, PAM, LDACOL, TNG, andPDLDA.

We use the NIPS dataset5. The NIPS collection is widely
used in the topic modeling literature. Note that the original
raw NIPS dataset consists of 17 years of conference papers.
But we supplemented this dataset by including some new
raw NIPS documents6 and it has 19 years of papers in total.
Our NIPS collection consists of 2741 documents comprising
of 453,606,9 non-unique words and 94961 words in the vo-
cabulary. In addition to the NIPS collection we also use the
OHSUMED collection.

5http://www.cs.nyu.edu/eroweis/data.html
6http://ai.stanford.edu/egal/Data/NIPS/

In order to calculate the likelihood of held-out data, we
must integrate out the sampled multinomials and sum over
all possible topic assignments which has no closed-form so-
lution. Griﬃths et al.
[13] have used Gibbs sampling for
computing such approximations. First, we randomly split
each of the datasets into 80% training and 20% testing. We
trained each of the topic models on the training set. We then
tested the models on the testing set by running the inference
algorithms ﬁve times for each word-topic and segment-topic
pair. We then took average value for all ﬁve runs. We varied
the number of segment-topics from 10 to 50 in steps of 20
and the number of word-topics from 20 to 200 in steps of
20 in the NIPS collection. As the OHSUMED collection is
larger compared with the NIPS collection, so we varied the
number of segment-topics from 50 to 150 in steps of 50 and
word-topics from 150 to 490 in steps of 20.

From the results in Figure 4 we can see that in the NIPS
collection, NTSeg performs better than the comparative meth-
ods especially when the number of segment-topics is 10.
However, its performance deteriorates a bit when the num-
ber of segment-topics is increased, but still remains com-
petitive with the comparative methods. Moreover, we no-
tice that as the number of word-topics increases, the perfor-
mance of NTSeg deteriorates to some extent in the NIPS col-
lection. However, in the OHSUMED collection, NTSeg again
performs better against the comparative methods when the
number of word-topics is increased. We can observe that
NTSeg outperforms the comparative methods considerably
when the number of segment-topics is 100. The results sug-
gest that NTSeg can perform very well on large document col-
lections as large collections provide richer information about
word co-occurrences.

2116. CONCLUSIONS AND FUTURE WORK
We have developed a generative topic discovery model,
known as NTSeg, which maintains the document’s structure
such as paragraphs and sentences and also keeps the order
of the words in the document intact. NTSeg incorporates
the notion of word-topics and segment-topics. We have
conducted extensive experiments and shown results using
both qualitative analysis where we show the n-gram words
in the correlation graph and quantitative performance. Ex-
perimental results demonstrate that by relaxing the bag-
of-words assumption in each segment improves the perfor-
mance of the model.

Giving an arbitrary number of word-topics and segment-
topics to the model is one issue that we would look into for
future work. We would attempt to work towards a model
which could automatically ﬁnd out the desirable number of
word-topics and segment-topics in the collection.

7. REFERENCES
[1] B. Adams, D. Phung, and S. Venkatesh. Discovery of latent
subcommunities in a blog’s readership. ACM Trans. on the
Web, 4(3):12:1–12:30, 2010.

[2] D. Beeferman, A. Berger, and J. Laﬀerty. Statistical models
for text segmentation. Machine Learning, 34:177–210, 1999.

[3] D. Blei and J. Laﬀerty. Correlated topic models. Proc. of

NIPS, pages 147–155, 2006.

[4] D. M. Blei and P. J. Moreno. Topic segmentation with an

aspect Hidden Markov model. In Proc. of SIGIR, pages
343–348, 2001.

[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet

Allocation. JMLR, 3:993–1022, 2003.

[6] K. L. Caballero, J. Barajas, and R. Akella. The Generalized
Dirichlet Distribution in enhanced topic detection. In Proc.
of CIKM, pages 773–782, 2012.

[7] G. Casella and E. I. George. Explaining the Gibbs sampler.

The American Statistician, 46(3):pp. 167–174, 1992.

[8] X. Chen and S. Chen. Subsequence-based text segmentation

and labeling. In Proc. of ECTS, pages 582–587, 2009.

[9] J. Chien and C. Chueh. Topic-based hierarchical

segmentation. IEEE Transactions on Audio, Speech, and
Language Processing, 20(1):55–66, 2012.

[10] P. Diggle and R. Gratton. Monte Carlo methods of

inference for implicit statistical models. Journal of the
Royal Statistical Society, pages 193–227, 1984.

[11] L. Du, W. Buntine, and H. Jin. A segmented topic model

based on the two-parameter Poisson-Dirichlet process.
Machine Learning, 81(1):5–19, 2010.

[12] J. Eisenstein and R. Barzilay. Bayesian unsupervised topic

segmentation. In Proc. of EMNLP, pages 334–343, 2008.
[13] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc topics.

National Academy of Sciences of the United States of
America, 101(Suppl 1):5228–5235, 2004.

[14] T. L. Griﬃths, M. Steyvers, and J. B. Tenenbaum. Topics

in semantic representation. Psychological Review,
114(2):211, 2007.

[15] A. Gruber, Y. Weiss, and M. Rosen-Zvi. Hidden topic

Markov models. In Proc. of AI and Statistics, pages
163–170, 2007.

[16] M. A. Hearst. TextTiling: Segmenting text into

multi-paragraph subtopic passages. Comput. Linguist.,
23(1):33–64, 1997.

[17] T. Hofmann. Unsupervised learning by Probabilistic Latent

Semantic Analysis. Machine Learning, 42:177–196, 2001.

[18] S. Jameel and W. Lam. An N-gram topic model for
time-stamped documents. In Proc. of ECIR, pages
292–304. 2013.

[19] N. Kawamae. Identifying sentiments over n-gram. In Proc.

of WWW, pages 541–542, 2012.

[20] H. D. Kim, D. H. Park, Y. Lu, and C. Zhai. Enriching text

representation with frequent pattern mining for
probabilistic topic modeling. Journal of American Society
for Information Science and Technology, 49(1):1–10, 2012.

[21] W. Kintsch. The role of knowledge in discourse

comprehension: A construction-integration model.
Psychological Review, 95(2):163, 1988.

[22] W. Li and A. McCallum. Pachinko allocation:

DAG-structured mixture models of topic correlations. In
Proc. of ICML, pages 577–584, 2006.

[23] R. V. Lindsey, W. P. Headden, and M. J. Stipicevic. A

phrase-discovering topic model using hierarchical
Pitman-Yor processes. In Proc. of EMNLP, pages 214–222,
2012.

[24] D. Metzler and W. B. Croft. A Markov Random Field
model for term dependencies. In Proc. of SIGIR, pages
472–479, 2005.

[25] H. Misra, F. Yvon, J. M. Jose, and O. Cappe. Text

segmentation via topic modeling: An analytical study. In
Proc. of CIKM, pages 1553–1556, 2009.

[26] A. Mukherjee and B. Liu. Mining contentions from

discussions and debates. In Proc. of KDD, pages 841–849,
2012.

[27] P. V. Mulbregt, I. Carp, L. Gillick, S. Lowe, and

J. Yamron. Text segmentation and topic tracking on
broadcast news via a hidden markov model approach. In
Proc. of ICSLP, pages 2519–2522, 1998.

[28] L. Pevzner and M. A. Hearst. A critique and improvement

of an evaluation metric for text segmentation. Comput.
Linguist., 28(1):19–36, 2002.

[29] M. Riedl and C. Biemann. How text segmentation

algorithms gain from topic models. In Proc. of NAACL
HLT, pages 553–557, 2012.

[30] M. Riedl and C. Biemann. TopicTiling: A text

segmentation algorithm based on LDA. In Proc. of ACL
2012 Student Research Workshop, pages 37–42, 2012.

[31] M. Shaﬁei and E. Milios. Latent Dirichlet Co-Clustering. In

Proc. of ICDM, pages 542 –551, 2006.

[32] M. Shaﬁei and E. Milios. A statistical model for topic
segmentation and clustering. In Proc. of Advances in
Artiﬁcial Intelligence, pages 283–295, 2008.

[33] A. Tagarelli and G. Karypis. A segment-based approach to

clustering multi-topic documents. In Proc. of SDM, pages
1–33, 2008.

[34] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical
Dirichlet processes. JASA, 101(476):1566–1581, 2006.

[35] H. M. Wallach. Topic modeling: Beyond bag-of-words. In

Proc. of ICML, pages 977–984, 2006.

[36] H. Wang and B. Lang. Online Ngram-enhanced topic model

for academic retrieval. In Proc. of ICDIM, pages 137–142,
2011.

[37] L. Wang, B. Wei, and J. Yuan. Topic discovery based on

LDACOL model and topic signiﬁcance re-ranking. Journal
of Computers, 6(8):1639–1647, 2011.

[38] X. Wang and A. McCallum. A note on Topical N-grams.

Technical report, DTIC Document, 2005.

[39] X. Wang, A. McCallum, and X. Wei. Topical N-Grams:

Phrase and topic discovery, with an application to
Information Retrieval. In Proc. of ICDM, pages 697 –702,
2007.

[40] Y. Wang, E. Agichtein, and M. Benzi. TM-LDA: Eﬃcient
online modeling of latent topic transitions in social media.
In Proc. of KDD, pages 123–131, 2012.

[41] Q. Wu, C. Zhang, and X. An. Topic segmentation model

based on ATNLDA and co-occurrence theory and its
application in stem cell ﬁeld. Journal of Information
Science, pages 1–14, 2012.

[42] Z. Yan and F. Li. News thread extraction based on Topical
N-gram model with a background distribution. In Proc. of
ICONIP, pages 416–424, 2011.

212