Task-Aware Query Recommendation

Henry Feild and James Allan

Center for Intelligent Information Retrieval

School of Computer Science
University of Massachusetts

Amherst, MA 01003

{hfeild,allan}@cs.umass.edu

ABSTRACT
When generating query recommendations for a user, a natu-
ral approach is to try and leverage not only the user’s most
recently submitted query, or reference query, but also in-
formation about the current search context, such as the
user’s recent search interactions. We focus on two important
classes of queries that make up search contexts: those that
address the same information need as the reference query
(on-task queries), and those that do not (oﬀ-task queries).
We analyze the eﬀects on query recommendation perfor-
mance of using contexts consisting of only on-task queries,
only oﬀ-task queries, and a mix of the two. Using TREC
Session Track data for simulations, we demonstrate that on-
task context is helpful on average but can be easily over-
whelmed when oﬀ-task queries are interleaved—a common
situation according to several analyses of commercial search
logs. To minimize the impact of oﬀ-task queries on recom-
mendation performance, we consider automatic methods of
identifying such queries using a state of the art search task
identiﬁcation technique. Our experimental results show that
automatic search task identiﬁcation can eliminate the eﬀect
of oﬀ-task queries in a mixed context.

We also introduce a novel generalized model for generat-
ing recommendations over a search context. While we only
consider query text in this study, the model can handle in-
tegration over arbitrary user search behavior, such as page
visits, dwell times, and query abandonment. In addition, it
can be used for other types of recommendation, including
personalized web search.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—search process, query formulation

Keywords
Query recommendation, context-aware recommendation,
search task identiﬁcation

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Figure 1: A search context with interleaved tasks.

1.

INTRODUCTION

Query recommendation is a common tool used by search
engines to assist users in reformulating queries. When an
information need, or task, requires multiple searches, the se-
quence of queries form a context around which new queries
can be recommended. Figure 1 illustrates a series of queries
issued by a user consisting of two tasks: 1) ﬁnding infor-
mation about the history of black powder ﬁrearms and 2)
preparing for the GMAT standardized test. Given this se-
quence, our goal is to generate a list of query suggestions
with respect to the most recently submitted query, or refer-
ence query, which is “black powder inventor” in this example.
Notice, however, that the user has interleaved the two tasks
such that no two adjacent queries are part of the same task.
If we use the entire context to generate recommendations,
two of the queries will be oﬀ-task with respect to the ref-
erence query and three (including the reference query) will
be on-task. This paper explores the eﬀects that on- and oﬀ-
task contexts have on query recommendation. While previ-
ous work has considered task-aware query recommendation
over logged user data, we are not aware of any work that
systematically explores the eﬀects of on-task, oﬀ-task, and
mixed contexts on recommendation performance.

Though the example in Figure 1 may seem an extreme
case, consider that Lucchese et al. found 74% of web queries
were part of multi-tasking search sessions [15] in a three-
month sample of AOL search logs; Jones and Klinkner ob-
served that 17% of tasks were interleaved in a 3-day sample
of Yahoo! web searches [10]; and Liao et al. [14] found 30%
of sessions contained multiple tasks and 5% of sessions con-
tained interleaved tasks in a sample of half a billion sessions
extracted from Bing search logs. In addition, in a labeled
sample of 503 AOL user sessions, we found 44% of search
tasks consisted of two or more queries (see Figure 2), but
there was only a 45% chance that any two adjacent queries

83d
o
o
h

i
l

e
k
L

i

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

Likelihood of tasks of at least length x

Likelihood of tasks of length x

s
k
s
a

t
 

i

 

n
g
n
e
e
s
 
f

 

o
d
o
o
h

i
l

e
k
L

i

2

4

6

8

10

Task length in number of queries (x)

Figure 2: Distribution of tasks lengths observed in
a labeled sample of the AOL query log.

were part of the same task. Figure 3 shows the likelihood
of seeing n tasks in any sequence of x queries, e.g., 10-query
sequences typically consist of 3–7 search tasks. This means
that a context consisting of the most recent n queries is very
likely to consist of sub-contexts for several disjoint tasks,
none of which may be a part of the same task as the refer-
ence query.

The goal of this paper is to better understand the eﬀects
of on-task, oﬀ-task, and mixed contexts on query recommen-
dation quality. We also present and analyze several methods
for handling mixed contexts. We address four questions con-
cerning query recommendation:

RQ1. How does on-task context aﬀect query recommen-

dation performance?

RQ2. How does oﬀ-task context aﬀect query recommen-

dation performance?

RQ3. How does mixed context (on- and oﬀ-task queries)

aﬀect query recommendation performance?

RQ4. How do the following three methods aﬀect query
recommendation performance in a mixed context?
(a.) using only the reference query, (b.) using the
most recent m queries, or (c.) using the most re-
cent m queries with same-task classiﬁcation scores
to weight the inﬂuence of each query.

To answer these questions, we perform a number of exper-
iments using simulated search sequences derived from the
TREC Session Track. For recommendation, we rely on ran-
dom walks over a query ﬂow graph formed from a subset of
the 2006 AOL query log. We measure query recommenda-
tion performance by the quality of the results returned for
a recommendation, focusing primarily on mean reciprocal
rank (MRR). Our results show that on-task context is usu-
ally helpful, while oﬀ-task and mixed contexts are extremely
harmful. However, automatic search task identiﬁcation is a
reliable way of detecting and discarding oﬀ-task queries.

There are four primary contributions of this paper: (1) an

10 tasks
9 tasks
8 tasks
7 tasks
6 tasks
5 tasks
4 tasks
3 tasks
2 tasks
1 task

0
1

.

.

8
0

6
0

.

.

4
0

2
0

.

.

0
0

2

3

4

5

6

7

8

9 10

Sequence length

Figure 3: The distribution of seeing n tasks in a
sequence of x queries as observed in a labeled sample
of the AOL query log.

analysis of task-aware query recommendation demonstrating
the usefulness of on-task query context, (2) an analysis of
the impact of automatic search task identiﬁcation on task-
aware recommendation, in which we show the state of the
art works very well, (3) an open source data set for evaluat-
ing context-aware query recommendation, and (4) a gener-
alized model of combining recommendations across a search
context, regardless of the recommendation algorithm.

2. RELATED WORK

Huang, Chien, and Oyang [9] introduced a search log-
based query recommendation algorithm that extracts sug-
gestions from search sessions in a query log that appeared
similar to the user’s current session, thereby incorporating
the surrounding search context. They found it outperformed
methods that extract suggestions from retrieved documents
in many aspects.

Filali et al. [8] presented a probabilistic model for gener-
ating rewrites based on an arbitrarily long user search his-
tory. Their model interpolates the same-task similarity of
a rewrite candidate to the reference query with the aver-
age similarity of that candidate to all on-task queries from
a user’s history, weighted by each query’s similarity to the
reference query. They found that giving some weight to the
history, but most weight to the reference query, did best on
a set of manually and automatically labeled data.

Liao et al. [14] explored the eﬀect of task-trails on three
applications, including query recommendation. They com-
pare two session-based, two task-based, and two single-query
recommendation models and found they retrieve comple-
mentary sets of suggestions, though the task-based models
provided the higher quality suggestions. To identify tasks,
they used an SVM model using features similar to Jones and
Klinkner [10], and the weighted connected components clus-
tering method described by Lucchese et al. [15]. Unlike Liao
et al., we focus on the eﬀects of on- and oﬀ-task context in

84various mixtures and evaluate on publicly accessible data.
Their work is complementary to ours and our ﬁndings sup-
port theirs concerning the quality of recommendations using
on-task recommendations.

Cao et al. [4] introduce a context-aware recommendation
system that converts a series of user queries into concept
sequences and builds a suﬃx tree of these from a large query
log. To produce recommendations, a concept sequence is
looked up in the suﬃx tree and the common next queries
are given as suggestions.

Cao et al. [5] explored an eﬃcient way to train a very large
variable length Hidden Markov Model (vlHMM), which con-
siders sequences of queries and clicks in order to produce
query and URL recommendations as well as document re-
ranking. The authors trained the vlHMM on a large com-
mercial search log. However, they did not analyze the ef-
fects of on- and oﬀ-task contexts on recommendation and
the vlHMM technique is tied to large search logs, limiting
its portability.

Boldi et al. [2] presented a technique for building a query
reformulation graph over user sessions, called a query ﬂow
graph. They considered two edge weighting techniques: one
uses the threshold weight output by a same-task classiﬁer
and the other uses the observed likelihood of one query be-
ing submitted after the other. They looked at two appli-
cations of query ﬂow graphs: (1) identifying sequences of
queries that share a common search task and (2) generating
query recommendations. The query suggestion component
involves random walks and can be conﬁgured to consider the
most recent n queries.

Several groups have investigated automatic methods for
segmenting and clustering user search interactions into
search tasks [2, 10, 13, 15, 18]. Most of these depend on ma-
chine learning algorithms—logistic regression, support vec-
tor machines, and decision trees—trained over large amounts
of manually or automatically labeled user data to classify a
pair of queries as belonging to the same task or not. We
decided to use the Lucchese et al. method [15], which is a
heuristic model that depends on lexical and semantic fea-
tures of the pair of queries being classiﬁed. This method
is the most recent and was shown to out-perform machine
learned models on a sample of AOL data. The other meth-
ods are, however, equally applicable to the analysis we con-
duct.

3. QUERY RECOMMENDATION

We only consider one query recommendation algorithm
based largely on the query ﬂow graph (QFG) work of Boldi
et al. [2] and the term-query graph (TQGraph) work of
Bonchi et al. [3]. We use a query ﬂow graph G in which the
vertices V are queries from a query log L and the edges E
represent reformulation probabilities. For any vertex v ∈ V ,
the weights of all outgoing edges must sum to 1. A reformu-
lation is deﬁned as an ordered pair of queries (qi, qj) such
that the pair occurs in a user search session in that order,
though not necessarily adjacent. A session is deﬁned to be
the maximal sequence of queries and result clicks such that
no more than t seconds separate any two adjacent events.
The outgoing edges of v are normalized across all sessions
and users in L.

While we do not require reformulations to be adjacent,
Boldi et al. did. By considering all reformulations—adjacent
and otherwise—within a session, we expand the coverage

of G beyond using adjacent query reformulations only but
avoid incorporating as many oﬀ-task reformulations as when
the scope is a user’s entire search history. We assume that
reformulations in which qi and qj are from diﬀerent tasks, qj
will be an infrequent follower of qi, and therefore statistically
insigniﬁcant among qi’s outgoing edges. In addition, Boldi et
al. used a thresholded and normalized chaining probability
for the edge weights, but we do not due to the sparseness of
our data (the AOL query logs).

To generate recommendations, we rely on a slight adapta-
tion of the query ﬂow graph, called the term-query graph [3].
This adds a layer to the QFG that consists of all terms that
occur in queries in the QFG, each of which points to the
queries in which it occurs. Given a query q, we ﬁnd recom-
mendations by ﬁrst producing random walk scores over all
queries in Q for each term t ∈ q.

To compute the random walk with restart for a given term
t, we must ﬁrst create a vector v of length |V | (i.e., with one
element per node in Q). Each element corresponding to a
query that contains t is set to 1 and all others are set to
0. This is our initialization vector. Next, we must select
the probability, c, of restarting our random walk to one of
the queries in our initialization vector. Given the adjacency
matrix of G, A, and a vector u that is initially set to v, we
then compute the following until convergence:

u = (1 − c)Au + cv

(1)

After convergence, the values in u are the random walk
scores of each corresponding query q′ for t. We denote this

One issue with using the random walk score for a query
is that it favors frequent queries. To address this, Boldi et
al. used the geometric mean rterm of the random walk score

as the term-level recommendation scorebrterm(q′|t).
brterm and its normalized score rterm. Given an initial query

q, the scores for an arbitrary query q′ can be computed by:

when the initialization vector v is uniform.

wherebruniform(q′) is the random walk score produced for q′

The ﬁnal query recommendation vector is computed us-
ing a component-wise product of the random walk vectors
for each term in the query. Speciﬁcally, for each query
q′ ∈ V , we compute the query-level recommendation score
rquery(q′|q) as follows:

rquery(q′|q) =Yt∈q

rterm(q′|t)

(4)

4. CONTEXT AWARE RECOMMENDATION

In this section, we introduce formal deﬁnitions of general
and task-based contexts as well as the automatic search task
identiﬁcation algorithm used for the experiments.

4.1 Generalized context model

The recommendation models described above, and recom-
mendation algorithms in general, that generate suggestions
with respect to a single query can be easily extended to
handle additional contextual information. The basic idea is

rterm(q′|q) = brterm(q′|q)
bruniform(q′)
rterm(q′|q) = pbrterm(q′|q) · rterm(q′|q)
brterm(q′|q)
pbruniform(q′)

=

(2)

(3)

85simple: when generating recommendations for a query, con-
sider the search context consisting of the m most recently
submitted queries from the user, weighting the inﬂuence of
each according to some measure of importance. Many func-
tions can be used to measure the importance of a context
query. The two we consider in this paper are how far back in
a user’s search history a query occurs and whether the query
appears to be related to a user’s current task. However, oth-
ers may include whether a context query was quickly aban-
doned or spell corrected, how many results the user visited,
the time of day they were visited, and other behavioral as-
pects. In this section, we introduce a generalized model that
makes it easier for us to talk about the various importance
functions we are interested in and can be used with addi-
tional functions explored in future work.

Assume that we have a search context C that contains
all the information about a user’s search behavior related
to a sequence of m queries, with the mth query, C[m], be-
ing the most recently submitted query. Also assume that
we have a set of n importance functions, θ1, θ2, . . . , θn and
corresponding weights λ1, λ2, . . . , λn that tell us how much
weight to give to each of the importance functions. We will
represent corresponding functions and weights as tuples in
a set F = {hλ1, θ1i hλ2, θ2i . . . , hλn, θni}. We compute the
context-aware recommendation score for a query suggestion
q′ as follows:

rcontext(q′|C, F ) =

λ · θ(i, m, C)

mXi=1

rquery(q′|C[i]) Xhλ,θi∈F

(5)
Each importance function θ, takes three parameters: i, m, C,
where i is the position of the query within context C for
which importance is being measured and m is the position
of the reference query. In this work, the reference query is
always the last query of C, but the model does not make
the assumption that this is always the case. The rcontext
recommendation scoring function scores q′ with respect to
each query in the context (C[i]) and adds that score to the
overall score with some weight that is computed as the lin-
ear combination of the importance function values for that
query.

4.2 Decaying context model

One of the importance functions we consider in this paper
is a decaying function, where queries earlier in a user’s con-
text are considered less important than more recent queries.
As such, queries submitted more recently have a greater in-
ﬂuence on recommendation scores. This has the intuitive
interpretation that users are less interested in older queries,
otherwise they would not have moved on to new queries.

Boldi et al. [2] discussed a decay weighting method for
entries in the random walk initialization vector (v in Eq. 1).
They proposed that each query in a search context receive a
weight proportional to βd, where d is the distance in query
count from the current query. For example, the second most
recent query would get a weight of β1, because it’s one query
away from the most recent query.

While the Boldi et al. method is speciﬁc to recommenda-
tions using random walks, we can transfer their exponential
decay function to our model as follows:

decay(i, j, C) = βj−i

rdecay(q′|C) = rcontext(q′|C, {h1.0, decayi})

(6)

(7)

4.3 Task context model

While decaying the inﬂuence of queries earlier in a search
context is a natural importance function, we are also inter-
ested in functions that incorporate the degree to which a
query is on the same task as the reference query. It is rea-
sonable to assume (an assumption we test) that queries from
a search context that are part of the same task should be
more helpful in the recommendation process than queries
that are not.

As we have stated earlier, Lucchese et al. observed that
74% of web queries are part of multi-task search sessions [15]
while Jones and Klinkner found that 17% of tasks are inter-
leaved in web search [10]. Using a labeled sample of the AOL
query log, we observed an exponential decrease in the likeli-
hood that the previous m queries are part of the same task
as m increases (see Figure 3). This suggests that using the m
most recent queries as the the search context for generating
recommendations will likely introduce oﬀ-topic information,
causing recommendations that seem out of place. There-
fore, it may be beneﬁcial to identify which queries from that
context share the same task as the reference query.

Formally, given a search context C with m queries, we
deﬁne a task context T to be the maximal subset of queries
in C that share a common task to C[m]:

T ⊆ C | ∀ i ∈ [1, m], C[i] ∈ T ⇐⇒ sametask(i, m, C) > τ
(8)
where sametask(i, m, C) is a function that outputs a pre-
diction in the range [0,1] as to how likely C[i] and C[m] are
to be part of the same search task given C and τ is the
decision threshold.

Once we have T , the natural question to pose is how do we
use it? One method would be to treat T just as C and use it
with the rdecay function, i.e., rdecay(q′|T ). However, it may
be that the oﬀ-task context is still useful, just not as useful
as T . To support both of these points of view, we can use
the following hard task recommendation scoring functions:

taskdecay(i, j, C) = β taskdist(i,j,C)

(9)

hardtask(i, j, C) =(taskdecay if sametask > τ,

otherwise.

0

rhardtask(q′|C) = rcontext(q′|C, {hλ, hardtaski,

h1 − λ, decayi})

(10)

(11)

where λ can be used to give more or less weight to the task
context and taskdist is the number of on-task queries be-
tween C[i] and C[j]. If we set λ = 1, we only use the task
context, whereas with λ = 0, we ignore the task context
altogether. If we use λ = 0.5, we use some of the task in-
formation, but still allow the greater context to have a pres-
ence. Note that we have left oﬀ the parameters to decay
and sametask in Eq. 16 for readability.

This approach may work well if one is comfortable with
setting a hard threshold τ on the output of the sametask
function. If, however, we want to provide a mechanism by
which we use the output of sametask as a conﬁdence, we
can use the following soft task recommendation scoring func-

86tions:

softtask(i, j, C) = sametask · decay

rsofttask(q′|C) = rcontext(q′|C, {hλ, softtaski,

h1 − λ, decayi})

(12)

(13)

Here, λ smooths between using and not using the same-task
scores to dampen the decay weights. As before, we have left
oﬀ the parameters to sametask and decay in Eq. 12.

Two additional models we consider are both variations of
what we call ﬁrm task recommendation, as they combine
aspects of both the hard and soft task models. The ﬁrst,
called ﬁrmtask1, behaves similarly to the soft task model,
except that the weight given to any queries with a same
task score at or below the threshold τ are weighted as 0.
The second, called ﬁrmtask2, is identical to the hard task
model, except that the task classiﬁcation score is used in
addition to the taskdecay weight. Mathematically, the ﬁrm
task recommendation models are described by:

sametask if sametask > τ,

× decay

0

otherwise.

ﬁrmtask1(i, j, C) =
ﬁrmtask2(i, j, C) =

rﬁrmtask1(q′|C) = rcontext(q′|C, {hλ, ﬁrmtask1i,

h1 − λ, decayi})

sametask

if sametask > τ,

× taskdecay

0

otherwise.

rﬁrmtask2(q′|C) = rcontext(q′|C, {hλ, ﬁrmtask2i,

h1 − λ, decayi})

Note that unlike the hard task model, the decay component
of the ﬁrmtask1 model is aﬀected by every query, not just
those above the threshold.

For example, suppose we have a context C with ﬁve
queries, q1, . . . , q5. Relative to the reference query, q5, sup-
pose applying sametask to each query produces the same-
task scores [0.4, 0.2, 0.1, 0.95, 1.0]. If we set τ = 0.2, then
T = [q1, q4, q5]. Using β = 0.8, notice in Figure 4 how the
importance weight of each query in the context changes be-
tween using only the decay function (a.) and setting λ = 1
for the task-aware recommendations (b.–e.). Note that when
λ = 0, the hard, ﬁrm, and soft task recommendation scores
are equivalent (they all reduce to using the decay-only scor-
ing function).

There are two primary diﬀerences between using hard-
and soft task recommendation. First, hard task recommen-
dation does not penalize on-task queries that occur prior to
a sequence of oﬀ-task queries, e.g. in Figure 4, we see that
q1 is on-task and hardtask treats it as the ﬁrst query in a
sequence of three: βn−1 = 0.82. Conversely, the soft task
recommendation model treats q1 as the ﬁrst in a sequence
of ﬁve: βm−1 = 0.84.

Second, soft task recommendation can only down-weight
a query’s importance weight, unlike-hard task recommenda-
tion, which we saw can signiﬁcantly increase the weight of an
on-task query further back in the context. At the same time,
however, soft task recommendation only allows a query to

(14)

(15)

(16)

(17)

sametask = [.8, .2, .1, .9, 1.0]
rdecay
≈ [.4, .5, .6, .8, 1.0]
rsofttask
≈ [.3, .1, .1, .7, 1.0]
rﬁrmtask1 = [.3, .0, .0, .7, 1.0]
rﬁrmtask2 = [.5, .0, .0, .7, 1.0]
rhardtask ≈ [.6, .0, .0, .8, 1.0]

a.
b.
c.
d.
e.

Figure 4: An example of the degree to which each
query in a context contributes (right column) given
its predicted same-task score (top row) for: (a.) de-
cay only, (b.) soft task, (c.) ﬁrm task-1, (d.) ﬁrm
task-2, and (e) hard task recommendation. We set
β = 0.8 for all, and λ = 1, τ = 0.2 for b.–e.

be given a zero weight if its same-task score is zero. The two
ﬁrm task models balance these aspects in diﬀerent ways.

4.4 Automatic search task identiﬁcation

In Section 3, we discussed how to use information about
tasks for query recommendation, but we did not say how
to generate the scores. We use the search task identiﬁcation
heuristic described by Luccehse et al. [15]. In deciding if two
queries qi and qj are part of the same task, we calculate two
similarity measures: a lexical and a semantic score, deﬁned
as follows.

The lexical scoring function slexical is the average of the
Jaccard coeﬃcient between term trigrams extracted from
the two queries and one minus the Levenshtein edit distance
of the two queries. The score is in the range [0,1]. Two
queries that are lexically very similar—ones where a single
term has been added, removed, or reordered, or queries that
have been spell corrected—should have an slexical score close
to one.

The semantic scoring function ssemantic is made of up two
components. The ﬁrst, swikipedia(qi, qj), creates the vectors
vi and vj consisting of the tf·idf scores of every Wikipedia
document relative to qi and qj, respectively. The func-
tion then returns the cosine similarity between these two
vectors. The second component, swiktionary(qi, qj) is simi-
larly computed, but over Wiktionary entries. We then set

ssemantic(qi, qj) = max(cid:0)swikipedia(qi, qj), swiktionary(qi, qj)(cid:1).

As with the lexical score, the range of the semantic score
is [0,1].

The combined similarity score, s, is deﬁned as follows:

s(qi, qj) = α · slexical(qi, qj) + (1 − α) · ssemantic(qi, qj)

We can deﬁne a same-task scoring function to use s directly,
as follows:

sametask1(i, j, C) = s(C[i], C[j])

(18)

Alternatively, we can run one extra step: single-link clus-
tering over the context C using s as the similarity measure.
Clustering allows us to boost the similarity score between
two queries that are only indirectly related. Similar to Liao
et al. [14], our choice of clustering follows the results of Luc-
chese et al. [15], who describe a weighted connected compo-
nents algorithm that is equivalent to single-link clustering
with a cutoﬀ of η. After clustering, we use the notation
KC [q] to represent the cluster or task associated with query
q in context C; if two queries q, q′ ∈ C are part of the same
task, then KC [q] = KC [q′], otherwise KC [q] 6= KC [q′]. A

87212 judged sessions from both years. The relevance judg-
ments in both cases are over the ClueWeb09 collection. Our
goal is to provide recommendations to retrieve documents
relevant to the last query in each session, thus we mark the
last query as the reference query.

Each session constitutes a single task, and henceforth we
refer to the sessions as tasks. Since the TREC data consists
of single tasks, we need some way of simulating the case that
multiple tasks are interleaved. We describe our approach for
this next.

5.3 Experiments

To answer our four research questions, we use the follow-
ing set of experiments. Throughout all of these experiments,
the baseline is to use only the reference query for generating
query recommendations.

Experiment 1. For RQ1, which seeks to understand
the eﬀect of including relevant context on recommendation
performance, we use each task T from the TREC Session
Track and recommend suggestions using the most recent m
queries for m = [1, |T |]. If incorporating context is helpful,
then we should see an improvement as m increases. Note
that m = 1 is the case in which only the reference query is
used.

Experiment 2. To address RQ2, which asks how oﬀ-
task context aﬀects recommendation performance, we mod-
ify the experiment described above to consider a context of
m = [1, |T |] queries such that queries 2–|T | are oﬀ-task. To
capture the randomness of oﬀ-task queries, we evaluate over
R random samples of oﬀ-task contexts (each query is inde-
pendently sampled from other tasks, excluding those with
the same TREC Session Track topic) for each task T and
each value of m > 1. If oﬀ-task context is harmful, we should
see a worsening trend in performance as m increases.

Experiment 3. To address RQ3, which asks how query
recommendation performance is aﬀected by a context that
is a mix of on- and oﬀ-task queries, we rely on a simulation
of mixed contexts. As we saw in Figure 3, the probability
that a sequence of m queries share the same task decreases
exponentially as m increases, and so the mixed context as-
sumed in RQ3 is realistic if not typical. We simulate mixed
contexts by taking each task T of length n and considering
a context window of length m = [1, n + R], where R is the
number of oﬀ-task queries to add into the context. The last
query in the context qm always corresponds to the last query
qn in T . Queries q1, . . . , qm−1 consist of a mix of the queries
from T and other tasks from the TREC Session Track. The
queries from T will always appear in the same order, but
not necessarily adjacent.

To incorporate noise, we initially set C = []. We select
R oﬀ-task queries as follows: ﬁrst, we randomly select an
oﬀ-topic task, O, from the TREC Session Track and take
the ﬁrst R queries from that task. If |O| < R, we randomly
selected an addition oﬀ-topic task and concatenate its ﬁrst
R−|O| queries to O. We continue the process until |O| = |R|.
We now randomly interleave T and O, the only rule being
that Tn—the reference query—must be the last query in C
(an easy rule to adhere to by simply removing Tn before
the randomized interleaving, and then concatenating it to
the end). For a given value of R, we can perform many
randomizations and graph the eﬀect of using the most recent
n + R queries to perform query recommendation.

Experiment 4. The ﬁnal research question, RQ4, asks

Figure 5: Examples of on-task/oﬀ-task segmenta-
tions using sametask1 and sametask2 scoring. The
reference query, q5, sits in the bolded center node.
Note that the edge (q1, q5) goes from 0.2 using
sametask1 to 0.6 under sametask2 due to q1’s strong
similarity to q3, which has a similarity score of 0.6
with q5.

scoring function that uses task clustering is the following:

sametask2(i, j, C) =

s(C[i], C[k])

(19)

max

k ∈ [1,|C|] :

k 6=i∧

KC [C[k]] =KC [C[j]]

Note that sametask2 will return the highest similarity be-
tween C[i] and any member of C[j]’s tasks, excluding C[i].
Figure 5 illustrates a case in which sametask2 improves
over sametask1; note, however, that sametask2 can also
be harmful when an oﬀ-task query is found to be similar to
an on-task query.

5. EXPERIMENTAL SETUP

In this section, we describe the data, settings, and

methodology used for the experiments.

5.1 Constructing a query ﬂow graph

We extracted query reformulations from the 2006 AOL
query log, which includes more than 10 million unique
queries making up 21 million query instances submitted
by 657,426 users between March–April 2006. Considering
all ordered pairs from a 30-query sliding window across
sessions with a maximum timeout of 26 minutes, we ex-
tracted 33,218,915 distinct query reformulations to construct
a query ﬂow graph (compared to 18,271,486 if we used only
adjacent pairs), ignoring all dash (“-”) queries, which corre-
spond to queries that AOL scrubbed or randomly replaced.
The inlink and outlink counts of the nodes in the graph both
have a median of 2 and a mean of about 5. If we were to
use only adjacent reformulations from the logs, the median
would be 1 and the mean just under 2.

5.2 Task data

We used the 2010 and 2011 TREC Session Track [11, 12]
data to generate task contexts. The 2010 track data con-
tains 136 judged sessions, each with two queries (totaling
272 queries), covering three reformulation types: drift, spe-
cialization, and generalization relative to the ﬁrst search.
We ignore the reformulation type. The 2011 track data con-
sists of 76 variable length sessions, 280 queries (average of
3.7 queries per session), and 62 judged topics. Several top-
ics have multiple corresponding sessions. In total, we use all

88how mixed contexts should be used in the query recommen-
dation process. We have limited ourselves to consider three
possibilities: (a.) using only the reference query (i.e., our
baseline throughout these experiments), (b.) using the most
recent n + R queries (i.e., the results from Experiment 3),
or (c.) incorporating same-task classiﬁcation scores. Exper-
iment 4 concentrates on (c.) and analyzes the eﬀect of in-
corporating same-task classiﬁcation scores during the search
context integration process. This is where we will compare
the task-aware recommendation models described in the pre-
vious section.

5.4 Technical details

For the query recommendation using the TQGraph, we
used a restart probability of c = 0.1, as was found to be
optimal by Bonchi et al. [3]. Note that they refer to the
restart value α, where c = 1 − α. To increase the speed of
our recommendation, we only stored the 100,000 top scoring
random walk results for each term. Bonchi et al. [3] found
this to have no or very limited eﬀects on performance when
used with c = 0.1.

For task classiﬁcation, we used the parameters found op-
timal by Lucchese et al. [15]: η = 0.2 (used during task
clustering) and α = 0.5 (used to weight the semantic and
lexical features). We also set τ = η since τ is used in much
the same way in the task-aware recommendation models.

To evaluate recommendations, we retrieved documents
from ClueWeb09 using the default query likelihood model
implemented in Indri 5.3 [17].1 We removed spam by us-
ing the Fusion spam score dataset [6] at a 75th percentile,
meaning we only kept the least spammy 25% of documents.2

6. RESULTS

In this section, we cover the results of each of the experi-
ments described in Section 5.3. We then discuss the meaning
of our ﬁndings as well as their broader implications.

6.1 Experimental results

In all experiments, we measured recommendation perfor-
mance using the mean reciprocal rank (MRR) of ClueWeb09
document retrieved for the top scored recommendation av-
eraged over the 212 TREC Session Track tasks. We found
similar trends using normalized discounted cumulative gain
(nDCG) and precision at 3, 5, and 10. There are several
ways one can calculate relevance over the document sets re-
trieved for recommendations, such as count any document
retrieved in the top ten for any of the context queries as non-
relevant (rather harsh), indiﬀerently (resulting in duplicate
documents), or by removing all such documents from the re-
sult lists of recommendations. We elected to go with the last
as it is a reasonable behavior to expect from a context-aware
system. We removed documents retrieved for any query in
the context, not just those that are on-task. This is a very
conservative evaluation and is reﬂected in the performance
metrics.

Experiment 1. With this experiment, our aim was to
quantify the eﬀect of on-task query context on recommen-
dation quality. Focusing on the top line with circles in Fig-
ure 6, the MRR of the top scored recommendation averaged
over the 212 tasks performs better than using only the ref-

1http://www.lemurproject.org/indri/
2http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/

0
1
0

.

8
0

.

0

6
0

.

0

4
0

.

0

2
0
0

.

R
R
M

On−task context; β=0.8
Reference query only
Off−task context; β=0.8

2

4

6

8

10

Context length (m)

Figure 6: The eﬀect of adding on-task (blue circles)
and oﬀ-task (red triangles) queries versus only the
reference query on recommendation MRR (black
squares). MRR is calculated on the top scoring rec-
ommendation.

e
c
n
e
r
e

f
f
i

 

d
R
R
M

e
c
n
e
r
e

f
f
i

 

d
R
R
M

0

.

1

0

.

0

Effect of on−task context per task

.

0
1
−
0
1

.

0
0

.

.

0
1
−

Effect of off−task context per task

Figure 7: The per session eﬀect of on- and oﬀ-task
context on the change in MRR of the top scoring
recommendation. The y-axis shows the diﬀerence
between the MRR of using context and using only
the reference query. A higher value means context
improved MRR. Note that 145 tasks were removed
as neither on- nor oﬀ-task context had an eﬀect.
The bars are not necessarily aligned between the
two plots and should not be compared.

erence query (middle line with squares). To generate these
scores, we used the rdecay model with β = 0.8, as set by
Boldi et al. [2] in their decay function. For each value of m,
if a particular task T has fewer than m queries, the value
at |T | is used. The MRR scores are low because for a large
number of tasks, none of the methods provide any useful rec-
ommendations. We performed evaluations where such tasks
were ignored and found that the MRR does indeed increase
and the relationship between the methods plotted stays the
same. However, in order to ensure comparability with future
work, we elected to report on all tasks.

While performance is better on average in Figure 6, the
top bar chart in Figure 7 breaks the performance down by
the TREC search tasks and we can see that there are many
tasks for which on-task context is very helpful, as well as
several where it hurts. Note that some of the tasks are not
displayed for readability.

Experiment 2. The goal of the second experiment was
to ascertain the eﬀect of oﬀ-task context on query recom-
mendation. We generated 50 random oﬀ-task contexts for

89each task and report the micro-average across all trials. The
bottom line with triangles in Figure 6 shows that adding
oﬀ-task queries under the rdecay model with β = 0.8 rapidly
decreases recommendation performance for low values of m
before more or less leveling oﬀ around m = 5 (it still de-
creases, but much slower).
Its performance is well below
that of the baseline of using only the reference query, mak-
ing it clear that oﬀ task context is extremely detrimental.

Turning to the bottom plot in Figure 7, we see that oﬀ-
task context has an almost entirely negative eﬀect (there is
an ever so slight increase in performance for the task rep-
resented by the far left bar). Interestingly, for the severely
compromised tasks on the far right, the eﬀect is not as nega-
tive as when on-task context hurts. We have not conducted
a full analysis to understand this phenomena, but one possi-
ble cause is the averaging over 50 trials that takes place for
the oﬀ-task contexts. We leave investigations into this for
future work.

Experiment 3. With Experiment 3, we wanted to un-
derstand the eﬀect of mixed contexts—consisting of both
on- and oﬀ-task queries—on query recommendation perfor-
mance. As explained earlier, the experiment explores the
performance of tasks when R noisy queries are added to the
entire set of on-task queries. The bottom line with trian-
gles in Figure 8 shows just this, using rdecay with β = 0.8.
The far left point, where R = 0, lines up with the far right
point of the on-task line in Figure 6. We randomly gener-
ated 50 noisy contexts per task for each value of R. The solid
line shows the micro-averaged MRR over all tasks’ samples.
The dotted lines on either side show the minimum and max-
imum values for the micro-average MRR on a set of 1,000
sub-samples (with replacement) of the original 50. As you
can see, the bounds indicate relatively low variance of the
micro-average across the 212 tasks. There are still certain
tasks for which performance is very high or very low (that
is, the bounds on the micro-average do not inform us of the
variance among tasks).

An important observation from this experiment is that
performance dips below that of the baseline when even a sin-
gle oﬀ-task query is mixed in. This is quite startling when
you consider that the chances of three queries (at R = 1, all
contexts are of at least length three) in a row belonging to
a single task are below 30% (see Figure 3) and that roughly
40% of tasks in the wild are of length three or more (see Fig-
ure 2). These results clearly show that blindly incorporating
mixed context is a poor method of incorporating context.

Experiment 4. In the ﬁnal experiment, we hoped to de-
termine the eﬀects of using recommendation models that
consider the reference query only, the entire context, or
the entire context, but in a task-aware manner. The ﬁrst
two were addressed in the previous experiments, where we
learned that using the reference query is more eﬀective than
blindly using the entire context. Figure 8 shows the re-
sults of using the models we introduced in Section 4.3. We
used the same randomizations as in Experiment 3 and like-
wise generated the minimum and maximum bounds around
each model’s performance line. For these experiments,
sametask1 scores were used to produce same-task scores.
We also performed the experiment using sametask2 and
found it was comparable. We used λ = 1 for all task-aware
models; setting it to anything less resulted in an extreme
degradation of performance.

There are several interesting observations. First, the ﬁrm-

0
1

.

0

8
0

.

0

6
0

.

0

4
0

.

0

2
0
0

.

R
R
M

Firm−task1 (λ=1, β=0.8)
Firm−task2 (λ=1, β=0.8)
Hard−task (λ=1, β=0.8)
Reference query only
Soft−task (λ=1, β=0.8)
Decay (β=0.8)

0

2

4

6

8

10

Noisy queries added (R)

Figure 8: The eﬀect of adding oﬀ-task queries to a
task context on MRR when same task classiﬁcation
is used and is not used versus only using the refer-
ence query (black squares). The sametask1 scoring
method is used for all task-aware recommendation
models. MRR is calculated on the top scoring rec-
ommendation.

task models performed best, though it is likely that the
performance of the rﬁrmtask1 model (the top line with x’s)
would decrease with larger amounts of noise because the de-
cay function depends on the length of the context, not the
number of queries predicted to be on-task. Thus, for on-task
queries occurring early on in very large contexts, the decay
weight will eﬀectively be 0. You may notice that this model
also increases for a bit starting at R = 2. This is likely
due to the decay function used: since every query in the
context, and not just the on-task queries, count toward the
distances between an on-task query and the reference query
under rﬁrmtask1, on-task queries are actually down-weighted
to a greater degree than in the rﬁrmtask2 and rhardtask mod-
els. The graph indicates that this change in weighting is
helpful. This also suggests that setting β diﬀerently may
improve the performance of the other models.

The rﬁrmtask2 model (diamonds) comes in at a close second
and narrowly outperforms rhardtask (circles). All three mod-
els outperform the baselines—using only the reference query
(squares) and rdecay over the entire context (triangles).

The rsofttask model, however, performs rather poorly.
While it can oﬀer some improvement over using just the
reference query for a small amount of noise, once the noise
level reaches four oﬀ-task queries, it is not longer viable.
It does, however, outperform the decay model applied in a
task-unaware manner.

Another interesting point is that the performance of the
task-aware models is actually better at R = 0 than if the
known tasks are used. The likely explanation is that the
same-task scores prevent on-task queries that are quite dif-
ferent from the reference query from aﬀecting the ﬁnal rec-
ommendation. These kinds of queries may introduce more
noise since their only recommendation overlap with the ref-
erence query may be generic queries, such as “google”. This
is not always the case, however. For example, one task con-
sists of the queries [“alan greenspan”, “longest serving Fed-
eral Reserve Chairman”]. The ﬁrst query is detected to be

90Rank Reference query RR Decay

1.
2.
3.
4.
5.

google
ebay
yahoo
yahoo.com
google.com

history of ﬁlms
the history of european culture of drinking
the history of european alcohol drinking. . .
the history of european alcohol drinking
aircraft crash testing

RR Hard task (λ = 1)
black powder sabots
black powder cannons
google
marlin ﬁrearms
black powder weapons for sale

RR
1.00
0.33

0.08

Figure 9: The top 5 suggestions generated from three of the models for the randomly generated context
shown in Figure 10. Reciprocal rank (RR) values of 0 are left blank.

No. Query context
5.
4.
3.
2.
1.

black powder inventor
wikipedia black powder
us geographic map
black powder ammunition
us political map

sametask1

1.00
0.52
0.06
0.75
0.03

Figure 10: An example of a randomly generated
mixed context along with the same-task scores. The
top query (No. 5) is the reference query. The bolded
queries are on-task.

oﬀ-task, however, it is imperative to generate decent recom-
mendations since the reference query generates generic Fed-
eral Reserve-related queries and not ones focused on Alan
Greenspan.

Overall, though, the same-task classiﬁcation with a thresh-
old τ = 0.2 worked well. The same-task classiﬁcation preci-
sion relative to the positive class was on average 80%. The
precision relative to the negative class varied across each
noise level, but was on average 99%. The average accuracy
was 93%. The decent same-task classiﬁcation is why the
three models at the top of Figure 8 are so ﬂat.

6.2 Discussion

The results of our experiments demonstrate not only the
usefulness of on-task context, but also the extreme impact of
oﬀ-task and mixed contexts. The results from Experiment
4 suggest that the appropriate model is one that balances a
hard threshold to remove any inﬂuence from context queries
predicted to be oﬀ-task, and to weight the importance of
the remaining queries by both their distance to the reference
query and by the conﬁdence of the same-task classiﬁcation.
Based on the results, we recommend the rﬁrmtask2 model,
since its performance will be consistent regardless of how
far back in a user’s history we go, unlike rﬁrmtask1.

We did not see any substantial eﬀects from using task clus-
tering, as Liao et al. [14] used. However, other task iden-
tiﬁcation schemes may perform diﬀerently; after all, as we
saw in Experiment 4, our task identiﬁcation method actually
caused slight improvements over using the true tasks.

To get a feel for the quality of the recommendations pro-
duced generally with the AOL query logs and speciﬁcally
by diﬀerent models, consider the randomly generated mixed
context in Figure 10. The top ﬁve recommendations from
three methods for this context are shown in Figure 9. No-
tice that using only the reference query produces popular
queries, none of which are related to the context in the least.
Meanwhile, blindly using the context produces suggestions
that are swamped by the oﬀ-task queries. This is in stark
contrast to using the hard task model, which suggests four
decent looking suggestions, three of which have non-zeros
reciprocal rank values.

Another observation from this example is the amount of

noise: very popular queries such as “google” appear in the
suggestion lists. This is in part due to not ﬁnely tuning
the various parameters of the underlying recommendation
algorithm we used—something we avoided since the recom-
mendation algorithm was not the focus of this work. It is
likely also due to the age and scope of the AOL query log,
which is not large compared to the commercial logs com-
monly used in query recommendation research. Nonetheless,
the context-aware recommendation models we presented in
Section 4 are compatible with any recommendation algo-
rithm that supplies a suﬃciently long, scored list of sugges-
tions. We leave the investigation as to whether performance
follows for future work.

Many of the tasks for which task-aware recommenda-
tion performed well involved specialization reformulations.
Some examples include: heart rate→slow fast heart rate,
bobcat tractor →bobcat tractor attachment, disneyland ho-
tel→disneyland hotel reviews, and elliptical trainer →elliptical
trainer beneﬁts. One possible reason for this is that incorpo-
rating recommendations for a context query that is a subset
of the reference query focuses the ﬁnal recommendations on
the most important concepts.

None of the experiments used notions of temporal sessions
or complete user histories. We did this mainly because the
mixed contexts were generated and not from actual user logs
where context windows could be tied to temporal bound-
aries, e.g., two-day windows. We believe that by focusing
on factors such as on- and oﬀ-task queries, we struck at the
core questions in this space. We leave testing whether the re-
sults from these experiments port to real user data to future
work, but we believe they will, especially given the results of
related studies, such as those conducted by Liao et al. [14]
and Filali et al. [8].

7. SUMMARY AND FUTURE WORK

In this work, we investigated four key research questions
surrounding context-aware query recommendation and the
eﬀects of on- and oﬀ-task recommendation: (RQ1) How does
on-task context aﬀect recommendation quality relative to
using only the most recently used query? (RQ2) How does
oﬀ-task context aﬀect recommendation quality? (RQ3) How
does mixed context consisting of on- and oﬀ-task queries
aﬀect recommendation quality? and (RQ4) How do task-
aware recommendation models aﬀect performance given a
mixed context?

We designed four sets of experiments that used random
walks over a term-query graph of the 2006 AOL query
logs for recommendations and the 2010–2011 TREC Session
Track data for tasks. We evaluated all models using micro-
averaged mean reciprocal rank (MRR) over the 212 TREC
tasks and used 50 trials when randomization was needed.
The results of our experiments show that on-task context
is, on average, very helpful, but can sometimes degrade per-

91formance substantially. On the other hand, oﬀ-task context
only degrades performance. We found that leveraging mixed
contexts, when used without regard of the task correspond-
ing to each of its constituent queries, reduced the quality of
recommendations. Finally, we demonstrated the eﬀective-
ness of four task-aware models, which rely on a state-of-the-
art search task identiﬁcation algorithm. Three of the four
models out-performed using only the most recently submit-
ted query when up to ten oﬀ-task queries were added. Re-
sults suggest that two of those models would maintain their
accuracy at large levels of noise since their weighting schemes
completely ignore any query classiﬁed as oﬀ-task.
In many ways these results are not surprising:

incorpo-
rating relevant queries improves query accuracy just as rel-
evance feedback improves document ranking. What is sur-
prising, though, is the extraordinary sensitivity of these ap-
proaches to oﬀ-task queries. Even making a single mistake
and including an oﬀ-task query can shift many approaches
in the wrong direction, away from relevant documents. This
work takes a methodical look at the relative impact of on-
and oﬀ-task queries and provides a deeper understanding of
their inter-relationships than had been reported previously.

In addition, our ﬁndings rely on publicly accessible datasets,

which eases others’ eﬀorts in validating and reproducing our
results. Furthermore, it allows others to compare directly
with our results if they so desire. The simulated contexts and
corresponding recommendations used in our experiments are
available on our website.3

There are many avenues of future work. One element
to explore is the portability of our ﬁndings to real user
search behavior over many more tasks. Another is to con-
sider other recommendation algorithms, including ones not
dependent on query logs [1] and ones that use other user
behavior [7, 19]. We only considered the eﬀectiveness of
query recommendations to address the current task, but
other measures of suggestion quality exists [16], such as at-
tractiveness and diversity; exploring these other evaluations
would provide a fuller picture of the value of task-aware rec-
ommendation. Other future directions include expanding
the analysis to other applications, such as website or task
recommendation [20].

8. ACKNOWLEDGMENTS

This work was supported in part by the Center for In-
telligent Information Retrieval and in part by NSF grant
#IIS-0910884. Any opinions, ﬁndings and conclusions or
recommendations expressed in this material are those of the
authors and do not necessarily reﬂect those of the sponsor.

References
[1] S. Bhatia, D. Majumdar, and P. Mitra. Query sugges-
tions in the absence of query logs. In Proc. of SIGIR,
pages 795–804, 2011.

[2] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gio-
nis, and S. Vigna. The query-ﬂow graph: model and
applications. In Proc. of CIKM, pages 609–618, 2008.
[3] F. Bonchi, R. Perego, F. Silvestri, H. Vahabi, and
R. Venturini. Eﬃcient query recommendations in the
long tail via center-piece subgraphs. In Proc. of SIGIR,
pages 345–354, 2012.

3http://ciir.cs.umass.edu/downloads/
task-aware-query-recommendation

[4] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and
H. Li. Context-aware query suggestion by mining click-
through and session data. In Proceeding of KDD, pages
875–883, 2008.

[5] H. Cao, D. Jiang, J. Pei, E. Chen, and H. Li. Towards
context-aware search by learning a very large variable
length hidden Markov model from search logs. In Proc.
of WWW, pages 191–200, 2009.

[6] G. Cormack, M. Smucker, and C. Clarke. Eﬃcient and
eﬀective spam ﬁltering and re-ranking for large web
datasets. Information retrieval, 14(5):441–465, 2011.

[7] S. Cucerzan and R. W. White. Query suggestion based
on user landing pages. In Proc. of SIGIR, pages 875–
876, 2007.

[8] K. Filali, A. Nair, and C. Leggetter. Transitive history-
based query disambiguation for query reformulation. In
Proc. of SIGIR, pages 849–850, 2010.

[9] C. Huang, L. Chien, and Y. Oyang. Relevant term sug-
gestion in interactive web search based on contextual
information in query session logs. JASIST, 54(7):638–
649, 2003.

[10] R. Jones and K. L. Klinkner. Beyond the session time-
out: automatic hierarchical segmentation of search top-
ics in query logs. Proceedings of CIKM 2008, 2008.

[11] E. Kanoulas, P. Clough, B. Carterette, and M. Sander-
son. Session track at TREC 2010. In Proc. of the SI-
GIR Workshop on the Simulation of Interaction, pages
13–14, 2010.

[12] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and
M. Sanderson. Overview of the TREC 2011 Session
Track. In Proc. of TREC, 2011.

[13] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais,
and J. Teevan. Modeling and analysis of cross-session
search tasks. In Proc. of SIGIR, pages 5–14, 2011.

[14] Z. Liao, Y. Song, L.-w. He, and Y. Huang. Evaluating
the eﬀectiveness of search task trails. In Proc. of WWW,
pages 489–498, 2012.

[15] C. Lucchese, S. Orlando, R. Perego, F. Silvestri, and
G. Tolomei.
Identifying task-based sessions in search
engine query logs. In Proc. of WSDM, pages 277–286,
2011.

[16] Z. Ma, Y. Chen, R. Song, T. Sakai, J. Lu, and J. Wen.
New assessment criteria for query suggestion. In Proc.
of SIGIR, pages 1109–1110, 2012.

[17] D. Metzler and W. Croft. Combining the language
model and inference network approaches to retrieval.
Information processing & management, 40(5):735–750,
2004.

[18] F. Radlinski and T. Joachims. Query chains: learning
to rank from implicit feedback. In Proc. of KDD, pages
239–248, 2005.

[19] Y. Song and L.-w. He. Optimal rare query suggestion
with implicit user feedback. In Proc. of WWW, pages
901–910, 2010.

[20] G. Tolomei, S. Orlando, and F. Silvestri. Towards
In

a task-based search and recommender systems.
ICDEW, pages 333–336, 2010.

92