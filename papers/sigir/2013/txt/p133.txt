An Effective Implicit Relevance Feedback Technique Using

Affective, Physiological and Behavioural Features

Yashar Moshfeghi

School of Computing Science

University of Glasgow

Glasgow, UK

Yashar.Moshfeghi@glasgow.ac.uk

Joemon M. Jose

School of Computing Science

University of Glasgow

Glasgow, UK

Joemon.Jose@glasgow.ac.uk

ABSTRACT
The eﬀectiveness of various behavioural signals for implicit
relevance feedback models has been exhaustively studied.
Despite the advantages of such techniques for a real time in-
formation retrieval system, most of the behavioural signals
are noisy and therefore not reliable enough to be employed.
Among many, a combination of dwell time and task infor-
mation has been shown to be eﬀective for relevance judge-
ment prediction. However, the task information might not
be available to the system at all times. Thus, there is a need
for other sources of information which can be used as a sub-
stitute for task information. Recently, aﬀective and phys-
iological signals have shown promise as a potential source
of information for relevance judgement prediction. How-
ever, their accuracy is not high enough to be applicable on
their own. This paper investigates whether aﬀective and
physiological signals can be used as a complementary source
of information for behavioural signals (i.e. dwell time) to
create a reliable signal for relevance judgement prediction.
Using a video retrieval system as a use case, we study and
compare the eﬀectiveness of the aﬀective and physiological
signals on their own, as well as in combination with be-
havioural signals for the relevance judgment prediction task
across four diﬀerent search intentions: seeking information,
re-ﬁnding a particular information object, and two diﬀerent
entertainment intentions (i.e. entertainment by adjusting
arousal level, and entertainment by adjusting mood). Our
experimental results show that the eﬀectiveness of studied
signals varies across diﬀerent search intentions, and when
aﬀective and physiological signals are combined with dwell
time, a signiﬁcant improvement can be achieved. Overall,
these ﬁndings will help to implement better search engines
in the future.

Categories and Subject Descriptors: H.3.3 Information
Storage and Retrieval - Information Search and Retrieval -
Search Process

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

General Terms: Performance, Experimentation
Keywords: Implicit Relevance Feedback, Aﬀective, Physi-
ological, Behavioural, Dwell Time, Search Intentions

1.

INTRODUCTION

It has been understood for some time that the formulated
queries in an information seeking process do not always pro-
vide an adequate description necessary to retrieve relevant
documents [39], since it is only an approximation of the ac-
tual information need [33]. Likewise, queries formulated may
not adequately deﬁne the characteristics of relevant doc-
uments, or indeed any relevant information because of an
ill-deﬁned information need situation. Since more complete
representations generally lead to more precise search results,
supplementing textual queries with additional sources of in-
formation, such as documents users have found relevant [26],
can be advantageous [37]. Research has shown that searchers
can specify the relevance of a document to their needs more
easily than explicitly specifying their information needs [31].
Therefore, in order to help the searcher, IR systems have pre-
viously employed a range of relevance feedback techniques,
where feedback is gathered through explicit [20], implicit
[37], and/or aﬀective feedback [1].

Despite the robustness of explicit feedback in improving
retrieval eﬀectiveness [20], it is not always applicable or re-
liable due to the cognitive burden that it places on users
[38]. To overcome this cognitive burden, implicit relevance
feedback (IRF) is proposed; with relevance inferred instead
from behaviour signals (i.e.
interactional data) in an indi-
rect and unobtrusive manner [18]. For example, researchers
try to understand how dwell time (the time spent by a user
on a retrieved document) [19] and task [37] relate to rele-
vance. Dwell time on a document has got a particular inter-
est since it can be detected in real-time [22]. Recent studies
have shown that dwell time combined with task information
can improve IRF algorithm performance [37]. However, in a
real-time situation, task information might not be apparent
or easy to obtain in many cases. Hence, there is a need for
other sources of information which can be used as a substi-
tute for the task information.

Due to recent advances in technology, new sensory devices
have been developed. It is now possible to leverage aﬀective
and physiological signals from user while they are interacting
with a system. For instance, facial expression recognition,
galvanic skin response, heart rate monitoring and electroen-
cephalography provide more information about interaction
than ever before. Currently, there are numerous eﬀorts in

133disciplines that extend beyond IR to understand how such
information can be used to improve human-machine inter-
actions, e.g., [29]. The output of these sensory devices has
gained notable attention from the IR community, in par-
ticular for their eﬀectiveness in predicting relevance. As a
result of such studies, aﬀective feedback has been proposed
[1] where the intention is to capture facial expression [4],
and physiological signals [3] (such as skin temperature) and
use them as implicit relevance judgement. Despite the great
potential of this information for IRF techniques, the accu-
racy of such information in the past was not high enough to
be used on its own in a real time system.

Our hypothesis is that signals obtained from sensory chan-
nels can be considered as a suitable replacement of task in-
formation for IRF techniques based on dwell time informa-
tion. This hypothesis is motivated by the ﬁndings of recent
studies from both Poddar et al. [30] where they show that
tasks with diﬀerent characteristics stimuli diﬀerent emotions
in the searcher, and Arapakis et al. [1] where they show that
relevance information that derives from the selected sen-
sory channels is correlated to user aﬀective behaviour. They
investigated whether they can deduce topical relevance by
measuring physiological signals taken from the participants,
in the context of online information seeking. However, the
eﬀect of combining signals obtained from sensory devices
with behavioural data has yet to be studied. Successful re-
sults steer the situation for using IRF in a real-time system.
This paper is an attempt in this vein of research.

To further investigate our hypothesis, we study the eﬀec-
tiveness of such IRF techniques for search tasks with dif-
ferent intentions. Recent advances in query log analysis for
search engines such as Google, Yahoo! and Bing has shown
that Web searchers’ intentions do not always ﬁt into the typi-
cal taxonomy of informational, navigational or transactional
intentions [34]. Although informational (i.e. to acquire in-
formation present on one or more sites), navigational (i.e.
to reach a particular site) and transactional (i.e. to perform
some Web-mediated activity) intentions are all commonly
found in query logs [7], there is an increasing body of evi-
dence showing that re-ﬁnding and re-retrieving the visited
information is a regular activity by searchers [34, 12].
In
addition, there is also recent interest in the notion of en-
tertainment, where the searchers do not have any particular
information need [13] and the prime motivation behind their
search is to satisfy an emotion need [27]. Thus the eﬀective-
ness of these sources of information on their own and in
combination with behavioural data (i.e. dwell time) has yet
to be studied for search processes with diﬀerent intentions,
i.e.
information seeking, re-ﬁnding information and/or en-
tertainment.

Therefore, two important questions emerge. First, whether
combining sensory channels and behavioural signals provide
a reliable set of features for IRF techniques. Second, what
would be the eﬀect of diﬀerent search intentions (e.g. seeking
information, re-ﬁnding or entertainment) on the relevance
judgement predictability of this set of features.
In order
to investigate our research questions, we use an interactive
information retrieval evaluation framework.
In particular,
we devised four search tasks each of which cover a search
intention. We then capture aﬀective and physiological sig-
nals of the participants while they were performing the tasks
along with their relevance assessment for the visited videos.
Finally, we investigate the discriminative power of the cap-

tured signals with and without dwell time for predicting the
relevance of the assessed videos across diﬀerent search in-
tentions.

This paper has three novel contributions. First, we stud-
ied and compared the discriminative power of multitude sen-
sory channels (i.e. EEG, heart rate, facial expression, skin
temperature, etc.) for relevance judgement prediction task.
Second, we investigated the possibility of substituting task
information with these sensory signals for dwell time based
IRF techniques. Finally, we investigated the prediction ac-
curacy of these signals for search processes with diﬀerent
intentions. The rest of the paper is organised as follows: re-
lated work is presented in Section 2, the experiment method-
ology is described in Section 3. Results and discussion are
presented in Section 4, and ﬁnally, the paper is concluded
in Section 5.

2. RELATED WORK

Dwell Time and IRF: Much past research has at-
tempted ﬁnding a reliable set of features for IRF techniques
[18]. One of the main features investigated researchers is
dwell time, due to its applicability for real time systems
[22]. Despite early studies by Morita et al.
[25] and Clay-
pool et al. [8] where they proposed dwell time as a reliable
behavioural signal for IRF, later studies conducted by Kelly
and Belkin [19] show that dwell time by its own is not reli-
able and that it can diﬀer signiﬁcantly according to speciﬁc
task, and according to speciﬁc user. More recent studies in
this path by White and Kelly [37] show that a combination
of dwell time and task information is a reliable source of in-
formation for IRF techniques. However, information about
the search task is not available all the time. Thus many
approaches try to predict this information. One of the com-
mon way to do so is to use the behavioural signals such as
dwell time [21]. However, this approach cannot be useful if
the predicted task information is required to be combined
with the dwell time itself for IRF. Therefore, clearly there
is a need for other sources of information that can be sub-
stituted with the task information for dwell time based IRF
techniques. This paper is an attempt to investigate this re-
search question.

Task and Emotion:

In recent years the relation be-
tween the search tasks within an information seeking process
(ISP) and emotion has gained much attention. For exam-
ple, several works studied the emotional impact of search
tasks within the ISP [30, 2, 23]. Poddar and Ruthven [30]
examined how participants emotion responses are inﬂuenced
by tasks of diﬀerent natures. In particular, their results in-
dicate that (i) artiﬁcial tasks have higher uncertainty and
less sense of ownership than genuine search tasks, and (ii)
more complex search tasks have lower positive emotions and
more uncertainty before and after searching. Similar ﬁnd-
ings were earlier reported also by [2] in an information seek-
ing activity. They concluded that users’ emotions progres-
sively transit from positive to negative valence as the de-
gree of task diﬃculty increases. They also found that emo-
tions both interweave with diﬀerent physiological, psycho-
logical and cognitive processes during an information seek-
ing process, and form distinctive patterns according to spe-
ciﬁc tasks. However, [23] reported no signiﬁcant relationship
between searchers’ mood and search tasks due to the com-
plexity involved in such studies. In another study, [17] in-
vestigated the relationship between the subjective (e.g. hap-

134piness levels, feeling lost during search, etc.) and objective
(e.g. search outcomes and search task characteristics, etc.)
factors in the ISP. Their results show that “higher happiness
levels before the search and during the search correlate with
better feelings after the search, but also correlate with worse
search outcomes and lower satisfaction, suggesting that, per-
haps, it pays oﬀ to feel some ‘pain’ during the search in or-
der to ‘gain’ quality outcomes” [17]. These valuable insights
into the relation between emotion and search task motivated
our research. This paper investigates the possibility of sub-
stituting task information with experienced emotion to be
combined with dwell time in order to have a reliable real
time features for IRF techniques. The question that might
be raised is how one can capture human emotion.

Aﬀective Relevance Feedback: Recent advances in
information technology have facilitated the development of
sensory channels to capture human emotion [28, 29, 11]. The
argument is that such changes are often expressed through
a psycho-physiological mobilisation that is reﬂected by a se-
ries of more or less observable cues, such as facial expres-
sions, body movements, localised changes in the electroder-
mal activity, variations in the skin temperature, and many
more [3]. The aﬀective and physiological (A&P) signals
obtained from such devices have provided researchers with
additional sources of information not previously available.
Thus these A&P signals have gained signiﬁcant attention in
the IR community, and their eﬀectiveness has been studied
for numerous tasks and purposes, e.g [1, 3, 4].

The most related work to this paper are the studies con-
ducted by Arapakis and his colleagues on the eﬀectiveness of
A&P information for relevance judgment prediction (known
as aﬀective feedback) [2]. For example, Arapakis et al. [4]
show that aﬀective features captured from facial expression
during an information seeking process can be used to develop
a multimodal recommendation system.
In another study,
[3] explored the role of aﬀective (captured
Arapakis et al.
via facial expression) and physiological signals (captured via
skin temperature and body movement) in designing multi-
media search systems. They investigated whether topical
relevance could be deduced by measuring key physiological
signals taken from the user. Finally, In a more recent study,
the role of aﬀective signals was studied in a personalised
system [1] for topical relevance.

However, two main limitations can be associated with
these studies. First, the accuracy of A&P information in
the past was not high enough to be used by its own in a real
time system. For example, Feild [16] found little beneﬁt
from the physiological signals when they tried to used them
for searcher frustration prediction where three physical sen-
sors were used: a mental state camera, a pressure sensitive
mouse, and a pressure sensitive chair. Given the relation
between task characteristics and stimulated emotion in the
searcher (explained above) and the possibility of extracting
emotion using A&P sensory channels, we hypothesis that
a combination of A&P signals with behavioural signals (in
particular dwell time) can lead to a reliable set of features
for IRF. Second, the eﬀectiveness of A&P information was
only investigated for information seeking scenarios where a
clear information need is deﬁned (i.e.
topical relevance).
Thus the eﬀectiveness of such sensory channels for a more
realistic representation of real life search process has yet to
be studied, where variety of tasks with diﬀerent intentions

are proposed. This paper is an attempt to investigate this
research problem.

Seeking Information on the Web: Traditionally, the
general belief of the Information Retrieval (IR) community
was that the dominant intention of people engaging in search
processes was informational [7]. However, Broder [7] shows
the existence of search intents other than the assumed infor-
mational intent, i.e. navigational and transactional. In more
recent studies, researchers have identiﬁed a wider range of
search intents that go beyond Broader’s taxonomy of inten-
tions, i.e. re-ﬁnding [34] and entertainment [13]. Re-ﬁnding
the visited information has been shown to be a regular ac-
tivity by searchers [34, 12]. Teevan et al. [34], by analysing
Yahoo! Web query logs showed that up to 40% of the sub-
mitted queries were re-ﬁnding queries. Elsweiler et al. [12],
in an empirical study, showed that it is possible to isolate
re-ﬁnding behaviour in search logs through various qualita-
tive and quantitative analyses. Given this evidence, it is
important to study the eﬀect of aﬀective, physiological and
behavioural signals in the re-ﬁnding intention.

In addition, recently there is an increased eﬀort in the
research community to explore search processes with enter-
tainment intents, e.g. “Entertain Me” [5] and “Search4FUN”
[14] workshops. Besides the works published in these work-
shops, [13] attempted to understand the needs and moti-
vation underlying leisure-based activities in the context of
television viewing. They reported that the nature of the
need and motivation are diﬀerent between leisure-based and
work-based situations where most of the needs reported for
leisure-based situation are motivated by a desire to change
mood, emotion, or arousal level. Further research related to
leisure-based activity is the work by [32] which studies plea-
sure reading behaviour. Their results show that pleasure
readers ﬁnd information without having any prior informa-
tion need. In addition, [40] and in a more in-depth study,
[15], reported that in casual search, the motivation is not to
resolve an information need, but hedonistic, e.g. entertain-
ment driven.

Given the emergence of entertainment seeking intentions
on the Web and their unexplored characteristics, it is also
important to study the eﬀect of aﬀective, physiological and
behavioural signals in entertainment seeking intentions. The
deﬁned entertainment seeking intentions in this paper are
based on research done by Zillmann [36], who was instru-
mental in the establishing a range of theories in this do-
main. Zillmann and his colleagues posit that entertainment
choices are a reﬂection of a basic human need to enhance or
retain positive states, and to lessen or steer clear of negative
ones [27]. They have suggested two possible states in which
there may be a need for regulation: physiological arousal and
aﬀect. In the case of physiological arousal, their study sug-
gests that users might be over-stimulated (i.e., stressed) or
under-stimulated (i.e., boredom). In the case of aﬀect states,
it suggests that users might be in negative (i.e. dysphoric)
or positive (i.e. upbeat) moods. An individual experiencing
such states will choose their entertainment content accord-
ing to their expectations of what would lead them back to an
optimal state [27]. Thus, we investigate two entertainment
seeking intentions: entertainment by adjusting arousal level
and entertainment by adjusting mood.

1353. EXPERIMENTAL METHODOLOGY
3.1 Experiment Design

This study used a within subject design. The independent
variable was the search intents (with four levels: “Informa-
tion Seeking” (INS), “Information Re-ﬁnding” (INF), “En-
tertainment by adjusting Arousal” (ENA), “Entertainment
by adjusting Mood” (ENM)), which was controlled by the
simulated search task given to the participants (see Section
3.2). We did not perform any control on the number of rele-
vant and irrelevant results, in order to simulate a real search
scenario situation as much as possible. The dependent vari-
ables were: (i) task perception and (ii) relevance judgment
prediction accuracy.
3.2 Tasks

Four search tasks were devised, each simulating a search
intent. The structural framework of simulated need situa-
tions [6] were used to present search tasks. By doing so,
short cover stories were introduced that helped us describe
to our participants the source of their information need, the
problem to be solved and the environment of the situation.
This facilitated a better understanding of the search objec-
tive and introduced a layer of realism, while preserving well-
deﬁned relevance criteria. In the following, each of the search
tasks is explained in detail.

INS Task: This search task simulates the information
seeking search intent. Information seeking is the most stud-
ied search intent in the information seeking and retrieval
domain. This task is designed as a control group since the
majority of studies on relevance judgement prediction in the
past were based on a similar search task intent. For this
search task, we prepared a number of search topics that cov-
ered a variety of contexts in order to capture participants’
interests as best as possible. The topics, presented in Ta-
ble 1, were all checked manually prior to the experiment,
to ensure the availability of relevant documents. The simu-
lated search scenario for INS task was as follows: “Imagine
you have graduated recently and are going to interview for
a job in a local company. As part of the interview process,
you are asked to explain and expand on the area you will be
working on. You feel very enthusiastic about the interview;
however, due to your lack of knowledge you would like to ﬁnd
out more about this particular topic before taking part in the
interview.” Each participant was then asked to choose one
of the topics that they were unfamiliar with but consider
interesting. Using the video retrieval system, they had to
ﬁnd as many relevant videos as possible so that they could
acquire a good knowledge about their selected topic.

Table 1: Search topics for the information seeking task scenario.

Obtaining information regarding contraception methods.
Investigating new knowledge on global warming.
Formulating an opinion about existing social networking sites.

INF Task: This search task simulates the information
re-ﬁnding search intent. There are two diﬀerences between
this task and the previous one: (i) there is only one docu-
ment that can satisfy the information need of the searcher,
and (ii) the searcher has seen the relevant documents at
some point before initiating the search process and is now at-
tempting to re-ﬁnd it. The similarity between this task and

the previous one is that in both cases searchers have an in-
formation need. For this search task, we prepared a number
of videos that covered a variety of contexts in order to cap-
ture participants’ interests as best as possible. The videos
were intentionally selected as they would likely be very hard
to ﬁnd because they lacked textual description. The moti-
vation was to simulate a challenging information re-ﬁnding
process where the recalled terms are very ambiguous and do
not lead directly to the relevant item. This would better
represent many realistic re-ﬁnding tasks, when the user can-
not recall the exact description of the item they are looking
for. The simulated search scenario for INF task was as fol-
lows: “Imagine you are discussing a video which you have
seen few days ago with your friends. They are interested in
seeing the video and have asked you to send them a link to it.
You can remember the content of the video but you cannot
remember its title or any textual information which can help
you in retrieving it.” Each participant was then asked to
select and watch one of the three videos presented to them
(an animal1, martial arts2, or a science video3) that they
are unfamiliar with and consider interesting. Once partici-
pants watched the video, they were asked to ﬁnd it as fast
as possible.

ENA Task: This search task simulates the entertainment-

based search intent where searchers adjust their arousal level.
The main diﬀerence between this task and the two tasks be-
fore is that the primary need is hedonistic rather than infor-
mational. Therefore, to accurately simulate such search pro-
cesses, we avoid introducing any explicit information need.
Thus, for this search task, we did not prepare any speciﬁc
search topic. This decision is motivated by the literature in
sociology [27] and the information seeking and retrieval do-
main on entertainment [13]. The simulated search scenario
for the ENA task was as follows: “Imagine you are working
in a factory as a night-guard. You have just ﬁnished your
routine checks and will be taking out more checks shortly.
You are tired so you decide to watch some videos to wake
yourself up and make yourself ready for your next round of
checks.” Each participant was asked to ﬁnd as many relevant
videos as possible that make them feel excited.

ENM Task: This search task simulates the entertainment-

based search intent where searchers adjust their mood. Sim-
ilar to ENA task, searchers engage in such search processes
with hedonistic rather than informational needs. Therefore,
to accurately simulate this search process, we avoid intro-
ducing any explicit information need. Thus, similar to the
ENA task, we did not provide any pre-prepared search topic.
The simulated search scenario for the ENM task was as fol-
lows: “Imagine your boyfriend/girlfriend is travelling and
communication access is very limited. It is now a few days
since he/she has gone and you are missing him/her very
much. You are feeling very sad and in order to change your
mood, you have decided to watch some videos.” Each partic-
ipant was asked to ﬁnd as many relevant videos as possible
that make them feel happy.

Questionnaires: At the beginning of the experiment,
the participants were given an entry questionnaire. This
gathered background and demographic information, and in-
quired about previous experience with online videos, in par-
ticular, browsing and searching habits including their inten-

1

2

3

www.youtube.com/v/wx7rY11qb0k&showinfo=0

www.youtube.com/v/pEUeP7hx8fs&showinfo=0

www.youtube.com/v/P8Cs2tO5w74&showinfo=0

136tions. At the end of each task, the participants completed
a post-task questionnaire to elicit the subject’s viewpoint
on certain aspects of the search process, in particular their
perception of the encountered task. All of the questions
included in these questionnaires were a forced-choice type.
Finally, an exit questionnaire was introduced at the end of
the study.
In this questionnaire we gathered information
about the encountered system as well as the user study in
general: which task they preferred and why, and their gen-
eral comments about the user study.

Procedure: The user study was carried out in the fol-
lowing manner. The formal meeting with the participants
took place in a laboratory setting. At the beginning of
the session the participants were given an information sheet
which explained the conditions of the experiment. They
were then asked to sign a Consent Form and were notiﬁed
about their right to withdraw at any point during the study,
without aﬀecting their legal rights or beneﬁts. Then, they
were given an Entry Questionnaire to ﬁll in.

The session proceeded with a brief tutorial on the use of
the search interface. Then participants were helped to wear
the sensory devices, followed by a calibration of the web-
camera. After completion of this step, each participant had
to complete four search tasks (explained in Section 3.2), one
for each level of search process intentions (see Section 3.1).
To negate the order and fatigue eﬀects we counter-balanced
the task distribution using a Latin Square design. The sub-
jects were asked every time to provide judgment for any
video that they watch, and were given 10 minutes to com-
plete their task, during which they were left unattended to
work. At the end of each task, the subjects were asked to
complete a post-task questionnaire. Questions in the post-
task questionnaire were randomised to avoid the eﬀect of
fatigue. Between each task, a cooling-oﬀ period was applied
to minimise the carry-over eﬀect. An exit questionnaire was
administered at the end of the session. Finally, the partici-
pants were asked to sign a payment form, prior to receiving
the payment of £12.

Each study took approximately 120 minutes to complete;
this is from the time they accepted the conditions until they
signed the payment receipt. Users could only participate
once in the study. The total cost of the evaluation was £348,
including the cost of the pilot studies. A user study with the
procedure explained above was conduced over a period of 10
days from 16th to 26th of July 2012. The results of these
studies are presented in Section 4.

Apparatus: For our experiment we used one desktop
computer, equipped with a monitor, keyboard, and mouse.
The computer provided access to a custom-made video re-
trieval system explained in Section 3.4. The web-camera
(Creative Live! Cam Optia AF with a 2.0 megapixels sen-
sor) was used in combination with eMotion [35], for real-
time facial expression analysis. In addition, we used three
unobtrusive wearable devices to capture participants’ phys-
iological signals: (i) Polar RS800 Heart Rate Monitor, and
(ii) BodyMedia SenseWear R(cid:13) Pro3 Armband (iii) NeuroSky
MindKit-EMTMheadset. All devices and systems were log-
ging using a common system time. Finally, we used entry,
post and exit questionnaires in each session.

3.3 Sensory Channels

Aﬀective Signals:

In this study we considered facial
expression as our aﬀective signal. This decision was moti-

vated by the ﬁndings of previous studies in IR related to
this paper [1] as well as ﬁndings from other domains show-
ing that emotions are primarily communicated through fa-
cial expressions [28] and facial expressions can be associated
with universally distinguished emotions, e.g. happiness, sad-
ness, anger, fear, disgust, and surprise [11]. We applied a
real-time facial expression analysis using an automatic fa-
cial expression recognition system with emotion-detection
capabilities (i.e. eMotion) [35]. The process of recognition
occurred as follows: initially, eMotion would locate certain
facial landmark features (eyebrows, corners of the mouth,
etc.) and construct a 3-dimensional wireframe model of the
face, consisting of surface patches wrapped around it. Af-
ter construction of the model, head motion or any other
facial deformation would be tracked and measured in terms
of motion-units (MU’s), and, ﬁnally, classiﬁed into one of
the six detectable emotion categories plus neutral.
eMo-
tion applies a generic classiﬁer that has been trained on
a diverse data set, combining data from the Cohn-Kanade
database. The main advantage of this system is its rea-
sonable performance across all individuals, irrespective of
the variation introduced from mixed-ethnicity groups. Re-
sults of the person-dependent and person-independent tests
presented in [35] support our performance-related assump-
tions. For additional information regarding the advantages
and limitations of eMotion the reader is referred to [35].

Physiological Signals:

In order to capture physiolog-
ical signals we consider multiple sensory channels including
heart rate monitoring, skin temperature, and neural activ-
ity. This decision was also motivated by the ﬁndings of pre-
vious studies in IR related to this paper [3]. Emotions can
be expressed through several sensory channels and are re-
ﬂected by a series of more or less observable cues, such as
localised changes in the electrodermal activity, variations
in skin temperature, neural activity and many more.
It
has been shown that transitions between emotional states
are correlated with temporal changes in physiological states
[9], which cannot be easily faked. In this work we monitor
participants’ physiological responses by using three sensory
devices: Polar RS800, BodyMedia SenseWear R(cid:13) Pro3 Arm-
band and NeuroSky’s MindKit-EMTM.

Polar RS800 Heart Rate Monitor consists of the Polar
RS800 Running Computer, a wrist-watch that displays and
records the heart rate data, an elastic strap with two elec-
trodes and Polar WearLink R(cid:13) 31, a wireless transmitter. The
elastic strap is worn around the chest (below the chest mus-
cles), allowing the built-in soft textile electrodes to detect
the heartbeat and then transmit the heart rate signal to the
running computer, via the Polar WearLink R 31, which is
attached to the strap.
BodyMedia SenseWear R(cid:13) Pro3 Armband is an unobtru-
sive, lightweight, multi-sensor hub, which is worn above the
triceps area of the right arm.
It can simultaneously mea-
sure ﬁve low-level physiological metrics: (i) galvanic skin re-
sponse, (ii) skin temperature, (iii) near-body ambient tem-
perature, (iv) heat ﬂux, and (v) motion, via a 3-axis ac-
celerometer. From those vital sign streams it can produce
accurate statements about the human body states and be-
haviours. Moreover, the existence of multiple sensors allows
for the disambiguation of contexts, which a single sensor
would have not interpreted accurately.

NeuroSky MindKit-EMTM consists of hardware and soft-
ware components for simple integration of bio-sensor tech-

137Figure 1: A snapshot of the video retrieval system for query “avengers”.

nology into consumer and industrial end-applications. Neu-
roSky MindKit-EMTM features two key technologies:
(i)
ThinkGear-EMTM headset and (ii) eSense-EMTM software
(i.e. brainwave interpretation software). The headset is
used to extract, ﬁlter, and amplify brainwave (EEG) signals
and convert that information into digital mental state out-
puts for eSense-EMTM software. The EEG signals read by
the MindKit-EMTM are detected on the forehead via points
Fp1 (electrode placement system by the International Fed-
eration in Encephalography and Clinical Neurophysiology).
The headset has three dry active sensors: one sensor located
on the forehead and two sensors are located behind the ears
as ground/reference sensors. It also has electronic circuitry
that ﬁlters and ampliﬁes the brainwaves. The eSense-EMTM
software further processes and analyses the obtained brain-
wave signals into two useful neuro-sensory values: the user’s
Attention4 and Meditation5 levels at any given moment.
The output of eSense-EMTM software has been tested over
a wide population and under diﬀerent environmental condi-
tions, to work across a wide spectrum of individuals.
3.3.1 Features
For our aﬀective features, we considered 19 features (re-
ferred to as “FX”) extracted from the output of eMotion: 7
features related to the classiﬁed emotions (i.e. happiness,
sadness, anger, fear, disgust, and surprise) plus neutral and
12 features related to the motion units (MU’s) data, which
is a low-level category of features very similar to Ekman’s
action-units (AU’s) [10]. For our physiological features, we
considered the heart rate data (referred to as “HR”) from the
output of the Polar RS800 Heart Rate Monitor; the galvanic
skin response, skin temperature, near-body ambient temper-
ature, heat ﬂux, and motion data (referred to as “AB”) from

4

Attention Meter: Combination of waves associated with being in
It relates to mental arithmetic,

a state of readiness and thinking.
concentration and creative thinking.
5

calm and tranquility.

Meditation Meter: Combination of waves associated with a state of

the output of the BodyMedia SenseWear R(cid:13) Pro3 Armband;
and the Attention or Meditation data (referred to as “NV”)
from the output of the eSense-EMTM software. For our be-
havioural signal, we considered the dwell time (referred to
as “DT”) logged by the system as our dwell time feature.
Finally the task intention was considered as task feature
(referred to as “Task”).

Preprocessing:

For each visited video, the value of
each sensory feature (for both aﬀective and physiological
features) was calculated by averaging the data logged by
its sensory device during the dwell time period. Since none
of the instruments we used normalised the data, we scaled
signal values before applying any classiﬁcation method, to
avoid having attributes in greater numeric ranges dominat-
ing those in smaller numeric ranges.
3.4 Video Retrieval System

For the completion of the search tasks we used a custom-
made search environment (named VideoHunt) that was de-
signed to resemble the basic layout of existing video retrieval
services, while retaining a minimum of graphical elements
and distractions. VideoHunt works on top of the Bing Video
Search API. For every query submitted, it returned a list of
100 results (36 results on each page), stripped of their title,
snippet and any other metadata. This layout was inten-
tional to ensure that the participants would judge the rele-
vance of videos only after they have examined them. Even
though this approach introduced our participants into artiﬁ-
cial search situations which diﬀer from real-life experiences,
it was a necessary trade-oﬀ for capturing aﬀective responses
exhibited towards the viewed content.

Search Interface: The layered architecture approach
of VideoHunt interface is shown in Figure 1. The ﬁrst layer
of the interface is dedicated to supporting any interaction
that occurs during the early stages of the search process
(such as query formulation and search execution). Any out-
put generated during this phase is presented in the second
layer (shown in Figure 1 - A). From there, participants could

BA138easily select and preview any of the retrieved clips. The con-
tent of a clip is shown on a separate panel, in the foreground
(shown in Figure 1 - B), which corresponds to the third layer
of our system. The main reason behind this layered archi-
tecture was to isolate the viewed content from all possible
distractions that reside on the desktop screen; therefore, es-
tablishing additional ground truth that allowed us to relate
participants’ aﬀective responses to the source of stimuli (in
our case, the perused videos). This was an important aspect
of our experimental methodology, since we were interested
in isolating content-speciﬁc emotions.
In addition, in the
context of video retrieval, we would expect less interaction
with the system during the dwell time making the captured
physiological signals particularly valuable in this type of en-
vironment. Upon viewing the clip, the participants had to
explicitly indicate the relevance of the video. Finally, the
length of time a user spent watching a clip (dwell time) was
monitored and logged by the search interface.

Pilot Studies: Prior to running the actual user study,
a pilot study was performed using 5 participants to conﬁrm
that the process worked correctly and smoothly. A number
of changes were made to the system based on feedback from
the pilot study. The changes consisted of modiﬁcations to
the system to improve logging capabilities and improvements
to the tasks. After the ﬁnal pilot, it was determined that the
participants were able to complete the user study without
problems and that the system was capturing all necessary
data.

Participants: Participants consisted of 24 healthy peo-
ple with equal gender distribution (12 female and 12 male)
all under the age of 41, with the largest group between the
ages of 18-23 (45.8%) followed by the group between ages
of 24-29 (36%). Participants tended to have a high school
diploma or equivalent (4.16%), college degree (4.16%), bach-
elors (41.66%) or graduate degree (50%). They were pri-
marily students (62.5%), though there were a number of
self-employed (16.6%), not employed (4.16%) and employed
by a company or organisation (16.6%).

4. RESULTS AND DISCUSSION

In this section we present the experimental ﬁndings of
our study, based on the 96 search sessions that were carried
out by 24 subjects. We ﬁrst discuss the task perception ex-
pressed in the questionnaire. Following this, we discuss the
predictability of search intentions based on features derived
from the user interaction with the system in Section 4.1.

Task Perception: Figures 2 shows the box plots for the
qualitative analysis of users’ perception of the four tasks (i.e.
INS, INF, ENA, and ENM). Each box plot reports data ag-
gregated from 24 participants, along with ﬁve key statistics:
the minimum, ﬁrst, second (median), third, and maximum
quartiles.6 We performed an ANOVA test between measures
obtained at each phase, across four search tasks for each user
to check the signiﬁcance of the diﬀerence among them. The
test is suitable for this data as we have four groups of data,
therefore we need to compare four means and variances. We
use (*) and (**) to denote the fact that a measure had re-
sults diﬀerent across four search tasks with the conﬁdence
levels (p < 0.05) and (p < 0.01), respectively.

In the post-task questionnaire we measured participants’
perception of their performed task in terms of the diﬃ-

6

Further information can be found in [24].

culty of the task, the familiarity of the participant with the
task, the extent to which they found the task stressful, in-
teresting and clear by asking the following question “The
task we asked you to perform was [easy/stressful/interesting/
clear/familiar] (answer: 1: “Strongly Disagree”, 2: “Dis-
agree”, 3: “Neutral”, 4: “Agree”, 5: “Strongly Agree”)”. The
results shown in Figure 2 indicate that participants found
the INF task diﬃcult and stressful, followed by the INS task,
whereas they found other two tasks easy and not-stressful
(the diﬀerences were statistically signiﬁcant). The diﬀer-
ence in the answer provided by the participants for interest-
ing, clear and familiar measures is not statistically signiﬁ-
cant. In the post-task questionnaire we also asked the opin-
ion of the participants with respect to the following state-
ment “I had enough time to do an eﬀective search.] (an-
swer: 1: “Strongly Disagree”, 2: “Disagree”, 3: “Neutral”,
4: “Agree”, 5: “Strongly Agree”)”. The results show that
they found the time given enough to do an eﬀective search
task, (INS: M=4.0 SD=0.88, INF: M=3.8 SD=0.96, ENA:
M=4.0 SD=0.97, ENM: M=4.3 SD=0.56, the diﬀerences are
however not statistically signiﬁcant across the tasks).

Figure 2: Box plot of the task perception based on the informa-
tion gathered from 24 participants questionnaires. The diamond
represents the mean value.
(*) and (**) indicate conﬁdence levels
(p < 0.05) and (p < 0.01) respectively.

4.1 Relevance Prediction

In this section, we investigate our research questions in-
troduced in Section 1. We ﬁrst study the main eﬀect of
using aﬀective, physiological and behavioural signals for the
relevance judgement prediction task, by combining the rele-
vance judgement obtained from our four search intents. We
then study the interaction eﬀect of these signals with search
intents for the relevant judgment prediction task. For this

**Easy**StressfulInterestingClearFamiliar12345INSAgreement Level**Easy**StressfulInterestingClearFamiliar12345INFAgreement Level**Easy**StressfulInterestingClearFamiliar12345ENAAgreement Level**Easy**StressfulInterestingClearFamiliar12345ENMAgreement LevelThe task perfomed was139purpose, the features explained in Section 3.3.1 were used.
In total, participants judged 488 videos (with 247 relevant,
50.60%), of which 141 (with 82 relevant, 64.53%), 128 (with
91 relevant, 64.06%), 133 (with 73 relevant, 54.88%), and 86
(with 1 relevant, 1.17%) videos belong to “ENM”, “ENA”,
“INS”, and “INF” respectively. Due to the ratio of relevant
to non-relevant videos for INF task, we consider two combi-
nation scenarios: one where the relevance judgments of all
tasks are combined (referred to as “ALL”) and another one
where the relevance judgments of all tasks are combined,
except those for the INF task (referred to as “ALL - INF”).
For relevance prediction, we have a binomial classiﬁca-
tion problem where the classes are “+1” (relevant) and “-1”
(non-relevant). We used SMO, an implementation of SVM
in Weka,7 to discriminate between the two classes explained
above. We trained our models using a polynomial kernel
which in the majority of cases outperformed other SVM ker-
nels (e.g. radial-basis) based on our analysis, not presented
due to the space limits. For each classiﬁcation method we
present only the model which achieved the best performance
among the rest in its category.

Table 2 shows the classiﬁcation performance averaged over
the 24 participants of the study across four diﬀerent search
tasks. It reports the accuracy of the model (i.e. fraction of
items in the test set for which the models’ predictions were
correct) using 10-fold cross-validation.

For the scenarios where only sensory signals are used to
train a model, the baseline represents an untrained model
where all its predictions are biased to a dominant class (re-
ferred to as “BL1”). This baseline is more realistic than
a baseline which is based on a random choice (i.e. where
its accuracy is set to 50%). Any feature set that results in
higher accuracy than what is reported for BL1 is as a re-
sult of the discriminative power of the set itself. For the
scenarios where dwell time and sensory signals are used to
train a model, the baseline represents a model trained on
the dwell time feature only (referred to as “BL2”). Finally,
for the scenarios where dwell time, task and sensory signals
are used to train a model, the baseline represents a model
trained based on dwell time and task features (referred to
as “BL3”). We also performed a paired Wilcoxon test be-
tween the predictions obtained for each model to check the
signiﬁcance of the diﬀerence with its baseline. We use [(*)
and (**)], [(†) and (††)], and [(‡) and (‡‡)] to denote the fact
that a model trained on a set of features had results diﬀerent
from that of “BL1”, “BL2”, and “BL3” with the conﬁdence
levels [(p < 0.05) and (p < 0.01)] respectively.

Main Results: The key ﬁndings which emerged from
the results are that combining dwell time with sensory sig-
nals provides a reliable set of features for IRF techniques.
This means that sensory signals can be used as a good sub-
stitution for task information for dwell time based IRF tech-
niques. We also observed that the eﬀectiveness of dwell time
as well as sensory channel signals vary across the search in-
tentions. In the remainder of this section we discuss each of
our research questions in more detail.

RQ1: which combination of aﬀective, physiologi-
cal and behavioural signals provide a reliable set of
features for IRF techniques? In order to answer this
research question we study the main eﬀect of these signals
for the relevance prediction task. This is an attempt to sim-

7

http://www.cs.waikato.ac.nz/ml/weka/

ulate a real life scenario where the system has to predict
the relevance judgement for search processes with diﬀerent
(and in the majority of the cases, unidentiﬁed) intentions.
Therefore, we only consider the results obtained for the sce-
narios where the model has to predict the relevance judge-
ment gathered from multiple search intentions (i.e. “All -
INF” and “ALL” columns).

Behavioural and Sensory Signals: Considering dwell time
with only one sensory signal at a time as a feature set, the
ﬁndings show that the combination of facial expression fea-
tures and dwell time results in the highest accuracy (i.e.
“DT+FX” row). The ﬁndings show that a higher accuracy
can be achieved when the model has been trained on a fea-
ture set that has dwell time and all the sensory signals (i.e.
“DT+FX+AB+HR+NV” row). This ﬁnding suggests the
complementary nature of the information obtained through
various behavioural and sensory signals. Furthermore, they
also outperform the scenario where dwell time and task fea-
tures are used to train a model, showing that this set of
features is a reliable substitute for IRF techniques

Sensory Signals: Considering each sensory signal indi-
vidually as a feature set, the ﬁndings show that for the sit-
uation where the model has to predict the relevance judge-
ment gathered for all studied search intentions (i.e. “ALL”
column), facial expression features results in the highest ac-
curacy (i.e. “FX” row). However such an observation cannot
be made for the situation where the model has to predict
the relevance judgement gathered for multiple search inten-
tions except those of the re-ﬁnding intention (i.e. “All -
INF” column). The diﬀerence in the accuracy of facial ex-
pression features between “ALL” and “All - INF” situations
suggest that facial expression features can discriminate rele-
vance judgments of the tasks which have a signiﬁcant diﬀer-
ence in their diﬃculty. The results indicate that if there is
no signiﬁcant diﬀerence in the task diﬃculty, the heart rate
feature provides the highest accuracy in relevance judgement
prediction (i.e. “HR” row). The ﬁndings show that a higher
accuracy again can be achieved when the model has been
trained on a feature set that is based on all sensory signals
(i.e. “FX+AB+HR+NV” row).

Behavioural Signals: Considering only dwell time, the
ﬁndings show that for the situation where the model has
to predict the relevance judgement gathered for all studied
search intentions (i.e. “ALL” column), the dwell time feature
results in a high accuracy level. However, once the model
has to predict the relevance judgements gathered for multi-
ple search intentions except those of re-ﬁnding intention (i.e.
“All - INF” column), its eﬀectiveness decreases. Our ﬁndings
also show that the combination of dwell time and task in-
formation improves the accuracy of the relevance judgement
prediction signiﬁcantly, only if the performed tasks varies in
some aspect, such as diﬃculty (as it is in our case). This
ﬁnding supports the results obtained by Kelly and Belkin
[19]. We observe that if the performed tasks does not vary
signiﬁcantly in their characteristics, task information do not
provide any additional value and it can also signiﬁcantly
harm the accuracy.

In the Presence of Task Information: Finally, we investi-
gated the eﬀect of having task information along with dwell
time and all the sensory signals on the accuracy of relevance
judgment prediction (i.e. “DT+Task+FX+AB+HR+NV”
row). The ﬁndings show that the prediction accuracy of a
model trained on these features is signiﬁcantly higher than

140Table 2: This table shows the prediction accuracy of a model trained on diﬀerent sets of features (presented as rows), given diﬀerent search
intentions (presented as columns). The best performing set of features for each condition and search intention is highlighted in bold.

Random [BL1](*)
DT [BL2](†)
DT+Task [BL3](‡)
FX

AB

HR

NV

FX+AB+HR+NV

DT+FX

DT+AB

DT+HR

DT+NV

DT+FX+AB+HR+NV

DT+Task+FX+AB+HR+NV

INS
54.88%
62.40%
–%
55.63%*
(+1.3%)
54.88%
(+0%)
57.89%**
(+5.4%)
55.63%*
(+1.3%)
55.63%*
(+1.3%)
67.66%††
(+8.4%)
66.91%††
(+7.2%)
63.15%†
(+1.2%)
64.41%††
(+3.2%)
66.16%††
(+6.0%)
–%

ENA
64.06%
65.62%
–%
66.4%**
(+3.6%)
64.06%
(+0%)
64.06%
(+0%)
64.06%
(+0%)
69.53%**
(+8.5%)
68.75%††
(+4.7%)
67.96%††
(+3.5%)
73.43%††
(+11.9%)
70.31%††
(+7.1%)
75%††
(+14.2%)
–%

ENM
64.53%
66.16%
–%
64.53%
(+0%)
64.53%
(+0%)
64.53%
(+0%)
64.53%
(+0%)
64.53%
(+0%)
71.63%††
(+8.2%)
81.56%††
(+23.2%)
82.26%††
(+24.3%)
74.46%††
(+12.5%)
80.14%††
(+21.1%)
–%

INF
98.83%
98.83%
–%
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
98.83%
(+0%)
–%

ALL - INF
61.19%
71.31%
69.65%
62.43%**
(+2.0%)
61.19%
(+0%)
62.93%**
(+2.8%)
61.19%
(+0%)
67.16%**
(+9.7%)
72.88%††
(+2.2%)
71.64%
(+0.4%)
72.13%†
(+1.1%)
72.13%†
(+1.1%)
75.37%††
(+5.6%)
76.36%‡‡
(+9.6%)

ALL
50.60%
72.74%
76.63%
64.54%**
(+27.4%)
50.60%
(+0%)
53.27%**
(+5.2%)
55.73%**
(+10.1%)
65.98%**
(+30.3%)
77.04%††
(+5.9%)
76.22%††
(+4.5%)
76.22%††
(+4.5%)
75.40%††
(+5.6%)
77.04%††
(+5.9%)
78.89%‡‡
(+2.9%)

the prediction accuracy of a model trained on dwell time and
task features signiﬁcantly (i.e. “DT+Task” row). The results
also show that the prediction accuracy of such a model is
even higher than a model trained on all features except task
one (i.e. “DT+FX+AB+HR+NV” row). This show that re-
searches on task prediction are complementary to this study
rather than contradictory.

RQ2: What would be the accuracy of these fea-
ture sets with respect to diﬀerent search intentions?
In order to answer this research question we study the in-
teraction eﬀect of these signals with search intentions for
relevance prediction task. Therefore, we consider the results
obtained for the scenarios where the model has to predict
the relevance judgement gathered from one search intention
at a time (i.e. “INS”, “ENM”, “ENA” and “INF” columns).
Overall, the results do not show any general pattern, mean-
ing that the eﬀectiveness of diﬀerent signals are task de-
pendent. We exclude the “INF” intention from our further
analysis, since for “INF” intention, almost all of the judged
videos were non-relevant, additionally the accuracy of “BL1”
model is too high to be outperformed by any of our models.
The ﬁndings suggests that sensory channels combined with
dwell time performs better for the search intentions which do
not have a clear information need (i.e. “ENM” and “ENA”)
than the ones that do (i.e “INS”). For search processes with
entertainment-based intentions, a combination of heart rate
and dwell time features results in the highest accuracy. For
search processes with information seeking intention, com-
bination of facial expression and dwell time feature is the
highest accuracy. In particular, the results suggest that for
the ENM task, heart rate and skin temperature; for ENA
task, heart rate and EEG; for INS task, facial expression and
skin temperature are the best two sensory signals, once they
are combined with dwell time. Search processes for which
the purpose is to satisfy an information need, facial expres-
sion and skin temperature are the best two sensory channels
once they are combined with dwell time information.

An interesting ﬁnding is that the discriminative power of
sensory signals changes once they are combined with dwell
time, even though they show no such power individually. For
example, a sensory feature that was not discriminative on
its own for a task (e.g. “HR” feature for “ENM” task), when
combined with dwell time, can result in the highest predic-
tion accuracy (i.e. “DT+HR” features for “ENM” task). In
addition, the ﬁndings suggest that across all search tasks,
combining each sensory signal with dwell time results in a
higher accuracy compared to the situation where only either
of them is used. The combination of all the sensory channels
does not result in the highest accuracy, except for the “ENA”
task. Similar behaviour is observed when these features are
combined with dwell time one.

Finally, comparison of the accuracy obtained from BL2
(dwell time) and BL1 show that dwell time performs bet-
ter for tasks based on information needs (i.e. “INS”) than
tasks based on hedonistic needs (i.e. “ENA” and “ENM”).
However, the ﬁndings show that the accuracy of the dwell
time feature and sensory channels by themselves is not high
and is task dependent. This ﬁnding supports the results ob-
tained by previous studies, i.e.
[19] for dwell time feature
and [3] for sensory features.

5. CONCLUSIONS

In this paper we investigated the discriminative power of
diﬀerent sensory channel signals as a reliable substitute for
task information for IRF techniques based on dwell time,
given diﬀerent search intentions. In order to do so, we de-
vised four search tasks, each simulating one of the following
search intentions: information seeking, re-ﬁnding, entertain-
ment adjusting arousal level and entertainment adjusting
mood. Using a video retrieval system as a use case, we con-
ducted a user study with 24 participants. We then studied
two research questions: (1) which combination of aﬀective,
physiological and behavioural signals provide a reliable set of
features for IRF techniques? and (2) what would be the ac-

141curacy of these feature sets with respect to diﬀerent search
intentions? The ﬁndings show that combining dwell time
with sensory signals provides a reliable set of features for
IRF techniques, and the sensory signals can be used as a
good substitution of task information for dwell time based
IRF techniques. We also observed that the eﬀectiveness of
both the dwell time feature as well as sensory channel sig-
nals vary across diﬀerent search intentions. The major im-
plication of our work is in developing real time interactive
systems.

Finally, although the ﬁndings of this paper are limited to
the video retrieval domain and not generalizable, they moti-
vate the exploration of similar hypotheses in other domains.
Even though signiﬁcant improvement in relevance prediction
were achieved by adding aﬀective and physiological measures
across diﬀerent search task types, we acknowledge the fol-
lowing limitations: ﬁrst, the experiment was conduced in an
artiﬁcial situation (i.e. a laboratory setting) which lacks the
ecological validity of a naturalistic study. Second, only one
behavioural signal (i.e. dwell time) was considered in this
experiment. Our positive ﬁndings encourage us to explore
more exhaustively behavioural signals such as click-through
and mouse movement as well as eye movement in combi-
nation with aﬀective, physiological signals for future work.
Finally, it will be some time before physiological signals can
be used by practical applications. Therefore, the study in
this area is in its infancy. There are still many research ques-
tions left to be answered. The ﬁndings of this paper lay the
foundation for further studies to address these areas.

6. ACKNOWLEDGEMENT

This work was supported by the EU FP7 LiMoSINe project

(288024).

7. REFERENCES

[1] I. Arapakis, K. Athanasakos, and J. M. Jose. A comparison of

general vs personalised aﬀective models for the prediction of
topical relevance. In SIGIR, pages 371–378, 2010.

[2] I. Arapakis, J. M. Jose, and P. D. Gray. Aﬀective feedback: an

investigation into the role of emotions in the information
seeking process. In SIGIR, pages 395–402, 2008.

[3] I. Arapakis, I. Konstas, and J. M. Jose. Using facial

expressions and peripheral physiological signals as implicit
indicators of topical relevance. In ICMR, pages 461–470, 2009.

[4] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, D. Hannah, and
J. M. Jose. Enriching user proﬁling with aﬀective features for
the improvement of a multimodal recommender system. CIVR,
2009.

[5] N. Belkin, C. Clarke, N. Gao, J. Kamps, and J. Karlgren.
Report on the sigir workshop on entertain me: supporting
complex search tasks. ACM SIGIR Forum, 45(2):51–59, 2012.
[6] P. Borlund and P. Ingwersen. The development of a method for

the evaluation of interactive information retrieval systems.
Journal of documentation, 53(3):225–250, 1997.

[7] A. Broder. A taxonomy of web search. ACM Sigir forum,

36(2):3–10, 2002.

[8] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit interest

indicators. In IUI, pages 33–40, 2001.

[9] R. Cornelius. Theoretical approaches to emotion. In ISCA
Tutorial and Research Workshop (ITRW) on Speech and
Emotion, 2000.

[10] P. Ekman. Facial expressions. Handbook of cognition and

emotion, pages 301–320, 1999.

[11] P. Ekman. Emotions revealed: Recognizing faces and feelings
to improve communication and emotional life. Times Books,
2003.

[12] D. Elsweiler, M. Harvey, and M. Hacker. Understanding

re-ﬁnding behavior in naturalistic email interaction logs. In
SIGIR, pages 35–44. ACM, 2011.

[13] D. Elsweiler, S. Mandl, and B. Kirkegaard Lunn.

Understanding casual-leisure information needs: a diary study
in the context of television viewing. In IIiX, pages 25–34, 2010.
[14] D. Elsweiler, M. Wilson, and M. Harvey. Searching4fun. ECIR

workshop, 2012.

[15] D. Elsweiler, M. Wilson, and B. Lunn. Understanding

casual-leisure information behaviour. Library and Information
Science, 211:241, 2011.

[16] H. A. Feild, J. Allan, and R. Jones. Predicting searcher

frustration. In SIGIR, pages 34–41, 2010.

[17] J. Gwizdka and I. Lopatovska. The role of subjective factors in

the information search process. JASIST, 60(12):2452–2464,
2009.

[18] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay.

Accurately interpreting clickthrough data as implicit feedback.
In SIGIR, pages 154–161, 2005.

[19] D. Kelly and N. J. Belkin. Display time as implicit feedback:

understanding task eﬀects. In SIGIR, pages 377–384, 2004.

[20] J. Koenemann and N. J. Belkin. A case for interaction: a study

of interactive information retrieval behavior and eﬀectiveness.
In SIGCHI, pages 205–212, 1996.

[21] J. Liu, M. J. Cole, C. Liu, R. Bierig, J. Gwizdka, N. J. Belkin,

J. Zhang, and X. Zhang. Search behaviors in diﬀerent task
types. In JCDL, pages 69–78, 2010.

[22] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task

diﬃculty for diﬀerent task types. In ASIS&T, volume 47,
pages 16:1–16:10, 2010.

[23] I. Lopatovska. Searching for good mood: Examining
relationships between search task and mood. ASIST,
46(1):1–13, 2009.

[24] R. McGill, J. W. Tukey, and W. A. Larsen. Variations of box

plots. American Statistician, 32(1):12–16, 1978.

[25] M. Morita and Y. Shinoda. Information ﬁltering based on user

behavior analysis and best match text retrieval. In SIGIR,
pages 272–281, 1994.

[26] R. Oddy. Information retrieval through man-machine dialogue.

Journal of documentation, 33(1):1–14, 1977.

[27] M. B. Oliver. Mood management and Selective Exposure.
Communication and Emotion: Essays in honor of Dolf
Zillmann, pages 85–106, 2003.

[28] M. Pantic and L. Rothkrantz. Expert system for automatic

analysis of facial expressions. Image and Vision Computing,
18(11):881–905, 2000.

[29] M. Pantic and L. Rothkrantz. Toward an aﬀect-sensitive

multimodal human-computer interaction. IEEE,
91(9):1370–1390, 2003.

[30] A. Poddar and I. Ruthven. The emotional impact of search

tasks. In IIiX, pages 35–44, 2010.

[31] Y.-W. Seo and B.-T. Zhang. Learning user’s preferences by

analyzing web-browsing behaviors. In Agents, pages 381–387,
2000.

[32] C. Sheldrick Ross. Finding without seeking: The information

encounter in the context of reading for pleasure. IPM,
35(6):783–799, 1999.

[33] R. Taylor. Question-negotiation an information-seeking in

libraries. Technical report, DTIC Document, 1967.

[34] J. Teevan, E. Adar, R. Jones, and M. Potts. Information

re-retrieval: repeat queries in yahoo’s logs. In SIGIR, pages
151–158, 2007.

[35] R. Valenti, N. Sebe, and T. Gevers. Facial expression

recognition: A fully integrated approach. In ICIAP Workshop,
pages 125–130, 2007.

[36] P. Vorderer. Entertainment theory. Communication and

emotion: Essays in honor of Dolf Zillmann, pages 131–153,
2003.

[37] R. White and D. Kelly. A study on the eﬀects of

personalization and task information on implicit feedback
performance. In CIKM, pages 297–306, 2006.

[38] R. W. White. Implicit Feedback for Interactive Information

Retrieval. PhD thesis, University of Glasgow, 2004.

[39] R. W. White, J. M. Jose, and I. Ruthven. A task-oriented

study on the inﬂuencing eﬀects of query-biased summarisation
in web searching. IPM, 39(5):707–733, 2003.

[40] M. Wilson and D. Elsweiler. Casual-leisure searching: the

exploratory search scenarios that break our current models. In
HCIR, pages 28–31, 2010.

142