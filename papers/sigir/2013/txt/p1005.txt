Finding Knowledgeable Groups in Enterprise Corpora

Shangsong Liang

Maarten de Rijke
derijke@uva.nl
s.liang@uva.nl
ISLA, University of Amsterdam, Amsterdam, The Netherlands

ABSTRACT
The task of ﬁnding groups is a natural extension of search tasks
aimed at retrieving individual entities. We introduce a group ﬁnd-
ing task: given a query topic, ﬁnd knowledgeable groups that have
expertise on that topic. We present four general strategies to this
task. The models are formalized using generative language mod-
els. Two of the models aggregate expertise scores of the experts
in the same group for the task, one locates documents associated
with experts in the group and then determines how closely the doc-
uments are associated with the topic, whilst the remaining model
directly estimates the degree to which a group is a knowledgeable
group for a given topic. We construct a test collections based on the
TREC 2005 and 2006 Enterprise collections. We ﬁnd signiﬁcant
differences between different ways of estimating the association
between a topic and a group. Experiments show that our knowl-
edgeable group ﬁnding models achieve high absolute scores.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis
and Indexing
Keywords
Group ﬁnding, expertise retrieval, language modeling
1.

INTRODUCTION

A major challenge within any organization is managing the ex-
pertise within the organization such that groups with expertise in
a particular area can be identiﬁed [2]. Rather than ﬁnding knowl-
edgeable individuals, sometimes locating a group with appropriate
skills and knowledge in an organization is of great importance to
the success of a project being undertaken [6].

Traditional approaches to ﬁnding knowledge, whether in indi-
viduals or in groups within an organization, often include two main
steps. For a given task the expertise of the experts in each group
is recorded and then the expertise of a group is computed by ag-
gregating the expertise values of all group members. Both steps
are traditionally done manually and require considerable effort. In

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

addition, this approach is usually restricted to a ﬁxed set of exper-
tise areas [7]. To reduce the effort of recording and evaluating the
expertise of people from their representations, many automatic ap-
proaches have been proposed. There has been an increasing move
to automatically extract such representations for evaluating exper-
tise from heterogeneous document collections [2]. To compute the
expertise values of a group, in principle, many aggregation opera-
tors are available, e.g., sum or average. These can be employed to
combine individual experts’ expertise values. There are at least 90
families of aggregation operators [11], which have been put to use
in a range of applications. But the problem of how to aggregate ex-
pertise values of experts within a group so that the expertise scores
of different groups can be easily compared and ranked is unknown.
We treat the problem of ﬁnding a knowledgeable group differ-
ently. Four distinct models are proposed. Our models are based
on probabilistic language modeling techniques. Each model ranks
groups according to the probability of the group being a knowl-
edgeable group for a query topic, but the models differ in how this
is performed. Three types of variable play a key role in our es-
timations: groups (G), queries (Q) and documents (D). The order
in which we estimate these is reﬂected in our naming conventions.
E.g., the model named GDQ proceeds by ﬁrst collecting evidence
of whether a group is knowledgeable on the topic via the experts
in the group (G), and then determining whether each expert in the
group has expertise on the topic via documents (D), and ﬁnally
whether a document is talking about the given query (Q) topic.
2. RELATED WORK

Signiﬁcant research effort has been invested in locating a group
of individuals in an organization. Yang et al. [10] try to ﬁnd a group
of attendees familiar with a given activity initiator, and ensure each
attendee in the group to have tight social relations with most of
the members in the group. Sozio and Gionis [9] study a query-
dependent variant of the community-detection problem: given a
graph, and a set of nodes in the graph as their input query, ﬁnd a
subgraph that contains the input query nodes and is densely con-
nected. Lappas et al. [6] study the problem of given a task, a pool
of individuals χ with different skills and a social network that cap-
tures the compatibility among them, ﬁnding a subset of χ, who to-
gether have the skills to complete a task with minimal communica-
tion costs. Kargar and An [5] design communication cost functions
for two types of communication structures.

The problem we deal with is different. We introduce a new group
ﬁnding task: given a topic query, determine a list of knowledgeable
groups within which the experts have expertise on the topic. Our
group ﬁnding problem includes two sub-problems. The ﬁrst is to
answer questions such as “Which groups are knowledgeable groups
on topic T ?” whilst the second is to answer the question “What
does group G know?” We focus on the ﬁrst sub-problem.

10053. MODELING GROUP FINDING

In our modeling of the knowledgeable group ﬁnding task, groups,
documents and queries are considered in different orders. Groups
are ranked according to how likely they have expertise on the given
query according to the estimated language model.
Problem deﬁnition and context. We address the following prob-
lem: what is the probability of a group g being a knowledgeable
group given query topic q? We have to estimate the probability of
a group g given a query q and then rank groups according to this
probability. The top k groups will be considered to be the most
knowledgeable groups for the given query topic. Instead of com-
puting this probability directly, we apply Bayes’ Theorem, and ob-
tain p(g|q) = p(q|g)p(g)p(q)−1, where p(q) is the probability of a
query and p(g) the probability of a group, both of which can be as-
sumed to be uniform for a query and a group, considering that q is
the same during retrieval and there is no group that is more likely to
be relevant. Hence, ranking groups according p(g|q) boils down to
ranking a query topic given a group: p(q|g). To determine p(g|q)
or p(q|g) we consider experts, groups, documents and queries in
different orders, so as to arrive at four distinct models.
Four group ﬁnding models. The ﬁrst of four models for group
ﬁnding is presented in some detail; because of lack of space, the
others are presented much more concisely. We start with two types
of aggregation model: the Group-Query-Document (GQD) model
and the Group-Document-Query (GDQ) model. The order of the
key terms in these names signiﬁes the following: GQD means that
the evidence of whether a group is knowledgeable on the topic is
collected via the experts in the group (G), then how likely each
expert in the group has expertise on each subtopics in the query (Q)
topic is computed via the documents (D). GDQ denotes that the
evidence of whether a group is knowledgeable is collected via the
experts in the group (G), then via each document (D) the expertise
of each expert in the group on the query (Q) topic is computed
directly via the documents. We assume that experts in the same
group g are conditionally independent given the group, such that:

p(g|q) =(cid:81)

ex∈g p(ex|q)as(ex,g),

where ex is an expert belonging to group g, p(ex|g) is the probabil-
ity of how likely an expert ex belonging to a group g, and as(ex, g)
is the association between an expert ex and the group g. Instead of
computing p(ex|g) directly, we apply Bayes’ Theorem, and obtain
p(ex|q) = p(q|ex)p(ex)p(q)−1, where p(q|ex) is the probability
of a query given an expert, p(ex) is the probability of an expert,
and p(q) is the probability of the query. As we assume that each
expert is equally important, p(ex) is assumed to be constant. Ad-
ditionally, for each query topic, p(q) is the same, hence, p(ex|q) is
proportional to p(q|ex). So, p(g|q) becomes

The GQD Model. To obtain p(q|ex), we assume that each term t
in query q is conditionally independent given expert ex, such that:

p(g|q) rank= (cid:81)

where p(t|ex) is the probability of a term given an expert and
n(t, q) is the number of occurrences of term t in query q. Com-
bined, we can rewrite p(g|q) as follows.

(cid:110)(cid:81)

t∈q p(t|ex)n(t,q)(cid:111)as(ex,g)

.

ex∈g

lection. This can be expressed as p(t|ex) = (cid:80)

To obtain p(t|ex), we take the sum over documents d in the col-
d p(t|d)p(d|ex),

p(g|q) rank= (cid:81)
p(q|ex) =(cid:81)

ex∈g p(q|ex)as(ex,g).

t∈q p(t|ex)n(t,q),

t∈q

ex∈g

(cid:8)(cid:80)

(cid:110)(cid:81)

where p(t|d) is the probability of term t given document d, and
p(d|ex) is the probability of d given expert ex. Now we can obtain
the probability of a group given a query, i.e., our GQD model:

d p(t|d)p(d|ex)(cid:9)n(t,q)(cid:111)as(ex,g)

p(g|q) rank= (cid:81)
expressed as: p(q|ex) = (cid:80)
(cid:110)(cid:81)
t∈q p(t|d)n(t,q)(cid:111)
p(g|q) rank= (cid:81)

The GDQ Model. We can compute the probability of a query
q given an expert ex in a different way. By taking the sum over
all documents d, p(q|ex) can be obtained. Formally, this can be
d p(q|d)p(d|ex), where p(q|d) and
p(d|ex) are the probability of query q given document d and of
document d given query q, respectively. Based on this, we obtain
(cid:111)as(ex,g)
our second aggregation model, i.e., our GDQ model:

(cid:110)(cid:80)

p(d|ex)

ex∈g

d

.

The DGQ model. Next we consider a document model. Instead
of aggregating expertise scores of all the experts within a group as
in our aggregation models, as the key terms DGQ in this model’s
name suggests, the probability g(g|q) can be computed directly via
the documents (D). For each we compute how likely the group (G)
is associated with it, and how likely it is talking about the given
d p(g|d)p(d|q), where
p(g|d) and p(d|q) are the probability of group g given document
d and the probability of d given query q, respectively. This, then, is
how p(g|q) can be represented, i.e., our DGQ model:

query (Q) topic, such that: p(g|q) = (cid:80)
ex∈g p(d|ex)as(ex,g)(cid:111)(cid:110)(cid:81)
p(g|q) rank= (cid:80)

t∈q p(t|d)n(t,q)(cid:111)

(cid:110)(cid:81)

d

.

The QDG model. Finally, we present a query model for group
ﬁnding. We use “query model” not in the sense of building rich
representations of a query but to indicate that our estimations of a
group ﬁnding model start with the query. As the name QDG indi-
cates, it ﬁrst considers how likely a group knows about a query (Q)
topic. QDG computes this via documents (D) and then determines
how likely each expert in the group (G) is associated with each doc-
ument. We collect evidence of how knowledgeable group g is via
d p(t|d)p(d|g),
where p(d|g) is the probability of document d given group g. The
ﬁnal version of p(g|q) can then be represented as:

all documents in the collection and obtain p(t|g) =(cid:80)
p(g|q) rank= (cid:81)

ex∈g p(d|ex)as(ex,g)(cid:111)n(t,q)

(cid:110)(cid:80)
d p(t|d)(cid:81)

t∈q

.

And this is our QDG model. It ﬁrst computes the probability of how
likely a group is talking about a query topic; it collects evidence of
how knowledgeable the group is for a given query via all documents
in the collection. For each expert within a group, we determine how
likely the expert is associated with the documents.
4. ASSOCIATIONS AND SMOOTHING
Expert-document associations. For all models described in the
previous section, we need to be able to estimate the probability
of an expert ex in group g being associated with document d. In
recent years, this problem has attracted considerable attention [2].
Following Balog et al. [1], to deﬁne this probability, we assume that
associations a(d, ex) between experts ex and documents d have
been calculated and deﬁne

(cid:80)

p(d|ex) =

a(d, ex)
d(cid:48)∈D a(d(cid:48), ex)

,

(1)

where D is the set of documents in the collection, and a(d, ex)
is simply deﬁned as to be 1 if if the full name or email address of
expert ex (exactly) appears in document d, otherwise a(d, ex) = 0.

1006Group-expert associations. For all of the group ﬁnding models
described in the previous section, we also need to be able to esti-
mate the strength of the association between expert ex and group
g to which the expert belongs. We deﬁne the following group ex-
pert association as(ex, g) = |g|−1, where |g| is the total number
of experts within the group to which they belong.
In our four models, the term p(g|q) may
Smoothing strategies.
contain zero probabilities due to data sparsity. E.g., in our aggrega-
tion models, GQD and GDQ, p(g|q) will contain zero probabilities
if there exist experts who have no expertise on the given query.
Hence, we have to infer a group model θg, such that the probability
of a group given a query model is p(θg|q). We employ Jelinek-
Mercer smoothing [4] to estimate p(θg|q); we consider two types.
To facilitate comparisons and for the sake of uniformity, instead
of estimating p(g|q) directly, we can easily infer a document model
θd such that the probability of term t given a document d model is
p(t|θd), and infer an expert model θex such that the probability
of a document d given an expert ex is p(d|θex). The document
model, then, is a linear interpolation of the background model p(t)
and the smoothed estimate: p(t|θd) = (1 − α)p(t|d) + αp(t),
where α is a smoothing parameter (0 < α < 1). The expert
model is a linear interpolation of the background model p(d) and
the smoothed estimate: p(d|θex) = (1−β)p(d|ex)+βp(d), where
β is a smoothing parameter (0 < β < 1). Let θ(α, t, d) be short
for p(t|θd) = (1 − α)p(t|d) + αp(t), and ϑ(β, d, ex) be short
for p(d|θex) = (1 − β)p(d|ex) + βp(d). Then, the group ﬁnding
model GQD can be smoothed and estimated as

dθ(α, t, d) · ϑ(β, d, ex)(cid:9)n(t,q)(cid:111)as(cid:48)
(cid:8)(cid:80)

p(g|q) rank= (cid:81)

(cid:110)(cid:81)

t∈q

ex∈g

ex∈g

,
where as(cid:48) abbreviates as(ex, g). The other group ﬁnding models,
GDQ, DGQ, and QDG, can be smoothed and estimated in an anal-
ogous manner. As to the GDQ model:

with as(cid:48) as before. For the DGQ model we have

t∈qθ(α, t, d)n(t,q)(cid:111)
(cid:110)(cid:81)
(cid:110)(cid:80)
(cid:110)(cid:81)
ex∈gϑ(β, d, ex)as(ex,g)(cid:81)
(cid:110)(cid:80)
dθ(α, t, d)(cid:81)

p(g|q) rank= (cid:81)
p(g|q) rank= (cid:80)
p(g|q) rank= (cid:81)
strategy with one parameter: p(g|q) rank= (cid:81)

(cid:111)as(cid:48)
t∈qθ(α, t, d)n(t,q)(cid:111)
ex∈gϑ(β, d, ex)as(ex,g)(cid:111)n(t,q)
t∈q{(1−λ)(cid:80)

ex∈g{(cid:81)

For the GQD model, we also consider a second type of smoothing

and the QDG model can be smoothed and estimated as

ϑ(β, d, ex)

t∈q

d

,

,

.

d

d

p(t|d)p(d|ex) + λp(t)}n(t,q)}as(cid:48)
.

5. EXPERIMENTAL SETUP

Next, we describe the experimental setup for testing our knowl-
edgeable group ﬁnding methods. We specify our research ques-
tions, describe our data set, and detail our ground truth.
Research questions. We consider the following questions. How
do different group ﬁnding models perform compared against each
other, under different ground truths or different evaluation metrics?
Are some queries harder than others for the same model? Finally:
Are the models different from each other?
Experimental collection. For evaluation purposes we use data
made available for the expert ﬁnding task at the TREC 2005 and
2006 Enterprise tracks [3, 8]. The document collection used is a
crawl of the World Wide Web Consortium (W3C; 330K documents,
5.7GB). The expert ﬁnding qrels for the two years differ: in 2005,

50 working group titles were the test topics for the expert ﬁnding
task, resulting in 1509 expert-group pairs, with 2 to 391 experts in
the same group and approximately 30 experts per group on aver-
age; names and e-mail addresses of 1092 expert candidates (W3C
members) are given as part of the collection. For the TREC 2006
expert ﬁnding task, 55 queries queries were created, but only 49 are
provided with expert ﬁnding ground truth.
Three types of ground truth. We use the qrels of the TREC 2006
expert ﬁnding task and propose three types of ground truth: binary,
graded and number.

Binary A working group g is considered relevant for topic q if
there is at least one expert ex who is a member of g (according to
the TREC 2005 expert ﬁnding qrels) and has expertise on the topic
q (according to the TREC 2006 expert ﬁnding qrels).

|{ex∈g:rel(ex,q)=1}|

Grade A slightly more sophisticated deﬁnition of group rele-
vance uses grades: the level of relevance of group g for query q
is deﬁned based on the fraction of the experts in the group. We
distinguish between |L| different levels of relevance, i.e., {0, 1, 2,
. . . , |L − 1|}. The relevance grade of group g for topic q is deﬁned
, where {ex ∈ g :
as follows: let f (g, q) =
rel(ex, q) = 1} is the set of experts in g with expertise on topic q
according to the TREC 2006 expert ﬁnding qrels and |g| is the total
number of experts in group g. If 1|L| · l ≤ f (g, q) < 1|L| · (l + 1),
the grade level for this group is l. In this paper, we set |L| = 10.
Number Here, the level of relevance of group g for query q is
deﬁned based on the number of experts in the group. For instance,
if there are 15 experts who have expertise on the given query topic,
then the level of the relevance for this group is 15. The level of
relevance ranges from 0 to 30 with a majority smaller than 4.
Runs. We run our experiments with all documents in the collection
for our four group ﬁnding models. We perform a grid search to ﬁnd
optimal settings of the smoothing parameters (with 0.1 increments).
We generate runs on the full collection and on subsets deﬁned by
taking the top n documents returned by a standard document re-
trieval run when using the topic as query. Evaluation measures
used are MAP, precision@5, 10, nDCG and nDCG@5, 10 against
our three types of ground truth. Evaluation was done using the
trec_eval program (available from http://trec.nist.gov).
6. RESULTS

|g|

We start by comparing the results of the optimized models with
two smoothing parameters. Next, we present the results of query
differences. Finally, we test whether the models smoothed by two
or one parameters are statistically signiﬁcantly different.
Model comparison. How do our knowledgeable group ﬁnding
models perform compared to each other? For each speciﬁc per-
formance evaluation metric, we compare the models using optimal
smoothing parameters. We use two parameters α and β to smooth
the proposed four knowledgeable group ﬁnding models, i.e., GQD,
GDQ, DGQ and QDG.

Table 1 lists the scores for the various metrics. Clearly, DGQ
outperforms the other models on all metrics using the binary and
graded ground truth, but GDQ outperforms DGQ on all metrics us-
ing the number ground truth. QDG model is the worst performing
model for all metrics and against all types of ground truth. The
table also shows that GQD, GDQ and DGQ have a similar perfor-
mance for all metrics against all types of ground truth. (The MAP
and p@N scores against the number ground truth are the same as
those against the binary ground truth, and are therefore omitted.)
Query differences. Our aim here is to ﬁnd out whether some
queries are harder than others for the same model against the met-
rics. We turn to a topic-level analysis of the MAP performance for

1007NDCG@

Table 1: Evaluation results for all optimal models with two
smoothing parameters, using the binary, graded and number
ground truths. For each metric, we report the evaluation re-
sults, followed by the optimal smoothing parameters α and β.
Ground
truth Model NDCG α β 5 α β 10 α β MAP α β 5 α β 10 α β
binary GQD .8861 .1 .2 .8165 .1 .2 .7850 .1 .3 .7127 .1 .7 .8041 .1 .9 .7306 .1 .3
GDQ .9009 .1 .9 .8291 .3 .5 .7936 .2 .5 .7552 .1 .9 .8122 .1 .6 .7408 .2 .8
DGQ .9133 .1 .9 .8680 .1 .9 .8420 .1 .9 .7772 .1 .9 .8571 .1 .9 .7918 .1 .9
QDG .7623 .1 .2 .5604 .1 .1 .5568 .1 .2 .4882 .1 .4 .5592 .1 .1 .5265 .1 .2
graded GQD .8245 .2 .3 .7237 .2 .3 .7595 .1 .2 .7403 .1 .3 .6367 .1 .1 .5224 .1 .2
GDQ .8457 .1 .4 .7675 .2 .3 .7945 .1 .4 .7673 .1 .4 .6776 .1 .3 .5510 .1 .4
DGQ .8631 .2 .9 .7991 .5 .9 .8160 .7 .8 .8092 .6 .8 .7102 .5 .6 .5531 .7 .9
QDG .3916 .1 .8 .1012 .1 .1 .1282 .1 .2 .2182 .1 .4 .1714 .1 .1 .1673 .1 .1

p@

number GQD .7964 .1 .9 .6210 .1 .8 .6730 .6 .8
GDQ .8160 .1 .9 .6496 .1 .9 .6905 .1 .9
DGQ .7907 .1 .9 .6062 .1 .9 .6758 .1 .9
QDG .6222 .1 .2 .3328 .1 .1 .4258 .1 .1

(a) GQD, binary

(b) GQD, graded

Figure 1: Topic-level differences from the mean scores for GQD
using the binary/number and graded ground truths.

each model against binary/number, graded ground truth. We plot
the differences in performances (per topic) between the average AP
score and the AP score per topic, sorted by performance difference.
Fig. 1(a) shows the plots for GQD when using the binary/number
ground truth, whilst Fig. 1(b), shows the plots for GQD when us-
ing the graded ground truth. From Fig. 1(a), it is clear that the
performance in MAP does not differ dramatically from the mean
when using the binary or number ground truths. In comparison, for
some topics the performance is dramatically worse than the mean
for GQD when using the graded ground truth. (We observed similar
phenomena for the other three models.)
Statistical signiﬁcance. Finally, we determine whether the ob-
served differences between our group ﬁnding approaches with two
smoothing parameters strategy are statistically signiﬁcant. We use
a two-tailed paired t-test between two models on NDCG and MAP
data and test for signiﬁcance differences at the 0.95 conﬁdence
level. Table 2 indicates that when using NDCG as a metric the
differences between GQD and GDQ against graded ground truth
are not signiﬁcant. This is also true for the differences between
GQD and DGQ, and between GDQ and DGQ against the graded
and number ground truth. When using MAP, the differences be-
tween all models are statistically signiﬁcant except for those be-
tween GQD and GDQ. We also test differences between the optimal
GQD model with smoothing with two parameters and with a single
smoothing parameter based on NDCG and MAP against three types
of ground truth; there are no statistically signiﬁcant differences at
the 0.95 conﬁdence level except based on NDCG against binary
ground truth where the p value is 0.0270. Hence, we cannot really
distingush between GQD with smoothing with two parameters and
GQD with smoothing with one parameter in our experiments.

metric ground

truth

Table 2: Two-tailed paired t-test between different models on
NDCG and MAP metrics.
GQD
vs.
DGQ
.0005
.0616
.4107
.0000
.0140

NDCG binary
graded
number
bin./numb.
graded

GDQ
vs.
QDG
.0000
.0000
.0000
.0000
.0000

DGQ
vs.
QDG
.0000
.0000
.0000
.0000
.0000

GQD
vs.
QDG
.0000
.0000
.0000
.0000
.0000

GDQ
vs.
DGQ
.0496
.2282
.0570
.0198
.0221

GQD
vs.
GDQ
.0107
.2349
.0086
.0013
.2647

MAP

7. CONCLUSIONS

We have introduced a group ﬁnding task. We proposed four mod-
els, GQD, GDQ, DGQ and QDG. We also constructed an experi-
mental collection by using the TREC 2005 and 2006 Enterprise col-
lections. We introduced three kinds of ground truth and evaluated
our models along many dimensions. Directly collecting expertise
evidence from documents is the most effective way to ﬁnd knowl-
edgeable groups when using the binary or graded ground truths,
and aggregating the expertise of each experts in the same group
can also be a good way to ﬁnd the groups. Our models are not very
sensitive to changes of the parameters when using a two parameter
smoothing strategy. We found statistically signiﬁcant differences
between the models when using MAP scores based on multiple
types of ground truth.
Acknowledgements. We thank our reviewers for their helpful com-
ments. This research was supported by the European Community’s
Seventh Framework Programme (FP7/2007-2013) under agreements
258191 (PROMISE) and 288024 (LiMoSINe), the Netherlands Or-
ganisation for Scientiﬁc Research (NWO) under nrs 640.004.802,
727.011.005, 612.001.116, HOR-11-10, the Center for Creation,
Content and Technology (CCCT), the BILAND project funded by
the CLARIN-nl program, the Dutch national program COMMIT,
the ESF Research Network Program ELIAS, the Elite Network
Shifts project funded by the Royal Dutch Academy of Sciences
(KNAW), and the Netherlands eScience Center under number 027.-
012.105.
8. REFERENCES
[1] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert

ﬁnding in enterprise corpora. In SIGIR’06, 2006.

[2] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si. Expertise
retrieval. Foundations and Trends in Information Retrieval, 6(2-3):
127–256, 2012.

[3] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the TREC

2005 enterprise track. In TREC’05, 2005.

[4] F. Jelinek and R. Mercer. Interpolated estimation of markov

sourceparameters from sparse data. In Proceedings of the Workshop
on Pattern Recognition in Practice, 1980.

[5] M. Kargar and A. An. Discovering top-k teams of experts

with/without a leader in social networks. In CIKM’11, 2011.

[6] T. Lappas, K. Liu, and E. Terzi. Finding a team of experts in social

networks. In KDD’09, pages 467–475, 2009.

[7] G. A. Pryor, J. W. Myles, D. R. R. Williams, and J. K. Anand. Team

management of the elderly patient with hip fracture. The Lancet,
pages 401–403, 1988.
I. Soboroff, A. P. de Vries, and N. Craswell. Overview of the TREC
2006 enterprise track. In TREC’06, 2006.

[8]

[9] M. Sozio and A. Gionis. The community-search problem and how to

plan a successful cocktail party. In KDD’10, pages 939–948, 2010.

[10] D.-N. Yang, Y.-L. Chen, W.-C. Lee, and M.-S. Chen. On

social-temporal group query with acquaintance constraint. In
VLDB’11, 2011.

[11] S.-M. Zhou, F. Chiclana, R. I. John, and J. M. Garibaldi. Alpha-level

aggregation. IEEE Trans. Knowl. and Data Eng., 23:1455–1468,
2011.

-1-0.5 0 0.5 1AP differencetopics-1-0.5 0 0.5 1AP differencetopics1008