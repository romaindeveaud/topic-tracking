Effectiveness/Efﬁciency Tradeoffs for Candidate
Generation in Multi-Stage Retrieval Architectures

Nima Asadi1,2, Jimmy Lin3,2,1

1Dept. of Computer Science, 2Institute for Advanced Computer Studies, 3The iSchool

University of Maryland, College Park

nima@cs.umd.edu, jimmylin@umd.edu

ABSTRACT
This paper examines a multi-stage retrieval architecture con-
sisting of a candidate generation stage, a feature extraction
stage, and a reranking stage using machine-learned models.
Given a ﬁxed set of features and a learning-to-rank model,
we explore eﬀectiveness/eﬃciency tradeoﬀs with three can-
didate generation approaches: postings intersection with SvS,
conjunctive query evaluation with Wand, and disjunctive
query evaluation with Wand. We ﬁnd no signiﬁcant dif-
ferences in end-to-end eﬀectiveness as measured by NDCG
between conjunctive and disjunctive Wand, but conjunc-
tive query evaluation is substantially faster. Postings in-
tersection with SvS, while fast, yields substantially lower
end-to-end eﬀectiveness, suggesting that document and term
frequencies remain important in the initial ranking stage.
These ﬁndings show that conjunctive Wand is the best over-
all candidate generation strategy of those we examined.
Categories and Subject Descriptors: H.3.3 [Information
Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: query evaluation; postings intersection

1.

INTRODUCTION

One possible architecture for web retrieval breaks docu-
ment ranking into three stages: candidate generation, fea-
ture extraction, and document reranking. There are two
main reasons for this multi-stage design. First, there is a
general consensus that learning to rank provides the best so-
lution to document ranking [13, 12]. As it is diﬃcult to apply
machine-learned models over the entire collection, in prac-
tice a candidate list of potentially-relevant documents is ﬁrst
generated. Thus, learning to rank is actually a reranking
problem (hence the ﬁrst and third stages). Second, separat-
ing candidate generation from feature extraction has the ad-
vantage of providing better control over cost/quality trade-
oﬀs. For example, term proximity features are signiﬁcantly
more costly to compute than unigram features; therefore, by

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

decoupling the ﬁrst two stages in the architecture, systems
can exploit “cheap” features to generate candidates quickly
and only compute “expensive” term proximity features when
necessary—thus decreasing overall query evaluation latency.
Given a ﬁxed set of features and a learning-to-rank model,
we explore eﬀectiveness/eﬃciency tradeoﬀs in the candidate
generation stage, comparing postings intersection with SvS,
conjunctive query evaluation with Wand, and disjunctive
query evaluation with Wand. Previous work has suggested
that conjunctive query evaluation yields early precision re-
sults that are at least as good as disjunctive query evalu-
ation, but is much faster. We experimentally conﬁrm this
observation, and additionally show that postings intersec-
tion (with results sorted by spam scores) yields substantially
worse output within the same framework.

The contribution of this work is an empirical evaluation
of common candidate generation algorithms in a multi-stage
retrieval architecture. By ﬁxing the feature generation and
reranking stages, we are able to isolate the end-to-end eﬀec-
tiveness and eﬃciency implications of diﬀerent algorithms.

2. BACKGROUND AND RELATED WORK
We begin with a more precise speciﬁcation of our three-
stage architecture, illustrated in Figure 1. The input to the
candidate generation stage is a query Q and the output is a
list of k document ids {d1, d2, ...dk}. In principle, this can be
considered a sorted list, but that detail is unimportant here.
These document ids serve as input to the feature extraction
stage, which returns a list of k feature vectors {f1, f2, ...fk},
each corresponding to a candidate document. These serve as
input to the third document reranking stage, which typically
applies a machine-learned model to produce a ﬁnal ranking.
This multi-stage retrieval architecture has recently been
explored by many researchers. Some have examined the en-
tire pipeline; for example, Macdonald et al. [14] assessed
the impact of variables such as the number of candidate
documents and the objective metric to use when training
the learning-to-rank model (however, they did not explore
eﬀectiveness/eﬃciency tradeoﬀs with candidate generation).
Others have speciﬁcally looked at candidate generation, e.g.,
postings intersection on multi-core architectures [17], dy-
namic pruning [18], and approximate techniques [3]. There
has been some work focused on the feature extraction stage,
exploring how to best represent positional information; two
studies have independently come to the conclusion that it
is advantageous to store document positions in a document
vector representation distinct from the inverted index [1, 2].
Finally, most work on learning to rank [13, 12] assumes such

997Figure 1: Illustration of a multi-stage retrieval archi-
tecture with distinct candidate generation, feature
extraction, and document reranking stages.

an architecture, because the input to the machine-learned
model is a set of feature vectors. Occasionally, this fact
is made explicit: for example, Cambazoglu et al. [6] experi-
mented with reranking 200 candidate documents to produce
the ﬁnal ranked list of 20 results.

There are two general approaches to candidate genera-
tion: conjunctive and disjunctive query processing. In the
ﬁrst, only documents that contain all query terms are con-
sidered, whereas in the second, any document with at least
one query term is potentially retrievable. One popular algo-
rithm is Wand [4], which can operate in either conjunctive or
disjunctive mode with respect to a particular scoring model
(e.g., BM25). Wand uses a pivot-based pointer movement
strategy to avoid needlessly evaluating documents that can-
not be in the ﬁnal top k ranking. An alternative approach to
candidate generation involves simple postings intersection,
without reference to a particular scoring model. Culpepper
and Moﬀat [8] demonstrated SvS to be the best algorithm
for accomplishing this. Note that since SvS works by it-
eratively intersecting the current result set with the next
shortest postings list, it must exhaustively compute the in-
tersection set. In this paper we explore both Wand and SvS
for candidate generation.

It is a well-known fact that disjunctive query processing
is much slower than conjunctive processing. However, it is
unclear what impact output quality at the candidate genera-
tion stage has on end-to-end eﬀectiveness. Although Broder
et al. [4] found conjunctive processing to yield higher early
precision, the results were not in the context of learning-
to-rank experiments. We hypothesize that end-to-end ef-
fectiveness is relatively insensitive to candidate generation
quality, due to an emphasis on early precision in most web
search tasks today—we simply need to make sure there are
enough relevant documents for the machine-learned model
to identify. Thus, there is an interesting possibility that con-
junctive query processing might lead to both good and fast
results. Our paper explores this hypothesis.

3. EXPERIMENTAL SETUP

3.1 Candidate Generation Algorithms

We examined four approaches to candidate generation,
outlined below. For each, we varied the number of candi-
dates generated k, where k ∈ {100, 250, 500, 1000}.
SvSSpam. Since SvS must compute the entire intersection
set, to obtain k documents, we sort the intersection results
by a static prior and return the top k.1

1One caveat worth mentioning: the common trick to renumber
docids based on the static prior doesn’t help much here since we
need to compute the complete intersection set regardless; it would
save the ﬁnal sort by score but time spent on that is negligible.

SvSBM25. In this approach, we ﬁrst compute the full inter-
section set, and then in a second pass we compute BM25
scores for every document in the intersection set. After the
second pass, the top k documents are returned.
WandCon. We evaluate the query in conjunctive mode and
return the top k documents based on BM25.
WandDis. We evaluate the query in disjunctive mode and
return the top k documents based on BM25.

All algorithms were implemented in C and an equal amount
of time was spent optimizing each to ensure a fair compari-
son. All conditions used the same (non-positional) inverted
index (even though SvSSpam does not need access to tf’s) and
for SvSBM25 we modiﬁed the accumulators to hold the tf’s
extracted from the postings for the second-pass scoring. We
assumed that all index structures are retained in memory, so
query evaluation never involves hitting disk. Experiments
were performed on a server running Red Hat Linux, with
dual Intel Xeon “Westmere” quad-core processors (E5620
2.4GHz) and 128GB RAM.
3.2 Test Collections and Metrics

We performed experiments on the ClueWeb09 collection, a
best-ﬁrst web crawl from early 2009. Our experiments used
only the ﬁrst English segment, which has 50 million docu-
ments (247GB compressed). The Waterloo spam scores [7]
were used as the static priors for SvS reranking.

For evaluation, we used three diﬀerent sets of queries: ﬁrst,
the TREC 2005 terabyte track “eﬃciency” queries (50,000
queries total).2 Since there are no relevance judgments for
these queries, they were used solely for eﬃciency experi-
ments. Second, a set of 100,000 queries sampled randomly
from the AOL query log [16]. Our sample retains the query
length distribution of the original dataset. Similar to the
TREC 2005 terabyte track queries, we used these queries
only to evaluate eﬃciency.

Finally, we used the TREC web track topics from 2009–
2011, 150 in total. These topics comprise a complete test
collection in that we have relevance judgments, but there
are too few queries for meaningful eﬃciency experiments.
We performed ﬁve fold cross validation, using three folds for
training, one for validation, and one for testing.

We collected two ﬁgures of merit: the ﬁrst was end-to-end
eﬀectiveness in terms of NDCG. For eﬃciency, we measured
query latency of the candidate generation stage, deﬁned as
the elapsed time between the moment a query is presented to
the system and the time when top k candidates are retrieved
and forwarded to the feature extraction stage. To capture
variance, we repeated runs ﬁve times.
3.3 Features and Ranking Model

We used a standard suite of features very similar to those
described in previous work [15, 18]. They consist of ba-
sic information retrieval scores (e.g., language modeling and
BM25 scores), term proximity features (exact phrases, or-
dered windows, unordered windows), query-independent fea-
tures (e.g., PageRank, content quality score [7], etc.). Rele-
vant features were computed across multiple ﬁelds: the entire
document, anchor text, as well as title ﬁelds. In total, there
are 91 features.3

2http://www-nlpir.nist.gov/projects/terabyte/
3The complete test collection, including feature descriptions, can
be found at https://github.com/lintool/ClueWeb09-TREC-LTR.

Document Rerankingddddddd1234567Candidates...dkRelevanceFinal ResultsFeature Extractionfffffff1234567Features...fkdddddddr...dCandidate GenerationQ1r2r3r4r5r6r7rk998NDCG@1
250
500

Model
100
1000
m BM25
0.06 0.09 0.09 0.11
a
Linear
0.06 0.08 0.10 0.14
p
S
λ-MART 0.05 0.09 0.10 0.15
0.25 0.25 0.25 0.25
. BM25
n
Linear
0.26 0.25 0.25 0.24
o
C
λ-MART 0.26 0.25 0.24 0.25
0.25 0.25 0.25 0.25
. BM25
Linear
0.26 0.25 0.25 0.23
λ-MART 0.26 0.25 0.25 0.24

s
i
D

250
0.06
0.06
0.07
0.22
0.24

NDCG@5
500
0.08
0.08
0.09
0.22
0.25

100
1000
0.05
0.10
0.05
0.10
0.05
0.11
0.22
0.22
0.25
0.23
0.28∗ 0.27∗ 0.25∗ 0.25∗
0.22
0.22
0.25
0.23
0.28∗ 0.27∗ 0.26∗ 0.25∗

0.22
0.24

0.22
0.24

500
0.06
0.07
0.06
0.21

NDCG@20
250
0.05
0.05
0.05
0.21

100
1000
0.04
0.08
0.03
0.08
0.03
0.08
0.21
0.21
0.25∗ 0.24∗ 0.23∗ 0.23∗
0.26∗ 0.25∗ 0.24∗ 0.23∗
0.21
0.21
0.25∗ 0.24∗ 0.23∗ 0.23∗
0.26∗ 0.25∗ 0.24∗ 0.23∗

0.21

0.21

250
0.044
0.045
0.045
0.280

NDCG
500
0.060
0.060
0.060
0.304

100
1000
0.027
0.077
0.027
0.079
0.027
0.079
0.230
0.327
0.248∗ 0.297∗ 0.317∗ 0.340
0.250∗ 0.300∗ 0.318∗ 0.340∗
0.331
0.230
0.248∗ 0.298∗ 0.320∗ 0.344∗
0.250∗ 0.301∗ 0.321∗ 0.344∗

0.281

0.308

Table 1: NDCG at diﬀerent cutoﬀs using diﬀerent candidate generation algorithms: SvSSpam (Spam), SvSBM25
and WANDCon (Con.), and WANDDis (Dis.). Vertical blocks show each metric, and columns show the number
of candidate documents retrieved. ∗ indicates statistical signiﬁcance vs. BM25 using the t-test (p<0.05).

For the third stage reranking model, we used two diﬀerent
learning-to-rank techniques. In the ﬁrst, we trained a lin-
ear model using the greedy feature selection approach [15].
The model is iteratively constructed by adding features, one
at a time, according to a greedy selection criterion. During
each iteration, the feature that provides the biggest gain in
eﬀectiveness (as measured by NDCG [11]) after being added
to the existing model is selected. This yields a sequence of
one-dimensional optimizations that can easily be solved us-
ing line search techniques. The algorithm stops when the
diﬀerence in NDCG between successive iterations drops be-
low a given threshold (10−4). This training procedure is
simple, fast, and fairly eﬀective.

Separately, we learned a LambdaMART model [5] using
the open-source jforests implementation4 [10]. To tune pa-
rameters, we used grid search as suggested by the authors
and selected the parameter setting that results in the largest
gain in NDCG on the validation set: max number of leaves
(9), feature and data sub-sampling (0.3), minimum observa-
tions per leaf (0.75), and the learning rate (0.05).

4. RESULTS

Table 1 summarizes NDCG at diﬀerent rank cutoﬀs and
no cutoﬀ for all combinations of candidate generation and
reranking models. The ﬁrst horizontal block of the table
labeled “Spam” refers to SvSSpam, “Con.” indicates SvSBM25
or WandCon (as both algorithms produce the same results),
and “Dis.” shows the use of WandDis. Within each block
of the table, “BM25” indicates reranking candidates with
BM25, which serves as the baseline (note this only alters the
postings intersection candidate results; in the other cases,
this is equivalent to a reranker that does nothing); “Linear”
is the linear model learned using Metzler’s greedy-feature
selection method; and “λ-MART” is LambdaMART. Each
column shows the number of candidate documents retrieved.
Candidates generated using SvSSpam yield very low end-
to-end eﬀectiveness. This shows that combining postings
intersection and with a static prior does not provide a suf-
ﬁciently discriminative signal to include relevant documents
in the top k, where k is small relative to the size of the
collection. The quality of the static prior is not to blame
here, as the Waterloo spam scores have been demonstrated
to be very helpful in reranking [7]. Of course, as we increase
the size of k, the results of SvSSpam will approach that of
the other candidate generation algorithms, but at the cost
of more time spent performing feature extraction. It is clear

4http://code.google.com/p/jforests/

Figure 2: Number of rel, non-rel, and unjudged doc-
uments in the top 20 for diﬀerent candidate sizes
using WANDDis (averaged across folds).

that both term frequencies and document frequencies cannot
be ignored in candidate generation.

On the other hand, WandCon (and SvSBM25, which pro-
duces exactly the same results) yields NDCG scores that
are statistically indistinguishable from those produced by
WandDis (with no cutoﬀ, WandDis yields slightly higher
scores, but the diﬀerences are not statistically signiﬁcant).
In other words, with BM25 scoring, conjunctive candidate
generation and disjunctive candidate generation are equally
good from the end-to-end eﬀectiveness perspective. This
ﬁnding is consistent with the results of Broder et al. [4] (but
in a learning-to-rank context).

Increasing k improves NDCG at all rank cutoﬀs for can-
didates obtained by SvSSpam. We observe the same trend
for NDCG without cutoﬀ in all settings. This is expected
since larger k translates into higher recall at the candidate
generation stage, i.e., more relevant documents are available
to the reranker. However, with conjunctive and disjunctive
query processing we see a decline in NDCG at rank cut-
oﬀs 5 and 20 as we increase the number of candidate doc-
uments; this trend is consistent for both the linear model
and LambdaMART. To examine this eﬀect in a bit more
detail, we computed the number of relevant, non-relevant,
and unjudged documents in the top 20 obtained by rerank-
ing WandDis. Figure 2 shows these statistics for diﬀerent
values of k. We see that as k increases, more unjudged doc-
uments ﬁnd their way to the top 20, while both the number
of relevant and non-relevant documents decreases. Thus,
this trend appears to be an artifact of the test collection
and not a property of the candidate generation algorithms.

 0 50 100 150 200 250 30010025050010001002505001000Number of documentsKNo judgmentsNon-relevantRelevantLinearλ-MART999(a) Terabyte

(b) AOL

Figure 3: Average candidate generation per-query latency across ﬁve trials.

Figure 3 shows per-query latency of the candidate gener-
ation stage for diﬀerent values of k. On average, SvSSpam
is the fastest since the ranking function is query indepen-
dent: score computation boils down to table lookups of spam
scores. We see that SvSBM25 is on par with WandCon for
larger k values. SvSBM25 retrieves the full intersection set
and computes scores for every document in the set regard-
less of k; therefore, increasing k has no impact on latency.
On the other hand, WandCon slows with increasing k (but
is faster with a small k). Conjunctive query evaluation with
Wand can only terminate when a postings list is fully con-
sumed. However, this termination is mostly independent of
k; the only factor aﬀected by k is the set of heap operations
performed during query evaluation. Finally, WandDis is not
only the slowest overall, but latency grows with larger values
of k faster than with conjunctive evaluation.

5. CONCLUSIONS

Our experiments show that conjunctive Wand is the best
candidate generation strategy of those examined: in terms
of end-to-end eﬀectiveness, it is statistically distinguishable
from disjunctive query evaluation using Wand, but much
faster in query evaluation. Note that the “block-max” opti-
mization to Wand proposed by Ding and Suel [9] does not
aﬀect this conclusion—although the optimization increases
disjunctive query evaluation speed, it remains slower than
conjunctive processing. In addition, we also show that post-
ings intersection with static priors yields very poor end-to-
end eﬀectiveness, at least with the size of the candidate sets
we examined. This suggests that postings intersection, while
an interesting algorithmic challenge since it represents the
more general problem of intersecting two lists of sorted in-
tegers, is not particularly useful by itself if one’s goal is to
build eﬀective and eﬃcient search systems.

6. ACKNOWLEDGMENTS

This work has been supported by NSF under awards IIS-
0916043, IIS-1144034, and IIS-1218043. Any opinions, ﬁnd-
ings, or conclusions are the authors’ and do not necessarily
reﬂect those of the sponsor. The ﬁrst author’s deepest grat-
itude goes to Katherine, for her invaluable encouragement
and wholehearted support. The second author is grateful to
Esther and Kiri for their loving support and dedicates this
work to Joshua and Jacob.

7. REFERENCES
[1] D. Arroyuelo, S. Gonz´alez, M. Marin, M. Oyarz´un, and

T. Suel. To index or not to index: Time-space trade-oﬀs in
search engines with positional ranking functions. SIGIR,
2012.

[2] N. Asadi and J. Lin. Document vector representations for

feature extraction in multi-stage document ranking. IRJ, in
press, 2012.

[3] N. Asadi and J. Lin. Fast candidate generation for

two-phase document ranking: Postings list intersection
with Bloom ﬁlters. CIKM, 2012.

[4] A. Broder, D. Carmel, M. Herscovici, A. Soﬀer, and
J. Zien. Eﬃcient query evaluation using a two-level
retrieval process. CIKM, 2003.

[5] C. Burges. From RankNet to LambdaRank to

LambdaMART: An overview. Technical Report
MSR-TR-2010-82, Microsoft Research, 2010.

[6] B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen,

C. Liao, Z. Zheng, and J. Degenhardt. Early exit
optimizations for additive machine learned ranking
systems. WSDM, 2010.

[7] G. Cormack, M. Smucker, and C. Clarke. Eﬃcient and

eﬀective spam ﬁltering and re-ranking for large web
datasets. arXiv:1004.5168v1, 2010.

[8] J. Culpepper and A. Moﬀat. Eﬃcient set intersection for

inverted indexing. TOIS, 29(1), 2010.

[9] S. Ding and T. Suel. Faster top-k document retrieval using

block-max indexes. SIGIR, 2011.

[10] Y. Ganjisaﬀar, R. Caruana, and C. Lopes. Bagging

gradient-boosted trees for high precision, low variance
ranking models. SIGIR, 2011.

[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulative gain-based

evaluation of IR techniques. TOIS, 20(4):422–446, 2002.

[12] H. Li. Learning to Rank for Information Retrieval and

Natural Language Processing. Morgan & Claypool, 2011.

[13] T.-Y. Liu. Learning to rank for information retrieval.

FnTIR, 3(3):225–331, 2009.

[14] C. Macdonald, R. Santos, and I. Ounis. The whens and

hows of learning to rank for web search. IRJ, in press, 2012.

[15] D. Metzler. Automatic feature selection in the Markov

random ﬁeld model for information retrieval. CIKM, 2007.

[16] G. Pass, A. Chowdhury, and C. Torgeson. A picture of

search. InfoScale, 2006.

[17] S. Tatikonda, B. Cambazoglu, and F. Junqueira. Posting
list intersection on multicore architectures. SIGIR, 2011.
[18] N. Tonellotto, C. Macdonald, and I. Ounis. Eﬃcient and
eﬀective retrieval using selective pruning. WSDM, 2013.

 0 20 40 60 80 100 120 1401002505001000Time (ms)Number of CandidatesSvS (Spam)SvS (BM25)WAND-ConjunctiveWAND-Disjunctive 0 50 100 150 200 250 300 350 4001002505001000Time (ms)Number of CandidatesSvS (Spam)SvS (BM25)WAND-ConjunctiveWAND-Disjunctive1000