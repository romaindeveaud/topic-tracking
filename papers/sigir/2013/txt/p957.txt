Ranking-Oriented Nearest-Neighbor Based Method for

Automatic Image Annotation

Chaoran Cui1

bruincui@gmail.com

Jun Ma1

majun@sdu.edu.cn

Tao Lian1

liantao1988@gmail.com

Xiaofang Wang1,2

ise_wangxf@ujn.edu.cn

Zhaochun Ren3
z.ren@uva.nl

1 School of Computer Science and Technology, Shandong University, Jinan, China
2 School of Information Science and Engineering, University of Jinan, Jinan, China

3 Intelligent Systems Lab Amsterdam, University of Amsterdam, Amsterdam, The Netherlands

ABSTRACT
Automatic image annotation plays a critical role in keyword-
based image retrieval systems. Recently, the nearest-neighbor
based scheme has been proposed and achieved good perfor-
mance for image annotation. Given a new image, the scheme
is to ﬁrst ﬁnd its most similar neighbors from labeled images,
and then propagate the keywords associated with the neigh-
bors to it. Many studies focused on designing a suitable dis-
tance metric between images so that all labeled images can
be ranked by their distance to the given image. However,
higher accuracy in distance prediction does not necessarily
lead to better ordering of labeled images. In this paper, we
propose a ranking-oriented neighbor search mechanism to
rank labeled images directly without going through the in-
termediate step of distance prediction. In particular, a new
learning to rank algorithm is developed, which exploits the
implicit preference information of labeled images and under-
lines the accuracy of the top-ranked results. Experiments on
two benchmark datasets demonstrate the eﬀectiveness of our
approach for image annotation.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing

Keywords
image annotation; nearest-neighbor based scheme; learning
to rank

1.

INTRODUCTION

In recent decades, the number of digital images has been
growing rapidly and there is an increasingly urgent demand
for eﬀective image retrieval techniques. Users often prefer

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

searching images with a textual query, which can be achieved
by ﬁrst annotating images manually, and then searching over
the annotations using the query. However, manual image an-
notation is a laborious and time-consuming process. There-
fore, many eﬀorts have been devoted to the research on au-
tomatic image annotation.

The goal of automatic image annotation is to assign a
few relevant keywords to an image that can reﬂect its visual
content. Recently, the nearest-neighbor based scheme [4]
has become increasingly attractive because of its superior
performance and straightforward framework.
It is on the
assumption that visually similar images are more likely to
share common keywords. Given a new image, the nearest-
neighbor based scheme ﬁrst ﬁnds a set of its most similar
neighbors from labeled images, and then propagates the key-
words associated with the neighbors to it.

In spite of the simplicity of the nearest-neighbor based
annotation framework, there are some critical issues that
remain to be addressed. One important aspect of the frame-
work is how to perform the process of the nearest neighbor
search eﬀectively. Many studies [4, 5, 6] focused on design-
ing a suitable visual distance metric between images, which
is then used to rank all labeled images according to their dis-
tance to the new image. Typically, the work aimed to weight
and combine the distances from diﬀerent dimensions in vi-
sual feature space. Despite the encouraging results achieved,
we argue that higher accuracy in distance prediction does
not necessarily lead to better ordering of labeled images,
which is the ultimate goal of our problem. For example, let
u and v denote two labeled images whose true distances to
the new image are 4 and 5 respectively. Suppose a method
has predicted the distance to be 5 for u and 4 for v. Although
it is a desirable result in terms of prediction error, it fails in
ensuring the correct ordering of u and v. In light of this, it
is necessary to shift our attention from approximating the
absolute distance between images to directly predicting the
relative ordering of labeled images.

In this paper, we propose a ranking-oriented neighbor
search mechanism, which uses the learning to rank [3] tech-
niques to directly produce the ordering of all labeled images
for a new image, without going through the intermediate
step of distance prediction. Unlike regular learning to rank
methods, our proposed ranking algorithm exploits the im-
plicit preference information hidden in the training images.
In addition, since only the k nearest neighbors are generally

957considered in the nearest-neighbor based scheme, we enforce
the ranking model to focus more on the correctness of the re-
sults in top-k positions. A boosting algorithm [1] is utilized
to solve the resulting optimization problem in our approach.

is estimated in a manner similar to F1 measure:

CON (y, ˆy) =

2pr
p + r

p =

yT ˆy
ˆyT ˆy

r =

yT ˆy
yT y

.

(1)

2. RANKING-ORIENTED NEIGHBOR

SEARCH MECHANISM

In this section, we introduce our ranking-oriented neigh-
bor search mechanism in details. For the ease of explanation,
we ﬁrst give some notations.
Let X be an image collection, and all keywords appearing
in the collection are W = {w1, w2, . . . , wc}, where c is the
total number of unique keywords. In image annotation task,
we are given n labeled training images, T = {x(i) ∈ X | i =
1, . . . n}, each of which is associated with a c-dimensional
label vector y(i) ∈ {0, 1}c, where y(i)
j = 1 if x(i) is labeled
by the jth keyword wj and y(i)
j = 0 otherwise. Given a
query image q ∈ X , our goal is to ﬁnd a ranking function
H : X ×T → R such that H(q, x(i)) represents the relevance
of the labeled image x(i) with respect to q, and x(i) is ranked
before x(j) if H(q, x(i)) > H(q, x(j)).

To resolve the above challenge, we seek to exploit the
learning to rank (hereinafter referred to as LTR for short)
techniques to learn the optimal ranking function H from the
training data. Although LTR has been extensively studied
[3], it is not straightforward to directly apply regular LTR
techniques to our problem for the following two reasons.
First, unlike standard LTR tasks where some preference in-
formation (in the forms of pairwise or listwise constraints)
is often explicitly given to supervise the learning process, in
our problem, preference information is only implicitly avail-
able in the training set. Moreover, generally we only con-
sider the k nearest neighbors for a new image to prohibit
the potential noisy keywords introduced by those distant
neighbors. Therefore, in our ranking problem, the correct
ordering of the top-k results is crucial, and the mistakes in
low ranks may not deteriorate the ﬁnal performance. It is
necessary to redesign the training procedure to ensure the
top-k results are as accurate as possible.

Based on the above analysis, to facilitate our ranking task,
in the following, we ﬁrst generate the implicit preference
information hidden in the training data. With the preference
information, we further present a new LTR algorithm that
underlines the accuracy of the top-k results.

2.1 Generation of Preference Information

As no explicit preference information is given for our prob-
lem, the ﬁrst step before LTR is to derive some preference
information from the training data. Speciﬁcally, we sepa-
rately submit each labeled image as a query and look for the
information that could indicate the relative ordering among
the other labeled images with respect to it.

It is notable that the intuition behind the nearest-neighbor
based methods is that similar images should share more
common keywords. This means that given an image, its
close neighbors may have a higher keyword agreement with
it compared to those distant neighbors. In accordance with
this principle, we consider measuring the relative distance
between labeled images by the consistency of their keywords.
Given two label vectors y and ˆy, their consistency CON (y, ˆy)

i

i

. . . rn

, ri+1

i . . . ri−1

With this deﬁnition, for a labeled image x(i), we deﬁne
Ri = {r1
i } to be the relevance degrees
of the other corresponding labeled images, i.e., T \x(i) =
{x(1) . . . x(i−1), x(i+1) . . . x(n)}, where rj
i = CON (y(i), y(j)).
Then, let πi denote the total ranking of T \x(i) with respect
to x(i), which can be derived in descending order of Ri, and
πi(xj) stands for the position of image xj ∈ T \x(i).

Although πi seems a natural way of representing the pref-
erence information associated with x(i), the variations in
the importance of partial orderings with diﬀerent ranking
positions cannot be easily reﬂected in such a form of linear
ordering. To allow encoding this kind of information, in this
paper, we construct the preference information in the form of
ordered pairs of images, and also assign each pair a weight to
represent the importance of its being satisﬁed. In particular,
a set of p ordered pairs Wi = {w(xjm (cid:4)i xqm ) | m = 1, . . . , p}
is further randomly picked up from πi, where xj (cid:4)i xq de-
notes a ordered pair indicating that the labeled image xj is
ranked before xq in πi, and w(xj (cid:4)i xq) is its corresponding
importance weight. To determine the value of w(xj (cid:4)i xq),
i as a new ranking of T \x(i), which ex-
we ﬁrst deﬁne π(cid:2)
changes the positions of xj and xq in πi. Then we calculate
the NDCG@k metric of π(cid:2)
i:

N DCGπ

i @k =
(cid:2)

1
Nk

2rl − 1
log2(1 + l)

,

(2)

k(cid:2)

l=1

where rl denotes the relevance degree of the image with po-
sition l in π(cid:2)
i, and Nk is a normalization factor chosen so that
the NDCG@k of the original ranking πi is 1. Furthermore,
the exact form of w(xj (cid:4)i xq) is given as

w(xj (cid:4)i xq) =

1 − N DCGπ

i @k πi(xj) ≤ k

(cid:2)

η

otherwise

.

(3)

(cid:3)

In the above, if xj or xq involves the top-k instances in πi,
we take the drops of π(cid:2)
i in terms of NDCG@k as the value
of w(xj (cid:4)i xq).
Intuitively, as NDCG includes a position
discount factor in its deﬁnition, incorrectly ordering higher
ranks of πi can lead to greater losses in terms of NDCG@k.
As a result, a large weight will be assigned to the ordered
pair at high positions. On the contrary, if both xj and xq ap-
pear behind the kth position in πi, there is no eﬀect on the
value of NDCG@k when their positions exchange. There-
fore, we set w(xj (cid:4)i xq) to be η, which is a small constant.
Finally, we repeat the above process for each labeled im-
age and give the ultimate set of preference information as
P = ∪n
the following LTR algorithm.
2.2 Top-k Focused Ranking Algorithm

i=1Wi , which will be used as input training data for

With the derived preference information set P, we now
present the formulation of the proposed top-k focused LTR
algorithm. The basic idea is that the optimal ranking func-
tion H should be consistent with the preference information
in P as much as possible. To this end, we deﬁne the ranking
error of H with respect to P as follows:

WijqI(Hiq ≥ Hij) .

(4)

(cid:2)

err =

xj(cid:3)ixq∈P

958Here, we introduce Wijq = w(xj(cid:4)ixq) and Hij = H(x(i), xj)
for the simplicity of description. I(·) is an indicator function
that outputs 1 if the input boolean variable is true and zero
otherwise. In fact, err measures the weighted number of the
preference pairs misordered by H. As described in Section
2.1, the preference pairs at high positions have relatively
larger weights, thus the incorrect orders of these pairs will
result in more severe ranking errors; whereas the pairs only
involving the instances behind the kth positions have been
assigned small weights, and misordering them may aﬀect
little on the error. As a result, through minimizing err, we
can ﬁnd the optimal ranking function H that gives priority
to ensuring the correctness of the top-k results.
However, the ranking error deﬁned in (4) is a non-smooth
function as the indicator function I(·) is non-smooth.
It
is well known that directly optimizing a non-smooth func-
tion is computationally infeasible. To address the problem,
we follow the idea of AdaBoost algorithm by replacing the
indicator function I(x ≥ y) with an exponential function
exp(x − y). The resulting new ranking error is:
Wijq exp(Hiq − Hij) .

(cid:4)err =

(cid:2)

(5)

xj(cid:3)ixq∈P

mizing the new error (cid:4)err, we eﬀectively reduce the original
Since it always holds that exp(x − y) ≥ I(x ≥ y), by mini-
ranking error err. Besides, another advantage of using (cid:4)err is

In our study, we utilize the RankBoost [1] algorithm to

from the theoretical property of AdaBoost, i.e., minimizing
the exponential loss can not only reduce the training errors
but also increase the margins of the training samples, and
the enlarged margins are the key to ensure a low generaliza-
tion error for test instances.

learn the optimal ranking function H by minimizing (cid:4)err. To
guarantee the correct execution of the algorithm, we need
to give a group of ranking features F = {f1, . . . , fg}, where
each ranking feature fi deﬁnes a linear ordering of the im-
ages to be ranked. To this purpose, we calculate the dis-
tance of all ranked images to the query image in the space
of a certain visual feature, and a ranking feature is gener-
ated in ascending order of the distance. It should be noted
that the ranking features are only related to the ordering of
the ranked images rather than the actual numerical values
of their distance. Algorithm 1 shows the details of the Rank-
Boost algorithm. The algorithm operates for T iterations.
For each successive iteration t = 1, 2, . . . , T , it maintains a
weight distribution D(t) over the preference pairs in P, and
ijq as the weight on the pair xj(cid:4)ixq. Initially, the
denotes D(t)
weights are set according to the importance of preference
pairs (line 1). At iteration t, a weak ranker ht is created
from F based on the current weight distribution D(t) (line
3). We use the same generation process of weak ranker as
described in [1]. Then the algorithm chooses a weight co-
eﬃcient αt for ht by measuring its ranking accuracy on all
preference pairs (line 4). Intuitively, a greater coeﬃcient is
given to the more accurate weak ranker. Meanwhile, the
weight distribution D(t) is updated according to the perfor-
mance of ht (line 5). The preference pairs misordered by
ht have their weights increased, whereas the weights are de-
creased for those pairs that are ordered correctly. Therefore,
the weak ranker in the next iteration ht+1 will concentrate
more on the “hard” pairs for ht. Once all the weak rankers
have been created, the algorithm outputs the ﬁnal ranking

Algorithm 1 Rankboost algorithm for minimizing (cid:4)err
Input: P, F and T
Output: H
1: Initialize a distribution D over all preference pairs in P:

(cid:5)

D(1)

ijq = Wijq

Z0 where Z0 =

Wijq

xj(cid:3)ixq

2: for t = 1, . . . , T do
3:
4:

Create a weak ranker ht : Rr × Rr → R from F
2 ln( 1+r
Compute αt = 1
1−r )
ijq(ht(x(i), xj) − ht(x(i), xq))
D(t)
where r =

(t)
ijq

D

exp(ht(x

(i)

,xq)−ht(x
Zt

(i)

,xj ))

ijq exp(ht(x(i), xq) − ht(x(i), xj))
D(t)

(cid:5)
(cid:5)

xj(cid:3)ixq

5: Update D(t+1)

ijq =

where Zt =

xj(cid:3)ixq

6: end for
7: return H(q, x) =

T(cid:5)

t=1

αtht(q, x)

function H through their weighted combination, and αt is
the corresponding contribution of ht (line 7).

After ﬁnding the optimal ranking function H, given a new
image q, we employ H to produce the total ranking of all
labeled images with respect to q, and take the top-k results
as its k nearest neighbors, which is denoted by NH(q) =
{N N1, . . . , N Nk}.

3.

IMAGE ANNOTATION WITH DERIVED
NEIGHBORS
With the set of neighbor images NH (q), the next step is
to evaluate the keyword relevance and propagate a certain
number of the most relevant keywords to the new image q. In
most previous work, researchers determined the relevance of
a keyword by the majority or weighted voting of the nearest
neighbors. However, there is no theoretical guarantee that
the keywords selected by this manner are always the suitable
annotations for q. In [2], Li et al. demonstrated that the
diﬀerence between the keyword frequency in local neighbor
set and that in entire image collection is a good keyword
relevance indicator. Therefore, in our study, we adopt the
similar method to compute the relevance of the keyword w
with respect to q:

rel(w, q) = kfNH (q)(w) − kfprior(w) ,

(6)

where kfNH (q)(w) is the number of labeled images contain-
ing w in NH(q), and kfprior(w) denotes the total frequency
of w in the entire training collection.

4. EXPERIMENTS
4.1 Experiment Settings

We conduct experiments on two benchmark datasets: Corel
5K and IAPR TC 12. The two datasets have been widely
used in previous studies so we can directly compare the ex-
periment results. Each image on both datasets is repre-
sented with the same visual features as described in [4].

On Corel 5K and IAPR TC12, all comparative methods
are required to annotate each image with 5 most relevant
keywords. The quality of predicted annotations is assessed
by retrieving test images using the keywords in annotation

959Table 1: Performance comparison in terms of
P %, R% and N + between our method and previous
published work.

Corel 5K

IAPR TC12

P% R% N+ P% R% N+

MSC
JEC
LASSO
GS
NW-RNN
RNN

25
27
24
30
29
31

32
32
29
33
32
34

136 — — —
250
139
246
127
252
146
149
259
255
149

29
29
29
30
31

28
28
32
28
33

Figure 1: Eﬀect of the variation of k on Corel 5K.
vocabulary. For a keyword w, its precision and recall is
computed as follows:

P recision(w) =

Nw
Np

Recall(w) =

Nw
Nr

,

(7)

where Nw denotes the number of images correctly annotated
with w, Np denotes the number of images predicted to have
w, and Nr is the number of images annotated with w in
ground-truth. The average precision (P ) and recall (R) are
computed over all keywords as two evaluation measures. In
addition, we also consider another measure to assess the
coverage of correctly annotated keywords, i.e., the number
of keywords with non-zero recall (N +).

In our approach, the number of neighbors considered in
the nearest-neighbor based scheme, k, is a parameter to be
determined. In the experiments, the optimal value of k is
found via an exhaustive search with a 5-fold cross-validation
on training set. Figure 1 presents the performance compar-
isons by varying k from 30 to 400 on Corel 5K. We can see
that the best results can be achieved when k = 200, and
a very small or large value of k degrades the performance.
This is reasonable because a small number of neighbors can-
not provide suﬃcient information to reﬂect the characteris-
tics of a new image, while too many neighbors may introduce
some information irrelevant to that image. On IAPR TC12,
similar variation trend of performance can be observed as k
changes, and the optimal value of k is around 500. There-
fore, we set k = 200 for Corel 5K and k = 500 for IAPR
TC12 in our later experiments.
4.2 Experiment Results

To investigate the eﬃcacy of our ranking-oriented nearest-
neighbor based method for image annotation, which is de-
noted by RNN, we compare it with some previous distance-
oriented methods, i.e., MSC [5], JEC [4], LASSO [4] and
GS [6]. Besides, we design a modiﬁed version of our origi-
nal method, NW-RNN, which adopts a similar ranking al-
gorithm to RNN but without considering the procedure of
preference pair weighting in equation (3).
Instead, NW-
RNN assigns equal weight to all preference pairs. As a re-
sult, it is diﬃcult for NW-RNN to ensure the correctness of
the top-ranked results suﬃciently. Table 1 shows the anno-
tation results of diﬀerent approaches.

As clearly observed in the table, on Corel 5K, NW-RNN
gains comparable performance with RNN in N +, but loses
a lot a in terms of P and R respectively. Such results under-
lines the importance of focusing more on the correctness of
the top-ranked results for the success of our ranking-oriented

neighbor search mechanism. In addition, RNN outperforms
all the distance-oriented approaches listed in diﬀerent eval-
uation measures. The performance increase over the best
distance-oriented method (GS) still achieves 1% , 1% and
3 in terms ofP , R and N +. On IAPR TC12, RNN is
superior to other approaches as well. These improvements
suggest that the annotation results provided by our method
is preferable.

5. CONCLUSIONS

In this paper, we have introduced a novel image annota-
tion method, which adapts the conventional nearest-neighbor
based approaches with a ranking-oriented neighbor search
mechanism. A new learning to rank algorithm is devel-
oped to directly produce the ordering of all labeled im-
ages.
It leverages the implicit preference information of
training data and underlines the accuracy of the top-ranked
results. Experiments have demonstrated the eﬀectiveness of
our method for image annotation. For future study, we plan
to examine the scalability of our method and experiment on
large-scale web image datasets.

6. ACKNOWLEDGMENTS

This work is supported by the Natural Science Founda-
tion of China (61272240,60970047,61103151), the Doctoral
Fund of Ministry of Education of China (20110131110028)
and the Natural Science Foundation of Shandong Province
(ZR2012FM037).

7. REFERENCES
[1] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An

eﬃcient boosting algorithm for combining preferences.
The Journal of Machine Learning Research, 4:933–969,
2003.

[2] X. Li, C. Snoek, and M. Worring. Learning social tag

relevance by neighbor voting. Multimedia, IEEE
Transactions on, 11(7):1310–1322, 2009.

[3] T.-Y. Liu. Learning to rank for information retrieval.

Found. Trends Inf. Retr., 3(3):225–331, 2009.

[4] A. Makadia, V. Pavlovic, and S. Kumar. A new baseline

for image annotation. In ECCV, pages 316–329, 2008.

[5] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang.
Multi-label sparse coding for automatic image
annotation. In CVPR, pages 1643–1650, 2009.

[6] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and

D. Metaxas. Automatic image annotation using group
sparsity. In CVPR, pages 3312–3319, 2010.

960