Search Engine Switching Detection Based on User

Personal Preferences and Behavior Patterns

Denis Savenkov, Dmitry Lagun, Qiaoling Liu

Mathematics & Computer Science

{denis.savenkov, dlagun, qiaoling.liu}@emory.edu

Emory University

Atlanta, US

ABSTRACT
Sometimes, during a search task users may switch from one
search engine to another for several reasons, e.g., dissatis-
faction with the current search results or desire for broader
topic coverage. Detecting the fact of switching is diﬃcult
but important for understanding users’ satisfaction with the
search engine and the complexity of their search tasks, lead-
ing to economic signiﬁcance for search providers. Previous
research on switching detection mainly focused on studying
diﬀerent signals useful for the task and particular reasons
for switching. Although it is known that switching is a per-
sonal choice of a user and diﬀerent users have diﬀerent search
behavior, little has been done to understand how these dif-
ferences could be used for switching detection. In this paper
we study the eﬀectiveness of learning personal behavior pat-
terns for switching detection and present a personalized ap-
proach which uses user’s session history containing sessions
with and without switches. Experiments show that users’
personal habits and behavior patterns are indeed among the
most informative signals. Our ﬁndings can be used by a
search log analyzer for engine switching detection and po-
tentially other log mining problems, thus providing valuable
signals for search providers to improve user experience.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
search engine switching

1.

INTRODUCTION

Search engines such as Google, Bing, Yandex, etc. facili-
tate access to enormous amount of information available on
the Word Wide Web. Among many available search engines,
users typically prefer to use one or two, performing major-
ity of their searches on a primary search engine. Decision

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

on which search engine to use as a primary search provider
could be based on many factors including search quality,
locale, satisfaction, usability of search interface and popu-
larity of a search engine [9]. Sometimes users switch from
one search engine to another during a search task. As the
majority of switching cases (57%) arise from users’ dissatis-
faction with the search engine [9], information on when users
switch provides a valuable signal for search providers to im-
prove user search experience in such cases. For some search
sessions, the fact of switching can be easily detected, for in-
stance via a web browser (maintained by a search engine),
a browser toolbar or search logs (e.g. some users ask nav-
igational query in the current search engine to open a new
one). However, in reality only a fraction of switches can be
monitored. The same users may switch in a way that cannot
be detected by a search engine, e.g. by opening a new tab.
Detecting the fact of switching, when such events cannot
be monitored directly is diﬃcult, but it is important for un-
derstanding users’ satisfaction and the complexity of search.
It also has the economic signiﬁcance to search providers as
switching is a low-level signal related to the market share.

Previous work [20, 21, 9] identiﬁed the most popular rea-
sons for switching as dissatisfaction with search results, de-
sire to get more relevant results for coverage and veriﬁcation
or users’ personal preferences to use a particular search en-
gine for some types of searches. This suggests that decision
to switch search engines depends not only on a task, but
also on user’s personal behavior patterns and habits. For
example, some users are more persistent than others and
can make more eﬀort exploring the search results. Previous
research made progress in detecting search engine switching,
but information on user’s typical behavior didn’t get enough
attention.

In this paper, we focus on learning users’ search behavior
patterns and habits and using it for search engine switching
detection. Our experiments show, that a state-of-the-art
existing model (semi-supervised generative model proposed
in [10]) beneﬁts from using this information. In addition, we
built a model based on multiple gradient boosting regression
trees, which utilizes more than 400 features based on user
preferences, search tasks, user behavior patterns, etc.

In our work we used publicly available dataset, which con-
tains search logs for 1 month period, collected by Yandex for
the Switching detection challenge1 (Oct 23, 2012 - Dec 22,
2012) . The logs have search engine switching actions de-
tected either by a browser toolbar or by clicks on URL of

1http://switchdetect.yandex.ru/en

33a diﬀerent search engine from the result page of the source
search engine.

The main contributions of this paper include:
• a personalized version of the generative model for web
search success prediction [10]. We show that learning
user’s personal action transition patterns signiﬁcantly
improves the performance of the model.

• machine learning based switching prediction model,
based on an ensemble of decision trees, which utilizes
over 400 features, including users’ personal search be-
havior information. We show that the personalized
model outperforms algorithms that do not use this in-
formation. This model outperformed other competi-
tors on the Yandex switching detection challenge.

• in-depth feature importance analysis, which shows that
users’ statistics are actually among the best predictors
in the feature set.

The rest of the paper is organized as follows: Section 2
brieﬂy discusses the related work, Section 3 describes the
dataset we used and explains why personalization is impor-
tant. Section 4 presents our personalized versions of the re-
cently proposed Markov model based algorithm [10] and our
machine learning based approach that harnesses rich person-
alized behavior features for switching detection. In Section
5 the results of the prediction experiments are presented and
Section 6 provides some discussions and implications of the
results. Finally, Section 7 concludes the paper.

2. RELATED WORK

Understanding and detecting search engine switching has
recently attracted much research attention [13][14][20][9].
Heath and White [13] proposed the task of predicting search
engine switching in a real-time setting, i.e. whether a user’s
next action during a search will be an engine switch. The
algorithm used in the paper is based on a string matching
idea, i.e., user’s most recent actions performed and pages vis-
ited in the session so far are encoded as a character sequence
and prediction is based on the number of times this sequence
was previously followed by a switch or a non-switch action.
Laxmant et al.
[14] later improved the idea by identifying
frequent predictive subsequences and estimating their asso-
ciated Hidden Markov Models. Since a sequence could not
catch all session information, White and Dumais [20] used
a richer set of features to predict switching, which were de-
rived from the user’s most recent query, session interaction
observed so far, and user search history. Guo et al. [9] inves-
tigated the reasons causing search engine switching based on
in-situ feedback from users and developed models to predict
the rationale for a given switching event. The above work
(especially [13][14][20]) are the closest to ours, which also in-
spired us on the features to start with; yet several diﬀerences
exist in between. First, we aim at oﬄine detection of user
switching using complete session information instead of on-
line scenario where only partial session information is avail-
able. Second, we focus on using personalization for learning
to detect search engine switching and we compare diﬀerent
personalization approaches, which have been little studied
in previous works.

As the most popular switching rationale is dissatisfaction
with search results, another relevant research area is predict-
ing search success [10][1][8][11][12]. Field et al.
[8] worked

on predicting user-reported frustration, and found a few sim-
ple query log features performed better than physical sensor
features. Ageev et al. [1] worked on modeling search success
via a game-like infrastructure and used Conditional Random
Field (CRF) models for prediction. Hassan et al.
[11][12]
proposed to use two Markov models for predicting search
success, and found that user behavior based models are more
predictive than document relevance based models. Hassan
[10] recently proposed a semi-supervised approach based on
a generative model and EM estimation to model search sat-
isfaction and found that adding unlabeled data improves the
eﬀectiveness. Other relevant work include using query logs
and user behavior to predict relevance of search results [7][2]
as well as studying the change in user behavior when search
becomes more diﬃcult [3]. User preference is another rea-
son causing search engine switching, e.g. sometimes a user
prefers one search engine to another for some types of search
tasks. Numerous researches show that user diﬀerences inﬂu-
ence their search strategies and behavior [5][22][17] and per-
sonalization is eﬀective to improve user experience in web
search [4][18] [6].

In our work, we take a close look at the main reasons for
engine switching, derive features shown useful for identifying
such scenarios and focus on learning users’ personal behavior
models. The results of this work show that such models
signiﬁcantly improve the performance of switching detection
and the approach can be applied to other similar tasks.

3. DATASET ANALYSIS
3.1 Dataset

For our work we used a publicly available dataset, pro-
vided by Yandex for its recent switching detection challenge.
The dataset contains a subset of search logs of 30 days, which
are about 1.5 years old and do not contain sessions with
queries that have commercial intent detected with Yandex
proprietary query classiﬁer. Search sessions contain unique
user identiﬁer and a sequence of records for search actions,
such as queries, result clicks and search engine switching ac-
tions, which were detected by a browser toolbar or by clicks
on a link to open another search engine from the search
engine results page. The information provided along with
search actions includes for each record the time elapsed from
the beginning of the session, for a click action the clicked
url, or for a query action the query and the urls shown
to the user. To allay privacy concerns the logs were fully
anonymized, so only meaningless query, user and url ids are
available. Also the dataset doesn’t contain any information
on the search engine the user switched to. The elapsed time
for all actions are speciﬁed in time units with unknown re-
lation to seconds. In total, the dataset includes 8,595,731
sessions, 1,457,533 sessions with switch action, 10,139,547
unique queries, 49,029,185 unique URLs, and 956,536 unique
users. According to the challenge setting, the data in the last
3 days are used as the test set, and all users who don’t have
switches in the ﬁrst 27 days were removed from the dataset.
3.2 Switching Behavior Analysis

Previous research [20] [9] showed that the main reasons
for search engine switching are: dissatisfaction in 57% of
the cases, verifying or ﬁnding additional information in 26%
of the cases and user preferences (e.g. a user prefers one en-
gine to another for some particular search tasks) in 12% of

34Figure 1: Histogram of switching rates for users with
≥ 20 sessions in the ﬁrst 27 days

Figure 2: Histogram of switching rates for users with
≥ 20 sessions in the ﬁrst 27 days, considering only
sessions with 7 to 30 queries

the cases. This suggests that switching behavior highly de-
pends on a search task, e.g. for an easy task the user is less
likely to switch to another search engine. In [20] a histogram
of probability of switching given session length is provided,
suggesting that search sessions with more queries are more
likely to contain a switch. But on the other hand switching
behavior also depends on users’ personal habits. Some users
might not switch search engines even when they are not sat-
isﬁed with the search results while some other users might
have a habit of verifying results by opening another search
engine even when the current results are relevant. Figure
1 shows the histogram of switching rates for users with at
least 20 sessions in the training period (ﬁrst 27 days). As
you can see, some users switch more often than others. To
reduce the inﬂuence of task diﬃculty on switching probabil-
ity, the histogram on Figure 2 considers only sessions with 7
to 30 queries, as for these sessions the probability of switch-
ing is relatively more stable. Nevertheless, the histogram
shows that even in this case diﬀerent users still have varying
switching rates. This supports the hypothesis that switch-
ing behavior have a strong personal component. Some users
are more experienced with using multiple search engines,
and others do this only occasionally. Another aspect relates
to the search task itself. For example, a user may switch
more frequently because he/she performs diﬃcult searches
more often, for which the results are more likely to be non-
relevant, or because he/she asks more queries of research
type and needs more search results. This means that per-
sonalization can help learn more about both users’ behavior
patterns and their search tasks.

As shown by White and Dumais [20], the likelihood that
a switch happens in a session increases as the session length
increases. We also found this result with our data by com-
paring the average number of queries in switching and non-
switching sessions across all users (the blue line in Figure
3 shows that on average more queries are issued in sessions
with a switch). However, another interesting observation is
that this eﬀect is actually diﬀerent for diﬀerent users. As
a session gets longer, the likelihood of switching increases
much more for some users than for others. Moreover, for
some users the switching likelihood even decreases when
more queries are issued (the left gray part in Figure 3).

Therefore, to better detect switching for these users it is
necessary to understand and model their personal behavior
patterns.
3.3 Task and Metric Description

In this paper we are focusing on detecting search sessions
that contain switch/switches to another search engine. Fol-
lowing the setting of Yandex switching detection challenge,
we use the AUC (Area Under Curve) measure [15] to eval-
uate the performance of our switching prediction models.
AUC is a popular measure used for evaluating classiﬁers,
which represents the probability that a classiﬁer will rank a
randomly chosen positive instance higher than a randomly
chosen negative one. To allow easier interpretation of the
performance of our models, we also present the correspond-
ing precision-recall curves in the evaluation.

4. PERSONALIZED SWITCH MODELING
4.1 Evaluation Set

The dataset contains sessions collected for 30 day period
from users who switched at least once in the ﬁrst 27 days.
Since sessions in days 28-30 were used as the test set in
Yandex switching detection challenge and the labels were
not available at the time of our experiments, we held out
the sessions in days 25-27 as our validation set. The rest of
the data (sessions in days 1-24) were used for training. To
make a validation set similar to the challenge test set, we
ﬁltered out sessions from users who didn’t switch in the ﬁrst
24 days.
4.2 Personalized Generative Models to Switch

Detection
Search trails, i.e.

sequences of actions a user performs
with the search engine (e.g. queries, clicks on a search re-
sult) were shown to be very useful signals for predicting user
satisfaction and search engine switching. For example, in
[20] pre- and post-switch motifs were studied and shown to
be useful for switching prediction. In [10] a semi-supervised
generative model for search success prediction was proposed,
which learns probabilities of transitions from one action to

35Table 2: Markov model transition probabilities

Non-switch sessions
E

Q

C

Switch sessions
Q

C

E

Q 0.217
C 0.207

0.750
0.470

0.033 Q 0.316
0.323 C 0.288

0.621
0.521

0.063
0.191

Table 3: Markov model transition probabilities for
a particular user

Non-switch sessions
E

Q

C

Switch sessions
Q

C

E

Q 0.417
C 0.219

0.527
0.352

0.056 Q 0.506
0.428 C 0.397

0.430
0.354

0.065
0.249

sessions into the two classes, not considered in transitions).
Moreover, in switch sessions users tend to ask more queries.
However, we were also aware that some users have very
diﬀerent behavior from the average. For example, Table 3
shows the transition probabilities estimated on sessions from
a particular user (who has 50 switching and 77 non-switching
sessions). The diﬀerences in the estimated probabilities for
this user and the general model (e.g., this user in general asks
more queries) suggests that taking personal transition habits
into account may improve the prediction performance.

Unfortunately, most users have just a few sessions and
hence the estimated transition models can be unreliable. In
this consideration, we smoothed each personalized model
with the global model for ﬁnal prediction. To get a good
smoothing, we used a machine learning approach. Three
features were generated for each session: prediction of the
global model, prediction of the user’s personal model, and
number of sessions in the history of the user. Then a logistic
regression was applied to get a combined prediction2.

The results of our experiments with generative models
are presented in Table 4. The personalized models signif-
icantly outperform the global models and other baselines.
In our experiment, the semi-supervised approach proposed
in [10] incorporating the EM algorithm did not improve the
performance of the supervised models, perhaps due to the
large amount of labeled data available. Unfortunately, en-
coding time information into action sequences did not help
the personalized models, probably because the larger alpha-
bet makes the action transitions more sparse and therefore
the derived personalized model more unreliable.

Another interesting observation is the performance of the
baseline using user switching rate compared to other base-
lines. Despite the fact that this predictor did not use any in-
formation about the session, it outperforms other baselines,
supporting our hypothesis that switching highly depends on
users’ habits and behavior patterns.
4.3 Personalized Machine Learning Approach

to Switch Detection

Machine learning approaches have been shown to be suc-
cessful for predicting search engine switching [13][14][20] as
it allows to combine various signals into a single model. In-
spired by these eﬀorts, we developed a broad set of features
that cover diﬀerent aspects of search behavior, including the
ones described in previous work and some new ones.

2We also tried other models, including decision trees, but in
this setup the results of diﬀerent models did not diﬀer much.

Figure 3: Diﬀerence between average number of
queries in switch and non-switch sessions for each
user

Table 1: Alphabets for search trails (type-I, type-II)

Action Description

Q
C
E
q
Q
K
D
S
P
E

query submission
click on a search result
end of the session
query with a short pause before next action
query with a long pause before next action
other query
short dwell-time click on a search result
long dwell-time click on a search result
other clicks on a search result
end of the session

another and uses this data for prediction. Unfortunately, in
our dataset only query and search result click actions are
available, thus we could not exploit other actions such as
clicks on ads or on spelling suggestions as used in previous
work. As a result, the basic (type-I) alphabet for search
trail encoding consists of {Q, C, E} as shown in Table 1.
Inspired by previous work, we also introduce an advanced
search trail encoding by distinguishing between short- and
long-dwell time clicks. In addition, we felt that queries with
diﬀerent pause until the next action may be also helpful, so
we encoded them as well. Finally, the advanced (type-II)
alphabet is {q, Q, K, D, S, P, E}, as shown in Table 1. Note
that, “short” is deﬁned by less than 200 time units while
“long” is deﬁned by more than 500 time units.

A key part of the model deﬁned in [10] for user satisfaction
prediction is the action transition parameters for the switch
and non-switch class respectively. Therefore, we presents the
estimated transition probabilities for switch and non-switch
sessions in Table 2. As you can see sessions with switches are
more likely to end after a query action than sessions without
switches (Note that switch actions were only used to split

36Table 4: Generative model search prediction results

Model
Baseline using query length
Baseline using session duration
Baseline using user switching rate (smoothed)
Generative model (type-I sequences)
Generative model (type-II sequences)
Semi-supervised model (type-I sequences)
Prediction of per-user model only (type-I)
Personalized generative model (type-I)
Personalized generative model (type-II)

AUC
0.6710
0.7257
0.7306
0.7058
0.7081
0.7064
0.6707
0.7725
0.7624

To build a personalized model we tried several approaches:
• training a separate model for each user and using its

prediction as a new feature for a global training;

• encoding information about the user directly into the
feature set as binary “user id” features (the total num-
ber of features becomes very large);

• calculating diﬀerent statistics based on each user’s ses-
sions and using these statistics in the feature set sep-
arately as well as for normalizing other features (e.g.
query clickthrough rate normalized by the average query
clickthrough rate for this user). Statistics can be fur-
ther calculated for users’ switch and non-switch ses-
sions separately to learn more about user behavior in
both cases. Yet we need to use a separate set from
the training set for computing such statistics to avoid
overﬁtting. Therefore, we splitted the sessions from
days 1-24 into two sets: a statistics set (sessions from
days 1-21) and a training set (sessions from days 22-
24). Then, from the training set we removed sessions
by users who never switched during the statistics pe-
riod, similarly as how the test set was created in the
challenge setting.

4.3.1 Feature Description
The feature set we developed includes 414 features. Some
of them are very similar to each other, only with diﬀerent
smoothing, normalization or other variations. Table 5 pro-
vides a summary of the features (not a complete list). The
Python code generating the features is available online3.

The features belong to one of the three groups:

1. features based on the current session only, e.g. session
duration (in time units, in queries and in clicks), num-
ber of unique and abandoned queries (queries without
clicks), average/min/max click dwell time and pause
between actions (pause between issuing a query and
the next action was also considered), average click po-
sition, number of SAT/DSAT clicks (as deﬁned in Ta-
ble 1), and some more.

2. features based on statistics computed from all switch/non-

switch sessions in the statistics period.

3. features based on statistics computed from user-speciﬁc

switch/non-switch sessions in the statistics period.

The later two groups of features are calculated on the
statistics dataset. They can be grouped into three subsets:
3http://mathcs.emory.edu/∼dsavenk/switch detect/

• all features in group 1 normalized by the corresponding
average statistics calculated across all switching/non-
switching sessions (features in group 2) and across user-
speciﬁc switching/non-switching sessions (features in
group 3). This gives us four diﬀerently normalized
versions for each feature in group 1.

• switching probability of a user/query/url. Diﬀerent
users have diﬀerent switching probabilities. The same
is true for diﬀerent search tasks. To include search task
related switching probability we considered queries and
urls separately. For each query we calculated the num-
ber of times the query was issued before (immediately
or earlier in a session) a switch in a session.

• features based on action sequences or search trails,
including probabilities of this sequence appearing in
switch/non-switch sessions, probabilities of this sequence
under the Markov model with transition probabilities
estimated on switch/non-switch sessions, and ﬁnally
statistics based on all n-grams (2,3,4-grams) of this se-
quence, namely average ratio of their frequencies in
switch and non-switch sessions.

4.3.2 Learning Algorithms
For our experiments we used the logistic regression algo-
rithm (scikit-learn4 implementation [16]) and the gradient
boosting tree algorithm (pGBRT5 implementation [19]).

5. EVALUATION
5.1 Personalization Performance

To test the performance we used the holdout validation
set, which covers the sessions in days 25-27. Sessions in the
previous 3 days (days 22-24) were used for training and the
rest of the dataset (days 1-21) was used to calculate the
aggregated statistics6.

The results of the personalization approaches described
in Section 4.3 are presented in Table 6. When building a
model per user in the ﬁrst approach we used the logistic
regression (with L1-regularization and parameter C=1.0).
In the second approach the number of features becomes very
large and we were unable to train a gradient boosting model,
so the result reported was also obtained by training a logistic
regression model. In the third approach a gradient boosting
tree algorithm was applied (with 400 iterations, maximum
tree depth = 5 and learning rate = 0.1). For comparison
purposes we also provided the performance of the logistic
regression model in the same setting.

The ﬁrst and second personalization methods (per-user
model prediction as a feature and user ids as features) are
prone to overﬁtting. The dataset contains a lot of users with
just a few sessions (which is also true in general) and thus
such models do not have good generalization properties. On
the other hand, the third approach which utilizes statistics
aggregated over user-speciﬁc switch and non-switch sessions
signiﬁcantly outperforms the other two. This model is also
easier to implement in practice. It does not require build-
ing a separate model whenever a new user arrives and not

4http://scikit-learn.org/
5http://machinelearning.wustl.edu/pmwiki.php/Main/Pgbrt
6Increasing the size of the training set by reducing the statis-
tics period did not improve the performance of the models.

37Table 5: Features used for switching detection (not a complete list)

- average, max, min dwell times of clicks

- number of queries
- number of clicks

Features based on current session only
q count
c count
time to 1click - time to ﬁrst click in a session (or large value if no clicks)
q abandoned - number of abandoned queries
dwell
duration - session duration (in time units and in actions - queries and clicks)
pause - mean, min, max pause between actions in session
unique queries - number of unique queries issued in a session
(d)sat clicks - number of sat/dsat clicks
ave click pos - average click position (11 if no clicks)
time between clicks - average time between clicks
last action - is the last action a query or a click
Features based on all switch/non-switch sessions
all above features normalized by the corresponding average statistics calculated across all switching/non-switching sessions
q switch freq - frequency of a query occuring before switch in a session (max, mean, min over queries in a session)
q ctr - ctr (sat, dsat, last) of a query (max, mean, min over queries)
q ave clickpos - average click position for a query (max, mean, min over queries)
q freq - query frequency (max, mean, min over queries)
clicked url ctr - ctr (sat, dsat, last) of a clicked url (max, mean, min over all clicked urls in a session)
url switch freq - frequency of a clicked url apperaring before switch in a session (max, mean, min over all clicked urls)
markov models - probability of the action sequence by Markov models estimated on switch/non-switch sessions
n-grams - average ratio of frequencies of the sequence n-grams in switch and non-switch sessions
seq prob - probability of this exact action sequence appearing in switch/non-switch sessions
Features based on user-speciﬁc switch/non-switch sessions
all session-based features normalized by the corresponding average statistics calculated across all user’s switch/non-switch
sessions
all average statistics of session features computed over this user’s switch/non-switch sessions
user switch prob - number of user sessions with a switching + 1 divided by the total number of user sessions + 10
user switch prob periods - similar to user switch prob but computed for each 3-day period within the statistics period
user last(middle,toolbar,serp) switch prob - number of user sessions with a switch (last/middle in session or of toolbar/serp
type) divided by the total number of user sessions
user session count - number of sessions of this user in the statistics period
ave time to switch - average time to switch for this user

Table 6: Performance of diﬀerent personalization
approaches

Method
Overall statistics used in feature set (boosting)
Per-user model prediction as feature
User ids as binary features
User statistics used in feature set (boosting)
User statistics used in feature set (logistic)

AUC
0.7782
0.7753
0.7587
0.8413
0.8348

require rebuilding a model to add a new user id. Note that
the gradient boosting algorithm performed better than the
logistic regression algorithm in this same setting, thanks to
the power of the former algorithm to catch the non-linear
interactions between the features.

Figure 4 shows how the prediction quality depends on
the size of user’s history (number of sessions), for personal-
ized and non-personalized approaches. We chose the best-
performing third personalization method using the gradi-
ent boosting algorithm and its non-personalized counterpart
(the only diﬀerence is that all features depending on user-
speciﬁc statistics are ignored) for this comparison. As we
can see the personalized approach improves prediction per-
formance for all kinds of users and the more information we

have the better the result we can achieve. We do not have
users without history in our dataset, but even if we have
just one session we can still learn something from it and im-
prove the results. The performance of the non-personalized
approach also improves as the number of sessions in user
history grows. The reasons could be that users who rarely
use the search engine have diﬀerent behavior from users who
frequently use the search engine, and that sessions by diﬀer-
ent user groups have diﬀerent level of individual diﬀerences
in behavior.
5.2 Individual Feature Importance

Table 7 presents the importance scores for some of the
features. We chose two scores:
information gain and Gini
index based score calculated on the boosting trees ensemble
(denoted as boosting score). Only a couple of features from
each group are presented in the table. The ranks of the
features in the list are computed by the boosting score.

The analysis shows that the most important signals for
switch detection are sequence n-gram statistics, user switch-
ing frequency, and average click position. Interestingly, the
average click position feature is ranked 3rd by the boost-
ing score but much worse (123th) by the information gain.
This feature can be an indicator of both diﬃcult search
tasks and tasks requiring more coverage. Sequence features

38Table 7: Feature importance scores (ranks are computed based on the boosting score)

Rank Feature

1

2

3
4
11

12

14

15
21

22

27
30
42
51

64
291

- total number of switches in user’s sessions during the statistics

user typeII 3gram - a ratio of 3-gram (type-II, see above) counts in user’s switch and
non-switch sessions averaged over all 2-grams in a search trail
user switches count
period
ave click pos - average position of clicks in a session
user switch prob - smoothed user’s switching rate in sessions from the statistics period
typeII 3gram - a ratio of 3-gram counts in all switch and non-switch sessions from the
statistics period averaged over all 3-grams in a search trail
time to 1click user normalized - time to the ﬁrst click in a session normalized by average
time to the ﬁrst click in user’s non-switch sessions
abandoned to ave abandoned - number of abandoned queries normalized by average num-
ber of abandoned queries in user’s non-switch sessions
unique queries - number of unique queries in a session
ave clickedurl switchprob - number of times a user switched after (later in a session)
clicking on the given url divided by the total number of clicks on this url averaged over
all clicked urls in the session
min pause user switch - minimum pause in a session normalized by average minimum
pause in user’s switch sessions
ave time to switch - average time before switching in user’s sessions
time 1click - time to ﬁrst click in a session
sat clicks - number of sat clicks in a session
user ave query switchprob - number of times the user switched after issuing the given
query divided by the total number of times the query was asked averaged over all queries
in the session
session duration - duration of the session in time units
query nonswitchprob - number of time a query was asked and there was no switch after-
wards in the session divided by the total number of times the query was asked averaged
over all queries in a session

Information Boosting

gain
0.0350

score
0.0230

0.0194

0.0222

0.0149
0.0309
0.0290

0.0219
0.0184
0.0134

0.0103

0.0129

0.0229

0.0114

0.0204
0.0100

0.0114
0.0103

0.0016

0.0101

0.0026
0.0078
0.0046
0.0013

0.0092
0.0082
0.0069
0.0059

0.0182
0.0001

0.0050
0.0000

arately) is useful, which supports the previous researches. It
was surprising that url switching statistics (probability that
a session contains a switch after a click on the given url) are
more important than query switching statistics. The possi-
ble explanation is that some urls “suggest” users to try an-
other search engine. Unfortunately we couldn’t explore this
deeper because the dataset is anonymized and actual urls
are not available. Another reason could be sparsity of the
statistics: only ∼ 48 % of the queries shown in sessions from
the validation period were also asked at least once in the
training/statistics period, and ∼ 67% of urls clicked during
the validation period were also clicked at least once during
the training period.

As the analysis showed that n-gram features are among
the most useful predictors, we decided to look at some n-
grams in more details. Table 8 shows some 3-grams over
type-II sequences ranked by the ratio of their frequencies in
switch and non-switch sessions. We can see that 3-grams
with Q (queries with long pause) tend to occur more fre-
quently in switch sessions, and on the contrary 3-grams with
S (SAT clicks) are indicators of non-switch sessions.

5.3 Feature Group Importance

To investigate how features interact with each other we
analyzed the prediction performance for each of the feature
groups described in Chapter 4.3.1. A special interest is to
see which of the following is the most important: frequency
of switches for users, queries, urls or search trails. Therefore,
we considered another way to group features into ﬁve dis-

Figure 4: AUC for users with diﬀerent size of search
history (number of sessions)

placed at the top of the ranked list, suggesting that Markov
models, whole sequence frequencies and n-gram statistics
are all useful. Comparing the two types of sequences, we
found that features calculated based on type-II sequences
are ranked best, which means that encoding diﬀerent di-
mensions of time information (i.e., distinguishing diﬀerent
lengths of pause after a query and of pause after a click sep-

39Table 8: 3-grams ranked by the ratio of frequencies
to appear in switching and non-switching sessions

Table 9: Feature ablation experiments

Single feature group runs

3-gram Ratio
QKQ 1.6332
KQQ 1.5983
QQQ 1.5895
QQK 1.5259
KQK 1.4143
KKQ 1.4041

3-gram Ratio
0.2096
0.2107
0.2449
0.2568
0.2649
0.2697

qSS
SSS
SSP
SPS
SSD
PSS
...

...

...
qqq

...

0.4384

DDD

0.4194

joint sets: 1) features based on session statistics7 (duration,
number of queries, number of clicks, etc); 2) features based
on user statistics8 (user switch probability and some modi-
ﬁcation); 3) features based on query statistics (frequency of
switches after a given query, ctr of queries in a session, etc);
4) features based on url statistics (frequency of switches after
a given url is clicked in a session, ctr of clicked urls in a ses-
sion, etc); and 5) features based on action sequence statistics
(Markov model probabilities, n-gram statistics, etc).

Based on the above two ways of feature grouping, we con-
ducted a series of feature ablation experiments. Table 9
shows the prediction performance when providing a single
feature group or when providing all feature groups except
the given one. Among the ﬁrst set of feature groups, over-
all statistics features alone are more eﬀective than session
features alone, which veriﬁes our intuition that aggregated
statistics on a separate period helps with switching predic-
tion. Moreover, user-speciﬁc statistics features gave even
more signals for predicting switches. This single feature
group run achieved 98.1% of the validation AUC obtained by
training on all features. And removal of this feature group
resulted in a signiﬁcant drop in prediction quality (from
0.8413 to 0.7782).

Among the second set of feature groups, the most eﬀec-
tive ones are session statistics features and sequence fea-
tures. Here we also see that url switching statistics features
alone obtained better AUC than user statistic features alone
(e.g. user switching frequency). Query statistic features
alone resulted in signiﬁcantly worse performance in predict-
ing switches. On the other hand, results of runs with a
feature group removed are very close to each other, indicat-
ing that signals in each of the ﬁve feature groups could be
covered to a certain extent by other four feature groups.

In Figure 5 we plotted the precision and recall curve for
the positive class (switch sessions) for models with diﬀerent
subsets of features. It shows that for the switching detec-
tion task, high precision (e.g., 0.78) could be achieved at
the cost of recall (e.g., 0.1). Also the ﬁgure suggests that
adding personalized statistics improves the prediction qual-
ity signiﬁcantly as you can see a gap between curves with
and without personalized statistics.
5.4 Model Tuning

Considering the training set is imbalanced (only 10% of

7Note that session statistics features here included all the
normalized versions by the corresponding average statistics
computed in the statistics period.
8Note that user statistics features here did not include those
relying on queries, urls, or action sequences in the session.

Feature group

Training Validation

Session features
Overall statistics features
User-speciﬁc statistics features
Session statistics features
User statistics features
Query statistics features
Url statistics features
Sequence statistics features

AUC
0.7563
0.7853
0.8381
0.8298
0.7503
0.6480
0.7488
0.8062
Without feature group runs
0.8532
0.8371
0.7882
0.8453
0.8490
0.8529
0.8466
0.8517
0.8534

Session features
Overall statistics features
User-speciﬁc statistics features
Session statistics features
User statistics features
Query statistics features
Url statistics features
Sequence statistics features
All features

AUC
0.7491
0.7777
0.8253
0.8183
0.7273
0.6352
0.7396
0.7952

0.8326
0.8290
0.7782
0.8348
0.8289
0.8380
0.8273
0.8397
0.8413

Figure 5: Precision-Recall curve for the positive
class (switch sessions)

the sessions contain switches and the rest are sessions with-
out a switch), we tried two approaches to balance the dataset,
namely increasing the weight of positive examples and sub-
sampling the negative examples. The results for both ap-
proaches are summarized in Table 10. In addition, to reduce
the variance of the prediction model, an averaging approach
was applied. Models for seven diﬀerent time splits (e.g., days
1-3 as training period while days 4-24 as statistics period,
days 4-6 as training period while days 1-3,7-24 as statistics
period, etc) were built and average of the individual model
predictions was used as the ﬁnal score. As the results show,
these strategies improve the prediction performance.

40Table 10: Results for dataset balancing and model
averaging

Train AUC Validation AUC

No balancing
Pos weight = 4
Subsampling
Model trained on d22-24
Averaging 8 models

0.8473
0.8534
0.8490
0.8534

-

0.8403
0.8413
0.8412
0.8413
0.8450

5.5 Comparison with Other Models

As baselines we decided to use the following algorithms:
ranking by session length in queries, by session duration in
time and by user switching rate on a separate period. Table
11 compares results of our model with other known algo-
rithms. Unfortunately, some of the features described in [10]
and [20] cannot be calculated base on our fully anonymized
dataset, thus we included just a subset of the described fea-
tures. Most of them were already a part of the feature set
used for this work. We should mention that in [20] the online
switch prediction problem is solved, i.e. given a part of a
session predict if the next action is a switch. This problem
is diﬀerent from (probably more diﬃcult than) the oﬄine
detection of switching, and thus we were unable to get high
AUC score with this online model. To train it we used gra-
dient boosting tree algorithm (200 iterations, depth = 5 and
learning rate = 0.1).

Table 11: Comparison of diﬀerent switching detec-
tion algorithms

Model
Baseline using query length
Baseline using session duration
Baseline using user switching rate (smoothed)
Semi-supervised model from [10]
Personalized generative model (type-I)
Online prediction model trained
on subset of features from [20]
Our personalized model

AUC
0.6710
0.7257
0.7306
0.7081
0.7725
0.7206

0.8450

The results in Table 11 show that the developed person-
alized switching prediction model outperforms other known
models for the switching detection task. Another interesting
observation is the performance of the baselines. On our task
and dataset, session duration and user switch rate predictors
show better performance than the generative model. And
user switching rate is the strongest baseline, which proves
that switching behavior depends highly on user habits. Yan-
dex Switching detection challenge also proved competitive-
ness of the presented personalized model as this method
placed 1st out about ∼ 100 teams.

6. DISCUSSION AND IMPLICATIONS

A primary focus of this research has been a better detec-
tion of search engine switches inside a session using person-
alized models. We showed that users have diﬀerent behav-
ior patterns and habits when using web search and mostly
ignored personal statistics information is very useful for un-
derstanding search logs. Tables 2 and 3 show an example of
diﬀerences between “average” search behavior and behavior
of a particular user. There are other examples, which sup-

port the idea that users do have diﬀerent search behavior
and we can beneﬁt from learning behavior of each individual
user. A simple baseline model, which predicts probability of
a session to contain switching action based on how frequent
the given user switched before performed as good as some
other more complicated models. Learning more about user’s
behavior can also help improve performance of existing mod-
els. As an example, we trained a personalized version of one
of the recently proposed models [10] - a generative model for
satisfaction prediction and applied it to switching detection.
Personalized model allows to achieve 9.45% improvement
over non-personalized model.

Another common approach to switching detection is a ma-
chine learning based approach. To learn how personaliza-
tion would aﬀect such models we developed a set of features
which include some of previously studied signals as well as
some other predictors and applied a gradient boosting tree
algorithm to learn a detector for session containing switch-
ing actions. In this paper we presented 3 possible person-
alization approaches, namely training a model for each user
as well as a global model, using user id as a feature and
calculating rich aggregated statistics based on user-speciﬁc
sessions as both independent features and normalizers for
other features. Unfortunately the ﬁrst two approaches didn’t
show any improvement as models became overﬁtted, but
the last approach allowed us to get a 8.1% improvement
over a similar model trained on unpersonalized feature set.
Our analysis showed that features based on user’s statis-
tics are among the most important features. The features
ablation experiments also support the conclusion on useful-
ness of user’s statistics. The group of features obtained by
considering statistics collected on user’s previous switching
and non-switching sessions performed better than the oth-
ers and removal of this group leads to signiﬁcant decline in
prediction quality.

A possible problem with this approach is the sparsity of
the statistics, as just a small fraction of users have good
search history. But as Figure 4 suggests even for users with
small history the model still does good prediction which is
even better then for non-personalized model. The same ex-
periment shows that the more history we have the better
(we only had enough data to look at users with up to 50
sessions), but the grows in the prediction performance sta-
bilizes after as few as 10 sessions in user’s history.

We believe that the features we designed and the per-
sonalization approach we used can be useful in some other
tasks, like user satisfaction prediction, relevance prediction
from click logs, etc. For example, some users click on more
results without paying too much attention to snippets and
some may examine results longer than others. This means
that the clicks of these users may weight diﬀerently when
estimating results relevance.

7. CONCLUSIONS

In this paper we presented a personalized approach to
search engine switching prediction. The approach allows a
model to learn user’s personal behavior patterns in switch-
ing and non-switching sessions and use this information to
improve the prediction performance. We showed that some
existing models can beneﬁt from using personalized informa-
tion, e.g. the recently proposed generative markov model
for user satisfaction prediction, when applied to switching
detection problem, can be improved by learning per-user

41models. We also designed a machine learning approach
which utilizes several kinds of personalized and aggregated
statistics, collected on a period separate from the training
set. All the features were designed to help learning diﬀer-
ent users’ switching habits and behavior patterns as well
as diﬀerent reasons for switching (i.e. dissatisfaction and
necessity in better coverage). We demonstrated that per-
sonalized statistics and session feature normalization allow
to improve switching prediction quality signiﬁcantly. We
believe that our approach can also be used in other log min-
ing tasks such as search success prediction and document
relevance prediction.

8. ACKNOWLEDGMENTS

This work was supported by the NSF grant IIS-1018321,
the DARPA grant D11AP00269, and the Yahoo! Faculty
Research Engagement Program.

9. REFERENCES
[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find
It If You Can: A Game for Modeling Diﬀerent Types
of Web Search Success Using Interaction Data. In
Proc. of SIGIR’11, pages 345–354, New York, New
York, USA, July 2011. ACM Press.

[2] E. Agichtein, E. Brill, and S. Dumais. Improving web

search ranking by incorporating user behavior
information. In Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’06, pages
19–26, New York, NY, USA, 2006. ACM.

[3] A. Aula, R. M. Khan, and Z. Guan. How does search
behavior change as search becomes more diﬃcult? In
Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems, CHI ’10, pages 35–44,
New York, NY, USA, 2010. ACM.

[4] P. N. Bennett, R. W. White, W. Chu, S. T. Dumais,

P. Bailey, F. Borisyuk, and X. Cui. Modeling the
impact of short- and long-term behavior on search
personalization. In Proceedings of the 35th
international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’12, pages
185–194, New York, NY, USA, 2012. ACM.

[5] G. Buscher, R. W. White, S. Dumais, and J. Huang.
Large-scale analysis of individual and task diﬀerences
in search result page examination strategies. In
Proceedings of the ﬁfth ACM international conference
on Web search and data mining, WSDM ’12, pages
373–382, New York, NY, USA, 2012. ACM.

[6] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and

H. Li. Context-aware query suggestion by mining
click-through and session data. In Proceedings of the
14th ACM SIGKDD international conference on
Knowledge discovery and data mining, KDD ’08, pages
875–883, New York, NY, USA, 2008. ACM.

[7] G. Dupret and C. Liao. A model to estimate intrinsic

document relevance from the clickthrough logs of a
web search engine. In Proceedings of the third ACM
international conference on Web search and data
mining, WSDM ’10, pages 181–190, New York, NY,
USA, 2010. ACM.

[8] H. A. Feild, J. Allan, and R. Jones. Predicting searcher

frustration. In Proc. of SIGIR’10, pages 34–41, New
York, New York, USA, July 2010. ACM Press.
[9] Q. Guo, R. White, Y. Zhang, B. Anderson, and

S. Dumais. Why searchers switch: understanding and
predicting engine switching rationales. In Proc. of
SIGIR’11, pages 335–344, 2011.

[10] A. Hassan. A semi-supervised approach to modeling
web search satisfaction. In Proc. of SIGIR’12, pages
275–284, New York, NY, USA, 2012. ACM.

[11] A. Hassan, R. Jones, and K. L. Klinkner. Beyond

DCG: User Behavior as a Predictor of a Successful
Search. In Proc. of WSDM’10, pages 221–230, New
York, New York, USA, Feb. 2010. ACM Press.

[12] A. Hassan, Y. Song, and L.-w. He. A task level metric

for measuring web search satisfaction and its
application on improving relevance estimation. In
Proc. of CIKM’11, pages 125–134, New York, New
York, USA, Oct. 2011. ACM Press.

[13] A. Heath and R. White. Defection detection:

Predicting search engine switching. In Proc. of
WWW’08, pages 1173–1174, 2008.

[14] S. Laxman, V. Tankasali, and R. W. White. Stream

prediction using a generative model based on frequent
episodes in event sequences. In Proc. of SIGKDD’08,
pages 453–461, New York, New York, USA, Aug.
2008. ACM Press.

[15] C. Ling, J. Huang, and H. Zhang. Auc: a statistically

consistent and more discriminating measure than
accuracy. In International Joint Conference on
Artiﬁcial Intelligence, volume 18, pages 519–526, 2003.
[16] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,

B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine Learning in
Python . Journal of Machine Learning Research,
12:2825–2830, 2011.

[17] S. Shen, B. Hu, W. Chen, and Q. Yang. Personalized

click model through collaborative ﬁltering. In
Proceedings of the ﬁfth ACM international conference
on Web search and data mining, WSDM ’12, pages
323–332, New York, NY, USA, 2012. ACM.

[18] D. Sontag, K. Collins-Thompson, P. N. Bennett,

R. W. White, S. Dumais, and B. Billerbeck.
Probabilistic models for personalizing web search. In
Proceedings of the ﬁfth ACM international conference
on Web search and data mining, WSDM ’12, pages
433–442, New York, NY, USA, 2012. ACM.

[19] S. Tyree, K. Weinberger, K. Agrawal, and J. Paykin.

Parallel boosted regression trees for web search
ranking. In Proc. of WWW’11, pages 387–396. ACM.

[20] R. White and S. Dumais. Characterizing and

predicting search engine switching behavior. In Proc.
of CIKM’09, pages 87–96. ACM, 2009.

[21] R. White, A. Kapoor, and S. Dumais. Modeling

long-term search engine usage. User Modeling,
Adaptation, and Personalization, pages 28–39, 2010.

[22] R. W. White and S. M. Drucker. Investigating

behavioral variability in web search. In Proceedings of
the 16th international conference on World Wide Web,
WWW ’07, pages 21–30, New York, NY, USA, 2007.
ACM.

42