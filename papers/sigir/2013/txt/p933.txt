Sequential Testing in Classiﬁer Evaluation
Yields Biased Estimates of Effectiveness

William Webber

Mossaab Bagdouri

iSchool, CS

University of Maryland

College Park, MD 20742, USA
{wew,mossaab}@umd.edu

David D. Lewis

Douglas W. Oard

David D. Lewis Consulting
Chicago, IL 60614, USA

sigir2013pap@

DavidDLewis.com

iSchool / UMIACS

University of Maryland

College Park, MD 20742, USA

oard@umd.edu

ABSTRACT
It is common to develop and validate classiﬁers through a process
of repeated testing, with nested training and/or test sets of increas-
ing size. We demonstrate in this paper that such repeated testing
leads to biased estimates of classiﬁer effectiveness. Experiments
on a range of text classiﬁcation tasks under three sequential testing
frameworks show all three lead to optimistic estimates of effective-
ness. We calculate empirical adjustments to unbias estimates on
our data set, and identify directions for research that could lead to
general techniques for avoiding bias while reducing labeling costs.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems and soft-
ware—performance evaluation.

General Terms
Performance

Keywords
Supervised learning, text categorization, evaluation, F-measure

1.

INTRODUCTION

Classiﬁers are frequently produced by supervised learning, with
the classiﬁer trained on one set of annotated examples, and then
tested on another set selected by random sampling. When this test-
ing is done a single time, textbook techniques of point estimation,
conﬁdence interval estimation, and hypothesis testing on effective-
ness are directly applicable.

In many practical circumstances, however, effectiveness is tested
multiple times. The developer may repeatedly add examples to
the training data, retrain classiﬁers, and test until estimated effec-
tiveness reaches a target level. Or the classiﬁer may already be
constructed, and the developer wishes to validate its effectiveness
while minimizing the number of test examples that need to be la-
beled. Or the developer may grow both training and tests sets at the
same time, seeking both to improve and to certify effectiveness.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Unfortunately, as we observe in this paper, repeated testing of
a classiﬁer’s effectiveness introduces statistical bias into estimates.
Statistical inference is based on the recognition that any particu-
lar measurement has an element of randomness to it. We may by
chance draw a sample that gives an optimistic estimate, leading us
to put into use a classiﬁer less good than it appears. Conversely, we
may reject a classiﬁer incorrectly based on a sample that gives too
pessimistic an estimate. But by drawing a sample randomly we can
quantify these risks, and reduce them through larger samples.

When we test repeatedly, however, the two dangers are not sym-
metric. An optimistic estimate will lead us to put a classiﬁer into
use and congratulate ourselves on having annotated little data. A
pessimistic result will lead us to do more training, tuning and/or
testing. We are therefore more likely to accept overestimates than
underestimates in sequential testing, leading to an optimistic bias.
In this paper, we explore the behavior of sequential testing for
the three scenarios outlined above (ﬁxed test and variable training;
ﬁxed training and variable test; and both test and training variable).
Two estimates of effectiveness are considered: a point estimate of
the F1 score, and the lower bound of a one-sided 95% conﬁdence
interval on F1. We empirically quantify the bias introduced by
sequential testing across a range of text classiﬁcation tasks, provide
guidance for practitioners, and point to a range of open questions.

2. MATERIALS AND METHODS

We use SVMperf (V. 3.0) with a linear kernel [Joachims, 2006]
to train text classiﬁers optimized for F1, using ﬂags “-c 1000 -l 1
-w 3”. Our data set is the RCV1-v2 collection of Reuters newswire
stories [Lewis et al., 2004]. Feature vectors were constructed using
the RCV1-v2 stopped, stemmed token ﬁles (On-Line Appendix 12
of Lewis et al. [2004]). Feature values were log TF x IDF weights,
with IDF equal to the natural log of the number of documents in the
collection divided by the number of documents the word occurs in.
We deﬁned 29 binary classiﬁcation tasks corresponding to the
29 Topics categories with at least 25,000 positive examples in the
804,414-document collection. The use of high frequency cate-
gories enabled studying several orders of magnitude of variation
in sample sizes. Our effectiveness measure is F1; that is, the har-
monic mean of precision and recall. We deﬁne F1 to equal 1.0
if a classiﬁer makes no positive predictions on a data set with no
positive examples [Lewis, 1995].

Our experiments compare F1 estimates with true population F1.
We derive a highly accurate estimate of the latter from a randomly
sampled “truth set” of 700,000 of the RCV1-v2 documents. The
mean width of the two-sided conﬁdence interval on that estimate is
0.0059—minuscule compared to the variations we see in estimates
from our smaller experimental test sets.

933The test and training sets in our experiments are drawn from
these 700,000 documents and from the remaining 104,414 doc-
uments respectively. In each run, the training and/or test set was
grown in a nested fashion, by adding 20 randomly selected anno-
tated documents, retraining the classiﬁer on the training set if nec-
essary, and estimating F1 based on the test set. The 20 documents
were either all added to test (Section 3.1), all added to training (Sec-
tion 3.2), or half added to training and half to test (Section 3.3).

We compute two estimates of F1 from each test set. First, the
F1 score on the test set is taken as a point estimate, ˆF1, of the true
F1. Second, we ﬁnd the lower bound, θ, of a lower one-sided 95%
conﬁdence interval on F1. We compute this conﬁdence interval
estimate by taking the 5th percentile of 40,000 Monte Carlo draws
on beta-binomial posteriors on the classiﬁer’s true positive and false
positive rates, using the method described by Webber [2013] (see
Goutte and Gaussier [2005] for a similar binomial posterior on F1).
(Similar sequential biases would arise for other interval methods, or
other effectiveness measures such as precision or recall.)

Webber [2013] shows that this Monte Carlo procedure gives cov-
erage probabilities very close to the nominal 95% level for two-
tailed intervals on recall, in contrast to commonly used normal ap-
proximations. Nevertheless, we checked the accuracy of the method
for one-tailed intervals on F1. We calculated an observed coverage;
namely, the proportion of testing points at which the lower bound
of the conﬁdence interval is at or below the true F1 score. An edge
case occurs when the classiﬁer achieves no true positives on the
test set, either because the classiﬁer is poor or the test set has no
positive examples. In this case, the lower bound of the conﬁdence
interval is 0.0, guaranteeing 100% coverage. To avoid crediting the
algorithm for this trivial case, we computed coverage only on cases
with at least 100 training and 100 test examples. With this restric-
tion, we ﬁnd that the observed coverage of Webber’s algorithm is
very near 95% in all experiments.

As will be seen, however, applying sequential stopping rules to
nominal 95% conﬁdence intervals leads to stopping conditions with
actual coverage lower than 95% in all three scenarios studied. One
way to adjust for this might be to compute the conﬁdence interval
at a nominal level higher than 95%, but assert only a 95% level
for the resulting interval. For each protocol, we calculate what the
nominal level must be to achieve 95% coverage with our data set.

3. RESULTS

We present results for three conditions: variable test, variable

training, and variable both test and training.
3.1 Variable Test, Fixed Training

First, we consider the case where a classiﬁer has been produced
using a ﬁxed training set. A developer wishes to certify, at a given
conﬁdence level, that the classiﬁer exceeds a target value of effec-
tiveness. We assume that a testing budget of 10,000 annotations
is available, but that the developer would prefer to pay for fewer
annotations if possible. This is the standard setting in sequential
sampling theory [Wetherill and Glazebrook, 1986]. We assume that
they iteratively annotate randomly selected examples, add them to
the test data, and compute the two estimates of F1 (point estimate
and lower bound of a one-sided lower 95% conﬁdence interval).
The developer decides to accept the classiﬁer (and stop testing) if
at some point the lower conﬁdence limit exceeds the target value of
F1. They reason (erroneously) that they will accept an inadequate
classiﬁer at most 5% of the time. They reject the classiﬁer if the
budget of 10,000 annotations is used up without accepting.

Figure 1 shows the results of one such classiﬁer evaluation run.
The target value of F1 is 0.5, while the true effectiveness (unbe-

Topic = C31 ; Frequency = 5.04%

F^
1
F1
θ

1
F

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

20

50

100 200

500

2000

5000

Testing documents

Figure 1: Variable test, ﬁxed training. The dashed line is true
effectiveness (F1); the dotted line is the target F1; the solid line
is estimated F1, for increasing test set sizes; and the bold line
is the lower bound of the 95% one-sided conﬁdence interval on
F1. Results are shown for a single run for Topic C31.

knownst to the developer) is 0.42. The point estimate ˆF1 happens
to almost always be above its true value, and for many test set sizes
above the substantially higher target value. The lower bound of the
conﬁdence interval is more conservative, but wanders above the
true value at several points, and above the developer’s target for
several test sets in the range of 100 to 200 documents. The devel-
oper would have mistakenly halted testing and accepted the classi-
ﬁer upon reaching the test set of size 120. For large test sets, both
the point estimate and the lower bound of the conﬁdence interval
approach the true value of effectiveness, but our developer would
never reach these test set sizes. Figure 1 is only for a single run, and
perhaps (from a reliable evaluation perspective) an unlucky one. A
95% conﬁdence interval is expected to fail roughly 5% of the time.
We therefore performed the same experiment across all 29 topics,
and made 20 runs for each topic.
In each case the target effec-
tiveness was set inﬁnitesimally above the true effectiveness of the
classiﬁer, so that the classiﬁer should be rejected in every run; not
doing so counts as a failed evaluation. Unsurprisingly, the point es-
timate exceeded the target value at some point on almost every run.
So using the point estimate would fail almost 100% of the time.
But even the lower bound of the conﬁdence interval exceeded the
target value 31.55% of the time (far more than the nominal 5% the
developer might expect), causing the classiﬁer to be erroneously
accepted. The observed coverage of the conﬁdence intervals was
95.68%, making clear that the problem is the developer’s sequen-
tial stopping rule, not the (single-test) conﬁdence interval estimate.
While the bias introduced by overﬁtting has long been a sub-
ject of study in machine learning [Toussaint, 1974], the bias intro-
duced by sequential testing appears to have been ignored. Sequen-
tial stopping rules, and techniques for unbiasing estimates when us-
ing them, have been studied in statistics and quality control [Sieg-
mund, 1985, Wetherill and Glazebrook, 1986]. The theory is well
developed (if not simple) for cases such as the binomial propor-
tion and the normal mean. Unfortunately, F1, as the ratio of two
unknown quantities, is substantially more difﬁcult to address ana-
lytically. We can, however, observe an empirical adjustment on our
dataset. To achieve actual 95% conﬁdence, assuming that lower-
bound effectiveness is tested every 20 documents, we should set a
nominal conﬁdence level of 99.5%.

934Topic = M132 ; Frequency = 3.33%

F^
1
F1
θ

1
F

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

20

50

100 200

500

2000

5000

Training documents

Figure 2: Fixed test, variable training. True classiﬁer effective-
ness (measured by F1) as the training set is increased is shown
as a dashed green line; the estimated effectiveness, based on a
test set of 1,067, is the solid blue line; and the 95% lower-bound
conﬁdence on that effectiveness is the bold gold line.

3.2 Fixed Test, Variable Training

Next we examine a common use of sequential testing in classiﬁer
construction. The developer of the classiﬁer has a ﬁxed, randomly
selected test set. Training examples are selected, annotated, and
used to improve the classiﬁer. At regular intervals, the classiﬁer is
tested on the test set. We assume that training and testing continue
until the estimated effectiveness of the classiﬁer reaches some tar-
get value, or when a total of 15,000 documents have been labeled.
We ﬁx the test set size at 1,067 documents (a commonly used value
that gives a maximum two-sided conﬁdence interval width of ±3%
on a binomial proportion). This corresponds to training budgets be-
tween 13 and 13,933, with increments of 20 documents. However,
the choice of a particular test set size is not critical to our results.

Figure 2 shows an example of this scenario. The developer has
set a target value of 0.7 on F1. Thinking themselves conservative,
they decide to stop training when the lower conﬁdence limit on F1
exceeds 0.7. They (erroneously) reason they will accept an inad-
equate classiﬁer at most 5% of the time they use this procedure.
They reject the classiﬁer if the combined budget of 15,000 annota-
tions is used up without accepting.

The true effectiveness of the classiﬁer rises fairly smoothly with
the addition of training data, as we would expect. The two estimates
of effectiveness are, however, far more erratic. Both the point esti-
mate (at 333 training examples) and, surprisingly, the lower bound
of the conﬁdence interval (at 473 examples) exceed the target value
well before the true effectiveness does (at 660 examples).

The erratic behavior evinced in Figure 2 seems at ﬁrst surprising
given a ﬁxed test set and steady growth in true effectiveness. How-
ever, while the true effectiveness of successive classiﬁers increases
fairly steadily as the training set grows in size, their behavior on a
particular test set of modest size can ﬂuctuate greatly from classiﬁer
to classiﬁer. It is not the case that training improvements smoothly
and monotonically convert test set errors into test set successes.
Thus, even though no additional sampling is being performed, ran-
dom ﬂuctuations in estimated effectiveness occur, and we are more
likely to stop when those ﬂuctuations lead to overstating effective-
ness than when they lead to understating it.

Does Figure 2 simply show an unlucky case? We again ran 29
topics and 20 runs per topic. In each run, we set the target value for
effectiveness to be 90% of the highest true effectiveness achieved
(usually at the largest training set size) during the run. The effec-
tiveness of the classiﬁer was estimated after each addition of 20
annotated examples to the training set. We examined two stop-
ping rules: stopping when the point estimate ﬁrst hits this target,
and stopping when the lower conﬁdence interval bound ﬁrst hits
this target. For unbiased point estimates, we might expect the ﬁrst
rule to stop early 50% of the time (the point estimate is as likely
to be above as below the true effectiveness), and the latter rule to
stop early only 5% of the time. In practice, across 29 topics and
20 runs per topic, we found that the point-estimate rule stopped
early 53.58% of the time. The lower-bound stopping rule stops
early 8.13% of the time. The latter of these values is signiﬁcantly
different from the expected proportion (p < 0.01 under an exact bi-
nomial test with 29∗ 20 = 580 observations); the former is not sig-
niﬁcantly different, but the jaggedness of the estimate curve means
that there inevitably is an optimistic bias, though our number of cat-
egories and runs was too small to demonstrate this at the p = 0.05
level. The observed conﬁdence interval coverage for this scenario
is 95.26%, so again it is the stopping rules that are the problem.

This bias is smaller than in Section 3.1, but also difﬁcult to at-
tack analytically. The theory of sequential testing has focused on
estimating ﬁxed distributions, not time varying ones. Computa-
tional learning theory has found the analysis of learning curves
challenging even in the generalization framework (corresponding
to our comparatively smooth curve of true effectiveness) [Haussler
et al., 1996]—we do not know of a treatment of learning curves
for ﬁnite test sets. Lacking analytical guidance, we observe that a
nominal conﬁdence level of 97% is required to achieve an actual
conﬁdence of 95%.

3.3 Variable Test, Variable Training

Finally, we consider the situation in which both test and training
set are increased over time. Though not a widely used approach, it
has the advantage of avoiding committing to too small or too large
a testing or training set size at the outset. Many adaptive policies
are possible; we consider the simple one of allocating as many doc-
uments to testing as we do to training (for our experiments, 10 to
training and 10 to test at each increment, then performing the test).
Figure 3 shows what happens for a single run on a single topic
under this regime. We assume the developer has set a target value of
0.6 on F1, and decides to stop training and testing when the lower
conﬁdence limit exceeds this target. They reject the classiﬁer if the
combined budget of 15,000 annotations is used up ﬁrst.

Again, the increase in true effectiveness with increasing train-
ing set sizes is relatively smooth. And again, the point estimate
on effectiveness is more erratic, here from a combination of both
changing classiﬁers (as the training set is increased) and the chang-
ing content of the test set. The lower bound once more varies with
the point estimate, shrinking the gap gradually, since half of the
annotations go to testing. Both the point estimate of F1 and the
lower bound of the conﬁdence interval hit the target of 0.6 before
true effectiveness reaches it: the developer would stop too early.

We once more estimate the degree of bias by observing behavior
across all 29 topics and 20 runs. Setting the target effectiveness at
90% of maximum actual effectiveness, we ﬁnd that the point esti-
mate crosses this threshold prematurely 68.38% of the time, while
the lower bound crosses it prematurely 9.40% of the time. The
observed coverage for this scenario is 95.39%, showing again that
stopping rules are at fault. Here, a nominal conﬁdence level of 98%
is required to achieve an actual conﬁdence of 95%.

935Topic = GPOL ; Frequency = 7.07%

8
.
0

6
.
0

1
F

4
.
0

2
.
0

0
.
0

)

%

(

l
e
v
e
l

e
c
n
e
d
ﬁ
n
o
c

l
a
u
t
c
A

F^
1
F1
θ

20

50

100 200

500

2000

5000

Training documents

Figure 3: Variable test, variable training. The dashed green
line gives true effectiveness (F1) of the classiﬁer at each train-
ing set size. The solid blue line gives a point estimate on effec-
tiveness of a test set as large as the training set at each point.
The bold gold line is the lower bound of a 95% lower one-sided
conﬁdence interval estimate.

3.4 Empirical Bias

To generalize our observations on the nominal conﬁdence of
95%, we tried a range of conﬁdence levels between 50% and 99%.
Figure 4 shows the actual conﬁdence of the three scenarios studied
in this paper. It is, in all cases, lower than the nominal conﬁdence
(as shown by the dashed line), and most markedly so for the vari-
able test, ﬁxed training case. These curves can serve as an indicator
to correct the bias resulting from sequential sampling, though it is
unclear how well they generalize to other conditions.

4. CONCLUSIONS AND FUTURE WORK

We have examined the bias involved in three sequential testing
methods: ﬁxed training and variable test; ﬁxed test and variable
training; and both test and training variable. The bias in ﬁxed train-
ing and variable test is severe: a nominal 95% conﬁdence interval
provides only 68% coverage in practice, for the scenario considered
here. Variable training and ﬁxed test also introduces a signiﬁcant
bias to estimates, with a nominal 95% giving 92% coverage. The
combination of variable training and variable test has only slightly
greater bias than variable training with ﬁxed test; a nominal cover-
age interval of 95% gives actual coverage of 91%. These are not
due to problems in the Monte Carlo method of calculating inter-
vals on F1, which gives coverage very close to nominal levels for
all three conditions. The bias lies solely in the use of sequential
testing.

For the case of ﬁxed training (and ﬁxed classiﬁers in general)
and variable test a sequential sampling analysis of the F-measure
would be of great practical value, enabling valid estimates while
saving labeling costs. Both traditional analytical approaches (e.g.
Brownian motion approximations [Siegmund, 1985]) and simula-
tion techniques are worth considering. For the cases in which the
training set varies, estimation must track the moving target of the
learning curve. Analytical solutions are likely impossible, and even
simulation techniques will require confronting issues of classiﬁer
stability [Kearns and Ron, 1999].

0
0
1

0
8

0
6

0
4

0
2

0

50

Variable test, fixed training
Fixed test, variable training
Variable test, variable training

70

60
90
Nominal conﬁdence level (%)

80

100

Figure 4: Actual conﬁdence corresponding to nominal conﬁ-
dence level.

In this paper, we have provided empirical adjustments to con-
ﬁdence limits, but these apply only to the particular scenarios and
dataset explored here. For other cases, they may provide some rule-
of-thumb guidance for developers, but they cannot stand in place of
a statistically well grounded validation. For now, a trustworthy val-
idation requires testing the ﬁnal classiﬁer once on a set of annotated
instances that is used for no other purpose.

Acknowledgments
This material is based upon work supported by the National Science Foun-
dation under Grant No. 1065250. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of the authors and
do not necessarily reﬂect the views of the National Science Foundation.

References
Cyril Goutte and Eric Gaussier. A probabilistic interpretation of precision,
recall and F-score, with implication for evaluation. In ECIR ’05, pages
345–359, March 2005.

David Haussler, Michael Kearns, H Sebastian Seung, and Naftali Tishby.
Rigorous learning curve bounds from statistical mechanics. Machine
Learning, 25(2):195–236, 1996.

Thorsten Joachims. Training linear SVMs in linear time. In ACM KDD ’06,

pages 217–226, 2006. ISBN 1-59593-339-5.

Michael Kearns and Dana Ron. Algorithmic stability and sanity-check
bounds for leave-one-out cross-validation. Neural Computation, 11(6):
1427–1453, 1999.

David D Lewis. Evaluating and optimizing autonomous text classiﬁcation

systems. In ACM SIGIR ’95, pages 246–254, 1995.

David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. RCV1: A new
benchmark collection for text categorization research. Journal of Ma-
chine Learning Research, 5:361–397, December 2004.

David Siegmund. Sequential Analysis: Tests and Conﬁdence Intervals.

Springer-Verlag, 1985.

Godfried T. Toussaint. Bibliography on estimation of misclassiﬁcation.
IEEE Transactions on Information Theory, IT-20(4):472–479, July 1974.

William Webber. Approximate recall conﬁdence intervals. ACM TOIS, 31

(1):2:1–2:33, January 2013.

G. Barrie Wetherill and Kevin D. Glazebrook. Sequential Methods in Statis-

tics. Chapman and Hall, 3rd edition, 1986.

936