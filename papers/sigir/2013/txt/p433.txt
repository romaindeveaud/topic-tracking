A Low Rank Structural Large Margin Method

for Cross-Modal Ranking

Xinyan Lu

Fei Wu

Siliang Tang

College of Computer Science

College of Computer Science

College of Computer Science

Zhejiang University, China
xinyanlu@zju.edu.cn

Zhejiang University, China
wufei@cs.zju.edu.cn

Zhejiang University, China

siliang@zju.edu.cn

Zhongfei Zhang

Xiaofei He

Yueting Zhuang

Department of Information

College of Computer Science

College of Computer Science

Science & Electronic

Engineering

Zhejiang University, China
zhongfei@zju.edu.cn

Zhejiang University, China

xiaofeihe@cad.zju.edu.cn

Zhejiang University, China
yzhuang@zju.edu.cn

ABSTRACT
Cross-modal retrieval is a classic research topic in multime-
dia information retrieval. The traditional approaches study
the problem as a pairwise similarity function problem. In
this paper, we consider this problem from a new perspec-
tive as a listwise ranking problem and propose a general
cross-modal ranking algorithm to optimize the listwise rank-
ing loss with a low rank embedding, which we call Latent
Semantic Cross-Modal Ranking (LSCMR). The latent low-
rank embedding space is discriminatively learned by struc-
tural large margin learning to optimize for certain ranking
criteria directly. We evaluate LSCMR on the Wikipedia and
NUS-WIDE dataset. Experimental results show that this
method obtains signiﬁcant improvements over the state-of-
the-art methods.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models

General Terms
Algorithms, Theory, Experimentation

Keywords
Cross-Modal Retrieval; Ranking; Low Rank Embedding

1.

INTRODUCTION

Nowadays many real-world applications involve multi-modal

data. The ranking of cross-modal retrieval is imperative to
many applications of the practical interest, such as ﬁnding

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

relevant textual documents of a tourist spot that best match
a given image of the spot or ﬁnding a set of images that vi-
sually best illustrate a given text description. Therefore, it
is desirable to support the ranking of multi-modal data, e.g.,
identifying the most relevant textual documents in response
to a query image or vice versa. It is obvious that a good
performance of ranking multi-modal data hinges upon an
appropriate learning of the similarity across diﬀerent modal-
ities. The heterogeneity-gap between multi-modal data has
been widely understood as a fundamental barrier for the
cross-modal metric learning.

In recent years, there has been a great deal of research de-
voted to the development of algorithms for learning an op-
timal direct similarity metric between diﬀerent modalities.
Among existing research of cross-modal metric learning, one
kind of popular approaches is to map the data with multiple
modalities into a common (or shared) space such that the
distance between two similar objects is minimized, while the
distance between two dissimilar objects is maximized. These
approaches (e.g., Canonical Correlation Analysis (CCA) [12]
and [33]) usually assume a training set of strictly paired data
and exploit the symbiosis of multiple-modality data which
is common to describe the rich literal and visual semantics,
such as a web image with loosely related narrative text de-
scriptions, and a news report with collateral text and images.
Another kind of approaches for multi-modal metric learning
is the extensions of Latent Dirichlet Allocation (LDA) [5]
which are conducted on two or more collections of multiple-
modality data in an unsupervised manner. The LDA-based
approaches tend to model correlations among multi-modal
documents at a latent semantic (topic) level across diﬀerent
modalities, e.g. correspondence LDA [4].

We are particularly interested in ranking the multi-modal
data (i.e. cross-modal ranking) in this paper. Diﬀerent from
the two aforementioned categories of approaches which do
not maximize a criterion related to the ﬁnal ranking perfor-
mance, recent years have witnessed the eﬀorts in learning
an optimal similarity (ranking) function between diﬀerent
modalities by using learning to rank techniques. These ap-
proaches (e.g. [11, 23, 2]) are supervised but do not enforce
a strict assumption that the trained multi-modal data must
be paired (e.g., one image is in pair-correspondence with its

433collateral text). They in fact need some lists of ranked data
related to the queries for training where the training exam-
ples can be easily obtained from the abundance of users’
clickthrough data with little overhead [18, 10]. In this way,
the learned metric for ranking multi-modal data is generally
optimized for a ranking-based loss function (evaluation cri-
terion) to preserve the orders of the relevance instead of the
purely absolute values of similarity (dis-similarity) between
multi-modal data.

A discriminative kernel-based method PAMIR proposed
in [11] solves the problem of cross-modal ranking by adapt-
ing the Passive-Aggressive algorithm [8]. However, PAMIR
is inherently a pairwise cross-modal ranking approach; its
ranking performance is limited by the distribution of the
pairs of items and skewed data may even deteriorates the
ranking result. These are also the same problems for another
pairwise approach SSI [2] proposed by Bai et al. Moreover
in the real world, a long search query (e.g., a whole docu-
ment) is beneﬁcial because users’ intents can be described in
more detail [31]. Therefore, it is appropriate to support long
textual documents as queries in cross-modal ranking. It is
obvious that PAMIR cannot easily expedite the long queries
and restrains itself from a more ﬂexible application of cross-
modal ranking in the setting of uncontrolled multi-modal
data involved.

This paper aims to bridge the gap between learning a la-
tent space and ranking for multi-modal data. We consider
the problem of cross-modal retrieval from a new perspec-
tive as a listwise ranking problem in this paper, and pro-
pose a general cross-modal ranking algorithm to optimize
the listwise ranking loss meanwhile considering a low rank
embedding, called Latent Semantic Cross-Modal Ranking
(LSCMR). LSCMR employs the structural SVM [29] to sup-
port the optimizations of various ranking evaluation mea-
sures (e.g, MAP [32] and NDCG [7]) under a uniﬁed algo-
rithmic framework. LSCMR also incorporates a low rank
embedding in the learning procedure in which the latent as-
pect space is induced to address the curse of dimensionality
and discover the correlations between diﬀerent modalities.

It is worthwhile to highlight the main motivations of the
proposed method. We would like the method to beneﬁt both
from the low rank embedding and the most recent advances
in learning to rank techniques:

• Similar to the ideas of the latent model SSI [2] which
discovers the “latent concept” from high-dimensional
space, LSCMR employs the low rank embedding to
eliminate textual/visual synonymy and polysemy of
multi-modal data and discovers the correlations be-
tween diﬀerent modalities. This is particularly appro-
priate to the settings where the distributions of query
data and the retrieved data are intrinsically diﬀerent
due to the aforementioned multi-modal heterogeneity-
gap. Furthermore, from a ﬂexible retrieval point of
view, LSCMR naturally supports long queries to dis-
cern users’ search intentions.

• By the introduction of structural large margin learning
into the optimization of a cross-modal ranking function
in a listwise manner, LSCMR explicitly minimizes the
ranking loss of a whole permutation listwise, not indi-
viduals or pairs of items. The ranking function by the
maximum margin is not biased toward the queries with

more data pairs, leading to a strong generalization due
to its empirical risk minimization.

We show experimental results on the ranking of cross-
modal data obtained from two real-world datasets. The
proposed LSCMR outperforms other cross-modal ranking
approaches. LSCMR is particularly appropriate for cross-
modal ranking due to its structural large margin and low-
rank listwise ranking pursuing.

The rest of this paper is organized as follows. In Section
2, we describe the method in detail and show its feasibility.
Section 3 discusses the existing work. We compare the pro-
posed LSCMR with other cross-modal ranking approaches
on two real-world datasets in Section 4. Conclusions are
given ﬁnally.

2. THE ALGORITHM OF LSCMR

The proposed method LSCMR is a general cross-modal
retrieval framework in the sense that it can be applied in
both directions of image-query-text retrieval and text-query-
image retrieval. Consequently, a query here may be either an
image or a text document. Similarly, a retrieved document
can either be an image or a text document. For a clear
articulation in the rest of this section, the algorithm is only
derived in the case of text-query-image retrieval. We report
the experiments in both scenarios.

2.1 Notation

All vectors are assumed to be column vectors and a super-
script T denotes the transpose of a matrix or vector. Denote
the text query as q ∈ Rm and the retrieved image as d ∈ Rn,
where m is the dimension of the text space (e.g., vocabu-
lary size of bag-of-words (BoW)) and n is the dimension of
the image space (e.g., vocabulary size of bag-of-visual-words
(BoVW) which is quantized by clustering from low-level vi-
sual features such as SIFT [22]). We are given a training
set of N samples, in which each contains a text query qi
(i = 1, . . . , N ) as well as a set of corresponding retrieved
images di with their true rankings y∗
i ∈ Y, where Y denotes
the set of all possible permutations (rankings). We formu-
late a ranking as a matrix of pair orderings as [32] does,
Y ⊂ {−1, 0, +1}|d|×|d| where the operator | · | denotes the
number of elements in a set. For any y ∈ Y, yij = +1 if
document di is ranked ahead of document dj , and yij = −1
if dj is ranked ahead di, and yij = 0 if di and dj have
equal rank. We consider only matrices with correspond to
valid rankings (i.e., obeying antisymmetry and transitivity).
Moreover, assume that the true ranking is a weak ranking
with two rank values (relevant and irrelevant). For any tex-
tual query qi, let d+
i denote the set of relevant and
irrelevant images in di, respectively. For simplicity, we omit
the subscript i of qi and di when it is clear from the context.

i and d−

2.2 The Low-Rank Learning Function

We consider that learning scores for ranking from a super-
vised manner, in which the ranking of images corresponding
to a given textual query is available for training. Unlike
the uni-modal data ranking, cross-modal ranking attempts
to learn a similarity function f (q, d) between a text query q
and an image d according to a pre-deﬁned ranking loss. The
learned function f maps each text-image pair to a ranking
score based on their semantic relevance.

434Query

Ranked list

Latent space

U

q

=

V

d

=

Latent space

Training Set

The learned  latent space as well as U and V

Ranked list results for queries in the latent space

Figure 1: The algorithmic illustration of the proposed LSCMR. Shapes represent semantics (e.g., classes
or categories); the same shape indicates the same (or quite relevant) semantic content. Colors represents
modalities (i.e., images and text documents); the same color indicates that the data objects come from the
same modality space. For simplicity in this illustration, we assume the two modalities to be text queries
and image targets. Ideally, given a text query, we would like those images having the same shape (semantic
content) with the query are ranked before those having diﬀerent shapes in the ranking list. LSCMR tends
to learn a low rank ranking function from the training set (seen queries and associated ranking lists) in a
supervised manner. All the seen m-dimensional queries q and n-dimensional target documents d are mapped
to a k-dimensional latent space by U and V respectively, in which those data objects with the same shape are
grouped to minimize certain listwise ranking loss (e.g., MAP) directly. For unseen queries and documents,
they are ﬁrst mapped to the learned latent space by U and V , respectively, and then the scoring is conducted
in the latent space to produce ranking results. (Figure best viewed in color)

Given a text query q ∈ Rm and an image d ∈ Rn , we tend
to learn a linear scoring function to measure the relevance
of d given q :

f (q, d) = qT W d =

m

n

X

X

i=1

j=1

qiWijdj

(1)

where f (q, d) is the score (or similarity) between the query q
and the image d, and the only parameter W ∈ Rm×n in the
linear model captures the correspondence of the two diﬀer-
ent modalities of data as a weighting matrix: Wij weights
the correlation between the ith dimension of the text space
and the jth dimension of the image space. Note that both
positive values and negative values in Wij are allowed in
which a negative value represents a negative correlation.

Motivated by the idea of low rank embedding, a low rank
prior is introduced into matrix W with W = U T V in equa-
tion (1), which results in a low rank ranking model [2]:

f (q, d) = qT U T V d = (U q)T V d

(2)
where U ∈ Rk×m and V ∈ Rk×n. U refers to map the
query text q from the m-dimensional text space to the k-
dimensional latent space by a liner mapping, and V refers
to map the retrieved image d from the n-dimensional image
space to the k-dimensional latent space. Therefore, the text
query and the retrieved image are mapped to a common
k-dimensional latent aspect space, and then their similarity
is measured by a dot product of the two vectors in the k-
dimensional space, which is commonly used to measure the
matching between textual vectors [1].

Intuitively, the low rank model in equation (2) helps us
deal with the problem of textual/visual synonymy and poly-
semy which particularly occur in cross-modal retrieval. Note
that Latent Semantic Indexing (LSI) [9] takes into account of
the correlations between textual words (synonym and poly-

semy) in a single modality in an unsupervised manner, while
the low rank model in equation (2) attempts to capture the
correlation across two diﬀerent modalities from a supervised
manner. By constraining the form of W with a low rank
form, the beneﬁts are similar to LSI: U and V not only in-
duce a k-dimensional latent aspect space but are also faster
to compute and lead to much smaller storage requirements
by representing the image documents in k dimensions in-
stead of the original n dimensions (k is chosen much smaller
than m or n). Besides, from the viewpoint of statistical
learning theory, fewer parameters (k(m + n) ≪ mn) lead to
a better stability and generalization in performance.
Similar to [2], here U and V are diﬀerent and there is
no assumption that the query texts and the target images
should be embedded to the latent space in the same way.
This is appealing to cross-modal ranking since the distribu-
tions of the query texts and the target images are inherently
diﬀerent due to the heterogeneity-gap.

We can obtain a prediction y for each input query q and its
corresponding ranked target images d by sorting the f (q, d)
in a descending order. The rest is to learn U and V . We
aim to obtain the values of U and V by the minimization of
the following empirical risk,

R∆(f ) =

1
N

N

X

i=1

∆(y∗

i , yi) ,

(3)

where the non-negative loss function ∆ : Y ×Y → R quanti-
ﬁes the penalty for making prediction yi if the correct output
is y∗
i , which is typically bounded in [0, 1]. For example, we
deﬁne the loss function ∆ with the average precision (AP,
detailed deﬁnition in Equation (14)) loss as follows:

∆ap(y∗, y) = 1 − AP(rank(y∗), rank(y))

435and then to minimize the empirical risk is to maximize the
Mean Average Precision (MAP). Note that one can con-
struct diﬀerent ranking objective problems by considering
diﬀerent ranking loss function ∆.

2.3 The Formulation of LSCMR

In this section, we present the formulation of LSCMR in
details. The proposed LSCMR is inspired by the structural
SVM framework [29], especially SVMmap [32] for optimiz-
ing the average precision. The algorithmic illustration of
LSCMR is presented in Figure 1.

The motivation of LSCMR is to learn a cross-modal rank-
ing function h : X → Y between an input space X (a text
query q as well as all possible target images) and output
space Y (rankings over the image set). Similar to SVM, we
can derive a prediction by ﬁnding the ranking y that maxi-
mizes the following discriminant function h:

h(q, d) = argmaxy∈Y F (q, d, y)

(4)

where F is considered as a compatibility function param-
eterized by U, V that measures how compatible the triple
(q, d, y) are.

By adapting the most commonly used partial order com-
bined feature representation in [19] to the cross-modal rank-
ing, we deﬁne F as:

F (q, d, y) = X
i∈d+

X

yij

j∈d−

(U q)T V (di − dj )

|d+| · |d−|

(5)

where yij = +1 if image di is more preferred (more relevant
to query q) than image dj , and yij = −1 otherwise since we
assume that the predicted rankings are complete.
One attractive property of F is that for the ﬁxed U and
V , the ranking y which maximizes function F (then the
predicted ranking) is simply sorted by descending f (q, d) =
(U q)T V d. To see this, we note that F is a summation over
the diﬀerences of all relevant/irrelevant document pairs since
we assume weak rankings with two rank values. Since F de-
composes linearly over the pairwise representation, we can
maximize F by optimizing each yij individually: if (U q)T V di
> (U q)T V dj, yij is set to be 1, and yij = −1 otherwise. This
is the same procedure as sorting documents by descending
f (q, d). More details can be obtained from [19]. We note
that this simple prediction rule establishes a connection be-
tween the compatibility function F and the aforementioned
low rank ranking model.

Since U and V are independent to the summation in equa-

tion (5), we rewrite F as a linear function of U T V :

F (q, d, y) =< U T V, Ψ(q, d, y) >

where

Ψ(q, d, y) = q X
i∈d+

X

yij

j∈d−

j

dT
i − dT
|d+| · |d−|

.

(6)

(7)

Here the combined feature function Ψ(q, d, y) is a summa-
tion over the vector diﬀerences of all the relevant/irrelevant
image pairs. By representing the scoring F as a Frobenius
inner product of U T V and Ψ, we see that it is straightfor-
ward to extend the idea of the structural SVM to learn the
cross-modal ranking function F .

For the purpose of learning to rank, the structural SVM
takes a set of vector-valued features which characterize the
relationship between the input query and a set of target

documents as the input, and returns a ranking list y ∈ Y
of the target documents. The structural SVM is applied to
maximize the margins between the true ranking list y∗ and
all the other possible lists y. In this paper, LSCMR takes
cross-modal ranking into consideration, for i = 1, . . . , N :

∀y ∈ Y : δF (qi, di, y) ≥ ∆(y∗

i , y) − ξi

(8)

where for compactness, we deﬁne

δF (qi, di, y) = F (qi, di, y∗

i ) − F (qi, di, y) .

Since we assume that the query texts and the target im-
ages are embedded into a common latent space, respectively,
LSCMR adapts the original structural SVM to learn the op-
timal U ∗ and V ∗ which maximize the margins between the
true ranking and all the other possible rankings of the target
images for each text query. Hence, we replace the standard
quadratic regularization λ
F where
k·kF denotes the Frobenius norm. Intuitively, this extension
simpliﬁes the model complexity, thereby promoting a better
generalization performance.

2 kV k2

2 kUk2

2 kwk2

2 with λ

F + λ

The optimization problem is then presented as follows:

Optimization Problem 1.

min
U,V,ξ

s.t.

N

X

i=1

F +

F +

1
N

λ
2 kV k2

λ
2kUk2
∀i ∈ {1, . . . , N}, ∀y ∈ Y :
δF (qi, di, y) ≥ ∆(y∗

ξi

(9)

i , y) − ξi .

(10)

For each triple (qi, di, yi) in the training set, a set of con-
straints (10) are added to the optimization problem. To see
how these constraints indeed work, note that during the pre-
diction the model chooses the ranking ¯yi which maximizes
F (qi, di, y) given the ﬁxed U and V . If the predicted ranking
is an incorrect ranking ¯y, i.e., F (qi, di, ¯yi) > F (qi, di, y∗
i )
where y∗
i is the true ranking, the corresponding slack vari-
able ξi must be at least ∆(y∗
i , ¯yi) to satisfy the constraint.
Considering all the triples (qi, di, yi), i = 1, . . . , N , the sum
of slacks (i.e., 1
i=1 ξi) upper bounds the empirical risk
R∆(f ) deﬁned in Equation (3). This is stated formally in
Proposition 1.

N PN

Proposition 1. Denote by ξ∗(U, V ) the optimal solution
of the slack variables in Optimization Problem 1 for the given
parameters U and V . Then 1
is an upper bound
on the empirical risk R∆(f ).

N PN

i=1 ξ∗

i

Similar to SVM, to avoid overﬁtting, the objective func-
tion (9) to be minimized is a tradeoﬀ between the model
complexity, and a hinge loss relaxation of ∆ loss. A pre-
chosen value of parameter λ controls this tradeoﬀ and can
be tuned to achieve a good performance via the cross vali-
dation procedure.

Note that by exploring the low rank property, the opti-
mization problem is not convex. The well-known kernel trick
is diﬃcult to be applied to (9), while kernel trick is consid-
ered as one of the main beneﬁts of the traditional support
vector machine. Fortunately, a linear-SVM without using
kernels has been shown to give competitive performances for
textual documents classiﬁcation [13]. On the other hand, ac-
cording to the cross-modal retrieval approach PAMIR [11],
a linear mapping of BoVW yeilds the highest performance
of the other kernel mapping methods. As a result, with

436the multi-modal data under a certain feature representa-
tion, we argue that the model can indeed capture the linear
structures of the multi-modal data to learn a cross-media
semantic representation.

2.4 Algorithm and Implementation

Since |Y| is super-exponential in the size of the train-
ing set, our algorithm for learning U and V is adapted
from the 1-slack margin-rescaling cutting-plane algorithm
of Joachims et al [20]. The algorithm alternates between
two steps, one optimizing the model parameters (U and V
in our case) and the other updating the constraints set with
a new batch of rankings (ˆy1, . . . , ˆyN ) (ˆyi is one ranking for
one query sample, i = 1, . . . , N ) which most violate the cur-
rent constraints. Once reaching a stopping criterion based
on the accuracy of the empirical risk (the new constraint
batch’s empirical risk is no more than that of the current
set of constraints within a tolerance ǫ > 0), the algorithm
terminates.

The general optimization procedure of LSCMR is listed
in Algorithm 1. The code is implemented in MATLAB. The
proof of the correctness can be easily extended from [20].

Algorithm 1 Latent Semantic Cross-Modal Ranking
(LSCMR).
Input: ranking examples (qi, di, y∗

i ), i = 1, . . . , N , trade-oﬀ
control parameter λ > 0, accuracy tolerance threshold
ǫ > 0

0

Output: mapping parameters U and V , slack variable ξ ≥
1: W ← ∅
2: repeat
3:

Solve for the optimal U , V and slack ξ:

min
U,V,ξ

s.t.

F +

F + ξ

λ
2 kV k2

λ
2 kUk2
∀(y1, . . . , yN ) ∈ W :
1
N

δF (qi, di, yi) ≥

N

X

i=1

1
N

N

X

i=1

∆(y∗

i , yi) − ξ

4:
5:

for i = 1 to N do
ˆyi ← argmax

y∈Y

∆(y∗

i , y) + F (qi, di, yi)

6:
7:
8: until

end for
W ← W ∪ (ˆy1, . . . , ˆyN )

The other key issue is how to solve the optimization prob-
lem in Step 3. Since the problem is not a convex problem,
the parameters U and V are initialized with their previ-
ous (local) optimal values while in the beginning they are
randomly initialized using a normal distribution with mean
zero and standard deviation one. We have implemented a
subgradient descent solver adapted from Pegasos algorithm
[27] originally proposed for solving a traditional support vec-
tor machine. The Pegasos algorithm is a simple iterative
algorithm which alternates between stochastic subgradient
descent and projection steps, and is shown to be eﬀective
to solve the primal problem of SVM. In the problem, the
subgradient descent is performed by iteratively picking the
most violated ranking tuple (ˆy1, ˆy2, . . . , ˆyN ) from the set W
to minimize the slack variable.

On iteration t, the update for U is given by:

Ut+ 1

2 ← (1 − ηtλ)Ut +

ηt
N

N

X

i=1

Vt(δΨ(qi, di, ˆyi))T

(11)

where ηt is the learning rate on iteration t which is adjustable
and δΨ(qi, di, ˆyi) , Ψ(qi, di, y∗
i ) − Ψ(qi, di, ˆyi). Ut+1 is
obtained by projecting Ut+ 1
onto the set for acceleration
(see [27]):

2

B = {U : kUkF ≤ 1/√λ}

(12)

The update for V can be derived similarly except for the
most violated ranking tuple which is computed using the
updated Ut+1. The update is calculated exactly as given
by:

Vt+ 1

2 ← (1 − ηtλ)Vt +

ηt
N

N

X

i=1

Ut+1δΨ(qi, di, ˆyi)

(13)

followed by the projection step (12).

2 (kUk2

F + kV k2

Moreover, our problem is a bit diﬀerent from Pegasos [27]:
the objective function is penalized by λ
F ) to
control the model perplexity. It should be noted that the
optimal U and V must satisfy the condition kUkF = kV kF
since the prediction rule uses the product U T V only. Thus
after each subgradient descent, the updated U and V are
forced to be multiplied with a constant respectively to en-
sure kUkF = kV kF while keeping kU T V kF ﬁxed. Let α =
pkUkF kV kF ,

U ← αU/kUkF
V ← αV /kV kF

1
N

N

X

i=1

∆(ˆy∗

i , ˆyi) −

1
N

N

X

i=1

δF (qi, di, ˆyi) ≤ ξ + ǫ

The experiments show that this strategy yields a much faster
convergence rate. For ﬁxing tolerance ǫ = 0.01, the loop in
Algorithm 1 usually terminates within 200 iterations.

9: return U, V, ξ;

To solve the optimization problem in Algorithm 1, there
are two key issues to be resolved. One is searching for the
most violated constraints, the so-called separation oracle,
in Step 5. For diﬀerent loss functions ∆(y∗, y), diﬀerent
methods are proposed to address this issue, for example,
[19] for AUC loss (deﬁned as 1 − AUC(y∗, y)) and [32] for
MAP loss. Recalling that F is the Frobenius inner product
of U T V and Ψ, their work [19, 32] can be easily applied to
this algorithm with a minor modiﬁcation in implementation
to reduce the computational complexity.

3. PRIOR WORK

There has been a great deal of research devoted to the de-
velopment of algorithms for learning the similarity between
the data with diﬀerent modalities in order to perform cross-
modal retrieval. Most of cross-modal metric learning ap-
proaches tend to project multi-modal data into a common
(or shared) subspace so that the correlation between multi-
modal data is preserved or maximized. As one of the most
popular approaches to ﬁnding a pair of linear transforma-
tions to maximize the correlations between two variables,
Canonical Correlation Analysis (CCA) [15] and its exten-
sions are applied in cross-modal similarity learning. For

437example, after the maximally correlated subspaces of text
and image features are obtained by CCA, logistic regression
is employed to cross-modal retrieval in [26]. As a super-
vised kernelizable extension of CCA, Generalized Multiview
Analysis [28] is conducted to map data in diﬀerent modality
spaces to a single (non)linear subspace. Motivated by the
fact that dictionary learning (DL) methods have the intrinsic
power of capturing the heterogeneous features by generat-
ing diﬀerent dictionaries for multi-modal data, multi-modal
dictionary learning is recently applied to cross-modal metric
learning [16, 24]. Following the the seminal work of Blei et
al. [5], LDA has been extended to learn the joint distribution
of multi-modal data (e.g., text and imagery) such as corre-
spondence LDA [4], topic-regression multi-modal LDA [25],
Multi-modal Document Random Field [17] and hierarchical
Dirichlet process (HDP)-based LDA [30].

The aforementioned approaches, either optimizing the sim-
ilarity (distance) between pairs of samples or optimizing the
likelihood of the topic models, do not optimize for the ﬁnal
ranking performance directly. While bearing a resemblance
to multi-modal metric learning which aims at learning the
similarity or the distance measure from multi-modal data,
the multi-modal ranking function is generally optimized by
an evaluation criterion or a loss function deﬁned over the
permutation space induced by the scoring function over the
target documents.

Traditionally, algorithms of learning to rank can be cat-
egorized into the pointwise approaches, the pairwise ap-
proaches, and the listwise approaches. The main diﬀerences
among these three categories of approaches actually lie in
the input representations and the loss functions employed
in training. It is observed that the listwise and pairwise ap-
proaches usually outperform the pointwise approaches [21].
In [18] Joachims et al. trained a Ranking Support Vec-
tor Machine (RankSVM) to learn the weights of the hand-
designed features in which the training set was a set of doc-
uments preference pairs obtained through the clickthrough
data from the query-log of a search engine. The retrieval
function is automatically learned by taking a support vector
machine. The goal of RankSVM is to minimize the aver-
age number of the inversions in ranking; thus the method is
considered as a pairwise preference satisfaction approach.

Unlike the pairwise approaches, Cao et al. [6] ﬁrst noticed
the fact that ranking was a prediction task on a list of docu-
ments and took the ranking lists as training instances. They
trained two probabilistic models, respectively referred to as
permutation probability and top k probability, to deﬁne a
listwise loss function for learning.

Yue et al. [32] proposed another listwise approach SVMmap
to solve the problem of learning to rank in a discriminative
way. The method uses the structural SVM framework [29]
with the loss function deﬁned as MAP (mean average preci-
sion) loss that globally optimizes a hinge-loss relaxation of
MAP loss. This method simpliﬁes the process of obtaining
ranking functions with a high MAP performance by avoiding
the additional intermediate steps and heuristics.

Chakrabarti et al.

[7] proposed almost-linear-time algo-
rithms to optimize MRR (mean reciprocal rank) and NDCG
(normalized discounted cumulative gain). Further, they folded
multiple ranking loss functions into a multi-criteria max-
margin optimization problem to develop a single, robust
ranking model with close to the best accuracy of the learners
trained on individual criterions.

Diﬀerent from the aforementioned uni -modal learning to
rank techniques, to the best of our knowledge, Passive Ag-
gressive Model for Image Retrieval (PAMIR) is the ﬁrst at-
tempt to address the problem of ranking images by text
query directly [11]. PAMIR formulates the cross-modal re-
trieval problem similar to RankSVM and derives an eﬃcient
training procedure by adapting the Passive-Aggressive algo-
rithm.

The authors of [23] studied metric learning as a prob-
lem of learning to rank. They presented a general metric
learning algorithm based on the structural SVM, to learn a
metric such that the ranking of data induced by the distance
from a query can be optimized again various ranking mea-
sures. Diﬀerent from LSCMR, they focused on learning a
intra-modality metric which restricts a positive semi-deﬁne
matrix W (see Eq. (1)) to learn a valid metric and does not
introduce a low rank embedding.

The text and imagery are usually represented as BoW and
BoVW in a high-dimensional vector space. However, the
high-dimensional vector space representation suﬀers from its
inability to cope with two classic problems, i.e., synonymy
and polysemy. To capture the latent semantic associations
of data and to address these problems, embedding words in
a low-dimensional latent space to capture the semantics is
a classic approach in text retrieval such as Latent Seman-
tic Indexing (LSI) [9] and pLSA [14]. The idea of low rank
embedding is introduced into Supervised Semantic Indexing
(SSI) for cross-lingual retrieval [2]. SSI deﬁnes a set of lin-
ear low rank models to take account of correlations between
words (synonymy and polysemy). Related to SSI, Polyno-
mial Semantic Indexing (PSI) [3] generalizes and extends the
SSI approach to general polynomial models which could be
used to capture the higher order relationships among words.

4. EXPERIMENTS AND RESULTS

The main goal of the experiments is to evaluate the ef-
fectiveness of the proposed LSCMR approach. To show its
competitive performance, LSCMR is compared with other
three state-of-the-art approaches (CCA, PAMIR and SSI)
for cross-modal ranking.

These comparative methods are elaborately chosen for
the fair comparisons. Comparing with the classical CCA
method aims to test LSCMR’s ability to learn a useful la-
tent space; PAMIR has been shown to outperform pLSA
and SVM [11]; however it does not consider a latent space;
SSI introduces the low rank parameterizations while it min-
imizes a pairwise ranking loss and lacks of parameter reg-
ularization. Since LSCMR can generate low rank matrices
U and V (see Eq. (2)), in the experiments, we demonstrate
that the learned model also discovers the latent correlations
between textual words and topics.

4.1 Experimental Setup

Two public real-world datasets are used in the compara-
tive experiments. They are the largest available multimodal
datasets that are fully paired and labeled (tagged), to the
best of our knowledge. Both datasets are bi-modal with the
image and the associated text modalities. The statistics of
the two datasets are summarized in Table 1.

The ﬁrst dataset, Wikipedia feature articles1, consists of
2,866 images, each with a short paragraph describing the

1http://www.svcl.ucsd.edu/projects/crossmodal/

438Table 1: The statistics of the datasets used.

Wikipedia

NUS-WIDE

BoVW vocabulary size
BoW vocabulary size

Avg. # of words/image

1000
5000
117.5

500
1000
7.73

Documents
Partitiona
Queries
Partitiona

1,500/500

2,664/23,977

866

106,567

1,500/500

2,664/2,000

2,000
a Partitions are ordered by training/validation/test.

866

image. The images are labeled with exactly one of the 10
diﬀerent semantic classes, such as art and geography.
In
the originally provided dataset, the text comes with a 10 di-
mensional feature vector representing the probabilistic dis-
tributions over the 10 topics, which are derived from a La-
tent Dirichlet Allocation (LDA) model [5]. We note that
LSCMR and the comparative methods all resort to the raw
low-level features rather than the high-level semantic fea-
tures. For the training text, we extract 5,000-dimensional
feature vectors using the bag of words (BoW) representation
with the TF-IDF weighting scheme. For images, we ﬁrst ex-
tract SIFT points from each images in the dataset. The
randomly selected SIFT points are clustered by k-means to
generate 1,000 centers as the visual dictionary. Then each
image is quantized into a 1,000 dimensional histogram fea-
ture vector using the bag-of-visual-words (BoVW) model.

The second dataset, NUS-WIDE2, contains 133,208 im-
ages with 1,000 tags and 81 concepts, which are pruned from
the NUS dataset by keeping the images that have at least
one tag and one concept. For the feature representation, we
use the publicly available 1,000 dimensional text feature vec-
tor (namely tags) and 500 dimensional image feature vector
based on SIFT BoVW kindly provided by the authors.

Another reason why we choose these two datasets is due
to the large diﬀerence on the average number of the tex-
tual words per image and the dimensionality of the text
space.
In Wiki dataset the textual descriptions are based
on Wikipedia surrounding paragraphs which yield a 5,000
dimension text space and in average there are 117.5 sur-
rounding words per image. The NUS dataset, on the other
hand, is based on Flickr user-provided tags which yield a
1,000 dimension text space and in average there are 7.73
words (tags) per image. A manual examination reveals that
the synonymy and polysemy problem may occur more fre-
quently in the Wiki dataset than in the NUS dataset. For
their diﬀerence, ﬁrst we want to examine our algorithm’s
ability to learn a latent space for the Wiki dataset and sec-
ond we want to see whether our algorithm decays rapidly
with the NUS dataset.

Note that the two datasets are both presented by pairs of
text and imagery where CCA can be trained by this setting.
For the other three methods (PAMIR, SSI and LSCMR), the
restriction of paired correspondence of a text document and
an image is not needed. On the contrary, the queries and
the corresponding ranking lists are needed as training exam-
ples of the three methods. These training examples originate
from both direction of the text-query-image retrieval and the
image-query-text retrieval. For this purpose, we ﬁrst deﬁne
the relevance assessment. For the Wiki dataset, we deﬁne

2http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

that a target document d is relevant to a query q if d and
q belong to the same semantic class. Similarly, for the NUS
dataset, a target document d is relevant if it shares at least
one concept with query q. The ranking examples are gen-
erated as follows: for each text (image) query, we randomly
selected 40 images (text documents) in the other modal-
ity in the training set as candidates and then the selected
target documents are automatically labeled as relevant or
irrelevant to form a ranking example.

For all the 2,866 generated ranking examples in the Wiki
dataset, we randomly sample 1,500 examples to form the
training set, of which 500 examples form the validation set.
The rest are used to form the testing set. For NUS, 2,664
ranking examples are randomly selected to be the training
samples and 2,000 to be validation samples (see Table 1).
To be fair, all the comparative approaches are trained and
tested on the same training set and testing set respectively.
For both datasets, performance evaluations are conducted
using standard information retrieval metrics. We use Mean
Average Precision (MAP) as the performance measures. Let
p∗ = rank(y∗) (true ranking with two rank value +1 and
−1) and p = rank(y) (predicted ranking with a total order).
Given a query and a set of R retrieved target documents,
the Average Precision (AP) is deﬁned as

AP(p∗, p) =

1
L

R

X

j=1

P rec(j) · Rel(j)

(14)

where L is the number of the relevant documents in the
retrieved set, P rec(j) is the percentage of the relevant doc-
uments in the top j documents in predicted ranking p and
Rel(j) is an indicator function equaling 1 if the item at rank
j in predicted ranking p is a relevant document, zero other-
wise. We then average the AP values from all the queries
in the query set to obtain the MAP score. The larger the
MAP, the better the performance.
In the experiments, R
is the number of the retrieved documents to be examined,
where we set R = 50 or R = all for all the retrieved docu-
ments. Recalling that our model can be optimized for var-
ious ranking measures, we implement the greedy algorithm
for optimizing the average precision proposed in [32].

We report the performance results on both directions of
ranking images from text queries (text-query-image) and
ranking text documents from image queries (image-query-
text). Besides, to give an pictorial demonstration of an al-
gorithm’s performance, the Precision-Recall curves are also
reported on all the approaches.

4.2 Results on the Wiki Dataset

Table 2 reports the performance of LSCMR and the other
comparative models on the testing set of the Wiki dataset,
showing that LSCMR outperforms all the comparative meth-
ods on both directions of the retrieval tasks. Compared to
the best comparative methods, the minimum relative im-
provement is 9.6 percent gained by LSCMR for the image-
query-text retrieval with R = 50 and the maximum is 25.3
percent also for the image-query-text retrieval with R = all.
This improvement is due to the latent semantic space.
To verify this, we note that the low-rank based SSI also
outperforms PAMIR in the image-query-text retrieval while
PAMIR even controls the model complexity by optimizing
an adapted cross-modal RankSVM model. Further, LSCMR
outperforms SSI due to the structural large margin that reg-

439i

i

n
o
s
c
e
r
P

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

 
0

 

0.3

 

0.22

 

0.4

CCA

PAMIR

SSI

LRMAP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

i

i

n
o
s
c
e
r
P

0.28

0.26

0.24

0.22

0.2

0.18

0.16

0.14

0.12

0.1

 
0

CCA

PAMIR

SSI

LRMAP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

i

i

n
o
s
c
e
r
P

CCA

PAMIR

SSI

LRMAP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

0.2

0.18

0.16

0.14

0.12

0.1

0.08

 
0

i

i

n
o
s
c
e
r
P

0.35

0.3

0.25

0.2

0.15

0.1

0.05

 
0

 

CCA

PAMIR

SSI

LRMAP

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Recall

Recall

Recall

Recall

(a) Wiki Text Query

(b) Wiki Image Query

(c) NUS Text Query

(d) NUS Image Query

Figure 2: Precision-Recall curves on the two datasets (LRMAP is short for LSCMR optimizing MAP).

Table 2: The performance comparison in terms of
MAP@R scores on the Wiki dataset. Each text doc-
ument is represented as 5000-D BoW and each im-
age is presented as 1000-D BoVW. Both directions
of ranking tasks are reported. The results shown in
boldface are the best results.

Text
Query
Image
Query

R = 50
R = all
R = 50
R = all

CCA PAMIR
0.2343
0.3093
0.1734
0.1433
0.1797
0.2208
0.1451
0.1779

SSI

0.2821
0.1664
0.2344
0.1759

LSCMR
0.3663
0.2021
0.2570
0.2229

Table 3: The performance comparison in terms of
MAP@R scores on the NUS dataset. Each text doc-
ument is represented as 1000-D BoW and each im-
age is represented as 500-D BovW. Both directions
of ranking tasks are reported. The results shown in
boldface are the best results.

Text
Query
Image
Query

R = 50
R = all
R = 50
R = all

CCA PAMIR
0.1497
0.2046
0.1184
0.0851
0.5003
0.1523
0.0883
0.2410

SSI

0.2156
0.1140
0.4101
0.1992

LSCMR
0.2781
0.1424
0.4997
0.2491

ularizes the model and optimizes for MAP ranking loss di-
rectly. The Precision-Recall curves on both directions are re-
ported in Figure 2(a) and 2(b). The Precision-Recall curves
further validate the superiority of LSCMR for the cross-
modal ranking.

Recall that the CCA model is trained by the pairs of data
objects with diﬀerent modalities and learns a uniﬁed model
for the retrieval tasks of both directions. Hence, CCA has
achieved nearly the same performance in both image-query-
text retrieval and text-query-image retrieval. On the other
hand, the performances of other three approaches (including
LSCMR) are noticeably diﬀerent in the corresponding both
directions of the retrieval.

4.3 Results on the NUS Dataset

The improvement of LSCMR on the NUS dataset is not
as signiﬁcant as that on the Wiki dataset. The MAP scores
of all the methods are shown in Table 3 and the Precision-
Recall curves are reported in Figure 2(c) and Figure 2(d).
For text-query-image retrieval, LSCMR outperforms the other
comparative methods again while for image-query-text re-
trieval LSCMR outperforms all the comparative methods in
all the cases except for the case of R = 50 where PAMIR
has a slightly better overall performance than LSCMR.

Recall that in the NUS-WIDE dataset, one image is as-
sociated with about seven annotated words in average. The
low rank embedding does not help much for querying short
texts in the task of image-query-text retrieval. PAMIR and
LSCMR both train a regularized model, and therefore the
performances of PAMIR and LSCMR is undoubtedly super
to SSI in the image-query-text retrieval.

Once again, it is observed that CCA has a very similar

performance in both directions of the retrieval.

4.4 The Performance Discussions

It is noted that LSCMR has a better overall performance
for text-query-image retrieval than for image-query-text re-
trieval in the Wiki dataset and a better overall performance
for image-query-text retrieval than for text-query-image re-
trieval in the NUS dataset. The reason exactly lies in the
low-rank embedding of LSCMR that is capable of discern-
ing the latent aspect space and consequently supports rich-
semantic queries. For the Wiki dataset, each document has
over one hundred words in average, resulting in a long text
query which is presumably much richer in semantics than
the case in the NUS dataset where each image has only
about seven text words in given as the annotation that is
much shorter when posed as a query. On the other hand,
the overall image quality in the NUS dataset is much richer
and more diverse in semantics than that in the Wiki dataset,
and thus resulting in a better overall performance for image-
query-text-retrieval.

As we have stated that for both datasets CCA achieves
very similar performances in both directions of the retrieval
tasks. The reason is that CCA learns a uniﬁed model from
paired multi-modal data in which the pair-correspondence
of images and text documents ensure an equal contribution
to the learned metric between both modalities. However, a
uniﬁed model like CCA does not give a good performance
which is veriﬁed in both datasets. Our explanation is as fol-
lows: it is not the problem of uniﬁed models but the problem
of taking strictly paired data as training instances; in such
settings CCA can not capture the complete ranking informa-
tion (e.g. dissimilarity between data in diﬀerent modalities).
Nevertheless, learning a uniﬁed model with ranking lists is
still our interests for future work.

Overall, the results on the Wiki dataset and NUS dataset
demonstrate that the use of LSCMR is advantageous for
rich-semantic queries and has a superior performance in both

440e
c
n
a
m
r
o
f
r
e
p

 

0
5
@
P
A
M

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

 

 

Text query
Image query

5

10

25

50

100

200

Dimensionality of latent space

Figure 3: The performance (MAP@50) comparisons
for the Wiki dataset when the dimensions of latent
space are set to diﬀerent values. The dimensional-
ity of 10 is reported to be the best for the overall
performance of text query and image query.

Table 4: Exemplar words along with their top 9
neighboring words from the Wiki dataset.

Fiction

Stories Manuscript Tales Language Theatre
Poetry Editor Publisher Powell

Football Teams Player NHL Goal Conference Compete

Olympics Matches Yards

DVD Moves Brand Broadcasting Movies Victims

Commented Sequel Direction Producer

Airport Passenger Oregon Democratic Communities

Bring Lands Telephone Sale Coverage

directions of the cross-modal retrieval than that of the peer
methods from the state-of-the-art literature.

4.5 The Embedding Latent Space

The results over the Wiki dataset outline the advantage of
LSCMR over the comparative solutions, especially PAMIR.
This observation is certainly due to the low rank embedding
used in LSCMR. Now we look into the learned embedding
latent aspect space for the Wiki dataset. Since the BoVW
features used in our experiments originate from the SIFT
points which are diﬃcult to illustrate, we only demonstrate
the latent space by textual words.

First, the dimensionality of the subspace (also the size
of U and V ) is a parameter to be determined before solv-
ing Optimization Problem 1. We tune this parameter via a
validation set and the MAP@50 performance over a candi-
date set of the chosen dimensionalities is reported in Figure
3. It is observed that LSCMR performs the best with the
optimal subspace dimensionality of 10. When the chosen
dimensionality of the latent subspace is larger (even much
larger) than 10, the performances do not decay rapidly at
the same time it is observed that the topics overlap. But we
ﬁnd something interesting that though some topics overlap,
some smaller but more precise topics are discovered (see the
two “Biology” topics in Table 5).

Second, consider the mapping of textual words into the
latent space in LSCMR. For a text query q, U q maps q into
the latent space. Note that U ∈ Rk×m where k represents
the dimensionality of the latent space. For each row in U ,

the row weights the contribution of all words to the corre-
sponding “topic” in the latent space. The larger the number
Uij , the more positive correlation between topic i and word
j. We then sort every row by which the most relevant words
are ranked ahead. We present some topic examples in Table
5. The columns in U acts a similar way like the rows except
for that each column represents the relevance between the
corresponding word and all topics. We deﬁne two words are
neighbors if their relevance with all topics are similar. Some
examples on the neighboring words are shown in Table 4.

5. CONCLUSIONS

In this work, we have presented a new approach to solving
the problem of cross-modal retrieval by casting the problem
as a problem of learning to rank in a supervised manner with
the idea of low rank embedding. We have demonstrated
the eﬀectiveness of our proposed method LSCMR and have
shown signiﬁcant improvements over the comparative meth-
ods especially on two datasets. We have also investigated
the interpretability of the leaned low rank model by show-
ing some examples on the textual topics and the neighboring
words.

6. ACKNOWLEDGEMENTS

This work is supported by 973 Program (No. 2012CB316400),

NSFC (61070068, 90920303), 863 program (2012AA012505),
Chinese Knowledge Center of Engineering Science and Tech-
nology (CKCEST) and China Academic Digital Associative
Library (CADAL). Zhongfei Zhang is also supported by US
NSF (IIS-0812114, CCF-1017828).

7. REFERENCES
[1] R. Baeza-Yates and B. Ribeiro-Neto. Modern
Information Retrieval. Addison-Wesley, 1999.
[2] B. Bai, J. Weston, D. Grangier, R. Collobert,

K. Sadamasa, Y. Qi, O. Chapelle, and K. Weinberger.
Learning to rank with (a lot of) word features.
Information Retrieval, 13(3):291–314, 2010.

[3] B. Bai, J. Weston, D. Grangier, R. Collobert,

K. Sadamasa, Y. Qi, C. Cortes, and M. Mohri.
Polynomial semantic indexing. Advances in Neural
Information Processing Systems, 22:64–72, 2009.

[4] D. Blei and M. Jordan. Modeling annotated data. In

Proceedings of the 26th Annual International ACM
SIGIR Conference on Research and Development in
Informaion Retrieval, pages 127–134, 2003.

[5] D. Blei, A. Ng, and M. Jordan. Latent dirichlet

allocation. the Journal of Machine Learning Research,
3:993–1022, 2003.

[6] Z. Cao, T. Qin, T. Liu, M. Tsai, and H. Li. Learning
to rank: from pairwise approach to listwise approach.
In Proceedings of the 24th International Conference on
Machine Learning, pages 129–136, 2007.

[7] S. Chakrabarti, R. Khanna, U. Sawant, and

C. Bhattacharyya. Structured learning for non-smooth
ranking losses. In Proceeding of the 14th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 88–96, 2008.

[8] K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,

and Y. Singer. Online passive-aggressive algorithms.
The Journal of Machine Learning Research,
7:551–585, 2006.

441Table 5: Exemplar topics from the Wiki dataset. We assign each learned topic to its most probable category.
Topic words are sorted by their importance values in the descending order.

Category

Topic Words

Geography & Places Guadalcanal Corps Sale Airport Battleships Iowa Aircraft Carriers Chicago Puerto

Biology
Warface

Subspecies Breeding Dinosaurs Fossils Wallace Genus Marsh Females Wings Virginia
Puerto Battalion Guadalcanal Oklahoma Soviet Soldier Tank Bishop Bradman Infantry

Literature & Theatre Hamlet Theatre Gilbert Uncle Stories Shakespeare Refugees Manuscript Fiction Punk
Sports & Recreation Tech Jordan Players Championship Tournament Olympic NHL Cricket EP Coach

Biology

Puerto Zoo Skull Nest Augustus Specimen Tail Teeth Organisms Darwin

[9] S. Deerwester, S. Dumais, G. Furnas, T. Landauer,

[23] B. McFee and G. Lanckriet. Metric learning to rank.

and R. Harshman. Indexing by latent semantic
analysis. Journal of the American Society for
Information Science, 41(6):391–407, 1990.

[10] J. Gao, W. Yuan, X. Li, K. Deng, and J. Nie.

Smoothing clickthrough data for web search ranking.
In Proceedings of the 32nd International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 355–362, 2009.
[11] D. Grangier and S. Bengio. A discriminative

kernel-based approach to rank images from text
queries. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 30(8):1371–1384, 2008.

[12] D. Hardoon, S. Szedmak, and J. Shawe-Taylor.

Canonical correlation analysis: An overview with
application to learning methods. Neural Computation,
16(12):2639–2664, 2004.

In Proceedings of the 27th International Conference on
Machine Learning. Citeseer, 2010.

[24] G. Monaci, P. Jost, P. Vandergheynst, B. Mailhe,
S. Lesage, and R. Gribonval. Learning multimodal
dictionaries. IEEE Transactions on Image Processing,
16(9):2272–2283, 2007.

[25] D. Putthividhy, H. Attias, and S. Nagarajan. Topic
regression multi-modal latent dirichlet allocation for
image annotation. In IEEE Conference on Computer
Vision and Pattern Recognition, pages 3408–3415,
2010.

[26] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle,

G. Lanckriet, R. Levy, and N. Vasconcelos. A new
approach to cross-modal multimedia retrieval. In
Proceedings of the International Conference on
Multimedia, pages 251–260, 2010.

[13] C. Ho and C. Lin. Large-scale linear support vector

[27] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos:

regression. Technical report, National Taiwan
University, 2012.

[14] T. Hofmann. Probabilistic latent semantic indexing. In

Proceedings of the 22nd Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 50–57, 1999.

[15] H. Hotelling. Relations between two sets of variates.

Biometrika, 28(3/4):321–377, 1936.

[16] Y. Jia, M. Salzmann, and T. Darrell. Factorized latent

spaces with structured sparsity. Advances in Neural
Information Processing Systems, 23:982–990, 2010.

Primal estimated sub-gradient solver for svm. In
Proceedings of the 24th International Conference on
Machine Learning, pages 807–814, 2007.

[28] A. Sharma, A. Kumar, H. Daume, and D. Jacobs.

Generalized multiview analysis: A discriminative
latent space. In IEEE Conference on Computer Vision
and Pattern Recognition, pages 2160–2167, 2012.
[29] I. Tsochantaridis, T. Joachims, T. Hofmann, and

Y. Altun. Large margin methods for structured and
interdependent output variables. Journal of Machine
Learning Research, 6(2):1453–1484, 2006.

[17] Y. Jia, M. Salzmann, and T. Darrell. Learning

[30] S. Virtanen, Y. Jia, A. Klami, and T. Darrell.

cross-modality similarity for multinomial data. In
IEEE International Conference on Computer Vision,
pages 2407–2414, 2011.

Factorized multi-modal topic model. In Proceedings of
the 28th Conference on Uncertainty in Artiﬁcial
Intelligence, pages 843–851, 2012.

[18] T. Joachims. Optimizing search engines using

[31] Y. Yang, N. Bansal, W. Dakka, P. Ipeirotis,

clickthrough data. In Proceedings of the 8th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, pages 133–142, 2002.

[19] T. Joachims. A support vector method for

multivariate performance measures. In Proceedings of
the 22nd International Conference on Machine
Learning, pages 377–384, 2005.

[20] T. Joachims, T. Finley, and C. Yu. Cutting-plane

training of structural svms. Machine Learning,
77(1):27–59, 2009.

[21] H. Li. Learning to rank for information retrieval and

natural language processing. Synthesis Lectures on
Human Language Technologies, 4(1):1–113, 2011.

[22] D. Lowe. Distinctive image features from

scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110, 2004.

N. Koudas, and D. Papadias. Query by document. In
Proceedings of the Second ACM International
Conference on Web Search and Data Mining, pages
34–43, 2009.

[32] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A

support vector method for optimizing average
precision. In Proceedings of the 30th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
271–278, 2007.

[33] Y.-T. Zhuang, Y. Yang, and F. Wu. Mining semantic

correlation of heterogeneous multimedia data for
cross-media retrieval. IEEE Transactions on
Multimedia, 10(2):221–229, 2008.

442