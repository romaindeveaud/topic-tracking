On the Measurement of Test Collection Reliability

Julián Urbano∗

jurbano@inf.uc3m.es

Mónica Marrero∗

mmarrero@inf.uc3m.es

Diego Martín†

dmartin@dit.upm.es

∗University Carlos III of Madrid

Department of Computer Science

Leganés, Spain

†Technical University of Madrid

Department of Telematics Engineering

Madrid, Spain

ABSTRACT
The reliability of a test collection is proportional to the
number of queries it contains. But building a collection
with many queries is expensive, so researchers have to ﬁnd
a balance between reliability and cost. Previous work on
the measurement of test collection reliability relied on data-
based approaches that contemplated random what if scenar-
ios, and provided indicators such as swap rates and Kendall
tau correlations. Generalizability Theory was proposed as
an alternative founded on analysis of variance that provides
reliability indicators based on statistical theory. However,
these reliability indicators are hard to interpret in practice,
because they do not correspond to well known indicators like
Kendall tau correlation. We empirically established these
relationships based on data from over 40 TREC collections,
thus ﬁlling the gap in the practical interpretation of Gener-
alizability Theory. We also review the computation of these
indicators, and show that they are extremely dependent on
the sample of systems and queries used, so much that the
required number of queries to achieve a certain level of relia-
bility can vary in orders of magnitude. We discuss the com-
putation of conﬁdence intervals for these statistics, providing
a much more reliable tool to measure test collection reliabil-
ity. Reﬂecting upon all these results, we review a wealth of
TREC test collections, arguing that they are possibly not as
reliable as generally accepted and that the common choice
of 50 queries is insuﬃcient even for stable rankings.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance evaluation.

General Terms
Experimentation, Measurement, Reliability.

Keywords
Test Collection, Evaluation, Reliability, Generalizability
Theory, TREC.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for prof t or commercial advantage and that copies bear
this notice and the full citation on the f rst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specif c permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

The purpose of evaluating the eﬀectiveness of an Informa-
tion Retrieval (IR) system is to assess how well it would sat-
isfy real users. The main tool used in these evaluations are
test collections, which comprise a collection of documents
to search, a set of queries Q, and a set of relevance judg-
ments that contains information as to what documents are
relevant, and to which degree, to the queries [16]. Given
the results returned by a system A for one of the queries
q ∈ Q, an eﬀectiveness measure uses the information in the
relevance judgments to compute a score λq,A that represents
the eﬀectiveness of the system for that query. After run-
ning the system for all queries in the collection, the average
λQ,A = 1
of system eﬀectiveness, representing the expected behavior
of the system for an arbitrary new query. When comparing
two systems A and B, the main measure reported is the av-
erage eﬀectiveness diﬀerence ∆λQ,AB = λQ,A − λQ,B. Based
on this diﬀerence, we conclude which system is better.

|Q| P λi,A is usually reported as the main measure

The immediate question to ask is: how reliable are those
conclusions about system eﬀectiveness? Ideally, researchers
would evaluate the system with the set of all possible queries
that a user might request. In such a case, we could be sure
that the true average performance of the system corresponds
to the score we computed with the collection. The prob-
lem is that building such a collection is either impractical
for requiring an enormous amount of queries and relevance
judgments, or just impossible if the potential query set is
not deﬁned, which use to be the case because we can not
account for future queries that do not yet exist. Therefore,
the query set Q in a test collection must be regarded as
a sample from the universe of all queries, and the sample
mean λQ,A as an estimate of the true eﬀectiveness mean λA.
But because we are estimating this score with a sample of
queries, our estimates are erroneous to some degree. The
results may change drastically with a diﬀerent query set Q′,
so much that diﬀerences between systems could be reversed.
An evaluation result is reliable if it can be replicated with
another collection: if the set of queries Q suggests that sys-
tem A outperforms system B, we can be very sure that the
conclusion would hold for a diﬀerent set of queries Q′, and in
the end, for the universe of all queries. A simple way to make
a collection reliable is to include many queries; the more we
employ the smaller the variance of the estimates and thus
the more reliable the conclusion. The problem is that more
queries also means more cost to create the collection, so re-
searchers have to ﬁnd a balance between the reliability of
the results and the cost of the collection. To this end, it is
necessary to develop indicators of test collection reliability.

393Several works in the last ﬁfteen years have studied the
problem of reliability in IR evaluation experiments. The ba-
sic methodology consisted in evaluating a series of systems
with two diﬀerent and random sets of queries, computing
several reliability indicators that measured how similar those
evaluations were. Using diﬀerent query sample sizes and ran-
domizing query selection, researchers were able to map query
set size to reliability and extrapolate results to larger query
sets. The data used consisted in runs submitted to several
TREC tracks (mostly the Ad Hoc tracks), and the sets of
queries employed in each edition. While these approaches
are clearly faithful to the data, they are limited in that the
full query set had to be partitioned in two disjoint sets to
comply with the assumption that they were independent.

In 2007 Bodoﬀ and Li [6] proposed Generalizability The-
ory (GT) as an alternative [7, 18]. GT is grounded on analy-
sis of variance components, which allows to dissect the vari-
ability in eﬀectiveness scores and ﬁgure out how much of it
is due to system diﬀerences, query diﬃculty, assessors, etc.
In an ideal evaluation setting, we would like all variance to
be due to actual diﬀerences between systems and not due
to query variability; if the queries in the collection are too
varied, or diﬀerences between systems too small, then we
need many queries to ensure that our estimates are reliable.
From these variance components GT allows researchers to
estimate the reliability of a test collection even before it has
been created. Based on some previous data, GT can es-
timate the reliability of a collection with a larger number
of queries, more than one assessor providing judgments for
each query, etc. GT provides indicators for the stability
of both the absolute scores and the relative diﬀerences by
computing diﬀerent variance ratios.

The main advantages of GT against the traditional data-
based approaches are that 1) it is based on statistical theory,
2) it is easy to employ because it does not require tedious
and repetitive what if scenarios, and 3) it allows us to es-
timate the reliability of a collection or experimental design
that does not exist yet. But it has disadvantages too: 1) it
is unknown the extent to which reliability indicators are af-
fected by the data used to estimate variance components,
and 2) it is very hard to interpret them in practical terms.
We address these two problems of GT applied to the mea-
surement of test collection reliability. In the next section we
review past work following data-based approaches and the
reliability indicators used. We then review the use of GT
and discuss the motivation for this work. In Section 3 we
show how the initial data used in GT studies has a very large
eﬀect on the results, discussing minimum sample sizes and
interval estimators. Section 4 reports a study to provide an
empirical mapping between GT-based indicators of reliabil-
ity and the well known data-based ones. Next we discuss the
reliability of several TREC collections based on the results
from previous sections, presenting conclusions in Section 6.

2.

INDICATORS OF RELIABILITY

Several indicators of test collection reliability have been
proposed in the literature. This section reviews traditional
indicators found in the early data-based studies and the GT-
based indicators more recently proposed.
2.1 Data-based Indicators

Given a query set Q and a similar set Q′ of the same size,
we can deﬁne the following data-based reliability indicators:

• Kendall correlation (τ ), compares the order in
which systems are ranked according to Q and Q′, re-
gardless of the magnitude of the diﬀerences ∆λAB. It
ranges from 1 (same rankings) to -1 (reversed rank-
ings), counting the number of system pairs that are
swapped between the two rankings. For Q to be reli-
able, τ must therefore tend to 1.

• AP correlation (τAP ), adds a top-heaviness compo-
nent to Kendall τ , such that swaps between systems
towards the top of the rankings are more penalized
than swaps towards the bottom [23].

• Power ratio (β), is the fraction of pairwise system
diﬀerences that result statistically signiﬁcant accord-
ing to query set Q. If the diﬀerence ∆λQ,AB between
two systems is deemed as statistically signiﬁcant, it
serves as further evidence that the true diﬀerence ∆λAB
has the same sign. For Q to be reliable, β must there-
fore tend to 100%. In this paper we compute standard
2-tailed t-tests at the 0.05 level [19].

• Minor Conﬂict ratio (α−), is the fraction of statis-
tically signiﬁcant diﬀerences with Q that have a sign
swap with Q′ but are not statistically signiﬁcant there.
α− is therefore the fraction of uncertain conclusions
when measuring statistical signiﬁcance, so for Q to be
reliable α− must therefore tend to 0%.

• Major Conﬂict ratio (α+), is the fraction of statis-
tically signiﬁcant diﬀerences with Q that are also sig-
niﬁcant with Q′ but have a sign swap. α+ is therefore
the fraction of incorrect conclusions when measuring
statistical signiﬁcance, so for Q to be reliable α+ must
therefore tend to 0% as well.

• Absolute Sensitivity (δa), is the minimum absolute
diﬀerence ∆λQ,AB that need be observed between any
two systems such that the diﬀerences with Q′ have
the same sign at least 95% of the times. For Q to
be reliable, δa must therefore tend to 0, meaning that
even small diﬀerences can be trusted.

• Relative Sensitivity (δr), is the minimum relative

diﬀerence ∆λQ,AB/ max(cid:0)λQ,A, λQ,B(cid:1) that need be ob-

served with Q such that the diﬀerences with Q′ have
the same sign at least 95% of the times. For Q to be
reliable, δr must therefore tend to 0% too.

• Root Mean Squared Error (ε), measures the dif-
ference between the absolute scores with Q and with
Q′. Thus, for Q to be reliable ε must tend to 0 too.

One of the ﬁrst reliability studies was conducted in 1998
by Voorhees [20], who analyzed the eﬀect of having diﬀer-
ent assessors provide relevance judgments. Employing a
methodology based on randomization, she concluded that
the absolute scores could suﬀer wide variations between as-
sessors, but that the ranking of systems was seldom altered,
establishing τ = 0.9 as the de facto minimum on ranking
similarity. She also studied swap rates as a function of ∆λ
and suggested a minimum of 25 queries to have a somewhat
stable ranking. Also in 1998, Zobel [24] studied the eﬀect of
pool depth on absolute system scores, extrapolating trends
to larger pool depths. He also compared diﬀerent statistical
procedures in terms of power and conﬂict ratios.

Buckley and Voorhees [8] compared in 2000 the reliability
of various eﬀectiveness measures by mapping eﬀectiveness
diﬀerences to error rates. Extrapolating to 50 queries, they
concluded that ∆λ ≥ 0.05 produced less than 1.5% sys-
tem swaps when computing Average Precision (AP), while

394other measures such as Precision at cutoﬀ 10 (P@10) pro-
duced 3.6% of swaps. In 2002, Voorhees and Buckley [22]
extended their work with other collections and methods, but
again extrapolating trends. They concluded that with 50
queries the sensitivity of AP was δa = 0.05, while increasing
the query set size to 100 would yield δa = 0.03. They also
reported large diﬀerences across collections and eﬀectiveness
measures. Lin and Hauptmann [13] showed that the empiri-
cal model used by Voorhees and Buckley can be derived the-
oretically, and that the three factors aﬀecting reliability are
query set size, mean eﬀectiveness scores, and variability of
scores. Sanderson and Zobel [17] also revisited this work by
computing relative sensitivity and incorporating statistical
procedures to account for score variability. They concluded
δr = 10% with AP if coupled with statistical signiﬁcance,
and δr = 25% if not. They observed very similar relative
sensitivity between AP and P@10, arguing the use of more
queries with fewer judgments as previous work suggested
that much of the score variability is due to queries [4].

In 2007 Sakai [15] used similar methods to compare the
reliability of several eﬀectiveness measures, though he did
not extrapolate to larger query sets. He computed τ cor-
relations, absolute sensitivity δa and a variation of δr, and
observed that these indicators were not very correlated with
statistical signiﬁcance, arguing the importance of consider-
ing score variability rather than just means. Voorhees revis-
ited in 2009 [21] the use of statistical procedures with the
TREC Robust 2004 collection, computing reliability indi-
cators with an unprecedented set of 100 queries, therefore
avoiding the need to extrapolate to the usual size 50. When
using AP, she observed power β = 47% and conﬂict ratios
α− = 2.7% and α+ = 0.04%. She showed again that P@10
is less reliable than AP also in these terms; and that nDCG
showed higher reliability (agreeing with Sakai [15]). She also
found that minor conﬂicts were usually coupled with large
relative diﬀerences, thus suggesting that researchers employ
several large collections to draw general conclusions.

2.2 GT-based Indicators

Bodoﬀ and Li [6] proposed Generalizability Theory [7, 18]
as an alternative to measure test collection reliability that
directly addresses variability of scores rather than just the
mean as was common before. GT has two stages: a Gener-
alizability study (G-study) to estimate variance components
based on previous data, and a Decision study (D-study) that
subsequently computes reliability indicators for a diﬀerent
experimental design. We consider a fully crossed design and
decompose variability of scores into three components: vari-
ance due to actual diﬀerences among systems (σ2
s ), vari-
ance due to diﬀerences in diﬃculty among queries (σ2
q ), and
variance due to the system-query interaction eﬀect whereby
some systems are particularly good (or bad) for some queries
(σ2
s:q). The variance due to other eﬀects, such as assessors,
is in our case confounded with the interaction eﬀect.

Using Analysis of Variance (ANOVA) procedures, these
variance components can be estimated from previous data:

ˆσ2
s:q = ˆσ2

e = EMresidual
EMs − ˆσ2
e

ˆσ2
s =

ˆσ2
q =

nq

EMq − ˆσ2
e

ns

(1)

(2)

(3)

where EMν is the expected Mean Square of component ν,
and ns and nq are the number of systems and queries [7, 18].
These estimates can be used to compute the proportion of
total variance that is due to each of the eﬀects, such as how
much of it is due to actual diﬀerences between systems.

In the D-study, we can use the variance estimates from
the G-study to compute the reliability of a larger query set.
To this end, two reliability indicators are usually employed:

• Generalizability Coeﬃcient (Eρ2), is the ratio of

system variance to itself plus relative error variance:

Eρ2(cid:0)n′

q(cid:1) =

σ2
s
s + σ2
σ2
n′

e

q

(4)

and it provides a measure of the stability of relative
diﬀerences between systems ∆λ. By extension, it mea-
sures the reliability of the ranking. For a collection to
be reliable, Eρ2 must therefore tend to 1.

• Index of Dependability (Φ), is the ratio of system

variance to itself plus absolute error variance:

Φ(cid:0)n′

q(cid:1) =

σ2
s
σ2
q

+σ2
e
n′

q

σ2

s +

(5)

and it provides a measure of the stability of absolute
eﬀectiveness scores λ. For a collection to be reliable,
Φ must therefore tend to 1 as well.

The main advantage of these indicators is that they allow
us to estimate the reliability of an arbitrary query set size n′
q,
so there is no need to follow the traditional methodologies
based on random what if scenarios and extrapolation. From
equations (4) and (5) it can be seen that the reliability of the
collection increases as n′
q increases, because the estimates of
query diﬃculty (i.e. average system performance per query)
are more precise. These indicators were used by Kanoulas
and Aslam [12] to derive the gain and discount functions of
nDCG that yield optimal reliability when n′

q is constant.

With simple algebraic manipulation, we can calculate the
minimum number of queries needed to reach some level of
relative or absolute stability π:

n′

e

σ2

Eρ2 (π) =(cid:24) π · σ2
Φ (π) =& π(cid:0)σ2

s (1 − π)(cid:25)
s (1 − π) '
e(cid:1)

q + σ2

n′

σ2

(6)

(7)

which can be used to estimate how many more queries we
need to add to our collection for it to be reliable. The main
use of this approach can be found in the TREC Million
Query Track [2, 1], which set out to study whether many
queries with a few judgments yield more reliable results than
a few queries with many judgments. The conclusion was that
n′
q ≈ 80 queries are suﬃcient for a reliable ranking, while
n′
q ≈ 130 are needed for reliable absolute scores.
2.3 Motivation

The two problems of GT can be clearly spotted at this
point. First, equations (1) to (3) show that we do not com-
pute the true σ2
ν variance components, but just estimates
ˆσ2
ν based on some previous data. If we use a diﬀerent, yet
similar set of systems or queries to estimate these variance
components, the resulting Eˆρ2 and ˆΦ scores might be very

395Ad Hoc 3
Ad Hoc 4
Ad Hoc 5
Ad Hoc 6
Ad Hoc 7
Ad Hoc 8
Web adhoc 8
Web adhoc 9
Web adhoc 2001
Web adhoc 2009
Web adhoc 2010
Web adhoc 2011
Web distillation 2002
Web distillation 2003
Web distillation 2004
Web diversity 2009
Web diversity 2010
Web diversity 2011
Novelty 2002
Novelty 2003
Novelty 2004
Genomics 2003
Genomics 2004
Genomics 2005
Robust 2003
Robust 2004
Robust 2005
Terabyte 2004
Terabyte 2005
Terabyte 2006
Terabyte all 2006
Enterprise 2005
Enterprise 2006
Enterprise 2007
Enterprise 2008
1MQ MTC 2007
1MQ MTC 2008
1MQ MTC 2009
1MQ statAP 2007
1MQ statAP 2008
1MQ statAP 2009
Medical 2011
Microblog 2011

s
n
o
i
t
a
v
r
e
s
b
o

 

2
^
E

 

 

 
f
o
%
5
9
g
n
i
r
e
v
o
c
 
n
a
p
S

s
n
o
i
t
a
v
r
e
s
b
o

 

^
 

 

 

f
o
%
5
9
g
n
i
r
e
v
o
c
 
n
a
p
S

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

Variability due to queries

s
n
o
i
t
a
v
r
e
s
b
o

 

2
^
E

 

 

 
f
o
%
5
9
g
n
i
r
e
v
o
c
 
n
a
p
S

20

40

60

80

100

Initial number of queries in the G−study

s
n
o
i
t
a
v
r
e
s
b
o

 

^
 

 

 

f
o
%
5
9
g
n
i
r
e
v
o
c
 
n
a
p
S

Variability due to systems

20

40

60

80

100

Initial number of systems in the G−study

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

20

40

60

80

100

20

40

60

80

100

Initial number of queries in the G−study

Initial number of systems in the G−study

Figure 1: Variability in Eˆρ2 (top) and ˆΦ (bottom) scores as a function of the initial number of queries (left)
and number of systems (right) used in the G-study to estimate variance components.

diﬀerent.
In a revised paper, Bodoﬀ [5, §4.6] brieﬂy dis-
cussed this issue and argued that diﬀerences are marginal.
However, he reports the results when randomly selecting
only one system per research group instead of all of them,
and only one trial of such experiment. We argue that this
situation is not representative because the full set of systems
and the reduced set after removing runs by the same groups
are actually very similar to begin with, so it is expected that
reliability scores do not change much. Also, only one such
randomly reduced set is compared, so there is really no ev-
idence to support that claim. Likewise, he further suggests
that as few as ﬁve queries or systems are often enough to
provide stable estimates of the variance components in the
G-study [5, §3.1]. We further analyze this issue in Section 3.
Second, equations (6) and (7) allow us to estimate the
minimum number of queries n′
q to reach some stability level
π, but the greater question is: how much is stable enough?
Bodoﬀ [5] mentions that in most Social Science applications
a stability coeﬃcient of 0.8 is acceptable, but there is no
similar standard for Engineering applications. Kanoulas and
Aslam [12] set Φ = 0.95 as the target in their experiments,
but this choice is arbitrary.
In their analysis of the Mil-
lion Query Track 2007 [2] and 2008 [1], Allan et al. [1] and
[9, 10] also set Eρ2 = 0.95 as the tar-
Carterette et al.
get. They mention in a footnote that in their experiments
Eρ2 = 0.95 approximately corresponded to τ = 0.9, but
details are omitted. We study this issue in Section 4 by
empirically mapping GT-based indicators onto data-based
indicators that are easier to understand and use in practice.

3. VARIABILITY OF GT INDICATORS

To measure the eﬀect of the number of queries and number
of systems used in the G-study to estimate variance compo-
nents, we use data from 43 TREC collections covering 12
tasks across 10 tracks, from TREC 3 to TREC 2011 (see
Table 1). As in previous studies [22, 17, 6, 21], we remove

the bottom 25% of systems so that our results are not ob-
scured by possibly buggy implementations. For each collec-
tion, we randomly selected nq = 5 queries and computed
the variance components using the full set of systems. We
then calculated Eρ2 and Φ for the full query set size, and the
required number of queries to reach 0.95 stability. This was
repeated with increments in nq of 5 queries, up the maxi-
mum permitted by the collection or 100. For each query set
size, we ran 200 random trials, each of which can be con-
sidered as the possible data available for a G-study when
analyzing a test collection design. The same process was
followed by varying the initial number of systems ns and
using the full set of queries instead.

Figure 1 shows the variability in G-study results1. For
each collection and initial number of queries used, the y-axis
plots the length of the span covering 95% of the Eˆρ2 and ˆΦ
observations in the 200 random trials. The right hand side
plots show the same span lengths, but for diﬀerent number of
systems used in the G-study. As expected, the queries have
a larger eﬀect. Most importantly, we see that the average
span length with just 5 queries is about 0.5 across collec-
tions. That is, the stability estimates could be as low as 0.3
or as high 0.8, for example, just depending on the particular
set of queries we use in the G-study. In fact, estimates of
the minimum number of queries required can vary in orders
of magnitude if not using enough data. For example, with
as many as 30 initial queries and all 184 systems from the
Microblog 2011 collection, GT may suggest from 63 to 133
queries to reach Eρ2 = 0.95. Similarly, from 40 initial sys-
tems and all 34 queries from the Medical 2011 collection, GT
may suggest from 109 to 566 queries. In general, at least 50
queries and 50 systems seem necessary for 95% of estimates
to be within a 0.1 span. This means that GT may be trusted
to measure the reliability of an existing collection, but that

1Given the amount of datapoints displayed in this paper, we
recommend to access the full-color version available online.

396r
F
r
F
(cid:18) Lζ − 1

nq

,

Uζ − 1

nq (cid:19) , where

(8)

Lζ =

Uζ =

Ms

MeFα:dfs,dfe

Ms

MeF1−α:dfs,dfe

(cid:18) nsLΛ

nsLΛ + nq
M 2

LΛ =

,

nsUΛ

nsUΛ + nq(cid:19) , where

s − Fα:dfs,∞MsMe + (Fα:dfs,∞ − Fα:dfs,dfe ) Fα,dfs,dfe M 2
e

(ns − 1)Fα:dfs,∞MsMe + Fα:dfs,dfq MsMq

(9)

UΛ =

M 2

s − F1−α:dfs,∞MsMe + (F1−α:dfs,∞ − F1−α:dfs,dfe ) F1−α,dfs,dfe M 2
e

(ns − 1)F1−α:dfs,∞MsMe + F1−α:dfs,dfq MsMq

researchers should be cautious when planning a collection
based on the results of a handful of systems and queries.

These results clearly evidence the need for a measure of
conﬁdence on GT indicators. Bodoﬀ [5] suggests the use of
conﬁdence intervals to account for this variability, but only
computes them for the variance components in the G-study.
Conﬁdence intervals for the ultimately more useful D-study
can be worked out from various variance ratios (see equa-
tions (8) and (9)2). Feldt [11] derived exact 100(1 − 2α)%
conﬁdence intervals for the ratio ζ = σ2
e under the as-
sumption of normally distributed scores. The conﬁdence
interval on Eρ2(n′
q) is computed using the endpoints in (8):

s /σ2

n′

qζ
1 + n′

qζ

(10)

Eρ2(cid:0)n′

q(cid:1) =

n′

qΛ

1 +(cid:0)n′

Arteaga et al. [3] derived approximate 100(1 − 2α)% conﬁ-
dence intervals for the ratio Λ = σ2
assuming a normal distribution of scores. The conﬁdence

q + σ2

s + σ2

s /(cid:0)σ2

e(cid:1), again
q(cid:1) is computed using the endpoints in (9):
Φ(cid:0)n′

q(cid:1) =

interval on Φ(cid:0)n′

Brennan [7, §6] discusses diﬀerent methods to compute
conﬁdence intervals in both G-studies and D-studies, show-
ing that the above intervals work reasonably well even when
the normality assumption is violated. The right hand side
of Table 1 reports the point and 95% interval estimates of
the stability of the 43 TREC collections we consider in this
paper. These intervals provide a more suitable estimate of
test collection reliability because they account for variabil-
ity in the G-study. For example, researchers could use these
intervals to infer the required number of queries to reach the
lower endpoint of the interval instead of the point estimate:

q − 1(cid:1) Λ

(11)

n′

π

ζ (1 − π)(cid:25)
Eρ2 (π) =(cid:24)
Φ (π) =(cid:24) π (1 − Λ)
Λ (1 − π)(cid:25)

n′

(12)

(13)

4.

INTERPRETING GT INDICATORS

To empirically derive a mapping between GT-based and
data-based reliability indicators, we again used the 43 TREC
collections in Table 1. For each collection we proceeded
as follows. Two random and disjoint query subsets of size
nq = 10 were selected from the full set of queries; let these
subsets be Q and Q′. The full set of systems was evaluated
with both query subsets, and all data-based reliability indi-
cators in Section 2.1 were computed, along with the two GT-
based indicators according to Q and Q′. This was repeated

2Fϕ:df1,df2 is the quantile function of the F distribution with
df1 and df2 degrees of freedom. In our fully crossed design,
dfs = ns − 1, dfq = nq − 1, and dfe = (ns − 1)(nq − 1).

with increments in nq of 10 queries, up to the maximum
permitted by the collection. For query subset size we ran
50 random trials, each trial providing us with 32 datapoints
(Eˆρ2 and ˆΦ according to Q and to Q′, mapped to ˆτ ,ˆτAP , ˆβ,
ˆα−, ˆα+, ˆδa, ˆδr and ˆε). Theoretically though, Eρ2 is better
related to τ , τAP , β, α−, α+ and δa because it measures the
stability of relative diﬀerences, while Φ is better related to
δr and ε because it measures the stability of absolute scores.
We thus mapped only these combinations.

Figure 2 shows the mappings. For each collection we ﬁtted
a model with all available datapoints. However, we dropped
points for which Eˆρ2 < 0.8 and ˆΦ < 0.5 so that the trends
were not aﬀected by mappings with such small stability to
be even practical. These thresholds were chosen based on
the observed stability of the 43 TREC collections; about
85% of them show larger stability scores (see Table 1). This
resulted in over 28,000 points for each plot. In the top three
plots (τ , τAP and β) we ﬁtted the model y = xa, where a is
the parameter to ﬁt. This resulted in the desired theoretical
behavior that limx→1 y = 1 and limx→0 y = 0, that is, when
all variability is due to system diﬀerences τ should be 1
because the ranking cannot be altered, and if all variance is
due to queries then τ should be 0 because the rankings are
completely random. Similarly, in the bottom four plots we
ﬁtted the model y = (1 − x)a, such that limx→1 y = 0 and
limx→0 y = 1, that is, ε should for example be 0 if there is
no variability due to queries.

As the ﬁrst plot shows, all 43 collections do actually need
Eρ2 > 0.95 to reach τ = 0.9. In general, Eρ2 = 0.95 cor-
responds to τ ≈ 0.85, and on average Eρ2 ≈ 0.97 is needed
across collections to reach τ = 0.9. The two clear exceptions
are found in the Million Query Track. The 2008 collection
is the one that reaches the target τ = 0.9 with the lowest
stability (Eρ2 ≈ 0.93), while the 2007 collection needs the
largest (Eρ2 ≈ 0.98). Note that these were the two collec-
tions for which the Eρ2 = 0.95 → τ = 0.9 correspondence
was established [1, 9, 10]. It should be noted here that these
ﬁts have an exponential-like shape, meaning that it is hard
to achieve a mid level of τ , but once Eρ2 is large enough
small improvements in stability translate into large improve-
ments in τ . However, the relation between n′
q and Eρ2 has a
logarithmic-like shape, meaning that it is increasingly more
expensive to improve Eρ2 to begin with. Thus, it should be
considered the required eﬀort for slight improvements in τ .
The second plot shows quite high τAP scores at these lev-
els of relative stability, but generally below τ . This suggests
that the swaps in the rankings are still happening between
systems at the top of the rankings [23]. The third plot shows
that at these stability levels it is expected to observe statis-
tical signiﬁcance in about 80% of system comparisons. In
the middle right plot we can see that the proportion of con-
ﬂicting results is generally below the α = 0.05 signiﬁcance
level when Eρ2 ≥ 0.9.

397Ad Hoc 3
Ad Hoc 4
Ad Hoc 5
Ad Hoc 6
Ad Hoc 7
Ad Hoc 8
Web adhoc 8
Web adhoc 9
Web adhoc 2001
Web adhoc 2009
Web adhoc 2010
Web adhoc 2011
Web distillation 2002
Web distillation 2003
Web distillation 2004
Web diversity 2009
Web diversity 2010
Web diversity 2011
Novelty 2002
Novelty 2003
Novelty 2004
Genomics 2003
Genomics 2004
Genomics 2005
Robust 2003
Robust 2004
Robust 2005
Terabyte 2004
Terabyte 2005
Terabyte 2006
Terabyte all 2006
Enterprise 2005
Enterprise 2006
Enterprise 2007
Enterprise 2008
1MQ MTC 2007
1MQ MTC 2008
1MQ MTC 2009
1MQ statAP 2007
1MQ statAP 2008
1MQ statAP 2009
Medical 2011
Microblog 2011

2
1

.

0

9
0

.

0

a

6
0

.

0

3
0

.

0

0
0

.

0

0.80

6

.

0

5

.

0

4

.

0

r

3

.

0

2

.

0

1

.

0

0

.

0

3
.
0

0.80

0.85

0.90
Er 2

Absolute Sensitivity

0.85

0.90
Er 2

Relative Sensitivity

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

0.5

Kendall correlation

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

AP correlation

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

P
A

Power ratio

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

0.95

1.00

3
.
0

0.80

0.85

0.90
Er 2

0.95

1.00

3
.
0

0.80

0.85

0.90
Er 2

0.95

1.00

Minor Conflict ratio

0
2

.

0

5
1

.

0

−

0
1

.

0

5
0

.

0

0.95

1.00

0
0

.

0

0.80

0.85

0.90
Er 2

RMS Error

0.95

1.00

0.5

0.6

0.7

0.8

0.9

1.0

0.6

0.7

0.8

0.9

1.0

Figure 2: Mapping from GT-based to data-based reliability indicators on a per-collection basis.

Researchers interested in the particular mapping for one
of these collections may use the estimates in Table 1 and
the plots in Figure 2 to get a better understanding of the
evaluation results and draw more informed conclusions. To
assess the reliability of future collections and guide in their
development process, we ﬁtted a single model using all avail-
able data instead of one model per collection. Figure 3
shows these ﬁts, along with 95% and 90% prediction inter-
vals that theoretically cover 95% and 90% of all future ob-
servations. In terms of sensitivity, the middle left plots show
that δa ≈ 0.03 for Eρ2 ≈ 0.9, which is about 60% of what
Voorhees and Buckley reported for the Ad Hoc tracks [22];
although the intervals cover their values well. In the bottom

left plot we see that δr ≈ 20% for Φ ≈ 0.75, generally agree-
ing with Sanderson and Zobel [17]. As to statistical signiﬁ-
cance, we replicated Voorhees’s [21] study with random sets
of 50 queries from the Ad Hoc 7-8 topics and Robust 2004
systems. The average relative stability is Eˆρ2 ∈ [0.81, 0.88],
which corresponds to β ∈ [37%, 54%], α− ∈ [3.9%, 7.8%]
and α+ ∈ [0.38%, 1.3%]. These are again larger than she
reported, but the intervals cover her values well.

Overall, these models produce a decent ﬁt on the data,
and they ﬁll the gap between data-based methodologies and
Generalizability Theory. They provide a valuable tool to
rapidly assess and easily understand the reliability of a test
collection design.

398t
t
b
d
a
F
d
F
e
Ad Hoc 3
Ad Hoc 4
Ad Hoc 5
Ad Hoc 6
Ad Hoc 7
Ad Hoc 8
Web adhoc 8
Web adhoc 9
Web adhoc 2001
Web adhoc 2009
Web adhoc 2010
Web adhoc 2011
Web distillation 2002
Web distillation 2003
Web distillation 2004
Web diversity 2009
Web diversity 2010
Web diversity 2011
Novelty 2002
Novelty 2003
Novelty 2004
Genomics 2003
Genomics 2004
Genomics 2005
Robust 2003
Robust 2004
Robust 2005
Terabyte 2004
Terabyte 2005
Terabyte 2006
Terabyte all 2006
Enterprise 2005
Enterprise 2006
Enterprise 2007
Enterprise 2008
1MQ MTC 2007
1MQ MTC 2008
1MQ MTC 2009
1MQ statAP 2007
1MQ statAP 2008
1MQ statAP 2009
Medical 2011
Microblog 2011

2
1

.

0

9
0

.

0

a

6
0

.

0

3
0

.

0

0
0

.

0

0.80

6

.

0

5

.

0

4

.

0

r

3

.

0

2

.

0

1

.

0

0

.

0

3
.
0

0.80

0.85

0.90
Er 2

Absolute Sensitivity

0.85

0.90
Er 2

Relative Sensitivity

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

0.5

Kendall correlation

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

AP correlation

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

P
A

Power ratio

0
.
1

9
.
0

8
.
0

7
.
0

6
.
0

5
.
0

4
.
0

0.95

1.00

3
.
0

0.80

0.85

0.90
Er 2

0.95

1.00

3
.
0

0.80

0.85

0.90
Er 2

0.95

1.00

Minor Conflict ratio

0
2

.

0

5
1

.

0

−

0
1

.

0

5
0

.

0

0.95

1.00

0
0

.

0

0.80

0.85

0.90
Er 2

RMS Error

0.95

1.00

0.5

0.6

0.7

0.8

0.9

1.0

0.6

0.7

0.8

0.9

1.0

Figure 3: General mapping from GT-based to data-based reliability indicators, with 95% (dark grey) and
90% (light grey) prediction intervals.

5. DISCUSSION

The last columns in Table 1 report point and 95% inter-
val estimates of the stability of the 43 TREC collections we
considered. Collections in the same group correspond to the
same tasks, providing a historical perspective on the reliabil-
ity of the collections used so far since 1994 and for a variety
of tasks. For example, the average relative stability in the
Ad Hoc collections was Eρ2 ∈ [0.86, 0.93], which according
to Figure 3 corresponds to τ ∈ [0.65, 0.81]. For the Web Ad
Hoc collections we ﬁnd Eρ2 ∈ [0.8, 0.93], which would corre-
spond to τ ∈ [0.53, 0.81]. There are large diﬀerences within
some tasks, such as Web Distillation, Genomics, Terabyte

and Enterprise. This is further evidence of the variability in
D-study results due to the data used in the G-study. Except
for a few particular cases though, the computation of con-
ﬁdence intervals smooths the problem. Across collections
the averages are Eρ2 = 0.88 and Φ = 0.74, with some tasks
having very low scores. According to Figure 3 the expected
τ correlation is 0.69 with variations from 0.49 to 0.95, that
is, much lower than desired.

Figure 4 plots the historical trend of test collection relia-
bility. The left plot shows that relative stability has varied in
the (0.8,1) interval for the most part, but most importantly
it suggests that the stability of collections has decreased very

399t
t
b
d
a
F
d
F
e
Track Documents
Ad Hoc 3 Disks 1 & 2
Ad Hoc 4 Disks 2 & 3
Ad Hoc 5 Disks 2 & 4
Ad Hoc 6 Disks 4 & 5
Ad Hoc 7 Disks 4 & 5
Ad Hoc 8 Disks 4 & 5
WT2g
WebAdHoc 8
WT10g
WebAdHoc 9
WebAdHoc 2001
WT10g
WebAdHoc 2009 ClueWeb09
WebAdHoc 2010 ClueWeb09
WebAdHoc 2011 ClueWeb09
.GOV
.GOV
.GOV
WebDiversity 2009 ClueWeb09
WebDiversity 2010 ClueWeb09
WebDiversity 2011 ClueWeb09
Novelty 2002 Disks 4 & 5
Novelty 2003 AQUAINT
Novelty 2004 AQUAINT
GenomicsAdHoc 2003 MEDLINE
GenomicsAdHoc 2004 MEDLINE
GenomicsAdHoc 2005 MEDLINE

WebDistillation 2002
WebDistillation 2003
WebDistillation 2004

50 from 300-450 ∗
N1-N50
N51-N100
“G1-G50”
“G51-G100”
“G101-150”
Robust 2003 Disks 4 & 5 50 from 301-450 & 601-650 ∗
301-450 & 601-700 ∗
Robust 2004 Disks 4 & 5
50 from 301-700 ∗
Robust 2005 AQUAINT
701-750 ∗
GOV2
Terabyte 2004
751-800 ∗
GOV2
Terabyte 2005
801-850 ∗
GOV2
Terabyte 2006
701-850 ∗
GOV2
TerabyteAll 2006
EX01-EX50
W3C
EnterpriseExpert 2005
EX51-EX105
W3C
EnterpriseExpert 2006
CE001-CE050
CERC
EnterpriseExpert 2007
CERC
CE051-CE127
EnterpriseExpert 2008
GOV2
1MQ 2007
1MQ 2008
GOV2
1MQ 2009 ClueWeb09
GOV2
1MQ 2007
1MQ 2008
GOV2
1MQ 2009 ClueWeb09
NLP
Microblog 2011 Tweets2011

“MQ1-MQ10000”
“MQ10001-MQ20000”
“MQ20001-MQ60000”
“M101-M135”
MB1-MB50

Medical 2011

Query Set
151-200
201-250
251-300
301-350 ∗
351-400 ∗
401-450 ∗
401-450 ∗
451-500
501-550
“W1-W50” ∗
“W51-W100” ∗
“W101-W150” ∗
551-600
TD1-TD50
“WT04”

Measure
nq
ns
50
40
AP
49
33
AP
50
94
AP
50
AP
74
50
AP 103
50
AP 129
50
AP
44
50
AP 104
50
97
AP
50
71
AP (MTC)
48
56
AP
50
37
AP
49
71
AP
50
AP
93
75
74
AP
“W1-W50” ∗ α-nDCG@20
50
48
“W51-W100” ∗ α-nDCG@20
50
32
“W101-W150” ∗ α-nDCG@20
50
25
49
F
42
50
55
F
50
60
F
50
49
AP
50
43
AP
49
AP
62
100
AP
78
249
AP 110
50
74
AP
49
70
bpref
50
58
bpref
50
bpref
80
149
61
bpref
50
37
AP
49
91
AP
50
55
AP
42
55
AP
“MQ1-MQ10000” AP (MTC)
29 1692
784
25
“MQ10001-MQ20000” AP (MTC)
35
“MQ20001-MQ60000” AP (MTC)
542
29 1153
statAP
564
25
statAP
statAP
35
475
34
bpref 127
P@30 184
49

Eˆρ2(nq)

ˆΦ(nq)

0.933 0.893-0.963 0.786 0.661-0.88
0.907 0.847-0.952 0.79
0.658-0.89
0.856 0.804-0.9
0.62
0.488-0.732
0.898 0.855-0.933 0.806 0.714-0.875
0.919 0.891-0.943 0.799 0.71-0.864
0.701 0.59-0.787
0.908 0.88-0.932
0.728-0.904
0.929 0.89-0.96
0.83
0.662-0.835
0.876 0.833-0.912 0.76
0.862 0.813-0.904 0.711 0.598-0.801
0.81
0.729-0.876 0.619 0.473-0.744
0.829 0.746-0.895 0.662 0.513-0.787
0.804 0.685-0.895 0.702 0.537-0.835
0.901 0.858-0.935 0.84
0.762-0.898
0.249-0.619 0.315 0.144-0.492
0.45
0.89
0.844-0.927 0.747 0.643-0.832
0.903 0.852-0.943 0.847 0.759-0.911
0.882 0.803-0.94
0.804 0.676-0.899
0.844 0.725-0.929 0.719 0.535-0.865
0.919 0.873-0.955 0.792 0.671-0.883
0.966 0.949-0.979 0.944 0.91-0.967
0.801 0.708-0.876 0.181 0.1-0.301
0.94
0.903 0.848-0.945 0.768 0.64-0.868
0.77
0.664-0.855 0.422 0.269-0.586
0.846 0.784-0.897 0.509 0.384-0.636
0.95
0.934-0.964 0.824 0.768-0.872
0.864 0.807-0.911 0.693 0.564-0.797
0.953 0.933-0.97
0.877 0.809-0.924
0.875 0.815-0.923 0.648 0.501-0.774
0.762 0.668-0.841 0.427 0.283-0.575
0.94
0.913-0.962 0.719 0.617-0.812
0.916 0.864-0.955 0.824 0.713-0.905
0.965 0.952-0.976 0.939 0.909-0.96
0.884 0.827-0.929 0.785 0.674-0.87
0.565 0.315-0.757 0.28
0.11-0.498
0.999 0.999-1
0.998 0.997-0.999
0.998 0.996-0.999 0.988 0.979-0.995
0.96
0.936-0.979 0.908 0.854-0.951
0.992 0.986-0.996 0.982 0.97-0.991
0.969 0.946-0.986
0.978 0.962-0.99
0.935-0.979 0.929 0.886-0.963
0.96
0.774 0.704-0.835 0.497 0.348-0.628
0.92
0.899-0.938 0.818 0.747-0.869

0.909-0.965 0.87

0.792-0.925

Table 1: Summary of all 43 TREC collections analyzed. Query sets with ∗ are used in more than one
collection. Query numbers in quotes are not oﬃcial, but arbitrarily named for this paper. The last two
columns report the point and 95% interval estimates of the GT-based reliability indicators.

slightly with the years. The clear exceptions are again the
Million Query Track collections, which speciﬁcally aimed at
increasing the number of queries. Within each task it ap-
pears that stability tended to decrease as the tasks got older
despite that query set sizes were normally unaltered. The
second plot shows that this decrease in stability could be
due to system variance getting smaller with the years. That
is, systems perform more similarly as the tasks get older,
indicating that retrieval techniques are generally improved.
The right plot shows that query diﬃculty also varied within
tasks. Sudden peaks may be explained by changes in the
document set or in the task deﬁnition. The general trend
suggests that queries are getting more alike with the years,
further contributing to the decrease in reliability.

Bodoﬀ [5, §5] discusses the incorporation of the document
set as another facet in Generalizability Theory, much like
queries and systems, to measure variability due to docu-
ments [14]. He argues that it does not make sense in gen-
eral, because we do no assign performance scores for indi-

vidual documents but for sets of documents (e.g. the ﬁrst k
retrieved when computing P @k). In our case we could com-
pare diﬀerent editions of the same task but with diﬀerent
document sets to get a (weak) clue of the variability due to
documents. For example, the Ad Hoc task of the Web Track
shows quite diﬀerent stability scores in the ﬁrst three edi-
tions (WT2g and WT10g collections) compared to the last
three editions (ClueWeb09), given that they all used the
standard query set size of 50. Similarly, the Expert Search
task in the Enterprise Track shows very diﬀerent stability
levels when using the W3C collection or the CERC collec-
tion. We must bear in mind though that these diﬀerences
might actually be due to the systems and queries used, which
varied from year to year.

From the conﬁdence intervals in Table 1, we used the mod-
els ﬁtted in Section 4 to provide in Table 2 the estimated
data-based reliability scores for all 43 collections. It is evi-
dent that expected τ correlations are well below the desired
0.9 in most cases. In that line, some collections are clearly

400Track

ˆα+ (%)

ˆδa ˆδr (%)

WebAdHoc 8
WebAdHoc 9

ˆβ (%)
58-83
45-79
35-60
47-72
58-76
54-72
57-82
42-65
37-62
22-53
25-59
16-59
48-73

ˆα− (%)
0.6-3.2
0.9-5.6
2.9-8.2
1.6-5.2
1.2-3.3
1.6-3.9
0.7-3.4
2.4-6.4
2.8-7.7
4.1-13.5
3.1-12.2
3.2-17
1.5-5

ˆτAP
ˆτ
0.637-0.86
Ad Hoc 3 0.725-0.898
0.622-0.87 0.515-0.823
Ad Hoc 4
Ad Hoc 5 0.537-0.741 0.418-0.657
Ad Hoc 6 0.641-0.821 0.537-0.758
0.72-0.846 0.631-0.791
Ad Hoc 7
Ad Hoc 8 0.695-0.819
0.6-0.756
0.718-0.89 0.629-0.849
0.595-0.77 0.484-0.694
WebAdHoc 2001 0.554-0.749 0.437-0.668
WebAdHoc 2009 0.406-0.686
0.283-0.59
WebAdHoc 2010 0.434-0.729 0.311-0.643
WebAdHoc 2011
0.34-0.728 0.221-0.642
WebDistillation 2002 0.647-0.827 0.544-0.766
WebDistillation 2003 0.019-0.255 0.004-0.148
WebDistillation 2004 0.617-0.807 0.508-0.741
WebDiversity 2009 0.633-0.847 0.528-0.792
WebDiversity 2010 0.535-0.839 0.416-0.782
WebDiversity 2011 0.401-0.811 0.278-0.746
Novelty 2002 0.679-0.877 0.582-0.833
Novelty 2003
0.81-0.919
Novelty 2004 0.374-0.685 0.252-0.589
GenomicsAdHoc 2003 0.762-0.903 0.684-0.867
GenomicsAdHoc 2004 0.624-0.852 0.517-0.799
GenomicsAdHoc 2005 0.311-0.641 0.195-0.537
Robust 2003
0.5-0.734 0.379-0.649
Robust 2004 0.823-0.902 0.761-0.865
Robust 2005 0.544-0.766 0.426-0.689
Terabyte 2004
0.82-0.916 0.758-0.884
Terabyte 2005 0.558-0.795 0.442-0.725
Terabyte 2006
0.2-0.5
TerabyteAll 2006 0.772-0.897 0.696-0.859
EnterpriseExpert 2005 0.661-0.877
0.56-0.831
EnterpriseExpert 2006 0.868-0.932 0.821-0.907
EnterpriseExpert 2007 0.582-0.812 0.468-0.746
EnterpriseExpert 2008 0.037-0.453
0.01-0.33

44-70
46-76
35-74
22-70
52-80
78-90
19-53
63-84
45-76
14-47
31-60
72-84
36-64
72-86
38-68
14-44
65-83
50-80
79-89
40-70
0-26
1MQ 2007 0.997-0.999 0.995-0.999 99-100
1MQ 2008 0.989-0.997 0.985-0.996 98-100
73-90
1MQ 2009 0.827-0.942
0.767-0.92
94-98
1MQ 2007 0.962-0.989 0.947-0.984
83-95
1MQ 2008 0.896-0.972 0.858-0.961
1MQ 2009 0.826-0.941 0.765-0.919
73-90
19-42
Medical 2011 0.368-0.598 0.246-0.486
0.74-0.833 0.656-0.774
60-74

0.02-0.28 0.01-0.03
0.03-0.72 0.01-0.06
0.23-1.38 0.03-0.08
0.08-0.62 0.02-0.05
0.05-0.29 0.01-0.03
0.08-0.38 0.02-0.04
0.02-0.3 0.01-0.03
0.17-0.9 0.02-0.06
0.21-1.22 0.03-0.08
0.41-3.24 0.04-0.13
0.27-2.73 0.03-0.12
0.27-4.81 0.03-0.17
0.07-0.59 0.01-0.05
0-10 22.8-64.4 7.89-47.06 0.23-0.64
0.1-0.76 0.02-0.06
1.8-5.8
0.05-0.66 0.01-0.05
1.2-5.4
0.06-1.4 0.01-0.08
1.3-8.3
0.09-3.35 0.02-0.14
1.7-13.8
0.03-0.44 0.01-0.04
0.9-4.2
0-0.01
0.3-1.1
0.42-3.93 0.04-0.15
4.1-15.1
0.02-0.18 0.01-0.02
0.6-2.5
0.05-0.71 0.01-0.05
1.2-5.6
0.62-5.69 0.05-0.19
5.2-18.8
0.25-1.78 0.03-0.09
3.1-9.6
0.02-0.08 0.01-0.02
0.6-1.6
0.17-1.32 0.02-0.08
2.5-8
0.01-0.08
0-0.02
0.5-1.6
0.12-1.19 0.02-0.07
2-7.5
0.8-5.52 0.06-0.18
6-18.5
0.02-0.16 0.01-0.02
0.7-2.4
0.03-0.52 0.01-0.05
0.9-4.7
0.01-0.03
0-0.01
0.3-1
1.7-6.8
0.09-1 0.02-0.07
11.4-56 2.41-37.02 0.11-0.56
0-0
0-0
0-0.01
0-0
0-0.01
0-0.01
0.88-4.08 0.06-0.15
0.07-0.24 0.01-0.03

0-0
0-0
0.3-1.5
0-0.1
0.1-0.7
0.3-1.5
6.3-15.5
1.4-3

0-0
0-0
0-0.07
0-0
0-0.02
0-0.08

Microblog 2011

0.316-0.61

0.86-0.941

0-0.04

1-4

5-18

ˆǫ
6-25 0.001-0.029
6-25
0.001-0.03
18-42 0.013-0.112
7-20 0.001-0.017
8-20 0.001-0.017
13-31 0.006-0.054
0-0.014
10-24 0.003-0.028
12-31 0.005-0.051
17-44 0.011-0.122
13-39 0.006-0.095
0.003-0.08
10-37
5-16 0.001-0.009
41-82
0.108-0.6
10-26 0.003-0.034
4-16
0-0.009
5-23 0.001-0.025
7-37 0.001-0.081
6-24 0.001-0.026
0-0
63-87 0.309-0.709
3-13
0-0.006
7-27 0.001-0.035
32-67 0.055-0.358
27-53 0.036-0.204
7-15 0.001-0.008
13-34 0.005-0.066
0-0.004
15-41 0.008-0.103
33-65
0.06-0.336
11-29 0.004-0.043
0-0.017
0-0
7-23 0.001-0.025
41-86 0.104-0.683
0-0
0-0
0-0.002
0-0
0-0
0-0.001
28-57 0.039-0.246
7-17 0.001-0.011

0-0
0-1
2-8
0-1
0-2
1-6

5-20
2-4

4-12

ˆn′
Eρ2(.95)
37-114
47-169
106-233
69-161
58-117
69-130
40-118
92-190
102-220
135-354
107-311
112-438
65-154
585-2862
112-264
58-166
61-234
73-360
44-136
21-52

ˆn′
Φ(.95)
130-487
116-484
348-999
136-381
150-389
257-662
102-355
189-484
236-640
327-1058
247-868
188-819
106-292
980-5631
288-791
93-301
107-457
149-826
124-457
33-94
135-392 2203-8579
78-250
35-95
146-536
56-171
657-2528
158-472
218-525 1087-3043
693-1428
175-336
242-733
94-227
77-220
30-68
279-947
80-217
181-474
702-2406
657-1761
111-269
100-383
46-149
39-93
24-48
73-200
143-459
335-2277 1053-8458
30-104
81-313
534-1756
196-685
156-616
352-1156
383-1208
141-315

11-38
16-59
219-710
88-304
107-421
194-628
129-273
62-105

Table 2: Predicted reliability of all 43 TREC collections analyzed. All conﬁdence intervals are based on the
ﬁts from Figure 3 at the endpoints of the 95% conﬁdence intervals computed with equations (10) and (11).

not reliable, such as the Web Distillation 2003, Genomics Ad
Hoc 2005, Terabyte 2006, Enterprise Expert Search 2008, or
the very recent Medical 2011 and Web Ad Hoc 2011. Re-
garding the expected RMS Error of absolute scores, we can
see that collections are somewhat stable, but with clear ex-
ceptions such as Web Distillation 2003, Novelty 2004 and
Enterprise Expert Search 2008.

The last two columns in Table 2 report intervals on the
number of queries, as per equations (12) and (13), required
to achieve 0.95 stability. In general the number of queries
needs to be at least doubled, and in many cases a few hun-
dred queries seem to be needed. This is particularly interest-
ing for the most recent collections, such as Web Ad Hoc 2010
and 2011, Medical 2011 and Microblog 2011, which stick to
the traditional size of 50 queries but need about 200. What
becomes clear from these ﬁgures is that the ideal size of a
collection depends greatly on the task it will be used for,
and thus it is not appropriate to ﬁx some acceptable size
such as 50 or 100 throughout tasks. Each task has diﬀerent
characteristics and should be analyzed accordingly.

6. CONCLUSIONS

In this paper we discussed the measurement of test col-
lection reliability from the perspective of traditional data-
based methodologies and of Generalizability Theory. GT
is regarded as a more appropriate, easy to use, and power-
ful method to assess reliability, but it has two drawbacks.
First, we showed that GT is very sensitive to the particular
sample of systems and queries used to estimate reliability of
a larger query set. We showed that about 50 systems and
50 queries are needed for robust estimates of collection re-
liability. Therefore, researchers should be cautious in using
GT when building new collections from scratch. To account
for all this variability we discussed a more robust approach
based on interval estimates of the stability indicators, which
helps in making more appropriate decisions regarding num-
ber of queries or diﬀerent structure in the experimental de-
sign. Second, we empirically established a mapping between
GT-based and traditional data-based indicators to help in-
terpreting results from GT which, otherwise, do not have a

401Relative Stability

Variability due to Systems

Variability due to Queries

Ad Hoc
Web adhoc
Web distillation
Web diversity
Novelty
Genomics
Robust
Terabyte
Enterprise
1MQ MTC
1MQ statAP
Medical
Microblog

Linear trend

0

.

1

9

.

0

8

.

0

7

.

0

6

.

0

5

.

0

2
^
E

)
l
a

t

o

t
 
f

 

o
%

(
 

s2
^

5
2

0
2

5
1

0
1

5

0

)
l
a

t

o

t
 
f

 

o
%

(
 

q2
^

0
9

0
8

0
7

0
6

0
5

0
4

0
3

1995

2000

2005

2010

1995

2000

2005

2010

1995

2000

2005

2010

Year

Year

Year

Figure 4: Historical trend of relative stability (left), variability due to systems (middle) and to queries (right).

clear and easily understandable meaning. Based on these
results, we reviewed the reliability of 43 TREC test collec-
tions, evidencing that some of them are very little reliable.
We show that the traditional choice of 50 queries is clearly
not enough even for stable rankings, and in most cases a
couple hundred queries are needed. Our results also show
that the ideal query set size varies signiﬁcantly across tasks,
suggesting that we avoid the use of some ﬁxed size such as 50
or 100 and that we analyze tasks and collections separately.
There are two clear lines for future research. First, we
completely ignored the assessor facet in our study. It is ev-
ident that diﬀerent assessors provide diﬀerent results, so it
would be interesting to include them in the analysis. Sec-
ond, although we ﬁtted the theoretically correct models, it
is clear that they can be improved (see for instance Power
and RMS Error in Figure 3).
IR evaluation experiments
generally violate assumptions of GT, such as normality of
distributions and random sampling, so diﬀerent models and
features to better ﬁt the actual data should be investigated.
We created some scripts for the statistical software R that
can help researchers perform all these computations to easily
assess the reliability of custom test collection designs. They
can be downloaded from http://julian-urbano.info.

7. REFERENCES
[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and

E. Kanoulas. Million Query Track 2008 Overview. In
Text REtrieval Conference, 2008.

[2] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu,

B. Dachev, and E. Kanoulas. Million Query Track
2007 Overview. In Text REtrieval Conference, 2007.

[3] C. Arteaga, S. Jeyaratnam, and G. A. Franklin.

Conﬁdence Intervals for Proportions of Total Variance
in the Two-Way Cross Component of Variance Model.
Communications in Statistics: Theory and Methods,
11(15):1643–1658, 1982.

[4] D. Banks, P. Over, and N.-F. Zhang. Blind Men and

Elephants: Six Approaches to TREC data.
Information Retrieval, 1(1-2):7–34, 1999.

[5] D. Bodoﬀ. Test Theory for Evaluating Reliability of

IR Test Collections. Information Processing and
Management, 44(3):1117–1145, 2008.

[6] D. Bodoﬀ and P. Li. Test Theory for Assessing IR Test

Collections. In ACM SIGIR, pages 367–374, 2007.
[7] R. L. Brennan. Generalizability Theory. Springer,

2001.

[8] C. Buckley and E. M. Voorhees. Evaluating Evaluation
Measure Stability. In ACM SIGIR, pages 33–34, 2000.

[9] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam,

and J. Allan. Evaluation Over Thousands of Queries.
In ACM SIGIR, pages 651–658, 2008.

[10] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam,
and J. Allan. If I Had a Million Queries. In ECIR,
pages 288–300, 2009.

[11] L. S. Feldt. The Approximate Sampling Distribution
of Kuder-Richardson Reliability Coeﬃcient Twenty.
Psychometrika, 30(3):357–370, 1965.

[12] E. Kanoulas and J. A. Aslam. Empirical Justiﬁcation

of the Gain and Discount Function for nDCG. In
ACM CIKM, pages 611–620, 2009.

[13] W.-H. Lin and A. Hauptmann. Revisiting the Eﬀect of

Topic Set Size on Retrieval Error. In ACM SIGIR,
pages 637–638, 2005.

[14] S. Robertson and E. Kanoulas. On Per-Topic Variance

in IR Evaluation. In ACM SIGIR, pages 891–900,
2012.

[15] T. Sakai. On the Reliability of Information Retrieval

Metrics Based on Graded Relevance. Information
Processing and Management, 43(2):531–548, 2007.
[16] M. Sanderson. Test Collection Based Evaluation of

Information Retrieval Systems. Foundations and
Trends in Information Retrieval, 4(4):247–375, 2010.

[17] M. Sanderson and J. Zobel. Information Retrieval

System Evaluation: Eﬀort, Sensitivity, and Reliability.
In ACM SIGIR, pages 162–169, 2005.

[18] R. J. Shavelson and N. M. Webb. Generalizability

Theory: A Primer. Sage Publications, 1991.

[19] J. Urbano, M. Marrero, and D. Mart´ın. A Comparison

of the Optimality of Statistical Signiﬁcance Tests for
Information Retrieval Evaluation. In ACM SIGIR,
2013.

[20] E. M. Voorhees. Variations in Relevance Judgments
and the Measurement of Retrieval Eﬀectiveness. In
ACM SIGIR, pages 315–323, 1998.

[21] E. M. Voorhees. Topic Set Size Redux. In ACM

SIGIR, pages 806–807, 2009.

[22] E. M. Voorhees and C. Buckley. The Eﬀect of Topic

Set Size on Retrieval Experiment Error. In ACM
SIGIR, pages 316–323, 2002.

[23] E. Yilmaz, J. A. Aslam, and S. Robertson. A New

Rank Correlation Coeﬃcient for Information
Retrieval. In ACM SIGIR, pages 587–594, 2008.

[24] J. Zobel. How Reliable are the Results of Large-Scale
Information Retrieval Experiments? In ACM SIGIR,
pages 307–314, 1998.

402r
s
s
