Collaborative Factorization for Recommender Systems

Chaosheng Fany∗
‡Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R.China

†School of Mathematical Sciences, Peking University, Beijing, P.R.China

, Yanyan Lanz, Jiafeng Guoz, Zuoquan Liny, Xueqi Chengz

fcs@pku.edu.cn, {lanyanyan,guojiafeng}@ict.ac.cn

lz@pku.edu.cn, cxq@ict.ac.cn

ABSTRACT
Recommender system has become an eﬀective tool for in-
formation ﬁltering, which usually provides the most useful
items to users by a top-k ranking list. Traditional recom-
mendation techniques such as Nearest Neighbors (NN) and
Matrix Factorization (MF) have been widely used in real rec-
ommender systems. However, neither approaches can well
accomplish recommendation task since that: (1) most N-
N methods leverage the neighbors’ behaviors for prediction,
which may suﬀer the severe data sparsity problem; (2) MF
methods are less sensitive to sparsity, but neighbors’ inﬂu-
ences on latent factors are not fully explored, since the latent
factors are often used independently. To overcome the above
problems, we propose a new framework for recommender
systems, called collaborative factorization. It expresses the
user as the combination of his own factors and those of the
neighbors’, called collaborative latent factors, and a ranking
loss is then utilized for optimization. The advantage of our
approach is that it can both enjoy the merits of NN and MF
methods. In this paper, we take the logistic loss in RankNet
and the likelihood loss in ListMLE as examples, and the
corresponding collaborative factorization methods are called
CoF-Net and CoF-MLE. Our experimental results on three
benchmark datasets show that they are more eﬀective than
several state-of-the-art recommendation methods.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
ﬁltering

Keywords
Recommender System, Nearest Neighbors, Matrix Factor-
ization, Learning to Rank
∗
The work was performed when the ﬁrst author was a visit-
ing student at Institute of Computing Technology, Chinese
Academy of Sciences.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

Recently, recommender system has become an eﬀective
tool for information ﬁltering, and has played an importan-
t role in many popular web services, such as Amazon, Y-
ouTube, Netﬂix, Yahoo! etc. In most applications, the core
task of a recommender system is to well predict the utili-
ties of diﬀerent items, and then return to users top-k useful
items by a ranking list.

In literature, the main recommendation techniques can
be divided into two categories, i.e. Nearest Neighbors (NN)
and Matrix Factorization (MF). Nearest neighbors methods
[2], leverage user’s neighbors behavior for recommendation.
However, this approach may fail as the severe data sparsity
in recommendation makes the computation of of neighbors
not accurate any more. Compared with NN, MF methods,
such as SVD [9] and SVD++ [4], adopt the latent factors
model to uncover the interests of a user, thus it is less sensi-
tive to data sparsity. However, the latent factors of diﬀerent
users are usually used independently, and the neighbors’ in-
ﬂuences are rarely explored.

In order to take the advantages of both techniques, we
propose a new framework called collaborative factorization
(CoF for short). The central idea is to introduce the col-
laborative philosophy into matrix factorization by deﬁning
the latent factors of a user as the combination of his own
factors and those of his neighbors’ in traditional MF, called
collaborative latent factors. This is also in accordance with
the intuition that when making a decision, the user’s own
opinion and his/her friends’ suggestions may both play an
important role.

In this framework, we treat recommendation as a ranking
problem, which is more consistent with the typical output
of recommender systems (i.e. a ranking list of most useful
items). A ranking loss deﬁned on the basis of the collab-
orative latent factors is then utilized for optimization. To
avoid overﬁtting, regularizations are further taken in CoF.
In this paper, we use both pairwise and listwise losses s-
ince they are two dominant ranking losses in the literature
of learning to rank. Speciﬁcally, we use the logistic loss in
RankNet [5] and the likelihood loss in ListMLE [6], and the
corresponding methods are called CoF-Net and CoF-MLE,
respectively.

Finally, we conduct extensive experiments on the dataset-
s of Movielens and Yahoo!Rand. The experimental results
show that our proposed CoF-Net and CoF-MLE can signif-
icantly outperform several state-of-the-art recommendation
methods, including NN, MF and the Hybrid methods.

To sum up, the contribution of this paper lies in that:

949(1) The introduction of collaborative latent factors, which
can be viewed as a novel approach to leverage collaborative
philosophy in matrix factorization;

(2) The proposal of a uniﬁed framework to solve the rec-
ommendation problem, which is convenient to accommodate
any form of ranking losses.

The rest of the paper is organized as follows. In Section 2
we brieﬂy discuss related work. The collaborative factoriza-
tion framework is presented in Section 3. The experimental
results are discussed in Section 4. We summarize our work
in the last section.

2. RELATED WORK

Nearest Neighbors [2] is a classical recommendation ap-
proach. It leverages users’ neighbors who have similar rating
behavior for prediction. However, the rating data is often
highly sparse, which makes the chosen neighbors unreliable.
Matrix factorization [1, 4, 8, 9, 10, 11] is another popular
recommendation technique. It explains ratings as the inner
product of the latent factors of items and users. The usage
of low rank latent factors makes it less sensitive to sparsity,
but the neighbors’ inﬂuences are highly ignored.

There are also some work trying to combine the two tech-
niques to enhance recommendation, such as re-ranking mod-
el and hybrid model [4, 12]. For example, Koren et al.
[4] proposed to directly combine matrix factorization mod-
el and item-based neighborhood model from algorithm-level
to make more accurate model. Our work is diﬀerent from
their methods: (1) they aim at minimizing rating predic-
tion error, while we use ranking based loss; (2) they do not
fully explore neighbors’ inﬂuences, while we introduce the
collaborative idea in expressing user latent factors.

Please note that Ma etc.

[7] conduct similar user repre-
sentation, however, the objective function in their work is
regression-based loss, and the social connections are explic-
itly given. Compared with their approach, our setting is
more general and the motivation is diﬀerent.

3. OUR APPROACH

Our approach uses latent factors model (i.e. matrix factor-
ization) as its centric form. A latent factors model for rec-
ommendation can be formalized in the following way. Given
a set of m users U, a set of n items I, and an observed rating
matrix R, one aims to learn two low rank matrixes P and Q
for users and items respectively by optimizing the following
objective function:

L(R; P; Q)+(cid:21)P∥P∥2

F +(cid:21)Q∥Q∥2
F ;

min
P;Q

(1)
where L(R; P; Q) is the loss function, P is a k × m matrix,
Q is a k × n matrix, and k is the dimension of latent fac-
tors. ∥P∥2
F are regularization items to overcome
overﬁtting and (cid:21)P and (cid:21)Q are parameters. The idea behind
latent factors model is that ratings can be explained as the
inner product of the latent factors of items and users.

F and ∥Q∥2

In traditional matrix factorization [9], recommendation
task is viewed as a regression problem where one aims to ap-
proximate the observed rating matrix R as the inner product
of matrix P and Q. Therefore, the loss function takes the
form of mean square error as following
(Rui − pT

L(R; P; Q) =

∑

u qi)2:

(2)

u2U ;i2I

where column vector pu in P stands for the factors of user
u, and column vector qi stands for the factors of item i.

From the above process, we can see that the latent fac-
tors are used independently, and the neighbors’ inﬂuences is
ignored in MF methods.

In our approach, we show how to introduce collaborative
philosophy into MF and propose a novel framework for rec-
ommendation, namely collaborative factorization. Here we
ﬁrst introduce the collaborative latent factors.
3.1 Collaborative Latent Factors

Inspired by the intuition that when making a decision,
the user’s own opinion and his friends’ suggestions are both
taken into consideration, we propose to model the latent
factors of a user as the combination of his/her own factors
and those of his/her neighbors’, as deﬁned as follows, called
collaborative latent factors. The formalization is as follows

˜pu = (cid:11)pu +(1 − (cid:11))

Suv

w2F (u) Suw

pv;

(3)

∑

∑

v2F (u)

where (cid:11) is the parameter to show the trade-oﬀ between the
user’s own interest and his neighbors’ inﬂuences, F (u) stand-
s for the set of neighbors of user u, and Suv (Suw) stands
for the similarity between user u and v (w). Any user-user
similarity metric can be used in the above deﬁnition.

With the collaborative latent factors, ratings Rui can be
represented as the inner product of the corresponding fac-
tors, formulated as follows.

∑

∑

v2F (u)

Rui = ˜pT

u qi = ((cid:11)pT

u + (1 − (cid:11))

Suv

w2F (u) Suw

pT
v )qi:

(4)

3.2 Collaborative Factorization

With the collaborative latent factors deﬁned above, we

∑

now formalize our collaborative factorization framework.
Since the typical output of a recommender system is usual-
ly a top-k ranking list, we naturally treat recommendation
as a ranking problem. Therefore, the goal of collaborative
factorization is to minimize the following subject:

L(Ru1;··· ; Run)+(cid:21)P∥P∥2

F +(cid:21)Q∥Q∥2
F :

u2U

min
P;Q

(5)
where L(Ru1;··· ; Run) is a ranking loss of user u’s ratings
on item set I, and the other notations are the same as men-
tioned before.

We can see that the proposed collaborative factorization
is a general framework which can accommodate any kind
of ranking losses. In this paper, we adopt the pairwise and
listwise losses, which are two dominant ranking losses in the
literature of learning to rank. We call the corresponding
approach as pairwise collaborative factorization and listwise
collaborative factorization.
3.2.1 Pairwise Collaborative Factorization
In pairwise collaborative factorization, a pairwise loss Lp
is used, and the ranking loss on the item sets are deﬁned as
the sum of losses on all the pairs:

L(Ru1;··· ; Run) =

Lp(Rui; Ruj):

(6)

∑

(i;j)2Du

where Du stands for the pairs constructed from user’s rating
proﬁle. For example, if a user has rated item i higher than
j, then the pair (i; j) is included in Du.

950Any pairwise ranking loss in learning to rank can be used
in Eq. (6), such as the hinge loss in RankSVM, the expo-
nential loss in RankBoost, and the logistic loss in RankNet.
In this paper, we take the logistic loss in RankNet as an ex-
ample, and obtain the following method, named CoF-Net.

1

1 + exp{−(Rui − Ruj)} ):

(7)

∑

∑

−

log(

(i;j)2Du

u2U

3.2.2 Listwise Collaborative Factorization
In listwise collaborative factorization, a listwise loss Ll is
used, and the ranking loss on the item sets are deﬁned as
the loss on the generated list:

L(Rui;··· ; Run) = Ll(Rui;··· ; Run; (cid:27)u):

(8)

where (cid:27)u stands for the permutation generated from user’s
rating proﬁle. For example, if a user has rated item i higher
than j, and j higher than k, then the generated permutation
is (i; j; k).

Any listwise ranking loss in learning to rank can be used in
Eq. (8), such as the likelihood loss in ListMLE, the entropy
loss in ListNet, and the cosine loss in RankCosine. In this
paper, we take the likelihood loss in ListMLE as an example,
and obtain the following method, named CoF-MLE.

∑

jRuj(cid:0)1∑

log

u2U

i=1

−

(
∑jRuj
exp{Ru(cid:27)(cid:0)1(i)
}
j=i exp{Ru(cid:27)(cid:0)1(j)

)

:

}

(9)

(cid:0)1(i) represents the item ranked in position i of (cid:27),

where (cid:27)
and |Ru| is the length of user u’s rating proﬁle.

For optimization, we just use stochastic gradient descent
in this paper. We omit the details of algorithm iteration
framework due to the lack of space and the reader can refer
to [8, 10] for similar description.

4. EXPERIMENT

In this section, we conduct extensive experiments on bench-
mark datasets to show the superiority of our proposed col-
laborative factorization methods.
4.1 Experimental Settings

Datasets: We use three benchmark datasets for experi-

ments: Movielens100K, Movielens1M1 and Yahoo!Rand2.

The Movielens100K dataset consists of about 100 K rat-
ings made by 943 users on 1628 movies. The Movielens1M
dataset consists of about 1 M ratings made by 6040 users on
3706 movies. Following the experimental strategies used in
[10, 13], for each user, we sample N ratings for training, and
sample 10 ratings from training set to tune hyper parame-
ters (validation set). After that, we ﬁx the hyper parameters
and the validation set are added to the original training set,
then we retrain the model. In our experiments, we set N as
20 and 50, and users with less than 30 and 60 ratings are
removed to ensure that NDCG@10 can be obtained.

The Yahoo!Rand dataset is a much newer dataset. It con-
tains ratings for songs collected from two diﬀerent sources.
The ﬁrst one consists of 300,000 ratings supplied by 15,400
users with at least 10 existing ratings during normal inter-
action with Yahoo! Music services between 2002 and 2006.
The second one consists of ratings for randomly selected

1http://www.grouplens.org/node/73
2http://webscope.sandbox.yahoo.com/

songs collected during an on-line survey conducted by Ya-
hoo! Research and hosted by Yahoo! Music between August
22, 2006 and September 7, 2006. 5400 Participants were
asked to rate 10 songs selected at random from a ﬁxed set of
1000 songs. All the ratings from the ﬁrst source are used for
training, and ratings from the second source is used for test-
ing. The experiments on this dataset are more convincing,
since the test data are collected on random sampled songs.
Baseline Methods: Besides the typical collaborative
ﬁltering methods such as user-based K-Nearest-Neighbors
(KNN) [2] and matrix factorization methods aim to mini-
mize RMSE (Root Mean Square Error), e.g. Probabilistic
Matrix Factorization (PMF) [9], we also use state-of-the-art
ranking based methods including BPR [8] and ListMLE-
MF [11, 10] as baselines. Furthermore, we also conduct the
experiments on a hybrid method (HYBRID) (Equation 16
in [4]). Here we modify the HYBRID method by learning
user-user weight (similarity) other than the original item-
item weight, since we focus on investigating the user-wise
neighborhood eﬀect throught this paper.

We ﬁnd that further precision improvements can be achi-
eved by extending global average rating, user rating bias,
item rating bias, item-based implicit feedbacks and item-
based neighbors information to our framework (we verify
this in separated experiments not reported here). But we
omit these terms like most of the collaborative ranking meth-
ods [1, 8, 9, 10, 11, 13, 14] to keep fairly comparison with
our baselines.

(cid:0)3; 10

(cid:0)1; 10

(cid:0)2; 10

(cid:0)4; 10

Hyper Parameters: Latent factors dimension is ﬁxed as
5 in all methods. Learning rate and regularization values are
chosen from {10
(cid:0)5} for every model.
The size of nearest neighbors (chosen by Pearson correlation
coeﬃcient) is set to 100 in HYBRID. For CoF-Net and CoF-
MLE, we choose neighbors by cosine similarity and set the
nearest neighbors size as 50 which makes them work very
well. We don’t investigate the eﬀect of other neighborhood
size and other similarity in detail due to the page limitation.
The best value of combination coeﬃcient (cid:11) is 0.15 for CoF-
Net, and 0.65 for CoF-MLE.

Evaluation Measures: The traditional evaluation mea-
sure RMSE places equal emphasis on high rating and low
rating. Therefore, it is not suitable for the top-k recom-
mendation scenario.
In this paper, we choose Normalized
Discount Cumulative Gain (NDCG) [3] as the evaluation
measure, which is a popular evaluation metric in the com-
munity of information retrieval. NDCG can leverage the
relevance judgment in terms of multiple ordered categories,
and has an explicit position discount factors in its deﬁnition
as follows:

K∑

1

u;(cid:25)(cid:0)1(p) − 1
2R
log2(p + 1)

;

p=1

NK (u)

NDCG(u; (cid:25))@K =

(10)
where (cid:25) : {1; 2; :::; N} → {1; 2; :::; N} is a permutation gen-
erated in the descending order of the predicted ratings, and
(cid:0)1(p) stands for the item that is ranked in position p of
(cid:25)
the ranking list (cid:25). NK (u) is the normalization factors. We
set K to 10 like in [1, 10, 14], and we report the average
NDCG@10 over all users in our experiment.

4.2 Experimental Results

The experiment results are listed in Table 1. From the

results, we can see that:

951Dataset
Algorithm

KNN
PMF

HYBRID

ListMLE-MF

BPR

CoF-MLE
CoF-Net

Table 1: Ranking Performance

Movielens100K

Movielens1M

N = 20
0.640
0.681
0.710
0.707
0.710
0.712
0.722

N = 50
0.675
0.691
0.714
0.711
0.714
0.715
0.726

N = 20
0.643
0.729
0.750
0.741
0.742
0.752
0.756

N = 50
0.657
0.737
0.754
0.748
0.755
0.757
0.775

Yahoo!Rand

jTest Itemsj = 10

0.802
0.823
0.830
0.825
0.835
0.828
0.849

(1) Models combining MF and NN will obtain enhanced
performances. For example, on Yahoo!Rand, the NDCG@10
of KNN, PMF and HYBRID is 0.802, 0.823, and 0.830,
respectively. KNN obtains the lowest performance due to
the unreliable neighbors computation on sparse data. PM-
F works better since it employ dimensionality reduction to
relieve sparsity. By combining MF and NN, HYBRID ob-
tains even better performance. Similar results can be found
in the ranking based methods. By introducing the collab-
orative latent factors, CoF-MLE consistently outperforms
ListMLE-MF (about 1%) and CoF-Net consistently outper-
forms BPR (about 2%).

(2) RMSE minimization is not optimal for top-k recom-
mendations. From the results in Table 1, we can see that
models which aim at minimizing rating prediction error per-
form worse than the ranking based models. For example,
PMF obtains lower NDCG@10 scores than all of our rank-
ing based CoF methods (i.e. CoF-MLE, CoF-Net, BPR,
LISTMLE-MF).

(3) As the methods combining MF and NN, the proposed
collaborative factorization (CoF) outperforms the tradition-
al hybrid methods (HYBRID). From Table 1, CoF-Net is the
best model and it signiﬁcantly outperform HYBRID (about
2%). We can see that by introducing the collaborative idea
to the representation of user latent factors, the neighbors’
eﬀect are better explored in CoF than HYBRID, which di-
rectly combines MF and NN from algorithm-level.
5. CONCLUSION

In this paper, we propose a new recommendation frame-
work to enjoy both the merits of nearest neighbors and ma-
trix factorization, called collaborative factorization. Firstly,
collaborative latent factors are deﬁned as the combination
of the user’s own factors and those of his neighbors’. Sec-
ondly, a ranking loss is utilized as the objective for top-k
recommendation. Our experiments on several benchmark
datasets show that our approach can signiﬁcantly outper-
form the traditional methods.

As for future work, we can further study whether there
exists other ways to introduce collaborative idea to matrix
factorization.
Acknowledgements
This research work was funded by the National Natural
Science Foundation of China under Grant No. 61232010,
No. 61203298, No. 61003166, No. 60973003, 863 Program of
China under Grants No. 2012AA011003, and National Key
Technology R&D Program under Grant No. 2011BAH11B02,
No. 2012BAH39B02, No. 2012BAH39B04. We also wish to
express our thanks to Yahoo!Research for providing the Ya-
hoo!Rand dataset to us.

6. REFERENCES
[1] S. Balakrishnan and S. Chopra. Collaborative ranking.

In Proc. of WSDM ’12, pages 143–152, New York,
USA, 2012.

[2] M. Deshpande and G. Karypis. Item-based top-n

recommendation algorithms. ACM Trans. Inf. Syst.,
22(1):143–177, Jan. 2004.

[3] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, Oct. 2002.

[4] Y. Koren. Factor in the neighbors: Scalable and

accurate collaborative ﬁltering. ACM Trans. Knowl.
Discov. Data, 4(1):1:1–1:24, Jan. 2010.

[5] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M.

Deeds, N. Hamilton and G. Hullender, Learning to
rank using gradient descent, In Proc. of ICML ’05,
pages 89–96, 2005.

[6] F. Xia, Tie-Yan Liu, J. Wang, W. S. Zhang and H. Li,
Listwise Approach to Learning to Rank - Theory and
Algorithm, In Proc. of ICML ’08, pages 1192–1199,
2008.

[7] H. Ma, I. King, and M. R. Lyu. Learning to

recommend with social trust ensemble. In Proc. of
SIGIR ’09, pages 203–210, New York, USA, 2009.

[8] S. Rendle, C. Freudenthaler, Z. Gantner, and

L. Schmidt-Thieme. BPR: Bayesian personalized
ranking from implicit feedback. In Proc. of UAI ’09,
pages 452–461, Arlington, USA, 2009.

[9] R. Salakhutdinov and A. Mnih. Probabilistic matrix

factorization. In Proc. of NIPS ’07, 2007.

[10] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning

to rank with matrix factorization for collaborative
ﬁltering. In Proc. of RecSys ’10, pages 269–272, New
York, USA, 2010.

[11] T. Tran, D. Q. Phung, and S. Venkatesh. Learning
from ordered sets and applications in collaborative
ranking. Journal of Machine Learning Research -
Proceedings Track, 25:427–442, 2012.

[12] Gideon Dror, Noam Koenigstein, Yehuda Koren,
Markus Weimer The Yahoo! Music Dataset and
KDD-Cup11. Journal of Machine Learning Research -
Proceedings Track, 18:318, 2012.

[13] M. N. Volkovs and R. S. Zemel. Collaborative ranking

with 17 parameters. In Proc. of NIPS ’12, 2012.

[14] M. Weimer, A. Karatzoglou, and M. Bruch. Maximum
margin matrix factorization for code recommendation.
In Proc. of RecSys ’09, pages 309–312, New York,
USA, 2009.

952