SIGIR 2013 Workshop on Modeling User Behavior for

Information Retrieval Evaluation

Charles L. A. Clarke1, Luanne Freund2, Mark D. Smucker3, and Emine Yilmaz4

1School of Computer Science, University of Waterloo, Canada

2School of Library, Archival and Information Studies, University of British Columbia, Canada

3Department of Management Sciences, University of Waterloo, Canada

4Department of Computer Science, University College London, UK

ABSTRACT
The SIGIR 2013 Workshop on Modeling User Behavior for
Information Retrieval Evaluation (MUBE 2013) brings to-
gether people to discuss existing and new approaches, ways
to collaborate, and other ideas and issues involved in improv-
ing information retrieval evaluation through the modeling of
user behavior.
Categories and Subject Descriptors: H.3.4 [Informa-
tion Storage and Retrieval]: Systems and Software — Per-
formance evaluation (eﬃciency and eﬀectiveness)

Keywords: Information retrieval, search evaluation, user
models

1. OVERVIEW

Information retrieval (IR) evaluation is concerned with
producing accurate estimates of the performance of retrieval
systems. Excluding in-situ studies of user behavior, IR eval-
uation is conducted via simulation of the usage of a retrieval
system. Oﬀering the best ﬁdelity to reality, laboratory-based
user studies provide simulated search tasks to users and then
measure outcomes, such as user satisfaction or success. Un-
fortunately, user studies can be costly and diﬃcult to scale
to the large number of evaluations required during the devel-
opment of new retrieval systems. Cranﬁeld-style evaluation
retains the simulated search tasks of laboratory-based user
studies, but replaces the user with a model of user behavior
and recorded user relevance judgments. After the initial cost
of obtaining the relevance judgments, Cranﬁeld-style evalu-
ation has a very low cost and scales easily. The quality of the
eﬀectiveness estimates produced by Cranﬁeld-style evalua-
tion is limited by the user model. The better the user model,
the closer Cranﬁeld-style evaluation is to the standard of the
laboratory-based user study.

During the past decade, there has been an increasing body
of research on ways to incorporate models of user behavior

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

into Cranﬁeld-style eﬀectiveness measures or to understand
existing metrics in terms of user models. At the same time,
another body of research has focused on the simulation of
user interaction with IR user interfaces to predict the value
of new user interfaces to retrieval systems. Researchers have
also used simulation of user behavior to better understand
information retrieval. Space limitations prevent us from cit-
ing all applicable work here; for a good starting point to
exploring the relevant literature, please see the references in
the following recent papers: [1, 2, 3].

These varied approaches to the evaluation of IR systems
all rely on the quality of their user models. The better the
user models, the better the predictions made by the evalua-
tion measures.

While there is some overlap between the researchers pur-
suing these diﬀerent approaches, we believe that it makes
sense to bring these groups together to enable fruitful cross-
pollination of ideas and methods. This workshop brings to-
gether people interested in discussing new approaches and
new ways to collaborate and create shared resources for the
development and application of user models for information
retrieval evaluation.

The workshop solicited two-page poster papers, which were
reviewed by a program committee. Accepted papers will be
presented during boaster and poster sessions.
In addition
to the accepted papers, the tentative schedule for the work-
shop includes invited speakers, breakout groups, and plenty
of time for discussion and presentation of breakout group
reports.

Acknowledgments
This work was supported in part by the Natural Sciences
and Engineering Research Council of Canada (NSERC), in
part by GRAND NCE, and in part by the University of
Waterloo.

References
[1] L. Azzopardi, K. J¨arvelin, J. Kamps, and M. D. Smucker.
Report on the SIGIR 2010 workshop on the simulation of
interaction. SIGIR Forum, 44:35–47, January 2011.

[2] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating sim-
In

ple user behavior for system eﬀectiveness evaluation.
CIKM’11, ACM, 2011.

[3] M. D. Smucker and C. L. A. Clarke. Time-based calibration

of eﬀectiveness measures. In SIGIR’12, ACM, 2012.

1134