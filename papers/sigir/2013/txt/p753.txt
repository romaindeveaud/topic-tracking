Report from the NTCIR-10 1CLICK-2 Japanese Subtask:

Baselines, Upperbounds and Evaluation Robustness

Makoto P. Kato

Kyoto University, Japan

kato@dl.kuis.kyoto-u.ac.jp

Tetsuya Sakai

Microsoft Research Asia, P.R.C.

tetsuyasakai@acm.org

Takehiro Yamamoto
Kyoto University, Japan

tyamamot@dl.kuis.kyoto-u.ac.jp

ABSTRACT
The One Click Access Task (1CLICK) of NTCIR requires systems
to return a concise multi-document summary of web pages in re-
sponse to a query which is assumed to have been submitted in a
mobile context. Systems are evaluated based on information units
(or iUnits), and are required to present important pieces of infor-
mation ﬁrst and to minimise the amount of text the user has to
read. Using the ofﬁcial Japanese results of the second round of
the 1CLICK task from NTCIR-10, we discuss our task setting and
evaluation framework. Our analyses show that: (1) Simple baseline
methods that leverage search engine snippets or Wikipedia are ef-
fective for “lookup” type queries but not necessarily for other query
types; (2) There is still a substantial gap between manual and auto-
matic runs; and (3) Our evaluation metrics are relatively robust to
the incompleteness of iUnits.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval
General Terms
Experimentation
Keywords
evaluation, information units, mobile environment, NTCIR, nuggets,
summaries, test collections

1.

INTRODUCTION

NTCIR (NII Testbeds and Community for Information access
Research) is a sesquiannual evaluation forum that focuses primarily
on Asian language information access. The One Click Access Task
(1CLICK) of NTCIR requires systems to return a concise multi-
document summary of web pages in response to a query which is
assumed to have been submitted in a mobile context1. Systems are
evaluated based on information units (or iUnits), and are required

1http://research.microsoft.com/1CLICK/

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Mayu Iwata

Osaka University, Japan

iwata.mayu@ist.osaka-u.ac.jp

to present important pieces of information ﬁrst and to minimise the
amount of text the user has to read. Compared to nugget-based
evaluations for summarisation and question answering, the task is
novel in that the position of each information unit is utilised to eval-
uate the system output. Using the ofﬁcial Japanese results of the
second round of the 1CLICK task from NTCIR-10, we discuss our
task setting and evaluation framework. Our analyses show that: (1)
Simple baseline methods that leverage search engine snippets or
Wikipedia are effective for “lookup” type queries but not necessar-
ily for other query types; (2) There is still a substantial gap between
manual and automatic runs; and (3) Our evaluation metrics are rel-
atively robust to the incompleteness of iUnits.

2. WHAT’S NEW AT 1CLICK-2

The 1CLICK task is deﬁned as: Given a query, return a textual
summary which presents as much relevant information as possible
within X characters, important pieces of information ﬁrst, while
minimising the amount of text the user has to read. For DESKTOP
runs, we let X = 500 (representing a few search engine snippets
shown on a desktop PC), while for MOBILE runs we let X = 140
(representing a short text like a “tweet” shown on a mobile phone
screen). For more details on the general task design of 1CLICK,
we refer the reader to the Overview of 1CLICK-1 [7].
The main new features of 1CLICK-2 are as follows:
• Based on a study on mobile query logs [3], 1CLICK-1 con-
sidered four query types: CELEBRITY, LOCAL, DEFINI-
TION and QA (question answering). However, after manu-
ally constructing iUnits for the 1CLICK-1 queries, we found
that more ﬁne-grained query types are necessary. For ex-
ample, a person looking for information on a politician and
a person looking for information on an actress probably want
different types of information even though they are both celebri-
ties. Therefore, at 1CLICK-2, we reﬁned the CELEBRITY
and LOCAL query types as follows: ARTIST, ACTOR, POLITI-
CIAN, ATHLETE, FACILITY and GEO2. Thus we have a
total of eight query types, which are shown below together
with their information need we assumed:

ARTIST, ACTOR, POLITICIAN, ATHLETE (10 each)

user wants important facts about celebrities;

FACILITY (15) user wants access and contact information

for a particular landmark, facility etc.;

2For Example, “Tokyo Sky Tree” is a FACILITY query, while
“Sushi bars near Tokyo Station” is a GEO query as it includes a
geographical constraint.

753GEO (15) user wants access and contact information for en-
tities with geographical constraints, e.g. sushi restau-
rants near Tokyo station;

DEFINITION (15) user wants to look up a phrase, etc.;
QA (15) user wants to know factual (but not necessarily fac-

toid) answers to a natural language question.

The number of queries for each type is shown in parentheses:
thus we have a total of 100 test queries. Furthermore, for
the four CELEBRITY query types, we included specialised
queries such as “jennifer gardner alias” as well as non-specialised
ones such as “ichiro suzuki” as we wanted encourage par-
ticipants to explore retrieval of highly focussed information
rather than to summarise generic information from (say) Wikipedia.

• We tried to deﬁne an iUnit clearly. An iUnit is a factual state-
ment, and usually satisﬁes the relevance (satisﬁes the infor-
mation need behind the query partially or wholly), atomicity
(cannot be further broken down into multiple iUnits), cred-
ibility (stated explicitly in at least one document) and tem-
poral validity (holds true as of the date speciﬁed, e.g. July
4, 2012 for 1CLICK-2) criteria. Table 1 provides a few ex-
amples translated from one of our Japanese sample queries:
here, iUnit I049 entails I050, but I049 is said to be atomic
as it cannot be broken down into multiple iUnits3. The vital
string column represents the minimal text required to con-
vey the semantics of the iUnit to the user, which is used for
computing our evaluation metrics. For example, without the
string “160cm” it would be difﬁcult for the system to tell
the user that Keiko Kitagawa is 160 centimeters tall. More-
over, as shown in the “w” column, each iUnit was assigned
a weight, which reﬂects the organisers’ votes obtained on a
ﬁve point scale.

• At 1CLICK-1, participating teams were allowed to use any
resources available, which made fair comparisons across sys-
tems difﬁcult. Therefore, at 1CLICK-2, we provided a set of
baseline search results to participants, which consists of top-
ranked Yahoo! API search results returned in response to
each query4, and asked them to produce textual output from
these search results only. Runs that followed this rule are
called mandatory runs. Following 1CLICK-1, we also al-
lowed oracle runs (runs that assume that the documents from
which the iUnits were extracted are known) and open runs
(runs that use any additional resource besides the above men-
tioned data, e.g., Wikipedia, their own search results, etc.).
• We introduced a length penalty to our evaluation scheme.
Figure 1 illustrates our ideas for evaluating the system out-
put which we refer to as the X-string [6]. As our goal is to
quickly satisfy the user’s information need, our S-measure, a
position-aware version of weighted recall, ranks System (b)
above System (a). Let I be the complete set of iUnits, M (⊆
I) be the set of iUnits found within an X-string, and w(i) be
the weight of i ∈ I. Then S is deﬁned as:

S-measure =

(cid:2)
(cid:2)

i∈M w(i) max(0, L − oﬀset(i))
i∈I w(i) max(0, L − oﬀset∗(i))

,

(1)

3However, in practice, we found it easier to construct iUnits by
allowing them to entail multiple iUnits.
4The number of documents per query k varied across queries as
some documents could not be downloaded. The average of k over
the query set is 390.

(a)

(b)

(c)

Nonrelevant
text

Relevant text

X’=300

Relevant text

Relevant text

Nonrelevant
text

X’=300

Nonrelevant
text

X’=500

Figure 1: Evaluating textual output at 1CLICK-2.

Table 1: Some translated examples of iUnits for an ACTOR
query: “Keiko Kitagawa”.

entails

I050

ID
I001
I004
I049
I050

semantics
height: 160cm
born in 1986
graduated from Meiji U. in 2009
graduated from Meiji U.

vital string
160cm
born 1986
2009
Meiji U. graduate

w
11
18
15
11

where oﬀset(i) is the offset position of i within the X-string,
while oﬀset∗(i) is the offset position of i within the Pseudo
Minimal Output, which is an “ideal” output used for normal-
isation [6]. Following a suggestion from previous work, we
primarily use L = 500, and use L = 250 for additional anal-
ysis. These settings correspond to giving the user 60 and 30
seconds to gather information, respectively [5].
In addition to S, we also used T-measure at 1CLICK-2, which
is a precision-like metric that takes into account the fact that
different pieces of information require different amount of
space within the mobile phone screen: note that the vital
strings vary in length in Table 1. Like precision, T ranks
System (c) higher than System (b) in Figure 1. Let X(cid:4)
be
the length of an X-string (not the length limit), and |v(i)|
be the vital string length of iUnit i ∈ M. T is deﬁned
as: T-measure =
. Furthermore, we use an F-
measure-like combination of S and T called S(cid:2) as the primary
metric for ranking participating systems at 1CLICK-25 [5].

i∈M |v(i)|

X(cid:3)

(cid:2)

The general ﬂow of the 1CLICK-2 task was as follows:
1. Organisers sampled queries from a mobile query log, and

manually constructed iUnits for each query;

2. Task participants produced an X-string for each query auto-

matically, and returned the results to the organisers;

3. Assessors used the iUnit matching interface [6] to identify

iUnits within the X-string and to record their positions6;

4. Organisers added some new iUnits to the gold standard data

based on feedback from the assessors;

5. Weighted recall, S, T and S(cid:2) were computed for each X-
string and these were averaged across the query set for each
submitted system.

We are currently exploring the possibility of semi-automating the
iUnit extraction and matching processes by following the method-
ology used in the 1CLICK-2 English subtask [2]. Even though
these attempts turned out to be successful to some extent, our cur-
rent approach of manually extracting and matching iUnits is a nec-
essary step to understand the problems of information access eval-
uation that goes beyond the evaluation of ranked document lists.
5S(cid:2) = (1+β2)T S
6Every X-string was assessed independently by two assessors so
that two sets of matched iUnits were obtained. In this study, we
deﬁne M (See Eq. 1) as the intersection of these two sets. For each
matched iUnit, we use the smaller of the two offset values obtained.

β2T +S , β = 10

754e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

overall

Figure 2: Mean S(cid:2)-measure performances (L = 500) at
1CLICK-2. The x axis represents runs sorted by Mean S(cid:2) with
the intersection iUnit match data.

3. OFFICIAL RESULTS AND DISCUSSIONS
Figure 2 shows the ofﬁcial mean S(cid:2)-measure performances (L =
500) of runs submitted to the 1CLICK-2 Japanese subtask. The
x axis represents run names sorted by mean S(cid:2). Figure 3 breaks
down Figure 2 by showing similar results per query type. For the
four CELEBRITY query types, we also show graphs for specialised
and non-specialised queries separately. Hereafter, we shall discuss
statistical signiﬁcance based on a randomised version of Tukey’s
Honestly Signiﬁcant Differences (HSD) test [1] at α = 0.05.

3.1 Performances of Baselines

It can be observed that the three “ORG” runs are the overall top
performers in Figure 2. These are actually simple baseline runs
submitted by the organisers’ team: ORG-J-D-MAND-1 is a DESK-
TOP mandatory run that outputs a concatenation of search engine
snippets from the baseline search results; ORG-J-D-MAND-2 is
a DESKTOP mandatory run that outputs the ﬁrst sentences of a
top-ranked Wikipedia article found in the baseline search results;
ORG-J-D-ORCL-3 is similar to ORGs-J-D-MAND-1 but uses the
sources of iUnits instead of the search results (an oracle run). These
three runs signiﬁcantly outperform the other runs, and are signif-
icantly indistinguishable from one another. Moreover, Figure 3
shows that these baseline runs outperform all participating runs
with the four CELEBRITY query types as well as DEFINITION,
while they are not as effective for FACILITY, GEO and QA. Fur-
thermore, the graphs for CELEBRITY (where the runs have been
sorted by the mean S(cid:2) over all CELEBRITY queries) reveals that
while the baseline runs are effective for the non-specialised queries,
they are not necessarily so for the specialised ones. Recall that
specialised CELEBRITY queries seek speciﬁc information about
a celebrity such as “michael jackson death.” Also, note that FA-
CILITY, GEO and QA queries also seek speciﬁc information, e.g.
contact information of restaurants, sentences that directly answer
the natural language questions, and so on.

In summary, the above results suggest that, while simple snippet-
based and Wikipedia-based approaches are effective for “lookup”
type queries (e.g. celebrity names and deﬁnitions), more sophis-
ticated techniques are required to satisfy the user for other query
types (e.g. queries that look for speciﬁc information).

3.2 Estimating Performance Upperbounds

We now address the following question: Given the 1CLICK eval-
uation framework, what is the performance upperbound?. To this
end, we hired four subjects and asked them to manually create an

CELEBRITY (ARTIST, ACTOR,
POLITICIAN, ATHLETE)
Specialised

Non-specialised

FACILITY

GEO

DEFINITION

QA

e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

e
r
u
s
a
e
m
-
#
S

e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

0.5

0.4

0.3

0.2

0.1

0

e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

Figure 3: Mean S(cid:2)-measure performances (L = 500) for each
query type at 1CLICK-2. The x axis represents runs sorted by
Mean S(cid:2) with the intersection iUnit match data.

X-string for 73 queries from the ofﬁcial query set7. They were al-
lowed to use any resources to formulate the X-strings: these four
MANUAL runs are therefore open runs, and were manually as-
sessed together with the submitted runs.

7We could not obtain manual X-string for the remaining 27 queries
as the query set had not been ﬁnalised at that time.

755e
r
u
s
a
e
m
-
#
S

0.5

0.4

0.3

0.2

0.1

0

overall

1.00

0.95

0.90

0.85

0.80

0.75

n
o
i
t
a
l
e
r
r
o
c
 
k
n
a
r
 
s
’
l
l
a
d
n
e
K

10

20

30

S#-measure (L=250)
S#-measure (L=500)
Weighted Recall

50

40
70
Reduction rate (%)

60

80

90

100

Figure 4: Comparison between MANUAL and submitted runs
in terms of mean S(cid:2)-measure (L = 500) over 73 queries. The
x axis represents runs sorted by Mean S(cid:2) with the intersection
iUnit match data.

Figure 4 shows the mean S(cid:2)-measure (L = 500) over the 73
queries for all runs including the MANUAL ones. It can be ob-
served that three of the four MANUAL runs far outperform the
submitted automatic runs: these three runs are statistically signiﬁ-
cantly better than the other runs, and are statistically indistinguish-
able from one another. These results suggest that there are a lot of
challenges for advancing the state-of-the-art of 1CLICK systems:
a highly effective 1CLICK system needs to (a) ﬁnd the right docu-
ments; (b) extract the right pieces from information from the doc-
uments; and (c) synthesise the extracted information to form an
understandable text. It should also be noted that S(cid:2) does not di-
rectly take into account the readability of text: in fact, all of the
runs were evaluated also in terms of readability (how easy it is for
the user to read and understand the text), and the MANUAL runs
outperformed the submitted runs in terms of this criterion as well.
In summary, the comparison with the MANUAL runs shows that
our task setting is challenging, and that there is a lot of room for
improvement for the automatic 1CLICK systems. We hope that the
future rounds of the 1CLICK task will help close the performance
gap.

3.3 Robustness of Evaluation Metrics

Evaluating 1CLICK systems requires much manpower: for the
1CLICK-2 task, the organisers manually extracted over 6,000 iU-
nits for the 100 queries in advance, and further added some new
iUnits based on assessors’ feedback. In this section we address the
following questions: Do we need to try to ﬁnd iUnits for a query
exhaustively? What happens to the system ranking if the sets of iU-
nits were substantially incomplete? To this end, we follow a prac-
tice from document retrieval evaluation for examining the effect of
incomplete relevance assessments (e.g. [4]): we randomly down-
sample from the ofﬁcial sets of iUnits and examine the changes in
the system ranking in terms of Kendall’s tau rank correlation.

Figure 5 shows the effect of downsampling the iUnits on the sys-
tem ranking for three of our evaluation metrics. It can be observed,
for example, that even if we only have 50% samples of the ofﬁ-
cial iUnit sets, the Kendall’s tau between the original system rank-
ing and the new ranking is around 0.90 (about seven pairs of runs
swapped) or higher for both S(cid:2) and weighted recall. Even with 10%
samples, the tau is above 0.80. These results suggest that our eval-
uation framework is fairly robust to the incompleteness of iUnits.
Since even randomly downsampled iUnits yield relatively reli-
able evaluation results, exploring (semi)automatic approaches to
iUnit extraction seems worthwhile. As we have mentioned earlier,

Figure 5: Reduction rate (x axis) vs. Kendall’s rank correlation
to the ranking based on a full iUnit set (y axis).

we are actually investigating how iUnit extraction and matching
can be semi-automated.

4. FUTURE DIRECTIONS

One Click Access is an ambitious task that aims to satisfy the
user immediately after he clicks the search button. Our analyses
with the ofﬁcial NTCIR-10 1CLICK-2 results showed that: (1)
Simple baseline methods that leverage search engine snippets or
Wikipedia are effective for “lookup” type queries but not necessar-
ily for other query types; (2) There is still a substantial gap between
manual and automatic runs; and (3) Our evaluation metrics are rel-
atively robust to the incompleteness of iUnits.

Our future work includes (a) Investigating the effect of removing
the “additional” iUnits (ones that were added after the run submis-
sions) and of “dependent” iUnits (those that are entailed by other
iUnits); (b) Semi-automating the iUnit extraction and matching
processes while ensuring their reliability; (c) User studies for in-
vestigating how our effectiveness metrics correlate with subjective
assessments; and (d) Evaluation with more realistic mobile infor-
mation needs that go beyond simple lookup (e.g. synthesising in-
formation from multiple sources).

5. ACKNOWLEDGEMENTS

We thank the NTCIR-10 1CLICK-2 participants for their effort
in producing the runs, and Lyn Zhu, Zeyong Xu, Yue Dai and
Sudong Chung for helping us access to the mobile query log.

This work has been supported in part by the project: Grants-in-
Aid for Scientiﬁc Research (No. 24240013) from MEXT of Japan.

6. REFERENCES
[1] B. Carterette. Multiple testing in statistical analysis of systems-based
information retrieval experiments. ACM TOIS, 30(1):4:1–4:34, 2012.
[2] M. Ekstrand-Abueg, V. Pavlu, M. P. Kato, T. Sakai, T. Yamamoto, and

M. Iwata. Exploring semi-automatic nugget extraction for japanese
one click access evaluation. In Proc. of ACM SIGIR 2013, to appear.
[3] J. Li, S. Huffman, and A. Tokuda. Good abandonment in mobile and
PC internet search. In Proc. of ACM SIGIR 2009, pages 43–50, 2009.
[4] T. Sakai and N. Kando. On information retrieval metrics designed for

evaluation with incomplete relevance assessments. Information
Retrieval, 11(5):447–470, 2008.

[5] T. Sakai and M. P. Kato. One click one revisited: Enhancing

evaluation based on information units. In Proc. of AIRS 2012 (LNCS
7675), pages 39–51, 2012.

[6] T. Sakai, M. P. Kato, and Y.-I. Song. Click the search button and be

happy: Evaluating direct and immediate information access. In Proc.
of ACM CIKM 2011, pages 621–630, 2011.

[7] T. Sakai, M. P. Kato, and Y.-I. Song. Overview of NTCIR-9 1CLICK.

In Proc. of NTCIR-9, pages 180–201, 2011.

756