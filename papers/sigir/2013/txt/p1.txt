Riding the Multimedia Big Data Wave

John R. Smith

Kitchawan Road

IBM T. J. Watson Research Center

Yorktown Heights, NY 10532 USA

jsmith@us.ibm.com

ABSTRACT
Across multiple generations of information technology that
have dealt with structured and unstructured data, the ex-
plosion of multimedia data is creating the biggest wave of
all. Huge volumes of multimedia – images, video and audio
are being generated and consumed daily. Currently, multi-
media makes up 60% of internet traﬃc, 70% of mobile phone
traﬃc and 70% of all available unstructured data. To give
speciﬁc examples, Web users are uploading 72 video-hours
to YouTube per minute, and on an average day, social media
users post 300 hundred million photos to Facebook. Con-
sumers using mobile phones and digital cameras are tak-
ing 500 billion photos per year, or 78 per person on the
planet [1]. Specialized domains are participating too. Medi-
cal institutions are acquiring one billion radiological images
per year, and cities are installing hundreds of millions of
video cameras worldwide for safety, security and law enforce-
ment. Industries across life sciences, petroleum exploration,
astronomy, insurance, retail and many others are faced with
huge and growing volumes of multimedia data.

Multimedia clearly is ”big data”. But, it it big data not
just because there is a lot of it. Multimedia is big data
because increasingly it is becoming a valuable source for in-
sights and information. Multimedia data can tell us about
things happening in the world, point out places, events or
topics of interest (memes), give clues about a person’s pref-
erences and even capture a rolling log of human history [1,
2]. However, the challenge with multimedia big data is that
images, video and audio require much more sophisticated al-
gorithms for content analysis than previous waves of struc-
tured and unstructured data. This is spurring on a tremen-
dous amount of research on eﬃcient and eﬀective techniques
for ”bridging the semantic gap” to enable large-scale multi-
media information extraction and retrieval [3, 4].

In this talk we present a perspective across multiple indus-
try problems, including safety and security, medical, Web,
social and mobile media, and motivate the need for large-
scale analysis and retrieval of multimedia data. We de-

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage, and that copies
bear this notice and the full citation on the ﬁrst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s). Copyright is held by the author/owner(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

scribe a multi-layer architecture that incorporates capabil-
ities for audio-visual feature extraction, machine learning
and semantic modeling and provides a powerful framework
for learning and classifying contents of multimedia data. We
discuss the role semantic ontologies for representing audio-
visual concepts and relationships, which are essential for
training semantic classiﬁers [5]. We discuss the importance
of using faceted classiﬁcation schemes in particular for or-
ganizing multimedia semantic concepts in order to achieve
eﬀective learning and retrieval [6]. We also show how train-
ing and scoring of multimedia semantics can be implemented
on big data distributed computing platforms to address both
massive-scale analysis and low-latency processing [7]. We
describe multiple eﬀorts at IBM on image and video analysis
and retrieval, including IBM Multimedia Analysis and Re-
trieval System (IMARS), and show recent results for semantic-
based classiﬁcation and retrieval. We conclude with future
directions for improving analysis of multimedia through in-
teractive and curriculum-based techniques for multimedia
semantics-based learning and retrieval.

Categories and Subject Descriptors
H.2.4 [Systems]: Multimedia databases; I.2.10 [Artiﬁcial
Intelligence]: Vision and Scene Understanding—Video anal-
ysis; I.2.4 [Artiﬁcial Intelligence]: Knowledge Represen-
tation Formalisms and Methods—Semantic networks; I.2.6
[Artiﬁcial Intelligence]: Learning—Concept learning

Keywords
Multimedia information retrieval, video analysis, content-
based search, machine learning, semantic modeling

Biography
Dr. John R. Smith is Senior Man-
ager, Intelligent Information Man-
agement Dept, IBM T. J. Wat-
son Research Center.
He re-
ceived his Ph.D., Electrical En-
gineering from Columbia Univer-
sity in 1997. He currently leads
IBM’s research in multimedia in-
formation retrieval
including im-
age and video content extraction,
multimedia content-based search,
video event detection and retrieval
and social media analysis. Dr. Smith is currently princi-
pal investigator for IBM Multimedia Analysis and Retrieval

1System (IMARS), which has been recognized by multiple
awards including a Wall St. Journal innovation award. Dr.
Smith is a long-time participant in the NIST TRECVID
video retrieval evaluation and co-led the development of the
Large Scale Concept Ontology for Multimedia (LSCOM),
which has been incorporated into the TRECVID evalua-
tion. From 2000-2004, Dr. Smith served as Chair, ISO/IEC
MPEG Multimedia Description Schemes Group and led the
development of multiple parts of the MPEG-7 Multime-
dia Metadata Standard and MPEG-21 Digital Framework
Standard. While a student with Prof. Shih-Fu Chang at
Columbia University, Dr. Smith conducted some of the earli-
est work on content-based image retrieval (VisualSEEk) and
Web image/video search (WebSEEk), which has been highly
inﬂuential for researchers and practitioners. Dr. Smith has
published more than two hundred papers (>14K citations,
h-index of 55, i-index of 166). Dr. Smith is currently a mem-
ber of ACM SIGMM, Fellow of IEEE and Editor-in-Chief of
IEEE MultiMedia.

1. REFERENCES
[1] John R. Smith. History made everyday. IEEE

MultiMedia, 18(2), July-Sept 2011.

[2] L. Xie, A. Natsev, J. R. Kender, M. Hill, and J. R.

Smith. Visual memes in social media: Tracking
real-world news in youtube videos. In Proc. of the 19th
ACM Intl. Conf. on Multimedia, pages 53–62. ACM,
November 2011.

[3] John R. Smith. Minding the gap. IEEE MultiMedia,

19(2), January-March 2012.

[4] A. Hanjalic, R. Lienhart, W.-Y. Ma, and J. R. Smith.
The holy grail of multimedia information retrieval: So
close or yet so far away? Proc. IEEE, 96(4):541–547,
2008.

[5] M. Naphade, J. R. Smith, J. Tesic, S.-F. Chang,

W. Hsu, L. Kenendy, A. Hauptmann, and J. Curtis.
Large-scale concept ontology for multimedia. IEEE
MultiMedia, 13(3), July-Sept 2006.

[6] John R. Smith. Just the facets. IEEE MultiMedia,

20(1), January-March 2013.

[7] R. Yan, M. O. Fleury, M. Merler, A. Natsev, and J. R.

Smith. Large-scale multimedia semantic concept
modeling using robust subspace bagging and
map-reduce. In Proc. of the 1st ACM Workshop on
Large-Scale Multimedia Retrieval. ACM, October 2009.

2