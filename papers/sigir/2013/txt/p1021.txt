Bias-Variance Decomposition of IR Evaluation

Peng Zhang1, Dawei Song1,2, Jun Wang3, Yuexian Hou1

1Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China

2The Computing Department, The Open University, United Kingdom

3Department of Computer Science, University College London, United Kingdom

{darcyzzj, dawei.song2010}@gmail.com, jun_wang@acm.org, yxhou@tju.edu.cn

ABSTRACT
It has been recognized that, when an information retrieval
(IR) system achieves improvement in mean retrieval eﬀec-
tiveness (e.g. mean average precision (MAP)) over all the
queries, the performance (e.g., average precision (AP)) of
some individual queries could be hurt, resulting in retrieval
instability. Some stability/robustness metrics have been
proposed. However, they are often deﬁned separately from
the mean eﬀectiveness metric. Consequently, there is a lack
of a uniﬁed formulation of eﬀectiveness, stability and over-
all retrieval quality (considering both).
In this paper, we
present a uniﬁed formulation based on the bias-variance de-
composition. Correspondingly, a novel evaluation methodol-
ogy is developed to evaluate the eﬀectiveness and stability in
an integrated manner. A case study applying the proposed
methodology to evaluation of query language modeling illus-
trates the usefulness and analytical power of our approach.

Category and Subject Descriptors: H.3.3 [Information
Search and Retrieval]

General Terms: Theory, Measurement, Performance

Keywords: Bias-Variance, Decomposition, Eﬀectiveness,
Stability, Robustness, Evaluation

1.

INTRODUCTION

Recently, it has been noticed that when we are trying
to improve the mean retrieval eﬀectiveness (e.g., measured
by MAP [3]) over all queries, the stability of performance
across diﬀerent individual queries could be hurt. For exam-
ple, compared with a baseline (using the original query in
the ﬁrst round retrieval), query expansion based on pseudo
relevance feedback can generally achieve better MAP, but it
can hurt the performance for some individual queries [2].
In the literature, various stability (or robustness) mea-
sures, e.g., R−Loss [2], Robustness Index [2], and < Init [9]
have been proposed. R − Loss computes the averaged net
loss of relevant documents (due to query expansion failure)
in the retrieved documents. <Init calculates the percent-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

age of queries for which the retrieval performance of a query
model is worse than that of the original query model. The
robustness index is deﬁned as RI(Q) = (n+ − n−)/|Q|
[2],
where n+ is the number of queries for which the perfor-
mance is improved, n− is the number of queries hurt, over
the original model, and |Q| is the number of all queries.

These existing measures have some limitations, as shown
by the example in Table 1. Let us consider model A as
the original query model, as well as regard B and C as two
query expansion models. The example shows that A is less
eﬀective (with a lower MAP), but more stable (with a lower
< Init) than B. In this case, we can not judge which model
(A or B) has the better overall performance (considering
both eﬀectiveness and stability). For comparison of B and
C, both <Init and robustness index can not distinguish the
stability between them. The MAPs of B and C are also
the same. Intuitively, B would seem more stable due to the
less variance of its AP values for diﬀerent queries. However,
the above stability metrics do not take into account such
variance (denoted as V ar in Table 1). For example, one
way of computing the variance of AP for model A can be
[(0.3 − 0.2)2 + (0.1 − 0.2)2]/2 = 0.01, which calculates the
derivation of AP from its MAP (i.e., 0.2). In Table 1, V ar
distinguishes the models A, B and C: the smaller V ar can
indicate the better retrieval stability.

Now let us change the AP values of C to 0.32 and 0.11 (for
q1 and q2 respectively), then its MAP, < Init and Robust-
ness Index become 0.215, 0 and 1 respectively, suggesting
C is more stable but less eﬀective than B. We are not sure
which one (B or C) is overall better when considering both
eﬀectiveness and stability. This is mainly because the ex-
isting stability metrics are often deﬁned separately from the
eﬀectiveness metric (MAP). There is a lack of a uniﬁed for-
mulation of the eﬀectiveness and stability to allow them to
be looked at in an integrated manner.

In this paper, we present a formulation based on a fun-
damental concept in Statistics, namely the bias-variance de-
composition, to tackle the problem. In a nutshell, we view
the unsatisfactory overall performance as one total error,
which can be decomposed into bias and variance of the AP
values of diﬀerent queries. The bias captures the expected
diﬀerence of the APs from their upper bounds (the best AP
obtained by model T in Table 1). The variance has been
illustrated earlier. The detailed formulation will be given
in the next section. Brieﬂy speaking, the smaller bias or
variance reﬂects the better retrieval eﬀectiveness or stabil-
ity, respectively. The total error (denoted as Bias2 +V ar)
can reﬂect the overall quality of a model. As an illustration,

1021Table 1: Examples of Bias-Variance (AP)

Model

AP
MAP
Robust Index
< Init
Bias
V ar
Bias2 + V ar

A

q1
0.3

q2
0.1

q1
0.6

B

q2
0.08

q1
0.65

C

q2
0.03

q1
0.7

T

q2
0.2

0.2
0
0

0.25
0.01

0.0725

0.34

0
0.5
0.11

0.0646
0.0797

0.34

0
0.5
0.11

0.0961
0.182

0.45

1
0
0

0.0625
0.0625

Table 2: Examples of Additional Bias-Variance ((cid:2)ρ)

Model

(cid:2)ρ
Bias
V ar
Bias2 + V ar

A

q2
0.1

q1
0.4

B

q2
0.12

q1
0.1

C

q2

0.17 0

q1
0.05

q1
0

0.25

0.0225
0.0850

0.11

0.0001
0.0122

0.11

0.0036
0.0157

q2
0

T

0
0
0

We now add up the bias and variance, yielding:

Bias2

(AP) + V ar(AP)

= [E(APT ) − E(AP)]
= E(AP − MAPT )

2

2

+ E(AP − E(AP))

2

(3)

It turns out E(AP− MAPT )2 is not exactly the expected
squared error E(AP − APT )2. However, we will show later
that E(AP− MAPT )2 is a simpliﬁed version of an expected
squared error E(AP−1)2 in Section 2.3. This error formula-
tion will help us understand the diﬀerence between diﬀerent
bias-variance decompositions (see Section 2.3).

2.2 Additional Bias-Variance
To clarify the above observation, let us ﬁrst look at the
decomposition of the expected squared error E(AP−APT )2.
We need to deﬁne another random variable

(cid:2)ρ = APT − AP

in Table 1, Bias2 +V ar indicates that (1) the target model
T has the best overall retrieval quality; (2) the model B is
more desirable than C; (3) in comparison with the baseline
A, both query expansion models B and C reduce the bias
but enlarge the variance as well as the total error.

Recently, Wang and Zhu [7] studied the mean-variance
analysis regarding the document relevance scores. The per-
topic variance of AP (of each query) was investigated in [6].
In this paper, we explore the variance of AP (across queries)
and the bias-variance decomposition.

2. BIAS-VARIANCE DECOMPOSITION OF

RETRIEVAL PERFORMANCE

In Statistics [1], bias and variance, which are measure-
ments of estimation quality in diﬀerent aspects, are decom-
posed from the expected squared error of the estimated val-
ues with respect to the target value. In this paper, we for-
mulate the bias-variance decomposition in the IR evaluation
scenario. Let us ﬁrst assume there exists a target model T
that can have an upper-bound performance for each query 1.

2.1 Bias-Variance of AP

We can let AP be a random variable over queries. Bias(AP)

essentially calculates the average diﬀerence between the tar-
get AP (i.e. the AP of the target model T, denoted APT )
and the actual AP of a retrieval model over all diﬀerent
queries. Speciﬁcally,

Bias(AP) = E(APT − AP) = E(APT ) − E(AP)

(1)
where the expectation E(·) is over a set of queries that are
assumed to be uniformly distributed. E(APT ) corresponds
to the target MAP (i.e., the MAP of the target model T,
denoted MAPT ), and E(AP) corresponds to the MAP of
the retrieval model under evaluation.

It turns out that the smaller bias indicates the better mean
retrieval eﬀectiveness over queries. In Table 1, the bias for
model A is 1
Eq. 1, it can be equivalently calculated as 0.45-0.2 = 0.25,
where 0.45 is the target MAP and 0.2 is the MAP of A.

2 [(0.7 − 0.3) + (0.2 − 0.1)] = 0.25. According to

Similarly, the variance of the APs for diﬀerent queries is

deﬁned as:

V ar(AP) = E(AP − E(AP))

2

(2)

The smaller V ar(AP) indicates the better retrieval stability
of AP across queries.

1We will relax this constraint in Sections 2.4.

As an illustration, for the model A in Table 2, (cid:2)ρ is 0.4 (0.7-
0.3) and 0.1 (0.2-0.1), for q1 and q2 respectively.
Let ρT be the target value of (cid:2)ρ, which is indeed 0, since for
model T, (cid:2)ρ = APT − APT =0. We need ρT in the following

analysis, since the target value is an important component
in the standard deﬁnition of bias. We deﬁne

Bias((cid:2)ρ) = E((cid:2)ρ) − ρT

where E((cid:2)ρ) is the averaged (cid:2)ρ over all queries. Since ρT is 0,
Bias((cid:2)ρ) equals to E((cid:2)ρ) = E(APT −AP), which is Bias(AP).
The variance based on (cid:2)ρ, denoted V ar((cid:2)ρ), computes the

variance of the diﬀerence between the AP (of the test model)
and the AP target across all queries. Formally,

V ar((cid:2)ρ) = E((cid:2)ρ − E((cid:2)ρ))

2

As shown in Table 2, V ar((cid:2)ρ) of B and C are smaller than
V ar((cid:2)ρ) of A, indicating that B and C are more stable than
A. This observation is diﬀerent from < Init and V ar(AP)
which shows that B and C are less stable than A in Table 1.

Now, we derive the decomposition of E(AP − APT )2:

(4)

(5)

(6)

E(AP − APT )

2

2

= E((cid:2)ρ − ρT )
= E((cid:2)ρ − E((cid:2)ρ))
= V ar((cid:2)ρ) + Bias2

2

+ (E((cid:2)ρ) − ρT )
((cid:2)ρ)

2

(7)

The above equations show the expected squared error
E(AP − APT )2 can be exactly decomposed into bias and
variance on (cid:2)ρ. The expected squared error E(AP − APT )2

always computes the error of the target model as zero, no
matter whether or not the target model still has room for
improvement. It is likely that the current best performance
for some queries can be further advanced in the near future.

10222.3 Further Investigation on Decomposition of

Expected Squared Error

In order to further investigate two aforementioned bias-
variance decompositions, we set 1 (the maximum AP) as the
upper-bound AP for each query. We can have an expected
squared error E(AP − 1)2 and its decomposition as:

E(AP − 1)

2

= E(AP − APT + APT − 1)
= E(AP − APT )

+ E(APT − 1)

2

2

2

+ 2E(AP − APT )(APT − 1)

(8)
which shows that E(AP−APT )2 is only one part of E(AP−
1)2, and the target model T still has an error E(APT − 1)2,
suggesting there is still a room for improvement.

We can also decompose E(AP − 1)2 as:

E(AP − 1)

2

2

= E[AP − E(AP) + E(AP) − 1]
= E(AP − E(AP))
= V ar(AP) + (MAP − 1)
= (1 − MAP)
+ V ar(AP)

+ [E(AP) − 1]

2

2

2

2

(9)

It turns out the variance parts in Eq. 3 and Eq. 9 are the
same (i.e., V ar(AP)). The term (1 − MAP)2 in Eq. 9 has
the same trend as Bias2(AP) (i.e., (MAPT −MAP)2), where
MAPT is the upper bound of the MAP. The above observa-
tions show that the decomposition in Eq. 3 can be considered
as a simpliﬁed version of the decomposition in Eq. 9.
2.4 Comparison between Two Bias-Variance
First, the bias-variance of (cid:2)ρ requires a target AP for ev-
ery query. On the other hand, the bias-variance of AP (in
given. Speciﬁcally, two biases (i.e., Bias(AP) and Bias((cid:2)ρ))
Eq. 3) can be used when only a target MAP (i.e., MAPT ) is
are equivalent and both can be computed by MAPT −MAP.
Regarding variance, the variance of (cid:2)ρ requires APT for each

query (see Eq. 6 and Eq. 4), while the variance of AP can
be calculated without knowing APT (see Eq. 2). Thus, the
Second, the bias-variance of (cid:2)ρ is an exact decomposition of
bias-variance based on AP is more ﬂexible for practical use.
E(AP − APT )2 and it always regards the target model as a
zero-error model. However, the decomposition of E(AP−1)2
in Eq. 8 shows that the target model can still has error. On
the other hand, under the bias-variance of AP, the variance
for the target model still exists, indicating that although we
can assume a target model as an upper-bound at a certain
stage, it can be further improved.
2.5 Bias-Variance Evaluation

To carry out IR evaluation based on the proposed the
bias-variance decomposition methods, one needs to choose
the upper-bound settings (i.e., the target model T). There
can be a number of ways. First, the upper-bound AP for
each query can be simply set as 1, which corresponds to a
perfect target model. Second, the upper-bound AP for each
query can be obtained based on the best AP among evalu-
ated retrieval models in the historic TREC results. Third,
one can simply set an upper-bound MAP (i.e., MAPT ).

The ﬁrst and second upper-bound settings correspond to
a virtual target model. For example, in the second setting,
the target model collects the best AP among many retrieval

models. The third setting can correspond to a real target
model (see the model settings in the experiments.) With
the ﬁrst and second upper-bound settings, either of bias-

variance metrics (based on AP or (cid:2)ρ) can be adopted. The
or (cid:2)ρ) are chosen, we can calculate bias-variance ﬁgures for

third setting is suitable for bias-variance of AP (see Sec-
tion 2.4). Once an upper-bound setting and a variable (AP

a series of models and then see which model can have the
minimum bias, or minimum variance, or the sum of them
(i.e., the total error). We can also get an overview on the
trend of bias and variance over parameters.

3. EMPIRICAL EVALUATION

We use the query modeling as an example of bias-variance
evaluation. Due to the page limit, we only report the eval-
uation results on two standard TREC collections, i.e., WSJ
(87-92) over queries 151-200 and ROBUST 2004 over queries
601-700. The title ﬁeld of the queries is used. Lemur 4.7 [5]
is used for indexing and retrieval. The top n = 30 ranked
documents in the initial ranking by the original query model
are selected as the pseudo-relevance feedback (PRF) docu-
ments. The number of expanded terms is ﬁxed as 100. 1000
documents are retrieved by the KL-divergence model [5].

Model Settings We customize a number of query mod-
els according to three factors (i.e., model complexity, model
combination, ground-truth data) that can generally inﬂu-
ence the bias-variance tradeoﬀ [1]. The ﬁrst model is the
original query model which is a maximum likelihood esti-
mation of original query. We also evaluate an expanded
query model RM (i.e., RM1 in [4]) which is generated from
feedback documents D. Compared with the original query
model, the expanded query model is more complex due to
the fact that it has more terms and additional assumption-
s (e.g., assuming that all feedback documents are relevan-
t). The above two models can be combined, leading to a
combined query model (also called as RM3). Let λ be the
combination coeﬃcient which is in the range (0,1). When
λ gets close to 0, the combined model is towards the R-
M, and otherwise towards the original model. In RM, we
can gradually remove a percentage (denoted by rn) of non-
relevant documents DN along the initial ranking of feedback
documents, based on the available relevance judgements (as
ground-truth) [8]. We ﬁnally derive a target model which
are generated by keeping only relevant documents DR in D:

p(w|θ(t)

q ) ∝

p(w|θd)

(10)

(cid:3)
d∈DR

which gives the best MAP over the aforementioned query
models.
In Eq. 10, the document weights are set as uni-
form (diﬀerent from the weights in RM), since the relevant
judgements of relevant documents are the same (i.e., 1).

Bias-Variance Metrics Setting We report the bias-
variance on AP in our evaluation, since the target query
model used in our evaluation only guarantees the upper-
bound MAP. According to the discussion in Sections 2.4
and 2.5, the bias-variance of AP is more suitable.

Evaluation Results We ﬁrst look at the bias-variance
results in Figure 1. Recall that the smaller bias or variance
(based on AP) corresponds to the better retrieval eﬀective-
ness or stability, respectively. Figure 1(a) shows that on
WSJ8792, the original query model has the smallest vari-
ance (the highest stability), but the largest bias (the lowest
MAP). This is a tradeoﬀ between bias and variance. One

1023 

Bias−Variance (AP)

Original Model
Expanded Model RM
Combinaed Model
Removing DN
Target Model

0.066

0.064

0.062

0.06

0.058

0.056

)

P
A
(
r
a
V

0.06

0.058

0.056

0.054

0.052

0.05

0.048

0.046

0.044

0.042

)

P
A
(
r
a
V

Bias−Variance (AP)

 

Original Model
Expanded Model RM
Combinaed Model
Removing DN
Target Model

0.054

 
0

0.005

0.01

0.015
0.02
Bias2(AP)

0.025

0.03

0.035

0.04

 
0

0.005

0.01

0.015

0.02
0.025
Bias2(AP)

0.03

0.035

0.04

0.045

(a) WSJ8792

(b) ROBUST2004

Figure 1: Results of Bias and Variance of AP of all the concerned query models. The x-axis shows the squared
bias and the y-axis shows the variance.

reason in Statistics language is that the expanded models are
all complex than the original one. The bias of the expanded
model RM is much smaller, while its variance is much larg-
er, than the original model. The diﬀerent variants of the
combined model (with λ in [0.1, 0.9] with increment 0.1) are
lying between the expanded and original models. We can
also see that 2 (out of 9) combined models (when λ=0.1 and
0.2) has smaller bias and smaller variance than the expand-
ed query model RM. This suggests that the combined query
model with good parameters can achieve better eﬀectiveness
and stability over RM. Among the combined query model-
s, bias and variance are negatively correlated, indicating a
bias-variance tradeoﬀ occurs. On the other hand, if we re-
move the non-relevant documents DN (with rn in [0.1, 1]
with increment 0.1) in RM, the bias and variance are posi-
tively correlated, and 9 (out of 10) models (when rn = 0.2
to 1) can reduce both the bias and variance over RM. The
target model has zero bias, although there is a variance of
its AP across diﬀerent queries. On ROBUST2004, the re-
sults are similar. The diﬀerences are that: 1) by removing
non-relevant documents, only 6 (out of 10) models (when
rn = 0.5 to 1) reduce both bias and variance over RM; 2)
there are 3 (out of 9) (when λ = 0.1 to 0.3) variants of the
combined model for which both bias and variance can be
smaller than those of RM.

Figure 2 shows Bias2 + V ar results for all the concerned
models. On both collections, the target method achieves the
smallest Bias2 + V ar and the original query model achieves
the largest Bias2 +V ar. Recall that Bias2 +V ar represents
the total error and the smaller error can reﬂect the better
overall quality (or performance), by considering both the
eﬀectiveness and stability in one single criterion (Bias2 +
V ar). The results also suggest that the model combination
and the relevance judgements are helpful to reduce Bias2 +
V ar over the original model and the expanded model RM.

4. CONCLUSIONS AND FUTURE WORK

In this paper, a novel evaluation strategy based on the
bias-variance decomposition has been presented. We for-
mulate two forms of the bias-variance decomposition and
theoretically compare them. We also use query expansion
as an example to demonstrate the use of the bias-variance
for IR evaluation and analysis, in terms of bias, variance or
sum of them. In the future, we will further explore the other
upper-bound settings and variables discussed in Section 2.5,

0.09

0.085

0.08

0.075

0.07

0.065

)

P
A
(
r
a
V
+
)
P
A
2
s
a
B

(

i

Bias−Variance (AP)

 

0.085

Bias−Variance (AP)

 

)

P
A
(
r
a
V
+
)
P
A
2
s
a
B

(

i

0.08

0.075

0.07

0.065

0.06

Original Model
Expanded Model RM
Combinaed Model
Removing DN
Target Model

Original Model
Expanded Model RM
Combinaed Model
Removing DN
Target Model

0.06

 
0

0.005

0.01

0.02
0.015
Bias2(AP)

0.025

0.03

0.035

0.055

 
0

0.005

0.01

0.015

0.025
0.02
Bias2(AP)

0.03

0.035

0.04

0.045

(a) WSJ8792

(b) ROBUST2004

Figure 2: Results of Bias2 and Bias2+V ar of AP of all
the concerned query models. The x-axis shows the
squared bias and the y-axis shows the Bias2 + V ar.

test with more retrieval models and explore the deep insights
behind the bias-variance trends.
5. ACKNOWLEDGMENTS

This work is supported in part by the Chinese Nation-
al Program on Key Basic Research Project (973 Program,
grant No. 2013CB329304), the Natural Science Foundation
of China (grant No. 61272265, 61070044), and EU’s FP7
QONTEXT project (grant No. 247590).
6. REFERENCES
[1] C. M. Bishop.Pattern Recognition and Machine Learning .

Springer-Verlag New York, Inc., 2006.

[2] K. Collins-Thompson. Reducing the risk of query expansion

via robust constrained optimization. In CIKM ’09, pages
837–846, 2009. ACM.

[3] G. V. Cormack and T. R. Lynam. Statistical Precision of

Information Retrieval Evaluation. In SIGIR ’06, pages
533–540, 2006. ACM.

[4] V. Lavrenko and W. B. Croft. Relevance-based language

models. In SIGIR ’01, pages 120–127, 2001.

[5] P. Ogilvie and J. Callan. Experiments using the lemur

toolkit. In TREC ’02, pages 103–108, 2002.

[6] S. E. Robertson and E. Kanoulas. On per-topic variance in

ir evaluation. In SIGIR ’12, pages 891–900, 2012.

[7] J. Wang and J. Zhu. Portfolio theory of information

retrieval. In SIGIR ’09, pages 115–122, 2009.

[8] P. Zhang, Y. Hou and D. Song. Approximating true

relevance distribution from a mixture model based on
irrelevance data. In SIGIR ’09, pages 107–114, 2009.

[9] L. Zighelnic and O. Kurland. Query-drift prevention for

robust query expansion. In SIGIR ’08, pages 825–826, 2008.

1024