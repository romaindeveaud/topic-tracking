Workshop on Benchmarking Adaptive Retrieval 

and Recommender Systems – BARS 2013

Pablo Castells 

Univ. Autónoma de Madrid 
Fco. Tomás y Valiente 11 

28049 Madrid, Spain 

Frank Hopfgartner 
Technische Univ. Berlin 

Ernst-Reuter-Platz 7 

10587 Berlin, Germany 

Alan Said 

Mounia Lalmas 

Cent. Wiskunde & Informatica 
Science Park 123, 1098 XG  

Amsterdam, Netherlands 

Yahoo! Labs Barcelona 
Avinguda Diagonal 177 
08018 Barcelona, Spain 

pablo.castells@uam.es 

frank.hopfgartner@tu-berlin.de 

alan.said@cwi.nl 

mounia@acm.org 

ABSTRACT 
Evaluating  adaptive  and  personalized  information  retrieval  tech-
niques is known to be a difficult endeavor. The rapid evolution of 
novel technologies in this scope raises additional challenges that 
further stress the need for new evaluation approaches and meth-
odologies. The BARS 2013 workshop seeks to provide a specific 
venue  for  work  on  novel,  personalization-centric  benchmarking 
approaches  to  evaluate  adaptive  retrieval  and  recommender  sys-
tems. 
Categories and Subject Descriptors 
H.3.3  [Information  Search  and  Retrieval]:  search  process, 
information filtering. 
Keywords 
Evaluation, adaptive information retrieval, recommender systems, 
benchmarking, metrics, methodology. 
1.  INTRODUCTION 
Great progress has been made in recent years in the development 
of recommendation, retrieval and personalization techniques. Yet 
the  evaluation of these systems is still based on traditional met-
rics, e.g. precision, recall or RMSE, often not taking the use-case 
and situation of the system into consideration, and failing to pro-
vide  a  suitable  proxy  of  user  satisfaction  and  business  goals. 
Moreover, the rapid evolution of novel information retrieval (IR) 
and  recommender  systems  foster  the  need  for  new  evaluation 
paradigms.  
New  evaluation  approaches  of  adaptive  systems  should evaluate 
both  functional  and  non-functional  requirements.  Functional  re-
quirements go beyond traditional relevance metrics and focus on 
user-centered utility metrics, such as novelty, diversity and seren-
dipity.  Non-functional  requirements  focus  on  performance  and 
technical aspects, e.g. scalability and reactivity.  
The evaluation of adaptive IR systems has been acknowledged to 
find  difficulty  in  fitting  in  established evaluation paradigms and 
methodologies,  which  can  be  identified  as  a  hurdle  to  research 
and progress in this area. Active research efforts and open discus-
sion  are  currently  taking  place  in  parallel  in  the  Recommender 
Systems  and  Adaptive  IR  fields,  where  devising  methodologies 
 
Permission to make digital or hard copies of part or all of this work for 
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s). 
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland. 
ACM 978-1-4503-2034-4/13/07. 

 
 

and metrics suiting the goals and task models of real applications 
is still a prominent open issue. 
The BARS 2013 workshop aimed to serve as a venue for work on 
novel, personalization-centric benchmarking approaches to evalu-
ate  adaptive  retrieval  and  recommender  systems.  The  workshop 
was  set  to  revise  and  leverage  the  latest  advances  in  this  area, 
identify the main issues to be addressed, and share ideas for con-
tinued  progress.  BARS  sought,  in  particular,  to  join  forces  and 
provide a meeting point for researchers working on largely over-
lapping and connected areas such as adaptive IR and recommend-
er systems, dealing with closely related problems but often from 
different backgrounds. 
2.  SCOPE 
The workshop gathered researchers and practitioners interested in 
developing  better,  clearer,  and/or  more  complete  evaluation 
methodologies,  and  addressing  the  challenges  involved  in  the 
evaluation  of  adaptive  retrieval  and  recommender  systems.  It 
provided an informal setting for exchanging and discussing ideas, 
sharing  experiences  and  viewpoints.  The  participants  worked 
together in the identification and better understanding of the cur-
rent  gaps  in  the  evaluation  methodologies,  seeking  to  lay  direc-
tions for progress in addressing them, and the consolidation and 
convergence of experimental methods and practice.  
The  accepted  papers  and  the  discussions  held  at  the  workshop 
addressed, among others, the following topics: 

  New metrics and methods for quality estimation of recom-

mender and adaptive IR systems. 

  Novel  frameworks  for  the  user-centric  evaluation  of  adap-

tive systems. 

  Validation of off-line methods with online studies. 
  Comparison of evaluation metrics and methods. 
  Comparison  of  recommender  and  IR  approaches  across 

multiple systems and domains. 

  Measuring technical constraints vs. accuracy. 
  New datasets for the evaluation of recommender and adap-

tive IR systems. 

  Benchmarking frameworks. 
  Multiple-objective benchmarking. 

The accepted papers and a summary of discussions are available 
in  the  workshop  proceedings,  which  can  be  reached  from  the 
workshop website at http://www.bars-workshop.org. 

 

1133