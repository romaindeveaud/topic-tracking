Ranking Document Clusters Using Markov Random Fields

Fiana Raiber

ﬁana@tx.technion.ac.il

Oren Kurland

kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion

Haifa 32000, Israel

ABSTRACT
An important challenge in cluster-based document retrieval
is ranking document clusters by their relevance to the query.
We present a novel cluster ranking approach that utilizes
Markov Random Fields (MRFs). MRFs enable the integra-
tion of various types of cluster-relevance evidence; e.g., the
query-similarity values of the cluster’s documents and query-
independent measures of the cluster. We use our method to
re-rank an initially retrieved document list by ranking clus-
ters that are created from the documents most highly ranked
in the list. The resultant retrieval eﬀectiveness is substan-
tially better than that of the initial list for several lists that
are produced by eﬀective retrieval methods. Furthermore,
our cluster ranking approach signiﬁcantly outperforms state-
of-the-art cluster ranking methods. We also show that our
method can be used to improve the performance of (state-
of-the-art) results-diversiﬁcation methods.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation

Keywords: ad hoc retrieval, cluster ranking, query-speciﬁc clus-
ters, markov random ﬁelds

1.

INTRODUCTION

The cluster hypothesis [33] gave rise to a large body of
work on using query-speciﬁc document clusters [35] for im-
proving retrieval eﬀectiveness. These clusters are created
from documents that are the most highly ranked by an ini-
tial search performed in response to the query.

For many queries there are query-speciﬁc clusters that
contain a very high percentage of relevant documents [8,
32, 25, 14]. Furthermore, positioning the constituent doc-
uments of these clusters at the top of the result list yields
highly eﬀective retrieval performance; speciﬁcally, much bet-
ter than that of state-of-the art retrieval methods that rank
documents directly [8, 32, 25, 14, 10].

As a result of these ﬁndings, there has been much work on
ranking query-speciﬁc clusters by their presumed relevance

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous
approaches to cluster ranking compare a representation of
the cluster with that of the query. A few methods integrate
additional types of information such as inter-cluster and
cluster-document similarities [18, 14, 15]. However, there
are no reports of fundamental cluster ranking frameworks
that enable to eﬀectively integrate various information types
that might attest to the relevance of a cluster to a query.

We present a novel cluster ranking approach that uses
Markov Random Fields. The approach is based on integrat-
ing various types of cluster-relevance evidence in a princi-
pled manner. These include the query-similarity values of
the cluster’s documents, inter-document similarities within
the cluster, and measures of query-independent properties
of the cluster, or more precisely, of its documents.

A large array of experiments conducted with a variety of
TREC datasets demonstrates the high eﬀectiveness of using
our cluster ranking method to re-rank an initially retrieved
document list. The resultant retrieval performance is sub-
stantially better than that of the initial ranking for several
eﬀective rankings. Furthermore, our method signiﬁcantly
outperforms state-of-the-art cluster ranking methods. Al-
though the method ranks clusters of similar documents, we
show that using it to induce document ranking can help to
substantially improve the eﬀectiveness of (state-of-the-art)
retrieval methods that diversify search results.

2. RETRIEVAL FRAMEWORK

Suppose that some search algorithm was employed over
a corpus of documents in response to a query. Let Dinit be
the list of the initially highest ranked documents. Our goal
is to re-rank Dinit so as to improve retrieval eﬀectiveness.

To that end, we employ a standard cluster-based retrieval
paradigm [34, 24, 18, 26, 15]. We ﬁrst apply some cluster-
ing method upon the documents in Dinit; C l(Dinit) is the
set of resultant clusters. Then, the clusters in C l(Dinit) are
ranked by their presumed relevance to the query. Finally,
the clusters’ ranking is transformed to a ranking of the doc-
uments in Dinit by replacing each cluster with its constituent
documents and omitting repeats in case the clusters overlap.
Documents in a cluster are ordered by their query similarity.
The motivation for employing the cluster-based approach
just described follows the cluster hypothesis [33]. That is,
letting similar documents provide relevance status support
to each other by the virtue of being members of the same
clusters. The challenge that we address here is devising a
(novel) cluster ranking method — i.e., we tackle the second
step of the cluster-based retrieval paradigm.

333Figure 1: The three types of cliques considered for graph G. G is composed of a query node (Q) and three
(for the sake of the example) nodes (d1, d2, and d3) that correspond to the documents in cluster C. (i) lQD
contains the query and a single document from C; (ii) lQC contains all nodes in G; and, (iii) lC contains only
the documents in C.

Formally, let C and Q denote random variables that take
as values document clusters and queries respectively. The
cluster ranking task amounts to estimating the probability
that a cluster is relevant to a query, p(C|Q):

p(C|Q) =

p(C, Q)

p(Q)

rank= p(C, Q).

(1)

The rank equivalence holds as clusters are ranked with re-
spect to a ﬁxed query.

To estimate p(C, Q), we use Markov Random Fields (MRFs).

As we discuss below, MRFs are a convenient framework for
integrating various types of cluster-relevance evidence.

2.1 Using MRFs to rank document clusters

An MRF is deﬁned over a graph G. Nodes represent
random variables and edges represent dependencies between
these variables. Two nodes that are not connected with an
edge correspond to random variables that are independent of
each other given all other random variables. The set of nodes
in the graph we construct is composed of a node representing
the query and nodes representing the cluster’s constituent
documents. The joint probability over G’s nodes, p(C, Q),
can be expressed as follows:

p(C, Q) = Ql∈L(G) ψl(l)

Z

;

(2)

L(G) is the set of cliques in G and l is a clique; ψl(l)
is a potential (i.e., positive function) deﬁned over l; Z =
PC,Q Ql∈L(G) ψl(l) is the normalization factor that serves
to ensure that p(C, Q) is a probability distribution. The
normalizer need not be computed here as we rank clusters
with respect to a ﬁxed query.

A common instantiation of potential functions is [28]:

ψl(l)

def
= exp(λlfl(l)),

where fl(l) is a feature function deﬁned over the clique l
and λl is the weight associated with this function. Accord-
ingly, omitting the normalizer from Equation 2, applying the
rank-preserving log transformation, and substituting the po-
tentials with the corresponding feature functions results in
our ClustMRF cluster ranking method:

p(C|Q) rank= X

l∈L(G)

λlfl(l).

(3)

This is a generic linear (in feature functions) cluster ranking
function that depends on the graph G. To instantiate a spe-
ciﬁc ranking method, we need to (i) determine G’s structure,

speciﬁcally, its clique set L(G); and, (ii) associate feature
functions with the cliques. We next address these two tasks.

2.1.1 Cliques and feature functions

We consider three types of cliques in the graph G. These
are depicted in Figure 1. In what follows we write d ∈ C to
indicate that document d is a member of cluster C.

The ﬁrst clique (type), lQD, contains the query and a sin-
gle document in the cluster. This clique serves for making
inferences based on the query similarities of the cluster’s
constituent documents when considered independently. The
second clique, lQC , contains all nodes of the graph; that is,
the query Q and all C’s constituent documents. This clique
is used for inducing information from the relations between
the query-similarity values of the cluster’s constituent docu-
ments. The third clique, lC, contains only the cluster’s con-
stituent documents. It is used to induce information based
on query-independent properties of the cluster’s documents.
In what follows we describe the feature functions deﬁned
over the cliques. In some cases a few feature functions are
deﬁned for the same clique, and these are used in the summa-
tion in Equation 3. Note that the sum of feature functions
is also a feature function. The weights associated with the
feature functions are set using a train set of queries. (Details
are provided in Section 4.1.)

The lQD clique. High query similarity exhibited by C’s
constituent documents can potentially imply to C’s rele-
vance [26]. Accordingly, let d (∈ C) be the document in
1
|C| ,
lQD. We deﬁne fgeo−qsim;lQD (lQD)
where |C| is the number of documents in C, and sim(·, ·) is
some inter-text similarity measure, details of which are pro-
vided in Section 4.1. Using this feature function in Equation
3 for all the lQD cliques of G amounts to using the geometric
mean of the query-similarity values of C’s constituent docu-
ments. All feature functions that we consider use logs so as
to have a conjunction semantics for the integration of their
assigned values when using Equation 3.1

def
= log sim(Q, d)

The lQC clique. Using the lQD clique from above results
in considering the query-similarity values of the cluster’s
documents independently of each other.
In contrast, the
lQC clique provides grounds for utilizing the relations be-
tween these similarity values. Speciﬁcally, we use the log

1Before applying the log function we employ add-ǫ (=
10−10) smoothing.

334of the minimal, maximal, and standard deviation2 of the
{sim(Q, d)}d∈C values as feature functions for lQC, denoted
min-qsim, max-qsim, and stdv-qsim, respectively.

The lC clique. Heretofore, the lQD and lQC cliques served
for inducing information from the query similarity values of
C’s documents. We now consider query-independent proper-
ties of C that can potentially attest to its relevance. Doing so
amounts to deﬁning feature functions over the lC clique that
contains C’s documents but not the query. All the feature
functions that we deﬁne for lC are constructed as follows.
We ﬁrst deﬁne a query-independent document measure, P,
and apply it to document d (∈ C) yielding the value P(d).
Then, we use log A({P(d)}d∈C) where A is an aggregator
function: minimum, maximum, and geometric mean. The
resultant feature functions are referred to as min-P, max-
P, and geo-P, respectively. We next describe the document
measures that serve as the basis for the feature functions.

The cluster hypothesis [33] implies that relevant docu-
ments should be similar to each other. Accordingly, we mea-
sure for document d in C its similarity with all documents
in C: Pdsim(d)

def
= 1

|C| Pdi∈C sim(d, di).

The next few query-independent document measures are
based on the following premise. The higher the breadth of
content in a document, the higher the probability it is rel-
evant to some query. Thus, a cluster containing documents
with broad content should be assigned with relatively high
probability of being relevant to some query.

High entropy of the term distribution in a document is a
potential indicator for content breadth [17, 3]. This is be-
cause the distribution is “spread” over many terms rather
than focused over a few ones. Accordingly, we deﬁne
= −Pw∈d p(w|d) log p(w|d), where w is a term
Pentropy(d)
and p(w|d) is the probability assigned to w by an unsmoothed
unigram language model (i.e., maximum likelihood estimate)
induced from d.

def

Inspired by work on Web spam classiﬁcation [9], we use
the inverse compression ratio of document d, Picompress(d),
as an additional measure. (Gzip is used for compression.)
High compression ratio presumably attests to reduced con-
tent breadth [9].

Two additional content-breadth measures that were pro-
posed in work on Web retrieval [3] are the ratio between the
number of stopwords and non-stopwords in the document,
Psw1(d); and, the fraction of stopwords in a stopword list
that appear in the document, Psw2(d). We use INQUERY’s
stopword list [2]. A document containing many stopwords
is presumably of richer language (and hence content) than
a document that does not contain many of these; e.g., a
document containing a table composed only of keywords [3].
For some of the Web collections used for evaluation in
Section 4, we also use the PageRank score [4] of the docu-
ment, Ppr(d), and the conﬁdence level that the document is
not spam, Pspam(d). The details of the spam classiﬁer are
provided in Section 4.1.

We note that using the feature functions that result from
applying the geometric mean aggregator upon the query-
independent document measures just described, except for

dsim, could have been described in an alternative way. That
1
|C| as a feature function over a clique con-
is, using log P(d)
taining a single document. Then, using these feature func-
tions in Equation 3 amounts to using the geometric mean.3

3. RELATED WORK

The work most related to ours is that on devising cluster
ranking methods. The standard approach is based on mea-
suring the similarity between a cluster representation and
that of the query [7, 34, 35, 16, 24, 25, 26]. Speciﬁcally, a
geometric-mean-based cluster representation was shown to
be highly eﬀective [26, 30, 15]. Indeed, ranking clusters by
the geometric mean of the query-similarity values of their
constituent documents is a state-of-the-art cluster ranking
approach [15]. This approach rose as an integration of fea-
ture functions used in ClustMRF, and is shown in Section 4
to substantially underperform ClustMRF.

Clusters were also ranked by the highest query similar-
ity exhibited by their constituent documents [22, 31] and by
the variance of these similarities [25, 19]. ClustMRF incor-
porates these methods as feature functions and is shown to
outperform each.

Some cluster ranking methods use inter-cluster and cluster-
document similarities [14, 15]. While ClustMRF does not
utilize such similarities, it is shown to substantially outper-
form one such state-of-the-art method [15].

A diﬀerent use of clusters in past work on cluster-based
retrieval is for “smoothing” (enriching) the representation of
documents [20, 16, 24, 13]. ClustMRF is shown to substan-
tially outperform one such state-of-the-art method [13].

To the best of our knowledge, our work is ﬁrst to use
MRFs for cluster ranking. In the context of retrieval tasks,
MRFs were ﬁrst introduced for ranking documents directly
[28]. We show that using ClustMRF to produce document
ranking substantially outperforms this retrieval approach;
and, that which augments the standard MRF retrieval model
with query-independent document measures [3]. MRFs were
also used, for example, for query expansion, passage-based
document retrieval, and weighted concept expansion [27].

4. EVALUATION

4.1 Experimental setup

corpus
AP

# of docs
242,918

data
Disks 1-3

ROBUST

528,155

Disks 4-5 (-CR)

1,692,096
25,205,179

WT10g
GOV2

queries
51-150
301-450,
600-700
451-550
701-850

WT10G
GOV2
ClueA
ClueAF
ClueB
ClueBF

503,903,810 ClueWeb09 (Category A)

1-150

50,220,423

ClueWeb09 (Category B)

1-150

Table 1: Datasets used for experiments.

The TREC datasets speciﬁed in Table 1 were used for
experiments. AP and ROBUST are small collections, com-
posed mostly of news articles. WT10G and GOV2 are Web

2It was recently argued that high variance of the query-
similarity values of the cluster’s documents might be an in-
dicator for the cluster’s relevance, as it presumably attests
to a low level of “query drift” [19].

3Similarly, we could have used the geometric mean of the
query-similarity values of the cluster constituent documents
as a feature function deﬁned over the lQC clique rather than
constructing it using the lQD cliques as we did above.

335collections; the latter is a crawl of the .gov domain. For
the ClueWeb Web collection both the English part of Cat-
egory A (ClueA) and the Category B subset (ClueB) were
used. ClueAF and ClueBF are two additional experimental
settings created from ClueWeb following previous work [6].
Speciﬁcally, documents assigned by Waterloo’s spam classi-
ﬁer [6] with a score below 70 and 50 for ClueA and ClueB,
respectively, were ﬁltered out from the initial corpus rank-
ing described below. The score indicates the percentage of
all documents in ClueWeb Category A that are presumably
“spammier” than the document at hand. The ranking of the
residual corpus was used to create the document list upon
which the various methods operate. Waterloo’s spam score
is also used for the Pspam(·) measure that was described in
Section 2.1. The Pspam(·) and Ppr(·) (PageRank score) mea-
sures are used only for the ClueWeb-based settings as these
information types are not available for the other settings.

The titles of TREC topics served for queries. All data
was stemmed using the Krovetz stemmer. Stopwords on
the INQUERY list were removed from queries but not from
documents. The Indri toolkit (www.lemurproject.org/indri)
was used for experiments.

Initial retrieval and clustering. As described in Section
2, we use the ClustMRF cluster ranking method to re-rank
an initially retrieved document list Dinit. Recall that af-
ter ClustMRF ranks the clusters created from Dinit, these
are “replaced” by their constituent documents while omit-
ting repeats. Documents within a cluster are ranked by
their query similarity, the measure of which is detailed be-
low. This cluster-based re-ranking approach is employed
by all the reference comparison methods that we use and
that rely on cluster ranking. Furthermore, ClustMRF and
all reference comparison approaches re-rank a list Dinit that
is composed of the 50 documents that are the most highly
ranked by some retrieval method speciﬁed below. Dinit is rel-
atively short following recommendations in previous work on
cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we
study the eﬀect of varying the list size on the performance
of ClustMRF and the reference comparisons.

We let all methods re-rank three diﬀerent initial lists Dinit.
The ﬁrst, denoted MRF, is used unless otherwise speciﬁed.
This list contains the documents in the corpus that are the
most highly ranked in response to the query when using the
state-of-the-art Markov Random Field approach with the
sequential dependence model (SDM) [28]. The free param-
eters that control the use of term proximity information in
SDM, λT , λO, and λU , are set to 0.85, 0.1, and 0.05, respec-
tively, following previous recommendations [28]. We also use
MRF’s SDM with its free parameters set using cross valida-
tion as one of the re-ranking reference comparisons. (De-
tails provided below.) All methods operating on the MRF
initial list use the exponent of the document score assigned
by SDM — which is a rank-equivalent estimate to that of
log p(Q, d) — as simM RF (Q, d), the document-query simi-
larity measure. This measure was used to induce the initial
ranking using which Dinit was created. More generally, for a
fair performance comparison we maintain in all the experi-
ments the invariant that the scoring function used to create
an initially retrieved list is rank equivalent to the document-
query similarity measure used in methods operating on the
list. Furthermore, the document-query similarity measure is

used in all methods that are based on cluster ranking (in-
cluding ClustMRF) to order documents within the clusters.
The second initial list used for re-ranking, DocMRF (dis-
cussed in Section 4.2.4), is created by enriching MRF’s SDM
with query-independent document measures [3].

The third initial list, LM, is addressed in Section 4.2.5.
The list is created using unigram language models. In con-
trast, the MRF and DocMRF lists were created using re-
trieval methods that use term proximity information. Let
pDir[µ]
(·) be the Dirichlet-smoothed unigram language model
z
induced from text z; µ is the smoothing parameter. The LM
similarity between texts x and y is simLM (x, y)
(·)(cid:17)(cid:17) [37, 17], where CE is
exp(cid:16)−CE (cid:16)pDir[0]
the cross entropy measure; µ is set to 1000.4 Accordingly,
the LM initial list is created by using simLM (Q, d) to rank
the entire corpus.5 This measure serves as the document-
query similarity measure for all methods operating over the
LM list, and for the inter-document similarity measure used
by the dsim feature function.

(·) (cid:12)(cid:12)(cid:12)

def
=

(cid:12)(cid:12)(cid:12)

x

pDir[µ]

y

Unless otherwise stated, to cluster any of the three ini-
tial lists Dinit, we use a simple nearest-neighbor clustering
approach [18, 25, 14, 26, 13, 15]. For each document d
(∈ Dinit), a cluster is created from d and the k − 1 docu-
ments di in Dinit (di 6= d) with the highest simLM (d, di); k
is set to a value in {5, 10, 20} using cross validation as de-
scribed below. Using such small overlapping clusters (all of
which contain k documents) was shown to be highly eﬀective
for cluster-based document retrieval [18, 25, 14, 26, 13, 15].
In Section 4.2.6 we also study the performance of ClustMRF
when using hierarchical agglomerative clustering.

Evaluation metrics and free parameters. We use MAP
(computed at cutoﬀ 50, the size of the list Dinit that is re-
ranked) and the precision of the top 5 documents (p@5) and
their NDCG (NDCG@5) for evaluation measures.6 The free
parameters of our ClustMRF method, as well as those of all
reference comparison methods, are set using 10-fold cross
validation performed over the queries in an experimental
setting. Query IDs are the basis for creating the folds. The
two-tailed paired t-test with p ≤ 0.05 was used for testing
statistical signiﬁcance of performance diﬀerences.

For our ClustMRF method, the free-parameter values are
set in two steps. First, SVMrank [12] is used to learn the val-
ues of the λl weights associated with the feature functions.
The NDCG@k of the k constituent documents of a cluster
serves as the cluster score used for ranking clusters in the
learning phase7. (Recall from above that documents in a

4The MRF SDM used above also uses Dirichlet-smoothed
unigram language models with µ = 1000.
5Queries for which there was not a single relevant document
in the MRF or LM initial lists were removed from the eval-
uation. For the ClueWeb settings, the same query set was
used for ClueX and ClueXF.
6We note that statAP, rather than AP, was the oﬃcial
TREC evaluation metric in 2009 for ClueWeb with queries
1–50. For consistency with the other queries for ClueWeb,
and following previous work [3], we use AP for all ClueWeb
queries by treating prel ﬁles as qrel ﬁles. We hasten to point
out that evaluation using statAP for the ClueWeb collections
with queries 1–50 yielded relative performance patterns that
are highly similar to those attained when using AP.
7Using MAP@k as the cluster score resulted in a slightly
less eﬀective performance. We also note that learning-to-

336AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

Init
10.1
50.7
50.6
19.9
51.0
52.5
15.8
37.5
37.2
12.7
59.3
48.6
4.5
19.1
12.6
8.6
46.3
32.4
12.5
33.1
24.4
15.8
44.8
33.2

TunedMRF ClustMRF

9.9
48.7
49.4
20.0
51.0
52.7
15.4
36.9
35.3i
12.7
60.8
49.5
4.9i
21.1
15.6i
8.7
47.8
33.1
13.5i
35.5
27.0
16.3i
46.8
34.3

10.8

53.0
54.4t
21.0i
t
52.4

54.7
18.0i
t
44.9i
t
42.8i
t
14.2i
t
70.1i
t
56.2i
t
6.3i
t
44.6i
t
29.4i
t

8.9

50.2

33.9
16.1i
t
48.7i
t
37.4i
t
17.0

48.5

36.9

Table 2: The performance of ClustMRF and a tuned
MRF (TunedMRF) when re-ranking the MRF ini-
tial list (Init). Boldface: the best result in a row. ’i’
and ’t’ mark statistically signiﬁcant diﬀerences with
Init and TunedMRF, respectively.

cluster are ordered based on their query similarity.) A rank-
ing of documents in Dinit is created from the cluster ranking,
which is performed for each cluster size k (∈ {5, 10, 20}), us-
ing the approach described above; k is then also set using
cross validation by optimizing the MAP performance of the
resulting document ranking. The train/test split for the
ﬁrst and second steps are the same — i.e., the same train
set used for learning the λl’s is the one used for setting the
cluster size. As is the case for ClustMRF, the ﬁnal docu-
ment ranking induced by any reference comparison method
is based on using cross validation to set free-parameter val-
ues; and, MAP serves as the optimization criterion in the
training (learning) phase.

Finally, we note that the main computational overhead,
on top of the initial ranking, incurred by using ClustMRF is
the clustering. That is, the feature functions used are either
query-independent, and therefore can be computed oﬄine;
or, use mainly document-query similarity values that have
already been computed to create the initial ranking. Clus-
tering of a few dozen documents can be computed eﬃciently;
e.g., based on document snippets.

4.2 Experimental results

4.2.1 Main result

Table 2 presents our main result. Namely, the perfor-
mance of ClustMRF when used to re-rank the MRF initial
list. Recall that the initial ranking was induced using MRF’s
SDM with free-parameter values set following previous rec-
ommendations [28]. Thus, we also present for reference the
re-ranking performance of using MRF’s SDM with its three
free parameters set using cross validation as is the case for

rank methods [23] other than SVMrank, which proved to
result in highly eﬀective performance as shown below, can
also be used for setting the values of the λl weights.

AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

ClustMRF

10.8

53.0

54.4

21.0

52.4

54.7

18.0

44.9

42.8

14.2

70.1

56.2

ClustMRF

6.3

44.6

29.4

8.9

50.2
33.9
16.1

48.7

37.4

17.0

48.5

36.9

max-
sw2
9.7

min-
geo-
stdv-
sw2
qsim
qsim
9.6
9.4
10.6
43.7c 44.6c 50.9
49.1
45.0c 45.8c 52.0
50.4
16.8c
19.0c 17.7c 20.6
44.7c
46.9c 50.4
50.7
45.9c
49.1c 52.4
52.4
15.4c 12.2c 16.3c 14.2c
38.4c 31.7c 39.3c 33.9c
37.8c 28.6c 39.0c 32.4c
12.7c 12.9c 13.2c 14.2
59.3c 62.3c 58.0c 66.3
48.2c 48.8c 46.6c 52.3
geo-
max-
sw2
qsim
5.4c
4.8c
28.7c 29.3c 18.7c 20.9c
20.3c 20.5c 12.4c 14.0c
8.6
8.6
48.7
47.2
33.9
32.5
14.2c 15.4
12.8c 12.9c
41.9c 42.9c 33.9c 34.2c
30.1c 32.5c 25.5c 25.6c
15.7c 14.8c 15.9
16.3
42.3c 42.9c 43.2
45.0
35.5
32.8
33.6

7.8c
8.3
40.4c 49.3
28.9c 34.3

max-
sw1
5.3c

max-
qsim
4.5c

32.8

Table 3: Using each of ClustMRF’s top-4 feature
functions by itself for ranking the clusters so as to
re-rank the MRF initial list. Boldface: the best per-
formance per row.
’c’ marks a statistically signiﬁ-
cant diﬀerence with ClustMRF.

the free parameters of ClustMRF; TunedMRF denotes this
method. We found that using exhaustive search for ﬁnding
SDM’s optimal parameter values in the training phase yields
better performance (on the test set) than using SVMrank
[12] and SVMmap [36]. Speciﬁcally, λT , λO, and λU were
set to values in {0, 0.05, . . . , 1} with λT + λO + λU = 1.

We ﬁrst see in Table 2 that while TunedMRF outperforms
the initial MRF ranking in most relevant comparisons (ex-
perimental setting × evaluation measure), there are cases
(e.g., for AP and WT10G) for which the reverse holds. The
latter ﬁnding implies that optimal free-parameter values of
MRF’s SDM do not necessarily generalize across queries.

More importantly, we see in Table 2 that ClustMRF out-
performs both the initial ranking and TunedMRF in all rel-
evant comparisons. Many of the improvements are substan-
tial and statistically signiﬁcant. These ﬁndings attest to the
high eﬀectiveness of using ClustMRF for re-ranking.

4.2.2 Analysis of feature functions

We now turn to analyze the relative importance attributed
to the diﬀerent feature functions used in ClustMRF; i.e., the
λl weights assigned to these functions in the training phase
by SVMrank. We ﬁrst average, per experimental setting and
cluster size, the weights assigned to a feature function using
the diﬀerent training folds. Then, the feature function is
assigned with a score that is the reciprocal rank of its cor-
responding (average) weight. Finally, the feature functions
are ordered by averaging their scores across experimental
settings and cluster sizes. Two feature functions, pr and
spam, are only used for the ClueWeb-based settings. Hence,
we perform the analysis separately for the ClueWeb and non-
ClueWeb (AP, ROBUST, WT10G, and GOV2) settings.

337AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

10.8
53.0
54.4
21.0i

Inter AMean GMean CRank CMRF
Init
10.4
10.6
10.1
55.9i 51.1
50.7
56.0i 52.2
50.6
19.9c 20.8i
20.3c
49.1c
52.2
51.0
52.5
51.2c
53.9
15.8c 15.1c 16.6i
c
37.5c 38.0c 39.6i
c
37.2c 36.8c 38.5c
12.7c 12.9c 13.1i
c
59.3c 62.9c 58.8c
48.6c 50.2c 47.8c
4.5c
19.1c 24.3c 19.3c
12.6c 17.8c 13.2c
8.6
8.9
44.8
46.3
32.4
32.6
12.5c 14.9i
33.1c 44.5i
24.4c 34.3i
16.7
15.8
48.2
44.8
33.2
36.4

10.0
50.0
50.5
19.7c
46.6i
c
49.1i
c
14.5c
34.2c
32.7i
c
12.7c
62.3c
48.4c
5.2c
24.3c
18.5i
c
8.3
41.5c
30.0
16.0i
46.6i
35.3i
17.7i
50.3
38.0i

10.6
50.9
52.0
20.6i
50.4
52.4
16.3c
39.3c
39.0c
13.2i
c
58.0c
46.6c
4.8c
20.9c
14.0c
8.6
48.7
33.9
12.9c
34.2c
25.6c
15.9
43.2
33.6

54.7
18.0i
44.9i
42.8i
14.2i
70.1i
56.2i
6.3i
44.6i
29.4i

8.8
49.8i
35.0i
13.0i
c
34.7c
26.1i
c
15.9
45.6
34.4

50.2
33.9
16.1i
48.7i
37.4i

17.0
48.5
36.9

5.3c

4.6c

52.4

8.9

Table 4: Comparison with cluster-based retrieval
methods used for re-ranking the MRF initial list.
(CMRF is a shorthand for ClustMRF.) Boldface
marks the best result in a row. ’i’ and ’c’ mark sta-
tistically signiﬁcant diﬀerences with the initial rank-
ing and ClustMRF, respectively.

For the non-ClueWeb settings, the feature functions, in
descending order of attributed importance, are: stdv-qsim,
max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim,
geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress,
max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy,
geo-sw1, max-entropy. For the ClueWeb settings the feature
functions are ordered as follows: max-sw2, max-sw1, max-
qsim, geo-qsim, max-spam, geo-sw2, min-icompress, min-
sw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, min-
dsim, min-entropy, max-entropy, min-spam, geo-icompress,
geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, min-
pr, max-dsim.

Two main observations rise. First, each of the three types
of cliques used in Section 2.1 for deﬁning the MRF has at
least one associated feature function that is assigned with
a relatively high weight. For example, the geo-qsim func-
tion deﬁned over lQD, the max-qsim function deﬁned over
lQC , and the max-sw2 function deﬁned over lC , are among
the 4, 6 and 2 most important functions in both cases (non-
ClueWeb and ClueWeb settings). Second, for the ClueWeb
settings, the feature functions deﬁned over the lC clique and
which are based on query-independent document measures
(e.g., max-sw1, max-sw2, max-spam) are attributed with
high importance.
In fact, among the top-10 feature func-
tions for the ClueWeb settings only two (max-qsim and geo-
qsim) are not based on a query-independent measure. This
is not the case for the non-ClueWeb settings where diﬀerent
statistics of the query-similarity values are among the top-
10 feature functions. We note that using some of the query-
independent document measures utilized here was shown in
work on Web retrieval to be eﬀective for ranking documents
directly [3]. We demonstrated the merits of using such mea-
sures for ranking document clusters.

In Table 3 we present the performance of using each of the
top-4 feature functions (for the non-ClueWeb and ClueWeb
settings) by itself as a cluster ranking method. As in Sec-
tion 4.2.1, we use the cluster ranking to re-rank the MRF
initial list. We see in Table 3 that in almost all relevant
comparisons ClustMRF is more eﬀective — often to a sub-
stantial and statistically signiﬁcant degree — than using one
of its top-4 feature functions alone. Thus, we conclude that
ClustMRF’s eﬀective performance cannot be attributed to
a single feature function that it utilizes.

We also performed ablation tests as follows. ClustMRF
was trained each time without one of its top-10 feature func-
tions. This resulted in a statistically signiﬁcant performance
decrease with respect to at least one of the three evaluation
metrics of concern (MAP, p@5 and NDCG@5) for all top-10
feature functions for the ClueWeb settings. (Actual num-
bers are omitted as they convey no additional insight.) Yet,
there was no statistically signiﬁcant performance decrease
for any of the top-10 feature functions for the non-ClueWeb
settings. These ﬁndings attest to the redundancy of feature
functions when employing ClustMRF for the non-ClueWeb
settings and to the lack thereof in the ClueWeb settings.

Finally, we computed the Pearson correlation of the learned
λl’s values (averaged over the train folds and cluster sizes)
between experimental settings. We found that for pairs of
non-ClueWeb settings, excluding AP, the correlation was
at least 0.5; however, the correlation with AP was much
smaller. For the ClueWeb settings, the correlation between
ClueB and ClueBF was high (0.83) while that for other pairs
of settings was lower than 0.5. Thus, we conclude that the
learned λl values can be collection, and setting, dependent.

4.2.3 Comparison with cluster-based methods

We next compare the performance of ClustMRF with that
of highly eﬀective cluster-based retrieval methods. All meth-
ods re-rank the MRF initial list.

The InterpolationF method (Inter in short) [13] ranks

documents directly using the score function:
Score(d; Q)

def
= (1 − λ)

sim(Q,d)

sim(Q,d′) +

Pd′∈Dinit

PC∈C l(Dinit ) sim(Q,C)sim(C,d)

λ

Pd′∈Dinit

PC∈C l(Dinit) sim(Q,C)sim(C,d′ ) . This state-of-the-
art re-ranking method represents the class of approaches
that use clusters to “smooth” document representations [13].
In contrast to Inter, ClustMRF belongs to a class of meth-
ods that rely on cluster ranking. Accordingly, the next ref-
erence comparison methods represent this class. Section 4.1
provided a description of how the cluster ranking is trans-
formed to a ranking of the documents in Dinit. The AMean
method [26, 15], for example, scores cluster C by the arith-
metic mean of the query similarity values of its constituent
|C| Pd∈C sim(Q, d).
documents. Formally, Score(C; Q)
Scoring C by the geometric mean of the query-similarity

def
= 1

values of its constituent documents, Score(C; Q)

def
=

|C|qQd∈C sim(Q, d), was shown to yield state-of-the-art clus-

ter ranking performance [15]. This approach, henceforth
referred to as GMean, results from aggregating several fea-
ture functions (geo-qsim) that are used in our ClustMRF
method. (See Section 2.1 for details.)

An additional state-of-the-art cluster ranking method is
ClustRanker (CRank in short) [15]. Cluster C is scored by

Score(C; Q)

def
= (1 − λ)

sim(Q,C)p(C)

PC′ ∈C l(Dinit) sim(Q,C′ )p(C′) +

338DocMRF ClustMRF

AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

9.9
50.7
51.0
20.3
52.1
54.0
17.1
42.0
40.4
15.0
66.3
54.0
9.8
42.4
28.4
9.5

52.6

35.7

16.6
45.6
33.6
17.6
50.3
37.5

11.0

53.5

53.5
21.2d
53.2

55.3

17.7

42.5
40.3
15.3

68.7

55.8

10.0
49.3d
33.4d

9.5
49.6
35.7
18.9d
52.9d
39.9d
19.4d
55.3d
41.9d

Table 5: Using ClustMRF to re-rank the DocMRF
[3] list. Boldface: best result in a row. ’d’ marks a
statistically signiﬁcant diﬀerence with DocMRF.

Pd∈C sim(Q,d)sim(C,d)p(d)

λ

PC′ ∈C l(Dinit) Pd∈C′ sim(Q,d)sim(C′ ,d)p(d) ; p(C) and p(d) are
estimated based on inter-cluster and inter-document (across
clusters) similarities, respectively. These similarities, com-
puted using the language-model-based measure simLM (·, ·),
are not utilized by ClustMRF that uses inter-document sim-
ilarities only within a cluster.

Following the original reports of Inter [13] and CRank [15],
we estimate sim(Q, C) and sim(C, d) in these methods using
simLM (·, ·); C is represented by the concatenation of its con-
stituent documents. For a fair comparison with ClustMRF,
sim(Q, d) is set in all reference comparisons considered here
to simM RF (·, ·), which was used to create the initial MRF
list that is re-ranked.

All free parameters of the methods are set using cross val-
idation. Speciﬁcally, λ which is used by Inter and CRank
is set to values in {0, 0.1, . . . , 1}. The graph out degree
and the dumping factor used by CRank are set to values
in {4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respec-
tively. The cluster size used by each method is selected from
{5, 10, 20} as is the case for ClustMRF. Table 4 presents the
performance numbers.

We can see in Table 4 that in a vast majority of the rele-
vant comparisons ClustMRF outperforms the reference com-
parison methods. Many of the improvements are substantial
and statistically signiﬁcant. In the few cases that ClustMRF
is outperformed by one of the other methods, the perfor-
mance diﬀerences are not statistically signiﬁcant.

4.2.4 Using ClustMRF to re-rank the DocMRF list

Heretofore, we studied the performance of ClustMRF when
used to re-rank the MRF initial list. The analysis presented
in Section 4.2.2 demonstrated the eﬀectiveness — especially
for the ClueWeb settings — of using feature functions that
utilize query-independent document measures. Thus, we
now turn to explore ClustMRF’s performance when em-
ployed over a document ranking that is already based on
using query-independent document measures.

To that end, we follow some recent work [3]. We re-rank
the 1000 documents that are the most highly ranked by
MRF’s SDM that was used above to create the MRF ini-
tial list. Re-ranking is performed using an MRF model that
is enriched with query-independent document measures [3].
We use the same document measures utilized by ClustMRF,
except for dsim which is based on inter-document similar-
ities and which was not considered in this past work that
ranked documents independently of each other [3]. The re-
sultant ranking, induced using SVMrank for learning param-
eter values, is denoted DocMRF. (SVMrank yielded better
performance than SVMmap.) We then let ClustMRF re-
rank the top-50 documents.
In doing so, we use the ex-
ponent of the score assigned by DocMRF to document d,
which is a rank equivalent estimate to that of log p(Q, d),
as the sim(Q, d) value used by ClustMRF. Thus, we main-
tain the invariant mentioned above that the scoring function
used to induce the ranking upon which ClustMRF operates
is rank equivalent to the document-query similarity measure
used in ClustMRF. We note that ClustMRF is diﬀerent from
DocMRF in two important respects. First, by the virtue of
ranking clusters ﬁrst and transforming the ranking to that
of documents rather than ranking documents directly as is
the case in DocMRF. Second, by the completely diﬀerent
ways that document-query similarities are used.

Comparing the performance of DocMRF in Table 5 with
that of the MRF initial ranking in Table 2 attests to the
merits of using DocMRF for re-ranking. We can also see
in Table 5 that applying ClustMRF over the DocMRF list
results in performance improvements in almost all relevant
comparisons. Many of the improvements for the ClueWeb
settings are substantial and statistically signiﬁcant.

4.2.5 Using ClustMRF to re-rank the LM list

The third list we re-rank using ClustMRF is LM, which
was created using unigram language models. For reference
comparison we use the cluster-based Inter method which was
used in Section 4.2.3. Experiments show — actual num-
bers are omitted due to space considerations — that for re-
ranking the LM list, the GMean cluster ranking method is
more eﬀective in most relevant comparisons than the other
two cluster ranking methods used in Section 4.2.3 for ref-
erence comparison (AMean and CRank). Hence, GMean is
used here as an additional reference comparison.

ClustMRF, Inter and GMean use the simLM (·, ·) similar-
ity measure, which was used for inducing the initial ranking,
for sim(Q, d). All other implementation details are the same
as those described above. As a result, ClustMRF, as well
as Inter and GMean, use only unigram language models in
the LM setting considered here. This is in contrast to the
MRF-list setting considered above where term-proximities
information was used.

An additional reference comparison that uses unigram lan-
guage models is relevance model number 3 [1], RM3, which
is a state-of-the-art query expansion approach. RM3 is also
used to re-rank the LM list. All (50) documents in the list
are used for constructing RM3.
Its free-parameter values
are set using cross validation. Speciﬁcally, the number of
expansion terms and the interpolation parameter that con-
trols the reliance on the original query are set to values in
{5, 10, 25, 50} and {0.1, 0.3, . . . , 0.9}, respectively. Dirichlet-
smoothed language models are used with µ = 1000.

339AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

Init

Inter
10.6i
9.9
56.1i
49.6
c
55.6i
49.9
19.3c 20.1i
49.5c 50.9
51.6c 53.1
14.9
15.0
36.4c 37.5
35.8
37.1
11.8c 12.6i
c
56.6c 62.4i
c
46.5c 50.4i
5.0i
3.3c
16.1c 24.6i
c
10.7c 17.9i
c
8.5i
8.0c
46.7
47.4
32.3
32.6
11.4c 13.8i
c
29.0c 40.5i
21.2c 29.6i
14.7c 15.6
42.9c 46.3
32.1c 34.6

GMean RM3 ClustMRF
10.8i
50.7
51.8
20.6i
52.1
53.8
14.9
37.5
35.5
12.4i
c
60.8i
c
48.8c
3.7i
c
17.2c
11.5c
8.2
45.7
32.3
12.0i
c
31.6i
c
23.4i
c
15.5
43.4
33.4c

10.5
9.9
51.3
49.1
51.7
49.3
19.7i
c 20.5i
49.7c 52.9i
52.1c 55.6i
14.5
14.6
36.6c 42.2i
35.9
39.3
13.5i
12.7i
c
60.4i
68.4i
c
49.1c 54.3i
3.8i
c
17.4c 43.3i
11.0c 27.7i
8.7i
47.6
34.3
35.6
13.9i
16.0i
c
40.2i 46.0i
30.0i 34.8i
16.4i 16.8i
48.9i 49.2i
36.6i 38.7i

8.7i
51.5

5.5i

Table 6: Re-ranking the LM initial list. Boldface:
the best result in a row. ’i’ and ’c’ mark statistically
signiﬁcant diﬀerences with the initial ranking and
ClustMRF, respectively.

We see in Table 6 that ClustMRF outperforms the ref-
erence comparisons in a vast majority of the relevant com-
parisons. Many of the improvements are substantial and
statistically signiﬁcant. These results, along with those pre-
sented in Sections 4.2.1 and 4.2.4, attest to the eﬀectiveness
of using ClustMRF to re-rank diﬀerent initial lists.

4.2.6 Varying the clustering algorithm

Thus far, we used ClustMRF and the reference compar-
isons with nearest-neighbor (NN) clustering. In Table 7 we
present the retrieval performance of using hierarchical ag-
glomerative clustering (HAC) with the complete link mea-
sure. This clustering was shown to be among the most ef-
fective hard clustering methods for cluster-based retrieval
[24, 13]. We use
simLM (d2,d1) for an inter-
document dissimilarity measure; and, cut the clustering den-
drogram so that the resultant average cluster size is the clos-
est to a value k (∈ {5, 10, 20}). Doing so somewhat equates
the comparison terms with using the NN clusters whose size
is in {5, 10, 20}. Cross validation is used in all cases for
setting the value of k.

simLM (d1 ,d2) +

1

1

The MRF initial list is clustered and serves as the ba-
sis for re-ranking. Experiments show (actual numbers are
omitted due to space considerations) that among the three
cluster ranking methods which were used above for refer-
ence comparison (AMean, GMean, and CRank) CRank is
the most eﬀective when using HAC. Hence, CRank serves
as a reference comparison here.

We see in Table 7 that in the majority of relevant com-
parisons, ClustMRF improves over the initial ranking when
using HAC. In contrast, CRank is outperformed by the ini-
tial ranking in most relevant comparisons for HAC. Indeed,
ClustMRF outperforms CRank in most cases for both NN
and HAC. We also see that ClustMRF is (much) more eﬀec-
tive when using the overlapping NN clusters than the hard

Init

HAC

NN

CRank ClustMRF CRank ClustMRF

AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

10.1 9.9
50.7 49.8
50.6 50.5
19.9 19.1
51.0 50.1
52.5 51.7
15.8 14.8
37.5 36.6
37.2 34.4
12.7 13.2i
59.3 61.5
48.6 49.7
5.6i
4.5
19.1 23.7
12.6 16.9i
8.6
46.3 43.9
32.4 32.0
12.5 14.4i
33.1 39.5i
24.4 30.6i
15.8 15.3
44.8 43.9
33.2 32.7

8.4

9.6i
46.5i
46.8i
19.6

50.4

51.9

15.8

38.2

37.0
13.6i

63.9

51.5
5.8i
31.7i
c
21.0i

9.2

48.9

33.4
14.5i
39.7i
30.3i
15.2
43.1
32.5

10.0
50.0
50.5
19.7
46.6i
49.1i
14.5
34.2
32.7i
12.7
62.3
48.4
5.2
24.3
18.5i
8.3
41.5
30.0
16.0i
46.6i
35.3i
17.7i
50.3
38.0i

10.8

53.0

54.4
21.0i
c

52.4

c

54.7
c
18.0i
c
44.9i
c
42.8i
c
14.2i
c
70.1i
c
56.2i
c
6.3i
c
44.6i
c
29.4i
c

8.9

50.2

c

33.9
16.1i
48.7i
37.4i

17.0
48.5
36.9

Table 7: Using nearest-neighbor clustering (NN) vs.
(complete link) hierarchical agglomerative cluster-
ing (HAC). The MRF initial list is used. Boldface:
the best result in a row per clustering algorithm; un-
derline: the best result in a row. ’i’ and ’c’: statisti-
cally signiﬁcant diﬀerences with the initial ranking
and CRank, respectively.

clusters created by HAC. The improved eﬀectiveness of us-
ing NN in comparison to HAC echoes ﬁndings in previous
work on cluster-based re-ranking [13]. For CRank, the per-
formance of using neither NN nor HAC dominates that of
using the other.

4.2.7 The effect of the size of the initial list

Until now, ClustMRF and all reference comparison meth-
ods were used to re-rank an initial list of 50 documents. Us-
ing a short list follows common practice in work on cluster-
based re-ranking [18, 25, 26, 13] as was mentioned in Section
4.1. We now turn to study ClustMRF’s performance when
re-ranking longer lists. To that end, we use for the initial list
the n (∈ {50, 100, 250, 500}) documents that are the most
highly ranked by MRF’s SDM [28] which was used above
for creating the MRF initial list. For reference comparisons
we use TunedMRF (see Section 4.2.1); and, the AMean and
GMean cluster ranking methods described in Section 4.2.3.
Nearest-neighbor clustering is used.

We see in Figure 2 that in almost all cases — i.e., ex-
perimental settings and values of n — ClustMRF outper-
forms both the initial ranking and TunedMRF; often, the
performance diﬀerences are quite substantial. Furthermore,
in most cases (with the notable exception of AP) ClustMRF
outperforms AMean and GMean.

4.2.8 Diversifying search results

We next explore how ClustMRF can be used to improve
the performance of search-results diversiﬁcation approaches.
Speciﬁcally, we use the MMR [5] and the state-of-the-art
xQuAD [29] diversiﬁcation methods.

340AP

ROBUST

WT10G

GOV2

P
A
M

P
A
M

13.5

13.0

12.5

12.0

11.5

11.0

10.5

10.0

9.5

11.0

10.0

9.0

8.0

7.0

6.0

5.0

4.0

50

100

250

n

ClueA

Init
TunedMRF
AMean
GMean
ClustMRF

Init
TunedMRF
AMean
GMean
ClustMRF

P
A
M

23.0

22.5

22.0

21.5

21.0

20.5

20.0

19.5

500

50

100

P
A
M

10.0

9.5

9.0

8.5

8.0

7.5

Init
TunedMRF
AMean
GMean
ClustMRF

250

n

ClueAF

Init
TunedMRF
AMean
GMean
ClustMRF

P
A
M

19.0

18.5

18.0

17.5

17.0

16.5

16.0

15.5

15.0

500

50

100

P
A
M

20.0

19.0

18.0

17.0

16.0

15.0

14.0

13.0

12.0

11.0

Init
TunedMRF
AMean
GMean
ClustMRF

250

n

ClueB

Init
TunedMRF
AMean
GMean
ClustMRF

P
A
M

18.0

17.0

16.0

15.0

14.0

13.0

12.0

500

50

100

P
A
M

19.0

18.0

17.0

16.0

15.0

14.0

Init
TunedMRF
AMean
GMean
ClustMRF

250

n

ClueBF

500

Init
TunedMRF
AMean
GMean
ClustMRF

50

100

250

n

500

50

100

250

n

500

50

100

250

n

500

50

100

250

n

500

Figure 2: The eﬀect on MAP(@50) performance of the size n of the MRF initial list that is re-ranked.

Init

MMR

xQuAD

MRF QClust ClustMRF MRF QClust ClustMRF

ClueA

ClueAF

ClueB

ClueBF

α-NDCG 24.5
16.0
ERR-IA
P-IA
11.8
α-NDCG 42.6
32.0
ERR-IA
P-IA
21.0
α-NDCG 33.2
ERR-IA
21.1
P-IA
15.4
α-NDCG 41.6
29.7
ERR-IA
P-IA
18.9

26.2c
17.3c
10.3c
42.9
32.3
20.2c
33.6c
21.3c
14.4i
c
42.6i
c
30.2i
c
18.4

25.4c
17.5c
9.6i
c
39.0i
c
29.8c
14.9i
c
33.9c
21.5c
12.8i
c
38.7i
c
27.0i
c
14.5i
c

38.7i
30.5i
16.7i

43.8

34.2
17.6i
43.7i
32.0i
17.4i
45.4i
33.3i
17.8

27.4i
c
17.9i
c
13.3c
44.3i
33.4i
21.0
39.7i
c
25.9i
c
19.4i
c
46.1i
c
33.2i
21.4i
c

28.9i
c
19.6i
c
13.6i
c
43.7
33.1
20.0
39.3i
c
25.3i
c
19.2i
c
44.2i
c
31.2c
20.9i
c

38.8i
30.6i
17.2i
45.5i
34.9i
20.6
45.5i
32.9i
21.0i
48.1i
34.8i
22.0i

Table 8: Diversifying search results. Underline and boldface mark the best result in a row, and per diver-
siﬁcation method in a row, respectively.
’i’ and ’c’ mark statistically signiﬁcant diﬀerences with the initial
ranking (Init) and ClustMRF, respectively. The MRF initial list is used.

MMR and xQuAD iteratively re-rank an initial list Dinit.
In each iteration the document in Dinit \ S assigned with
the highest score is added to the set S; S is empty at the
beginning. The ﬁnal ranking is determined by the order of
insertion to S.

The score MMR assigns to document d (∈ Dinit \ S) is
βsim1(Q, d)−(1−β) maxdi∈S sim2(d, di); β is a free parame-
ter; sim1(·, ·) and sim2(·, ·) are discussed below. In contrast
to MMR, xQuAD uses information about Q’s subtopics,
T (Q), and assigns d with the score βp(d|Q)+

(1 − β)Pt∈T (Q) hp(t|Q)p(d|t)Qdi∈S(1 − p(di|t))i; p(t|Q) is

the relative importance of subtopic t with respect to Q;
p(d|Q) and p(d|t) are the estimates of d’s relevance to Q
and t, respectively.

The parameter β controls in both methods the tradeoﬀ
between using relevance estimation and applying diversiﬁ-
cation. Our focus is on improving the former and evaluat-
ing the resulting (diversiﬁcation based) performance. This
was also the case in previous work that used cluster ranking

for results diversiﬁcation [11]. Hence, this work serves for
reference comparison below.8

We study three diﬀerent estimates for sim1(Q, d) (used
in MMR) which we also use for p(d|Q) (used in xQuAD).9
The ﬁrst, simM RF (Q, d), is that employed in the evalua-
tion above to create the MRF initial list that is also used
here for re-ranking. (Further details are provided below.)
The next two estimates are based on applying cluster rank-
ing and transforming it to document ranking using the ap-
1
proach described in Section 4.1. In these cases,
r(d) serves
for sim1(Q, d), where r(d) is the rank of d in the document
result list produced by using the cluster ranking method.
The ﬁrst cluster ranking method is ClustMRF. The second,
QClust, was used in the work mentioned above on utilizing
cluster ranking for results diversiﬁcation [11]. Speciﬁcally,
cluster C is scored by simLM (Q, C) (see Section 4.1 for de-

8There is work on using information induced from clusters
for the diverisiﬁcation itself (e.g., [21]). Using ClustMRF for
cluster ranking in these approaches is future work.
9For scale compatibility, the two resultant quantities that
are interpolated (using β) in MMR and xQuAD are sum
normalized with respect to all documents in Dinit before the
interpolation is performed.

341tails of simLM (·, ·)); C is represented by the concatenation
of its documents.

We use MMR and xQuAD to re-rank the MRF initial
list that contains 50 documents. simLM (·, ·) serves for the
sim2(·, ·) measure used in MMR and for p(d|t) that is used in
xQuAD. The oﬃcial TREC subtopics, which are available
for the ClueWeb settings that we use here, were used for
def
experiments. Following the ﬁndings in [29], we set p(t|Q)
=
|T (Q)| . The value of β is selected from {0.1, 0.2, . . . , 0.9}
using cross validation; α-NDCG (@20) is the optimization
metric. In addition to α-NDCG (@20), ERR-IA (@20) and
P-IA (@20) are used for evaluation.

1

Table 8 presents the results. We see that using the MRF
similarity measure in MMR and xQuAD outperforms the ini-
tial ranking, which was created using this measure, in most
relevant comparisons. This attests to the diversiﬁcation ef-
fectiveness of MMR and xQuAD. Using QClust outperforms
the initial ranking in most cases, but is consistently out-
performed by using the MRF measure and our ClustMRF
method. More generally, the best performance for each di-
versiﬁcation method (MMR and xQuAD) is almost always
attained by ClustMRF, which often outperforms the other
methods in a substantial and statistically signiﬁcant man-
ner. Thus, although ClustMRF ranks clusters of similar
documents, using the resultant document ranking can help
to much improve results-diversiﬁcation performance.

5. CONCLUSIONS

We presented a novel approach to ranking (query speciﬁc)
document clusters by their presumed relevance to the query.
Our approach uses Markov Random Fields that enable the
integration of various types of cluster-relevance evidence.
Empirical evaluation demonstrated the eﬀectiveness of us-
ing our approach to re-rank diﬀerent initially retrieved lists.
The approach also substantially outperforms state-of-the-art
cluster ranking methods and can be used to substantially
improve the performance of results diversiﬁcation methods.

6. ACKNOWLEDGMENTS

We thank the reviewers for their comments. This work has
been supported by and carried out at the Technion-Microsoft
Electronic Commerce Research Center.

7. REFERENCES
[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey,

X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 —
novelty and hard. In Proc. of TREC-13, 2004.

[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher,

and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, 2000.

[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased

ranking of web documents. In Proc. of WSDM, pages 95–104,
2011.

[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual

web search engine. In Proc. of WWW, pages 107–117, 1998.

[5] J. G. Carbonell and J. Goldstein. The use of MMR,

diversity-based reranking for reordering documents and
producing summaries. In Proc. of SIGIR, pages 335–336, 1998.
[6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Eﬃcient

and eﬀective spam ﬁltering and re-ranking for large web
datasets. Informaltiom Retrieval Journal, 14(5):441–465, 2011.

[7] W. B. Croft. A model of cluster searching based on

classiﬁcation. Information Systems, 5:189–195, 1980.

[8] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey.

Scatter/Gather: A cluster-based approach to browsing large
document collections. In Proc. of SIGIR, pages 318–329, 1992.

[9] D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam,

and statistics: Using statistical analysis to locate spam web
pages. In Proc. of WebDB, pages 1–6, 2004.

[10] N. Fuhr, M. Lechtenfeld, B. Stein, and T. Gollub. The

optimum clustering framework: implementing the cluster
hypothesis. Information Retrieval Journal, 15(2):93–115, 2012.
[11] J. He, E. Meij, and M. de Rijke. Result diversiﬁcation based on

query-speciﬁc cluster ranking. JASIST, 62(3):550–571, 2011.
[12] T. Joachims. Training linear svms in linear time. In Proc. of

KDD, pages 217–226, 2006.

[13] O. Kurland. Re-ranking search results using language models of

query-speciﬁc clusters. Journal of Information Retrieval,
12(4):437–460, August 2009.

[14] O. Kurland and C. Domshlak. A rank-aggregation approach to

searching for optimal query-speciﬁc clusters. In Proc. of
SIGIR, pages 547–554, 2008.

[15] O. Kurland and E. Krikon. The opposite of smoothing: A

language model approach to ranking query-speciﬁc document
clusters. Journal of Artiﬁcial Intelligence Research (JAIR),
41:367–395, 2011.

[16] O. Kurland and L. Lee. Corpus structure, language models, and

ad hoc information retrieval. In Proc. of SIGIR, pages
194–201, 2004.

[17] O. Kurland and L. Lee. PageRank without hyperlinks:

Structural re-ranking using links induced by language models.
In Proc. of SIGIR, pages 306–313, 2005.

[18] O. Kurland and L. Lee. Respect my authority! HITS without

hyperlinks utilizing cluster-based language models. In Proc. of
SIGIR, pages 83–90, 2006.

[19] O. Kurland, F. Raiber, and A. Shtok. Query-performance

prediction and cluster ranking: Two sides of the same coin. In
Proc. of CIKM, pages 2459–2462, 2012.

[20] K.-S. Lee, Y.-C. Park, and K.-S. Choi. Re-ranking model based
on document clusters. Inf. Process. Manage., 37(1):1–14, 2001.
[21] T. Leelanupab, G. Zuccon, and J. M. Jose. When two is better
than one: A study of ranking paradigms and their integrations
for subtopic retrieval. In Proc. of AIRS, pages 162–172, 2010.

[22] A. Leuski. Evaluating document clustering for interactive

information retrieval. In Proc. of CIKM, pages 33–40, 2001.

[23] T.-Y. Liu. Learning to Rank for Information Retrieval.

Springer, 2011.

[24] X. Liu and W. B. Croft. Cluster-based retrieval using language

models. In Proc. of SIGIR, pages 186–193, 2004.

[25] X. Liu and W. B. Croft. Experiments on retrieval of optimal

clusters. Technical Report IR-478, Center for Intelligent
Information Retrieval (CIIR), University of Massachusetts,
2006.

[26] X. Liu and W. B. Croft. Evaluating text representations for
retrieval of the best group of documents. In Proc. of ECIR,
pages 454–462, 2008.

[27] D. Metzler. A feature-centric view of information retrieval.

Springer, 2011.

[28] D. Metzler and W. B. Croft. A Markov random ﬁeld model for

term dependencies. In Proc. of SIGIR, pages 472–479, 2005.

[29] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query
reformulations for web search result diversiﬁcation. In Proc. of
WWW, pages 881–890, 2010.

[30] J. Seo and W. B. Croft. Geometric representations for multiple

documents. In Proc. of SIGIR, pages 251–258, 2010.

[31] J. G. Shanahan, J. Bennett, D. A. Evans, D. A. Hull, and

J. Montgomery. Clairvoyance Corporation experiments in the
TREC 2003. High accuracy retrieval from documents (HARD)
track. In Proc. of TREC-12, pages 152–160, 2003.

[32] A. Tombros, R. Villa, and C. van Rijsbergen. The eﬀectiveness

of query-speciﬁc hierarchic clustering in information retrieval.
Inf. Process. Manage., 38(4):559–582, 2002.

[33] C. J. van Rijsbergen. Information Retrieval. Butterworths,

second edition, 1979.

[34] E. M. Voorhees. The cluster hypothesis revisited. In Proc. of

SIGIR, pages 188–196, 1985.

[35] P. Willett. Query speciﬁc automatic document classiﬁcation.

International Forum on Information and Documentation,
10(2):28–32, 1985.

[36] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support

vector method for optimizing average precision. In Proc. of
SIGIR, pages 271–278, 2007.

[37] C. Zhai and J. D. Laﬀerty. A study of smoothing methods for

language models applied to ad hoc information retrieval. In
Proc. of SIGIR, pages 334–342, 2001.

342