IRWR: Incremental Random Walk with Restart

Weiren Yu†‡, Xuemin Lin(cid:2)†

†The University of New South Wales, Australia

‡NICTA, Australia

(cid:2)East China Normal University, China

{weirenyu, lxue}@cse.unsw.edu.au

ABSTRACT
Random Walk with Restart (RWR) has become an appeal-
ing measure of node proximities in emerging applications
e.g., recommender systems and automatic image captioning.
In practice, a real graph is typically large, and is frequently
updated with small changes. It is often cost-inhibitive to re-
compute proximities from scratch via batch algorithms when
the graph is updated. This paper focuses on the incremental
computations of RWR in a dynamic graph, whose edges
often change over time. The prior attempt of RWR [1]
deploys k-dash to ﬁnd top-k highest proximity nodes for
a given query, which involves a strategy to incrementally
estimate upper proximity bounds. However, due to its aim
to prune needless calculation, such an incremental strategy
in O(1) time for each node. The main
is approximate:
contribution of this paper is to devise an exact and fast
incremental algorithm of RWR for edge updates. Our solu-
tion, IRWR , can incrementally compute any node proximity
in O(1) time for each edge update without loss of exact-
ness. The empirical evaluations show the high eﬃciency and
exactness of IRWR for computing proximities on dynamic
networks against its batch counterparts.
Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Storage and Retrieval
Keywords
Random Walk with Restart; Proximity; Dynamic graph

1.

INTRODUCTION

Measuring node proximities in graphs is a key task of web
search. Due to various applications in recommender systems
and social networks, many proximity metrics have come into
play. For instance, Brin and Page [2] invented PageRank
to determine the ranking of web pages. Jeh and Widom
[3] proposed SimRank to assess node-to-node proximities.
Random Walk with Restart (RWR) [4] is one of such useful
proximity metrics for ranking nodes in order of relevance to

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

G (exclude edge

(u1, u5))

u6

u5

u1

u2

query
node

u4

u7

P =

u3

an edge to be
inserted to G
(in ΔG)

0.25 × [P]

− 0.32 × [P]

(cid:2),u5

(cid:2),u1

⎡

⎢⎢⎢⎢⎢⎢⎢⎣

.31 .25 .18 .14 .13 .11 .10
.08 .26 .07 .06 .10 .04 .04
.11 .09 .30 .24 .14 .19 .17
.14 .11 .13 .30 .17 .24 .22
.29 .23 .26 .21 .36 .17 .15
.08 .06 .07 .06 .10 .24 .12
0 .20

0

0

0

0

0

(cid:2),u2

⎡

⎤

+ 0.06 × e5 = [ΔP]
⎤
.036
−.011
−.016
−.020
.022
−.011
0

⎥⎥⎥⎥⎥⎥⎥⎦

⎥⎥⎥⎥⎥⎥⎥⎦

⎢⎢⎢⎢⎢⎢⎢⎣

adding edge (u1, u5) incurs change [ΔP]

(cid:2),u2

Figure 1: Computing RWR Incrementally

a query node. In RWR, the proximity of node u w.r.t. query
node q is deﬁned as the limiting probability that a random
surfer, starting from q, and then iteratively either moving
to one of its out-neighbors with probability weighted by the
edge weights, or restarting from q with probability c, will
eventually arrive at node u. Recently, RWR has received
increasing attention (e.g., for collaborative ﬁltering [1] and
image labeling [5]) since it can fairly capture the global
structure of graphs, and relations in interlinked networks [6].
Prior RWR computing methods are based on static graphs,
which is costly: Given a graph G(V, E), and a query q ∈ V ,
k-dash [1] yields, in the worse case, O(|V |2) time and space,
which, in practice, can be bounded by O(|E| + |V |), to ﬁnd
top-k highest proximity nodes. B LIN and NB LIN [4] need
O(|V |2) time and space for computing all node proximities.
In general, real graphs are often constantly updated with
small changes. This calls for the need for incremental algo-
rithms to compute proximities. We state the problem below:
Problem (Incremental Update for RWR)
Given a graph G, proximities P for G, changes ΔG to G,
a query node q, and a restarting probability c ∈ (0, 1).

Compute changes to the proximities w.r.t. q exactly.
Here, P is a proximity matrix whose entry [P]i,j denotes the
proximity of node i w.r.t. query j, and ΔG is comprised of
a set of edges to be inserted into or deleted from G.

In contrast with the existing batch algorithms [1, 4] that
recompute the updated proximities from scratch, our incre-
mental algorithm can exploit the dynamic nature of graphs
by pre-computing proximities only once on the entire graph
via a batch algorithm, and then incrementally computing
their changes in response to updates. The response time of
RWR can be greatly improved by maximal use of previous
computation, as shown in Example 1.

Example 1. Figure 1 depicts a graph G, taken from [1].
Given the query u2, the old proximities P for G, and c =
0.2, we want to compute new proximities w.r.t. u2 when
there is an edge (u1, u5) inserted into G, denoted by ΔG.
The existing methods, k-dash and B LIN, have to recompute
the new proximities in G ∪ ΔG from scratch, without using
the previously computed proximities in G, which is costly.

10171 to the old
However, we observe that the increment [ΔP](cid:3),u2
[P](cid:3),u2 is the linear combination of [P](cid:3),u1 and [P](cid:3),u5 ,i.e.,

2

[ΔP](cid:3),u2 = α · [P](cid:3),u1 + β · [P](cid:3),u5 + λ · e5

(1)
with α = 0.25, β = −0.32, λ = 0.06. Hence, there are op-
portunities to incrementally compute the changes [ΔP](cid:3),u2
by fully utilizing the old proximities of P. As opposed to
k-dash and B LIN involving matrix-vector multiplications,
computing [ΔP](cid:3),u2 via Eq.(1) only needs vector scaling and
additions, thus greatly improving the response time.

As suggested by Example 1, when the graph G is updated,
it is imperative to incrementally compute new proximities by
leveraging information from the old proximities. However, it
is a grand challenge to characterize the changes [ΔP](cid:3),q in
terms of a linear combination of the columns in old P, since
it seems hard to determine the scalars α, β, λ for Eq.(1).
Worse still, much less is known about how to extract a subset
of columns from the old P (e.g., why [P](cid:3),u1 and [P](cid:3),u5 are
chosen from P in Eq.(1)), to express the changes [ΔP](cid:3),q.
Contributions. This paper aims to tackle these problems.
To the best of our knowledge, this work makes the ﬁrst eﬀort
to study incremental RWR computing in evolving graphs,
with no loss of exactness. 1) We ﬁrst consider unit update,
i.e., a single-edge insertion or deletion, and derive an elegant
formula that characterizes the proximity changes as a linear
combination of the columns from the old proximity matrix.
2) We then devise an incremental algorithm for batch update,
i.e., a list of edge deletions and insertions mixed together,
and show that any node proximity can be computed in O(1)
time for every edge update, with no sacriﬁce in accuracy.
3) Our empirical study demonstrates that the incremental
approach greatly outperforms k-dash [1], a batch algorithm
that is reported as the best for RWR proximity computing,
when networks are constantly updated.
Organization. Section 2 overviews the background of RWR.
The incremental RWR is studied in Section 3. Section 4
gives empirical results, followed by open issues in Section 5.
Related Work. Incremental algorithms have proved useful
in various node proximity computations on evolving graphs,
such as the personalized PageRank [7] and SimRank [8].
However, very few results are known on incremental RWR
computing, far less than their batch counterparts [1, 4, 9].
k-dash [1] is the best known approach to ﬁnding top-k high-
est RWR proximity nodes for a given query, which involves a
strategy to incrementally estimate upper proximity bounds.
Nevertheless, such an incremental strategy is approximate:
in O(1) time for each node, which is mainly developed for
pruning unnecessary computation.
In contrast, our incre-
mental algorithm can, without loss of exactness, compute
any node proximity in O(1) time for every edge update.
Moore et al. [10] leveraged a sampling approach with branch
and bound pruning to ﬁnd near neighbors of a query w.h.p..
However, their incremental algorithm is probabilistic. Later,
Zhou et al. [9] generalized the original RWR by incorporat-
ing node attributes into link structure for graph clustering.
Based on this, an incremental version of [9] was proposed by
Cheng et al. [11], with the focus to support attribute update.
It diﬀers from this work in that our incremental algorithm is
designed for structure update. Thus, [11] cannot cope with
hyperlink changes incrementally in dynamic graphs.

1[X](cid:3),j denotes the j-th column vector of matrix X.
2ei is the |V | × 1 unit vector with a 1 in the i-th entry.

2. PRELIMINARIES

We formally overview the background of this paper. Graphs

studies here are directed graphs with no multiple edges.
RWR Formula [1]. In a graph G(V, E), let A be the tran-
sition matrix (i.e., column normalized adjacency matrix) of
dv if (u, v) ∈ E, and 0 otherwise.
G, whose entry [A]u,v = 1
Here, dv denotes the in-degree of v. Given query node q ∈ V ,
and restart probability c ∈ (0, 1), the proximity of node u
w.r.t. q, denoted by [P]u,q, is recursively deﬁned as follows:

[P](cid:3),q = (1 − c) · A · [P](cid:3),q + c · eq

(2)

where [P](cid:3),q is the |V |× 1 proximity vector w.r.t. q (i.e., the
q-th column of matrix P), whose u-th entry equals to [P]u,q.
eq is the |V | ×1 unit query vector, whoseq -th entry is 1.

Intuitively, [P]u,q is the limiting probability, denoting the
long-term visit rate of node u, given a bias toward query q.
The RWR proximity deﬁned in Eq.(2) can be rewritten as

[P](cid:3),q = c(I − (1 − c) · A)
where I is the |V | × |V | identity matrix.

−1 · eq

(3)

Existing methods of computing RWR are in a batch style,
with the aim to accelerate the matrix inversion in Eq.(3).
For instance, k-dash [1] uses LU decomposition and an incre-
mental pruning strategy to speed up the matrix inversion.

INCREMENTAL RWR COMPUTING

3.
We now study the incremental RWR computation. Given
the old P for G, changes ΔG to G, query q, and c ∈ (0, 1),
the goal is to compute [ΔP](cid:3),q for ΔG. The key idea of our
approach is to maximally reuse the previous computation,
by characterizing [ΔP](cid:3),q as a linear combination of the
columns from the old P. The main result is as follows.

Theorem 1. Any node proximity of a given query can be
incrementally computed in O(1) time for each edge update.

To prove Theorem 1, we ﬁrst consider unit edge update,
and then devise an incremental algorithm for batch updates,
with the desired complexity bound.
Unit Update. The update (insertion/deletion) of an edge
from G may lead to the changes [ΔP](cid:3),q of the proximity.
We incrementally compute [ΔP](cid:3),q based on the following.
Proposition 1. Given a query q, and the old proximity
matrix P for G, if there is an edge insertion (i, j) into G,
then the changes [ΔP](cid:3),q w.r.t. q can be computed as

(1−c)[P]j,q
1−(1−c)[y]j

· y with

(4)

[ΔP](cid:3),q =

(cid:2) 1
c [P](cid:3),i
c(dj +1) ([P](cid:3),i − 1

1

(cid:2) − 1

y =

(dj = 0)
(dj > 0)
where dj is the in-degree of node j in the old G, and [y]j is
the j-th entry of vector y.

1−c [P](cid:3),j ) +

(1−c)(dj +1)

If there is an edge deletion (i, j) from G, then [ΔP](cid:3),q can

1

ej

also be computed via Eq.(4) with y being replaced by

y =

c [P](cid:3),i
c(dj−1) ( 1
1

1−c [P](cid:3),j − [P](cid:3),i) −

1

(1−c)(dj−1)

ej

(dj = 1)
(dj > 1)

As opposed to the traditional methods, e.g., k-dash and
B LIN, that requires matrix-vector multiplications to com-
pute new proximities via Eq.(2), Proposition 1 allows merely
vector scaling and additions for eﬃciently computing [ΔP](cid:3),q.
The proof of Proposition 1 is attained by combining the

three following lemmas.

1018(cid:2)

Lemma 1. Let A be the old transition matrix of G.

If
there is an edge insertion (i, j) into G, then the new transi-
tion matrix ˜A is updated by

˜A = A + aeT

j with a =

ei

1

dj +1 (ei − [A](cid:3),j )

(dj = 0)
(dj > 0)

(5)

If there is an edge deletion (i, j) from G, then the new ˜A

is also updated as Eq.(5) with a being replaced by

(cid:2)

a =

ei

1

dj−1 ([A](cid:3),j − ei)

(dj = 1)
(dj > 1)

(6)

Proof. Due to space limits, we shall merely prove the

dj

insertion case. A similar proof holds for the deletion case.

eieT

, i.e., A ⇒ A + 1
eieT
j ]

(i) When dj = 0, [A](cid:3),j = 0. Thus, for an inserted edge
(i, j), [A]i,j will be updated from 0 to 1, i.e., ˜A = A + eieT
j .
(ii) When dj > 0, all the nonzero entries of [A](cid:3),j are 1
.
dj
Thus, for an inserted edge (i, j), we ﬁrst update [A]i,j from
0 to 1
j , and then change all nonzero
dj
entries of [A + 1
from 1
dj +1 . Recall from
dj
dj
the elementary matrix property that multiplying the j-th
column of a matrix by α (cid:5)= 0 can be accomplished by using
I − (1 − α)ejeT
j as a right-hand multiplier on the matrix.
(cid:3)
Hence, scaling [A + 1
dj
A + 1
dj
eieT
= A + 1
dj
dj +1 (ei − [A](cid:3),j )eT
= A + 1

by α =
I − (1 − dj
dj +1 (A + 1
dj

(cid:4)(cid:3)
j − 1

dj +1 )ej eT
eieT

dj
dj +1 yields

eieT
j ]
eieT
j

j )ej eT
j

˜A =

(cid:4)

to

(cid:3),j

(cid:3),j

1

j

j

Combining (i) and (ii), Eq.(5) is derived.
Lemma 1 suggests that each edge change will incur a rank-
one update of A. To see how the update of A aﬀects the
changes to P, we have the following lemma.

Lemma 2. Let P be the old proximity matrix for G. If
there is an edge update( i, j) to G, then the new proximity
˜P w.r.t. a given query q is updated as

[ ˜P](cid:3),q = [P](cid:3),q + (1− c)γ · z

with γ =

[P]j,q

1−(1−c)·[z]j

and z = (I − (1 − c)A)

−1a,

where the vector a is deﬁned by Lemma 1.

Proof. By RWR deﬁnition in Eq.(3), [ ˜P](cid:3),q satisﬁes

where ˜A is the new transition matrix that is expressed as
˜A = A + aeT

j by Lemma 1. Thus, Eq.(8) is rewritten as

(I − (1 − c) ˜A) · [ ˜P](cid:3),q = ceq
(cid:6)

(cid:6)(cid:5)

(cid:5)

(cid:6)

(cid:5)

I − (1 − c)A −(1 − c)a

−1

[ ˜P](cid:3),q

γ

=

ceq
0

eT
j

(cid:4)

(cid:3)
(cid:3)

To solve [ ˜P](cid:3),q and γ in Eq.(9), we apply block elimination,
by using block elementary row operations, and starting with
the associated augmented matrix:

I − (1 − c)A −(1 − c)a ceq
0

−1

eT
j

j (I−(1−c)A)−1·Row1
Row2−eT
−−−−−−−−−−−−−−−−−−−−→

0

→

(1 − c)eT

−(1 − c)a

I − (1 − c)A
(cid:5)

j (I − (1 − c)A)

ceq
−1a − 1 −ceT
j [P](cid:3),q
The ﬁnal array represents the following equations:
(cid:8)

(cid:7)
(I − (1 − c)A) [ ˜P](cid:3),q − γ(1 − c)a = ceq
(1 − c)eT

j (I − (1 − c)A)

γ = −c[P]j,q

−1a − 1

(7)

(8)

(9)

(cid:4)

Back substitution, along with Eq.(3), yields Eq.(7).

Algorithm 1: IRWR (G, P, q,Δ G, c)
Input : graph G, old proximities P for G, query node q,
updates ΔG to G, and restarting probability c.

Output: new proximities [ ˜P](cid:3),q w.r.t. q.

1 foreach edge (i, j) ∈ ΔG to be updated do

2
3
4

5

6
7
8

9

10

11
12
13

14

15

1−c [P](cid:3),j ) + 1

(1−c)

else if edge (i, j) is to be deleted then

1

dj +1

dj := in-degree of node j in G ;
if edge (i, j) is to be inserted then
c [P](cid:3),i ;
c ([P](cid:3),i − 1

(cid:3)
if dj = 0 then y := 1
else y := 1
G := G ∪ {(i, j)} ;
(cid:3)
if dj = 1 then y := − 1
else y := 1
dj−1
G := G\{(i, j)} ;
ΔG := ΔG\{(i, j)} ;
if ΔG (cid:4)= ∅ then

c [P](cid:3),i ;

c ( 1

1

foreach v ∈ {vertices in ΔG} ∪ {q} do

1−c [P](cid:3),j − [P](cid:3),i) − 1
(1−c)

(cid:4)

(cid:4)

ej

ej

;

;

γ :=

[P]j,v

1−(1−c)[y]j
[P]j,q

else γ :=

1−(1−c)[y]j

, [P](cid:3),v := [P](cid:3),v + (1 − c)γy ;

, [ ˜P](cid:3),q := [P](cid:3),q + (1 − c)γy ;

16 return [ ˜P](cid:3),q ;

Lemma 2 tells that for each edge update, the changes to
P are just associated with the scaling operation of vector z.
However, it is costly to compute z via Eq.(7) as it involves
the inversion of a matrix. Lemma 3 provides an eﬃcient way
of computing (I − (1 − c)A)
−1a from a few columns of P.
Lemma 3. Suppose there is an edge update (i, j) to G,
−1a = y

and a is deﬁned by Lemma 1. Then, (I − (1 − c)A)
with y being deﬁned in Proposition 1.

Proof. Due to space limits, we shall only prove the edge

insertion case. A similar proof holds for the deletion case.

(i) When dj = 0, a = ei. Then, Eq.(3) implies that

(I − (1 − c)A)
(ii) When dj > 0, a = 1
(I − (1 − c)A)

−1ei = 1

c [P](cid:3),i

dj +1 (ei − [A]∗,j). Then,

−1(ei − [A](cid:3),j )

−1[A](cid:3),j )
c [P](cid:3),i − (I − (1 − c)A)
−1[A](cid:3),j, we apply the property

−1a = 1
dj +1 (I − (1 − c)A)
= 1
dj +1 ( 1
(cid:6)∞
To solve (I − (1 − c)A)
(cid:9)∞
k=0 Xk (for (cid:7)X(cid:7)1 < 1) and obtain
−1 =
−1A=
1−c ((I − (1 − c)A)
= 1

(cid:9)∞
−1 − I) = 1
1−c ( 1
Substituting this back produces the ﬁnal results.

Thus, we have (I − (1 − c)A)

(1 − c)kAk
P − I)
1−c ( 1
c [P](cid:3),j−ej).

that (I − X)
(I − (1 − c)A)

(1 − c)kAk+1 = 1
1−c

−1[A](cid:3),j = 1

k=0

k=1

c

Combining Lemmas 1–3 together proves Proposition 1.

Algorithm for Batch Updates. Based on Proposition 1,
we devise IRWR, an incremental RWR algorithm to handle
a set ΔG of edge insertions and deletions (batch update).

IRWR is shown in Algorithm 1. Given the old P for G
w.r.t. query q, and the batch edge updates ΔG, it computes
new proximities w.r.t. q in G∪ΔG without loss of exactness.
It works as follows. For each edge (i, j) to be updated, it ﬁrst
computes the auxiliary vector y from a linear combination
of only a few columns in P (lines 2–10). Using y, it then
(i) removes (i, j) from ΔG (line 11) and (ii) updates the
proximities w.r.t. each remaining node in ΔG (lines 12–14).
After all the edges are eliminated from ΔG, IRWR ﬁnally
calculates the new proximities [ ˜P](cid:3),q from y (line 15).

1019[P]u5 ,u2
1−(1−c)[y]5

= 0.25 × [P](cid:3),u1

Before proceeding with the edge deletion, let us look at the

Example 2. Recall P and G of Figure 1. Consider batch
updates ΔG, which insert edge (u1, u5) and delete (u4, u6),
IRWR computes the
where (u1, u5) is given in Example 1.
new proximities [P](cid:3),u2 w.r.t. query u2 in G+ΔG as follows:
For the edge insertion (u1, u5), since du5 = 3 and c = 0.2,
y = 1.25 × [P](cid:3),u1
− 1.56 × [P](cid:3),u5 + 0.31 × e5 (via line 5).
changes [ΔP](cid:3),u2 (via line 14) for the inserted (u1, u5):
[ΔP](cid:3),u2 = (1 − c)γ · y with γ =
= 0.254
− 0.32 × [P](cid:3),u5 + 0.06 × e5,
which explains why the values of α, β, λ are chosen for Eq.(1).
IRWR then removes (u1, u5) from ΔG (line 11). Using
y, it updates proximities w.r.t. u4, u6 ∈ ΔG (lines 12–14).
Thus, [P](cid:3),u4 = (.17, .05, .23, .28, .23, .05, 0)T , and [P](cid:3),u6 =
(.14, .04, .18, .23, .18, .24, 0)T after (u1, u5) is added to G.
Likewise, for the edge deletion (u4, u6), du6 = 1 implies
y = − 1
× [P](cid:3),u4 (line 8). Then, (u4, u6) is removed from
ΔG (line 11). Since ΔG = ∅, the changes [ΔP](cid:3),u2 for the
deleted (u4, u6) is obtained (via line 15):
[ΔP](cid:3),u2 = 0.8γ · y = −0.17 × [P](cid:3),u4 with γ =
⇒ [ ˜P](cid:3),u2 = [P](cid:3),u2 + [ΔP](cid:3),u2 = (.25, .24, .04, .04, .22, .04, 0)T .
Correctness & Complexity. To complete the proof of
Theorem 1, we notice that (i) IRWR can correctly com-
pute RWR proximities, which is veriﬁed by Proposition 1.
Moreover, IRWR always terminates, since the size of ΔG is
monotonically decreasing. (ii) One can readily verify that
for each edge update, IRWR involves only vector scaling and
additions, which is in O(1) time for each node proximity.

[P]u6 ,u2
1−(1−c)[y]6

= 0.04

0.2

4. EXPERIMENTAL EVALUATION

We present an empirical study on real and synthetic data
to evaluate the eﬃciency of IRWR for incremental computa-
tion, as compared with (a) its batch counterpart B LIN [4],
(b) k-dash [1], the best known algorithm for top-k search,
and (c) IncPPR [7], the incremental personalized PageRank.
Two real datasets are adopted: (a) p2p-Gnutella, a Gnutella
P2P digraph, in which nodes represent hosts, and edges host
connections. The dataset has 62.5K nodes and 147.9K edges.
(b) cit-HepPh, a citation network from Arxiv, where nodes
denote papers, and edges paper citations. We extracted a
snapshot with 27.7K nodes and 352.8K edges.
GraphGen3 is used to build synthetic graphs and updates.
Graphs are controlled by (a) the number of nodes |V |, and
(b) the number of edges |E|; updates by (a) update type
(edge insertion or deletion), and (b) the size of updates |ΔG|.
All the algorithms are implemented in Visual C++ 10.0.
We used a machine with an Intel Core(TM) 3.10GHz CPU
and 8GB RAM, running Windows 7.

We set restarting probability c = 0.2 in our experiments.

Experimental Results. We next present our ﬁndings.
1) Incremental Eﬃciency. We ﬁrst evaluate the computa-
tional time of IRWR on both real and synthetic data.
Figures 2(a) and 2(b) depict the results for edges inserted
to p2p-Gnutella (|V |=62.5K) and cit-HepPh (|V |=27.7K), re-
spectively. Fixing |V |, we vary |E| as shown in the x-axis.
Here, the updates are the diﬀerence of snapshots w.r.t. the
collection time of datasets, reﬂecting their real-life evolution.
We ﬁnd that (a) IRWR outperforms k-dash on p2p-Gnutella for
92.7% (resp. cit-HepPh for 97.5%) of edge updates. When
the changes are 61.9% on p2p-Gnutella (83.8% on cit-HepPh),
3http://www.cse.ust.hk/graphgen/

)
c
e
s
(

i

e
m
T
d
e
s
p
a
l
E

104

103

IRWR
k-dash
B LIN
IncPPR

102

91K 107K 123K 139K

(a) p2p-Gnutella

)
c
e
s
(

i

e
m
T
d
e
s
p
a
l
E

103

102

IRWR
k-dash
B LIN
IncPPR

101

350K 330K 310K 290K

)
c
e
s
(

i

e
m
T
d
e
s
p
a
l
E

l

G
C
D
N

103

102

IRWR
k-dash
B LIN
IncPPR

101

296K 312K 328K 344K

(b) cit-HepPh

)
c
e
s
(

i

e
m
T
d
e
s
p
a
l
E

103

102

IRWR
k-dash
B LIN
IncPPR

101

280K 300K 320K 340K

(c) edge insertions

1

0.9

0.8

0.7

IRWR

B LIN

k-dash

IncPPR

l = 20

l = 40

l = 60

l

G
C
D
N

1

0.9

0.8

0.7

0.6

IRWR

B LIN

k-dash

IncPPR

l = 20

l = 40

l = 60

(d) edge deletions

(e) p2p-Gnutella

(f) cit-HepPh

Figure 2: Performance Evaluation of IRWR

IRWR improves k-dash by over 5.1x (resp. 4.4x). This is
because IRWR reuses the old information in G for incremen-
tally updating proximities via vector scaling and additions,
without the need for expensive LU decomposition of k-dash.
(b) IRWR always is better than B LIN by nearly one order of
magnitude as B LIN requires costly block matrix inversions.
(c) IRWR outperforms IncPPR for over 95% of insertions, due
to the extra cost of IncPPR for doing short random walks.
(d) IRWR is sensitive to |ΔG| as the larger |ΔG| is, the larger
the aﬀected area is, so is the computation cost, as expected.
Fixing |V |=50K on synthetic data, we varied |E| from
280K to 350K (resp. from 350K to 280K) in 10K increments
(resp. decrements). The results are shown in Figures 2(c)
and 2(d), respectively, analogous to those on real datasets.
2) Exactness. To measure IRWR accuracy, we adopted NDCGl
(Normalized Discounted Cumulative Gain) for ranking top-l
node proximities with l = 20, 40, 60, and chose the ranking
results of k-dash as the benchmark, due to its exactness. The
results on p2p-Gnutella and cit-HepPh are reported in Figures
2(e) and 2(f), indicating that IRWR never scariﬁes accuracy
for achieving high eﬃciency, superior to other approaches.
5. CONCLUSIONS

We showed how RWR proximities can be computed very
eﬃciently in an incremental update model, where the edges
of a graph are constantly changed. We also empirically eval-
uated that IRWR greatly outperforms the other approaches
on both real and synthetic graphs without loss of exactness.
Our future work will further predict up to what fraction of
updated edges IRWR is faster than its batch counterparts.
6. REFERENCES
[1] Y. Fujiwara, M. Nakatsuji, M. Onizuka, and M. Kitsuregawa, “Fast and exact

top-k search for random walk with restart,” PVLDB, vol. 5, pp. 442–453, 2012.

[2] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation
ranking: Bringing order to the web,” tech. rep., Stanford InfoLab, 1999.

[3] G. Jeh and J. Widom, “SimRank: A measure of structural-context similarity,” in

KDD, pp. 538–543, 2002.

[4] H. Tong, C. Faloutsos, and J. Pan, “Fast random walk with restart and its

applications,” in ICDM, pp. 613–622, 2006.

[5] C. Wang, F. Jing, L. Zhang, and H. Zhang, “Image annotation reﬁnement using

random walk with restarts,” in ACM Multimedia, pp. 647–650, 2006.

[6] H. Tong, C. Faloutsos, and J. Pan, “Random walk with restart: Fast solutions

and applications,” Knowl. Inf. Syst., vol. 14, no. 3, pp. 327–346, 2008.

[7] B. Bahmani, A. Chowdhury, and A. Goel, “Fast incremental and personalized

PageRank,” PVLDB, vol. 4, no. 3, pp. 173–184, 2010.

[8] C. Li, J. Han, G. He, X. Jin, Y. Sun, Y. Yu, and T. Wu, “Fast computation of

SimRank for static and dynamic information networks,” in EDBT, 2010.

[9] Y. Zhou, H. Cheng, and J. X. Yu, “Graph clustering based on structural/attribute

similarities,” PVLDB, vol. 2, no. 1, pp. 718–729, 2009.

[10] P. Sarkar, A. W. Moore, and A. Prakash, “Fast incremental proximity search in

large graphs,” in ICML, pp. 896–903, 2008.

[11] H. Cheng, Y. Zhou, X. Huang, and J. X. Yu, “Clustering large attributed

information networks: An efﬁcient incremental computing approach,” Data
Min. Knowl. Discov., vol. 25, no. 3, pp. 450–477, 2012.

1020