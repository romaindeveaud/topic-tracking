Sentiment Analysis of User Comments for

One-Class Collaborative Filtering over TED Talks

Nikolaos Pappas

Idiap Research Institute

Rue Marconi 19

CH-1920 Martigny, Switzerland
nikolaos.pappas@idiap.ch

Andrei Popescu-Belis
Idiap Research Institute

Rue Marconi 19

CH-1920 Martigny, Switzerland

andrei.popescu-belis@idiap.ch

ABSTRACT
User-generated texts such as reviews, comments or discus-
sions are valuable indicators of users’ preferences. Unlike
previous works which focus on labeled data from user-con-
tributed reviews, we focus here on user comments which are
not accompanied by explicit rating labels. We investigate
their utility for a one-class collaborative (cid:12)ltering task such
as bookmarking, where only the user actions are given as
ground truth. We propose a sentiment-aware nearest neigh-
bor model (SANN) for multimedia recommendations over
TED talks, which makes use of user comments. The model
outperforms signi(cid:12)cantly, by more than 25% on unseen data,
several competitive baselines.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval|Information (cid:12)ltering
Keywords
Sentiment Analysis; One-Class Collaborative Filtering
1.

INTRODUCTION

Many problems in collaborative (cid:12)ltering (CF) such as so-
cial bookmarking, news and video recommendations make
use of binary user ratings in terms of ‘action’ or lack thereof,
i.e. ‘inaction’. The diﬃculty of such one-class CF prob-
lems [6] comes from the lack of a negative class: it is inher-
ently unsure whether user inaction means that an item was
not seen or was not liked (hence not bookmarked). In this
paper, we study the one-class CF problem of lecture recom-
mendation over TED talks. We show how to infer additional
user ratings by performing sentiment analysis (SA) of user
comments and integrating its output in a nearest neighbor
(NN) model. The resulting SANN model outperforms sev-
eral competitive baselines, is robust to noisy SA results and
improves its performance with the number of comments.

Work on inferring user ratings has mostly focused on re-
views with explicit ratings [3, 11, 4]. Here, however, we show

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

that unlabeled comments can also be leveraged to improve
recommendations, in a challenging one-class setting. Other
studies have used comments to enrich user pro(cid:12)les for news
recommendation, but without attempting sentiment analy-
sis and the inference of item ratings [9, 5].
2. SENTIMENT ANALYSIS

The (cid:12)rst stage of our proposal is the sentiment classi(cid:12)-
cation of user comments, with two possible labels: positive
(pos) and negative (neg). Given the lack of ground-truth
labels, we focused on dictionary-based methods and specif-
ically on an extension of the rule-based sentiment classi(cid:12)er
presented in [10] and implemented in [7]1. The classi(cid:12)er
uses the MPQA polarity lexicon and can deal with negation,
intensi(cid:12)ers, and polarity shifters. The rule-based classi(cid:12)er
estimates the polarity of a sentence as a positive or negative
numerical value. This value determines the sentiment label,
pos or neg of the sentence; for zero, the neutral label (neu)
is applied. The polarity of a comment is the sum over the
polarities of the sentences that compose it, and its label (pos
or neg) is given by the sign of the total.

To test the sentiment analysis component, we performed
human labeling of a subset of the TED comments, with
pos, neg or neu polarity labels (the latter included also
undecided cases)1. Six human judges annotated 160 com-
ments with 320 sentences, randomly selected from the TED
data, with some overlap to assess agreement, using Fleiss’
kappa ((cid:20)) metric. We obtained 260 labels for sentences and
135 for comments (excluding neu). Among these, 61 sen-
tences and 29 comments were common across annotators,
and agreement was found to be, respectively, (cid:20) = 0:834 and
(cid:20) = 0:650. As agreement was substantial, we used the entire
set as ground truth to evaluate automatic sentiment analysis
(cases of disagreements were reconciled by a majority vote).
The results of our rule-based classi(cid:12)er (RB) and of a ran-
dom baseline (Rand) are shown in Table 1. Our system
reaches F-score of 74:90% and 72:60% on sentences and re-
spectively comments for the classi(cid:12)cation task (plus mod-
erate agreement with the ground truth of (cid:20) = 0:53 and
(cid:20) = 0:43), a level that is suﬃcient to improve the one-class
recommendation task, as we will show.
3. ONE-CLASS CF MODELS
The one-class collaborative (cid:12)ltering problem can be for-
malized as follows. Let U be the set of users of size jUj = M
and I the set of items of size jIj = N . The matrix of user-
item ratings is R (of size M (cid:2) N ), with rui = 1 indicating an
1https://github.com/nik0spapp/unsupervised sentiment

773Sentences (pos=123, neg =137)

Comments (pos=76, neg =59)

Methods
Rule Based (RB)
Baseline (Rand)
Annotators

Precision Recall
76.42
48.64

73.43
47.36

-

-

F

74.90
47.90

-

Kappa Precision Recall
69.73
53.42

75.71
56.24

0.53
-0.01
0.83

-

-

F

72.60
54.63

-

Kappa

0.43
-0.02
0.65

Table 1: Performance of annotators, ruled-based and random classi(cid:12)er measured with Precision, Recall, F1
and Fleiss’ kappa ((cid:20)). Inter-annotator agreement is substantial ((cid:20) (cid:21) 0:65).

‘action’ rating (favorite item) and rui = 0 an ‘inaction’ one
(not seen or not liked). Our goal is to predict the preference
of the users in the future, therefore (as in previous studies)
we hide for evaluation a certain proportion of ‘1’ values per
user and measure how well we predict them.
3.1 Neighborhood Models

Nearest neighbor (NN) models are often used in collabo-
rative (cid:12)ltering and have been proven quite eﬀective despite
their simplicity [2]. Here, we use item-based neighborhood
models which are described by Eq. 1. The prediction func-
tion ^rui estimates the rating of a user u for an unseen item
i. It relies on the bias estimate bui of the user u for vari-
ous items j (given in Eq. 2) and on a score computed using
the k most similar items to i that the user u has already
rated, i.e. the neighborhood Dk(u; i). The denominator is a
normalization factor.

∑

∑
j2Dk(u;i) dij(ruj (cid:0) buj)

j2Dk(u;i) dij

^rui = bui +

(1)

The bias estimate bui is computed as the sum of the aver-
age ratings (cid:22) of items in a given dataset, the average rating
bu of a user u and the average rating bi for a given item i.
The coeﬃcient dij is the similarity between item i and item
j (Eq. 2) and is computed using the similarity sij between
items i and j, weighted by the normalized importance of the
number of common raters nij (close to 1 if nij ≫ (cid:21)). We
determine the optimal values of (cid:21) and of the neighborhood
size k using cross-validation.

dij = sij

nij

nij + (cid:21)

; bui = (cid:22) + bu + bi

(2)

∑

The similarity sij can be computed using a function such
as cosine similarity or Pearson’s correlation between vectors
representing i and j in the N (cid:2) N co-rating matrix derived
from R. However, given that ratings are binary and not
real-valued, the biases should not be computed linearly, but
using the formulas proposed below, which take into account
the number of the items and the maximally rated items.

(cid:22) =

1jIj

ri

; bu =

rujIj ; bi =

ri

rmax

(3)

i2I

rmax

3.2 Sentiment-Aware Neighborhood Model

We propose a sentiment-aware nearest neighbor model
(SANN) which integrates into a NN model the preferences
of the users that are extracted from user-generated text by
sentiment analysis. In order to achieve that, the model in
Eq. 1 must be modi(cid:12)ed as follows: (cid:12)rstly, the neighborhood
Dk(u; i) must account for the additional training data, and
secondly, the rating function ruj must map the output of
sentiment analysis over comments to rating values that are
understandable by the model. Thus, we modify the Eq. 1 of
the traditional neighborhood models as follows:

∑

^rui = bui +

j2Dk

c (u;i)

uj (cid:0) buj)
′

dij(r

(4)

In this equation, Dk

c (u; i) is the neighborhood of the k
most similar items that the user has already rated or com-
′
mented and r
uj is the result of the mapping function that
accounts for both explicit ratings and those inferred from
comments, de(cid:12)ned as follows:

{

′
uj =

r

1;
cuj;

if ruj = 1
if ruj ̸= 1

(5)

cuj is a mapping function which maps the polarity level of
a comment Cj of user u to a rating understandable by the
SANN model. We will compare three such mapping func-
tions, formally de(cid:12)ned in Table 2: two of them, noted ‘rand-
SANN’ and ‘SANN’, generate 3-way ratings (1, 0, -1) from
the random (Rand) and respectively the rule-based (RB)
sentiment classi(cid:12)ers, while the third one, noted ‘polSANN’,
generates a polarity value between -1 and 1 by combining the
polarity scores of the sentences that constitute a comment.
For all the three functions, the pos class has a positive eﬀect
on ^rui, while neu and neg classes have a negative eﬀect. An
optimal function (per user or global) could also be learned
from the data, in future work.

Moreover, the additional training data from commented
items are considered for the creation of the co-rating matrix
(N (cid:2) N ) used for the similarity sij in Eq. 2.

Mapping function
cuj = signrand(Cj)
cuj = signRB(Cj)

cuj = 1 + zj (cid:1)∑

s2Cj

(
polRB(s)=jsj)

Notation
randSANN
SANN
polSANN

Table 2: Three mapping functions for the SANN
model. Notations: sign(Cj) is the sentiment of com-
ment Cj (1 for pos, 0 for neu and -1 for neg ); pol (s)
returns the polarity of a sentence s; and zj is a nor-
malization factor over all comments C u of a user u,
de(cid:12)ned as zj = 1=(1 + jC uj (cid:1) jfC s:t: C 2 C u ^ sign(C) =
sign(Cj)gj).
4. EXPERIMENTAL SETUP
4.1 The TED Dataset

To evaluate our models on the one-class problem, we focus
on multimedia recommendations on the TED dataset [8]2.
TED (www.ted.com) is a popular online repository of public
talks and user-contributed material (favorites, comments)
under a Creative Commons license. We crawled the TED
dataset in September 2012 and gathered data from 74,760
users and 1,203 talks, with 134,533 favorites and 209,566
comments. According to our RB classi(cid:12)er, 63% are positive,
27% are negative and 10% are neutral comments.

For this study, we selected users with at least 4 favorites
(5,657 users) and included only comments that appear in
the (cid:12)rst level of the commenting area, i.e. excluding all the
replies to other comments, because the target of their po-
larity judgment is uncertain (comments on comments rather

2https://www.idiap.ch/dataset/ted

774Methods
TopPopular
normNN(PC)
NN(COS)
NN(PC)
SANN(COS)
SANN(PC)
Improvement

k parameter (1 to 50)

(cid:21) parameter (1 to 100)

MAP@50 MAR@50 MAF@50 MAP@50 MAR@50 MAF@50

3.46
4.06
4.70
4.75
5.07
5.27

+10.9%

12.69
13.70
16.30
16.47
17.90
18.68
+13.4%

5.44
6.27
7.29
7.37
7.91
8.22

3.46
4.13
4.67
4.76
6.12
6.27

+11.5%

+31.8%

12.69
14.00
16.31
16.58
20.65
20.99
+26.5%

5.44
6.37
7.27
7.40
9.45
9.66

+30.6%

Table 3: Performance of various methods using 5-fold cross validation. The last row displays the improvement
of the SANN model over the best NN model, which is always signi(cid:12)cant at the p < 0:01 level.

than on the talk). The resulting data set has 129,633 ratings
(favorites) and 20,792 comments (see Table 4).
4.2 Evaluation Protocol

For evaluation, we split the dataset into a training set
(80%) and a testing set (20%). More precisely, for each
user we keep 80% of their positive ratings (‘1’ values) for
training and hold out 20% for testing. Furthermore, we
divide the test set in two subsets based on comment sparsity:
a sparse and a dense one (see Table 4). For the sparse set,
we keep all users with at least 12 ratings and for the dense
one, all users with at least 12 ratings and one comment.
We optimize the parameters on the training set using 5-fold
cross-validation. When training, we use all users to obtain
good approximations of item similarities (sij in Eq. 2).
Favorites Comments Users
5,657
2,409
1,181

Sparse held-out
Dense held-out

108,256
17,227
9,303

Set

Training

20,792
8,728
8,728

Table 4: Statistics of the TED dataset [8].

5. EXPERIMENTAL RESULTS

We evaluate the SANN model, comparing it with several
baselines, and show the following: (i) the use of sentiment
analysis of comments as additional training data improves
performance over competitive baselines; (ii) the RB senti-
ment classi(cid:12)er is the reason for the improvement as com-
pared to a random classi(cid:12)er; and (iii) performance increases
with the number of comments.We present (cid:12)rst the parame-
ter selection using cross-validation (5.1), and then we eval-
uate the best con(cid:12)gurations on the two held-out sets (5.2).
5.1 Parameter Selection

Both NN models and SANN models rely on two parame-
ters: the (cid:12)rst one is the k parameter (the neighborhood size)
and the second one is the (cid:21) parameter (the shrinking factor).
For the selection of k, we (cid:12)xed (cid:21) = 60 for all models. For
the selection of (cid:21), we then (cid:12)xed k to the optimal values that
were obtained for each model. Moreover, two other options
can be selected, namely the similarity function (Pearson’s
correlation (PC) or cosine similarity (COS)) and the use of
normalization in Eq. 1 (noted as ‘norm’).

In Table 3, we present the results of 5-fold cross-validation
on the training set for six diﬀerent models, namely: TopPop-
ular baseline which provides (cid:12)xed recommendations based
on the popularity of items, NN(COS), normNN(PC), NN(PC),
SANN(COS) and SANN(PC). We also experimented with
other normalized NN models, but as their results were infe-
rior, we do not discuss them here.

Figure 1 displays the eﬀect of the neighborhood size k on
the performance of the models. All models perform consid-
erably better than the TopPopular, as expected from CF

Figure 1: The eﬀect of the neighborhood size k on
MAP values at 50, using 5-fold cross-validation.

models. The best performing model over all k values is the
SANN(PC) model (with signi(cid:12)cance at the p < 0:01 level).
Both NN and SANN models stabilize their performance as
k increases, which conforms to the theory of stability for
k-nearest neighbor algorithms [1]. The decrease in perfor-
mance of SANN as k increases is due to the inclusion of noisy
ratings in the training data (along with ground-truth ones),
while the performance of NN increases with k as no noise
is included in this case. For the evaluation on the held-out
sets (next section), the following values were selected based
on cross-validation: k = 1; (cid:21) = 7 for the SANN(PC) model
and k = 28; (cid:21) = 19 for the NN(PC) model.
5.2 Evaluation of the SANN Model

We compare the performance of the SANN model with
the best performing baseline NN model (NN(PC)) and with
the TopPopular baseline on the two held-out sets. We con-
sider the two variants of the SANN model, namely rand-
SANN(PC) and polSANN(PC) (also with k = 1 and (cid:21) = 7)
to demonstrate the value of aggregated polarities.

Figure 2 displays performance on the dense held-out test
set. The SANN model performs signi(cid:12)cantly better than
the baseline models on unseen data. The ordering of the
methods based on performance is the same as in the cross-
validation experiments. The randSANN(PC) model per-
forms slightly better than the NN(PC) model (the diﬀerence
is not signi(cid:12)cant) but considerably worse than SANN(PC)
and polSANN(PC), demonstrating the utility of sentiment
analysis over comments.

Table 5 shows the results on both held-out test sets with
MAP, MAR and MAF scores. The polSANN(PC) model
outperforms signi(cid:12)cantly the other models, showing a rela-
tive improvement of 26.8% on mean average f-metric (MAF)
on the dense held-out set and 11.4% on the sparse held-out
set. When averaged over MAP values from 1 to 50, the
improvement is signi(cid:12)cant at the p < 0:01 level (t-test).

775Methods
TopPopular
NN(PC)
randSANN(PC)
SANN(PC)
polSANN(PC)
Improvement

Dense held-out test set

Sparse held-out test set

MAP@50 MAR@50 MAF@50 MAP@50 MAR@50 MAF@50

3.85
5.67
5.88
6.90
7.29

+28.5%

13.52
18.07
17.79
20.72
22.01
+21.8%

5.99
8.63
8.84
10.35
10.95
+26.8%

3.61
5.23
5.22
5.69
5.89

+12.6%

13.48
18.06
17.56
18.85
19.48
+7.8%

5.70
8.11
8.05
8.75
9.04
11.4%

Table 5: Performance of various methods on the two held-out test sets. The last row displays the improvement
of the polSANN model over the best NN model.

Figure 2: Method comparison in terms of average
precision (AP) and recall (AR) (1 to 50).

Figure 3: SANN models’ performance (MAP at 50)
when varying the number of comments for training.

The (cid:12)nal experiment concerns the learning ability of the
algorithm in relation to the proportion of comments used for
training. Figure 3 displays the performance of two SANN
models on the dense held-out set, when varying the propor-
tion of training comments for each user. The increase in the
proportion of comments leads to a clear increase in perfor-
mance. When fewer than 20% of the comments are taken
into account, the models perform similarly to the NN(PC)
model for neighborhood size k = 1 (see also Fig. 1).
6. CONCLUSION AND FUTURE WORK

The proposed SANN models were shown experimentally
to be able to overcome some diﬃculties of the one-class col-
laborative (cid:12)ltering task and to outperform signi(cid:12)cantly sev-
eral competitive baselines. Moreover, they demonstrated
robustness to noise and exhibited appropriate learning abil-
ities with respect to the amount of available user-generated
text. The results of this study suggest several directions
for future work: (i) extending the results to other datasets;
(ii) learning the optimal mapping function from sentiment
analysis scores to ratings; and (iii) generalizing the proposed
approach to more advanced recommendation models.
7. ACKNOWLEDGMENTS

The work described in this article was supported by the
European Union through the inEvent project FP7-ICT n.
287872 (see http://www.inevent-project.eu).
8. REFERENCES
[1] O. Bousquet and A. Elisseeﬀ. Stability and

generalization. J. of Machine Learning Research, 2002.

[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance

of recommender algorithms on top-N recommendation
tasks. In 4th Int. Conf. on Recommender Systems,
Barcelona, Spain, 2010.

[3] G. Ganu, N. Elhadad, and A. Marian. Beyond the

stars: Improving rating predictions using review text

content. In 12th Int. Workshop on the Web and
Databases, Rhode Island, USA, 2009.

[4] C. Leung, S. Chan, F.-L. Chung, and G. Ngai. A

probabilistic rating inference framework for mining
user preferences from reviews. World Wide Web, 2011.
[5] A. Messenger and J. Whittle. Recommendations based

on user-generated comments in social media. In 3rd
Int. Conf. on Social Computing, Boston, USA, 2011.

[6] R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose,

M. Scholz, and Q. Yang. One-class collaborative
(cid:12)ltering. In 8th Int. Conf. on Data Mining, Pisa, Italy,
2008.

[7] N. Pappas, G. Katsimpras, and E. Stamatatos.
Distinguishing the popularity between topics: A
system for up-to-date opinion retrieval and mining in
the Web. In 14th Int. Conf. on Intelligent Text Proc.
and Computational Linguistics, Samos, Greece, 2013.
[8] N. Pappas and A. Popescu-Belis. Combining content
with user preferences for TED lecture recommenda-
tion. In 11th Int. Workshop on Content Based
Multimedia Indexing, Veszpr(cid:19)em, Hungary, 2013.

[9] J. Wang, Q. Li, Y. P. Chen, and Z. Lin.

Recommendation in Internet forums and blogs. In
48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden, 2010.
[10] T. Wilson, J. Wiebe, and P. Hoﬀmann. Recognizing

contextual polarity in phrase-level sentiment analysis.
In Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing,
Vancouver, Canada, 2005.

[11] W. Zhang, G. Ding, L. Chen, and C. Li. Augmenting

Chinese online video recommendations by using
virtual ratings predicted by review sentiment
classi(cid:12)cation. In Int. Conf. on Data Mining
Workshops, Washington, USA, 2010.

776