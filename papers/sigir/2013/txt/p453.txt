Utilizing Query Change for Session Search

Dongyi Guan, Sicong Zhang, Hui Yang

Department of Computer Science

Georgetown University

37th and O Street, NW, Washington, DC, 20057

{dg372, sz303}@georgetown.edu, huiyang@cs.georgetown.edu

ABSTRACT
Session search is the Information Retrieval (IR) task that
performs document retrieval for a search session. During a
session, a user constantly modiﬁes queries in order to ﬁnd
relevant documents that fulﬁll the information need. This
paper proposes a novel query change retrieval model (QCM),
which utilizes syntactic editing changes between adjacent
queries as well as the relationship between query change and
previously retrieved documents to enhance session search.
We propose to model session search as a Markov Decision
Process (MDP). We consider two agents in this MDP: the
user agent and the search engine agent. The user agent’s
actions are query changes that we observe and the search
agent’s actions are proposed in this paper. Experiments
show that our approach is highly eﬀective and outperforms
top session search systems in TREC 2011 and 2012.

Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and
Retrieval—Information Search and Retrieval

Keywords
Session search; query change model; retrieval model

1.

INTRODUCTION

Session search is the Information Retrieval (IR) task that
retrieves documents for a search session [4, 8, 13, 14, 15, 25,
32]. During a search session, a user keeps modifying queries
in order to ﬁnd relevant documents that fulﬁll his/her infor-
mation needs. In session search, many factors, such as rel-
evance feedback, clicked data, changes in queries, and user
intentions, are intertwined together and make it a quite chal-
lenging IR task. TREC (Text REtrieval Conference) 2010-
2012 Session tracks [18, 19, 20] studied session search with
a focus on the “current query” task, which retrieves relevant
documents for the current/last query in a session based on
previous queries and interactions. Table 1 shows examples
from the TREC 2012 Session track.1

1All examples mentioned in this paper are from TREC 2012.
For simplicity, we use ‘sx’ to refer to a TREC 2012 session
where x is the session identiﬁcation number.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior speciﬁc permission and/or a fee.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Table 1: Examples of TREC 2012 Session queries.

session 6
1.pocono mountains pennsylvania
2.pocono mountains pennsylvania hotels
3.pocono mountains pennsylvania things to do
4.pocono mountains pennsylvania hotels
5.pocono mountains camelbeach
6.pocono mountains camelbeach hotel
7.pocono mountains chateau resort
8.pocono mountains chateau resort attractions
9.pocono mountains chateau resort getting to
10.chateau resort getting to
11.pocono mountains chateau resort directions
session 85
1.glass blowing
2.glass blowing science
3.scientiﬁc glass blowing

session 28
1.france world
cup 98 reaction
stock market
2.france world
cup 98 reaction
3.france world
cup 98
session 32
1.bollywood
legislation
2.bollywood law
session 37
1.Merck lobbists
2.Merck lobbying
US policy

From Table 1, we notice that queries change constantly
in a session. The patterns of query changes include general
to speciﬁc (pocono mountains → pocono mountains park),
speciﬁc to general (france world cup 98 reaction → france
world cup 98), drifting from one to another (pocono moun-
tains park → pocono mountains shopping), or slightly diﬀer-
ent expressions for the same information need (glass blowing
science → scientiﬁc glass blowing). These changes vary and
sometimes even look random (gun homicides australia →
martin bryant port arthur massacre), which increases the
diﬃculty of understanding user intention. However, since
query changes are made after the user examines search re-
sults, we believe that query change is an important form
of feedback. We hence propose to study and utilize query
changes to facilitate better session search.

One approach to handle query change is to classify them
based on various types of explorations [20], such as speciﬁca-
tion, generalization, drifting, or slight change, then perform
retrieval. Another approach is mapping queries into seman-
tic graphical representations, such as ontologies [7] or query
ﬂow graphs developed from query logs [2], then studying
how queries move in the graphs. However, ontology map-
ping is challenging [17], which may introduce inaccurate in-
termediate results and hurt the search accuracy. Moreover,
relying on large scale query logs may not be applicable due
to lack of such data. Therefore, although these approaches
have been applied to IR tasks such as query reformulation
[3] and query suggestion [2, 30], they have yet to be directly
applied to session search. It is therefore necessary to explore
new solutions to utilize query change for session search.

We propose to model session search as a Markov Decision
Process (MDP) [16, 28], which is applicable to many human
decision processes. MDP models a state space and an action
space for all agents participating in the process. Actions
from the agents inﬂuence the environment/states and the
environment/states inﬂuence the agents’ subsequent actions

453Table 2: Evidence that query change ∆q appears in previous
search results Di−1.

# in TREC’11

/∈ Di−1

∈ Di−1
184
80
141

20
124
63

204 adjacent query
pairs, 76 sessions

/∈ Di−1

# in TREC’12

∈ Di−1
178
97
112

21
102
87
199 adjacent query
pairs, 98 sessions

qtheme
+∆q
−∆q
Total

Figure 1: Session search MDP. (example is from s32)

based on certain policies. A transition model between states
indicates the dynamics of the entire system. In our MDP,
queries are modeled as states. Previous queries that the
user wrote inﬂuence the search results; the search results
again inﬂuence the user’s decision of the next query. This
interaction continues until the search stops.

As illustrated in Figure 1, we consider two agents in this
entire process: the user agent and the search engine agent.
The user agent’s actions are mainly human actions that are
able to change search results, such as adding and deleting
query terms, i.e. query change. Clicking is a human action;
however, it does not explicitly impact the retrieval. There-
fore, it is not considered as a user action here. Query change
is the only form of user action in this paper. Based on the
user actions, we design corresponding policies for the search
engine agent; which is the main focus of this paper.

It is diﬃcult to interpret the user intent [5, 31] behind
query change. For instance, for a query change from Kuro-
sawa to Kurosawa wife (s38), there is no indication about
‘wife’ in the search results returned for the ﬁrst query. How-
ever, Kurosawa’s wife is actually among the information
needs provided to the user by NIST’s topic descriptions. Our
experience with TREC Session tracks suggests that informa-
tion needs and previous search results are two main factors
that inﬂuence query change. However, knowing information
needs before search could not easily be achieved. This pa-
per focuses on utilizing evidence found in previous search
results and the relationship between previous search results
and query change to improve session search.

In this paper, we summarize various types of query changes
based on potential user intents into user agent policies. We
further propose corresponding policies for the search engine
agent and model them in the query change retrieval model
(QCM), a novel reinforcement learning [16] inspired frame-
work. The relevance of a document to the current query
is recursively calculated as the reward beginning from the
starting query and continuing until the current query. This
research is perhaps the ﬁrst to employ reinforcement learn-
ing to tackle session search. Our experiments demonstrate
that the proposed approach is highly eﬀective and outper-
forms the best performing TREC 2011 and 2012 session
search systems.

The remainder of this paper is organized as follows. Sec-
tion 2 analyzes query change and summarizes policies for
the user agent. Section 3 proposes policies for the search en-
gine agent. Section 4 elaborates the query change retrieval
model. Section 5 discusses how to handle duplicated queries.
Section 6 evaluates our approach, followed by a discussion in
Section 7. Section 8 presents the related work and Section
9 concludes the paper.

2. USER AGENT: QUERY CHANGE AS A

FORM OF FEEDBACK

We deﬁne a search session S = {Q,D,C} as a combination
of a series of queries Q = {q1, ..., qi, ..., qn}, retrieved docu-
ment sets D = {D1, ..., Di, ..., Dn}, and clicked information
C = {C1, .., Ci, ..., Cn}, where n is the number of queries in
the session (i.e., the session length) and i indexes the queries.
In TREC 2010-2012 Session tracks, each retrieved document
set Di contains the top 10 retrieval results di1, ..., di10 ranked
in decreasing relevance for qi. Each clicked data Ci contains
the user-clicked documents, clicking order, and dwell time.
For instance, for s6 q6, pocono mountains camelbeach hotel
(Table 1), C6 tells us that the user clicked the 4th ranked
search result, followed by the 2nd, with dwell time 15 seconds
and 17 seconds, respectively.

TREC 2010-2012 Session Tracks aim to retrieve a list of
documents for the current query, i.e. the last query qn in
a session, ordered in decreasing relevance. Without loss of
speciﬁcity, we assume that any query between q1 to qn could
be the last query. We therefore study the problem of retriev-
ing relevant documents for qi, given all previous queries q1
to qi−1, previous retrieval results D1 to Di−1, and previous
clicked data C1 to Ci−1.

We deﬁne query change ∆qi as the syntactic editing changes

between two adjacent queries qi−1 and qi:

∆qi = qi − qi−1

qi can be written as a combination of the shared portion
between qi and qi−1 and query change: qi = (qi∩qi−1)+∆qi.
The query change ∆qi comes from two sources. First, the
added terms, which we call positive ∆q, are new terms that
the user adds to the previous query. Second, the removed
terms, which we call negative ∆q, are terms that the user
deletes from the previous query. For example, in Table 1 s37,
‘US’ and ‘policy’ are the added terms; while in s28, ‘stock’
and ‘market’ are the removed terms.

We call the common terms shared by two adjacent queries
theme terms since they often represent the main topic of a
session. For example, in Table 1 s37 the theme terms are
“Merck lobby”.2

We thus decompose a query into three parts as theme

terms, added terms, and removed terms and write it as:
qi = (qi∩qi−1)+(+∆qi)−(−∆qi) = qtheme+(+∆qi)−(−∆qi)
where qtheme are the theme terms, +∆qi and −∆qi represent
added terms and removed terms, respectively.

Our observations suggest that documents that have been
examined by the user factor in deciding the next query
change. We therefore propose the following important as-
sumption between ∆qi, the query change between adjacent

2We perform K-stemming to all query terms. For instance,
‘lobbists’ and ‘lobbying’ are both stemmed to ‘lobby’.

454Table 3: User agent’s policies and actions about a query term t ∈ qi−1. (Refer to sessions shown in Table 1)

user intention

1. ﬁnd more in-
formation about
t
2.
satisﬁed &
move to the next
information need
3. satisﬁed

t ∈ Di−1

t /∈ Di−1

4.
inspired by
terms t(cid:48) in Di−1

user likes Di−1
user action
add new terms
t(cid:48) about t

example
s85
q1 → q2

remove
t &
add new terms
t(cid:48) as new focus
keep t

add terms t(cid:48)(cid:48)
about t(cid:48)

s6
q8 → q9

theme
term

s37
q1 → q2

user dislikes Di−1

type

speciﬁcation

user intention

5. remove the wrong
terms

user action
remove t

drift

no change

speciﬁcation

6.
not satisﬁed &
move to the next in-
formation need
7.
pression for t

try diﬀerent ex-

8.
try diﬀerent ex-
pression for t to get
more documents for t

remove
t
& add new
terms t(cid:48)
slight
change of t
to t(cid:48)
slight
change of t
to t(cid:48)

example
s28
q1 → q2

s6
q6 → q7

s85
q2 → q3

s32
q1 → q2

type

generalization

drift

slight change

slight change

queries qi and qi−1, and Di−1, the search results for qi−1:

∆qi ← Di−1.

The assumption basically says that previous search results
decide query change. In fact, previous search results Di−1
could inﬂuence query change ∆qi in quite complex ways. For
instance, the added terms in s37 (Table 1) q1 to q2, are ‘US’
and ‘policy’. D1 contains several mentions of ‘policy’, such
as “A lobbyist who until 2004 worked as senior policy advisor
to Canadian Prime Minister Stephen Harper was hired last
month by Merck”. However, these ‘policy’-related mentions
are about “Canada policy” whereas the user adds “US policy”
in q2. This suggests that the user might have been inspired
by ‘policy’ in D1, however he preferred the policy in US, not
in Canada. Therefore, instead of simply cutting and pasting
identical terms from Di−1, the user creates related terms to
add for the next search.

In another example, s28 (Table 1) q1, ‘stock’ and ‘market’
are frequent terms that are similar to stopwords. Docu-
ments in D1 are hence all about them and totally ignore the
theme terms “france world cup 98.” In q2, the user removes
“stock market” to boost rankings for documents about the
theme terms. In this case, removing terms is not only about
generalization, but also about document re-ranking.

To provide a convincing foundation for our approach, we
look for evidence to support our assumption. We investigate
whether ∆qi (at least) appears in Di−1. Table 2 shows how
often theme terms, added terms, and removed terms are
present in Di−1 for both TREC 2011 and 2012 datasets.
Around 90% of the time theme terms occur in Di−1 and
most removed terms (>60%) appear in Di−1.3 Added terms
are new terms for the previous query qi−1; we thus expect
to see few occurrences of added terms in Di−1. Surprisingly,
however, more than a third of them appear in Di−1.
It
suggests that it is quite probable that previous search results
motivate the subsequent query change.

Table 3 summarizes various types of query changes into
possible policies for the user agent. This table mainly serves
as a guide for us to design the policies for the search engine
agent. We do not perform a thorough user study to validate
this table. However, we believe that it is a good representa-
tive of various search scenarios and can help design a good
session search agent.

Along two dimensions, Table 3 summarizes the user agent’s
actions and possible policies. The dimensions are whether
a previous query term t ∈ qi−1 appears in previous search

3A third of query terms that do not appear in Di−1 are
removed by the user.

results Di−1 (the left most column) and whether the user
likes Di−1 and the occurrence of t in Di−1 (the top most
row). Combinations of the two dimensions yield 4 main
cases (as in a contingency table) and 8 sub-cases. For each
case, we identify four items: a rough guess of user intention,
the user’s actual action, an example, and the semantic ex-
ploration type for this action. For example, query change
in s6 q8 → q9, pocono mountains chateau resort attractions
→ pocono mountains chateau resort getting to can be inter-
preted as the following. Previous query term ‘attractions’
appears in Di−1 and the user likes the returned documents
Di−1. One possibility is that he is satisﬁed with what he
reads and moves to the next information need. Therefore,
the user removes ‘attraction’ and adds new terms “getting
to” as the new query focus. This is a drift in search focus.
(case 2 in Table 3)

We further group the cases in Table 3 by types of user

actions, i.e., query change, and summarize them into:
• Theme terms (qtheme), terms that appear in both qi−1
and qi.
In fact, they often appear in many queries in
a session. It implies a strong preference for those terms
from the user. If they appear in Di−1, it shows that the
user favors them since the user issues them again in qi. If
they do not appear in Di−1, the user still favors towards
them and insists to include them in the new query. This
corresponds to t in cases 1 and 3 in Table 3.
• Added terms (+∆q), terms that appear only in qi, not in
qi−1. They indicate speciﬁcation, destination of drifting,
or destination of slight change. If they appear in Di−1,
for the sake of novelty [14], they will not be favored in Di.
If they do not appear in Di−1, which means that they are
novel and the user favors them now. This corresponds to
t(cid:48) in cases 1, 2, 6, 7, and 8, and t(cid:48)(cid:48) in case 4 in Table 3.
• Removed terms (−∆q), terms that appear only in qi−1,
not in qi. They indicate generalization, source of drift-
ing, and source of slight change. If they appear in Di−1,
removing them means that the user observes them and
dislikes them. If they do not appear in Di−1, the user still
dislikes the terms since they are not in qi anyway. This
corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.

3. SEARCH ENGINE AGENT: STRATEGIES

TO IMPROVE SEARCH

The search engine agent observes query change from the
user agent and takes corresponding actions. For each type of
query change, theme terms, added terms, and removed terms,
we propose to adjust the term weights accordingly for better
retrieval accuracy. The search engine agent’s action include

455Table 4: Search engine agent’s policy. Actions are adjust-
ments on the term weights. ↑ means increasing, ↓ means
decreasing, and → means keeping the original term weight.

∈ Di−1

qtheme

+∆q
−∆q

Y

N

Y
N
Y
N

action Example
↑
↑
↓
↑
↓
→ ‘legislation’ in s32, q2 → q3

“pocono mountain” in s6
“france world cup 98 reaction” in
s28, q1 → q2
‘policy’ in s37, q1 → q2
‘US’ in s37, q1 → q2
‘reaction’ in s28, q2 → q3

increasing, decreasing, and maintaining the term weights.
Based on the observed query change as well as whether the
query terms appeared in the previous search results Di−1,
we can sense whether the user will favor the query terms
in the current run of search. Table 4 illustrates the policies
that we propose for the search engine agent.

As shown in Section 2, theme terms qtheme often appear
in many queries in a session and there is a strong preference
for them. Thus, we propose to increase the weights of theme
terms no matter whether they appeared in Di−1 or not (rows
1 and 2 in Table 4). In the latter case, if a theme term was
not found in Di−1 (top retrieval results), it is likely that the
documents containing them were ranked low. Therefore, the
weights of theme terms need to be raised to boost the rank-
ings of those documents (row 2 in Table 4). However, since
theme terms are topic words in a session, they could appear
like stopwords within the session. To avoid biasing too much
towards them, we lower their term weights proportionally to
their numbers of occurrences in Di−1.

For added terms +∆q, if they occurred in previous search
results Di−1, we propose to decrease their term weights
for the sake of novelty [14]. For example, in s5 q1 → q2,
“pocono mountains”→“pocono mountains park”, the added
term ‘park’ appeared in a document in D5. If we use the
original weight of ‘park’, this document might still be ranked
high in D2 and the user may dislike it since he read it before.
We hence decrease added terms’ weights if they are in Di−1
(row 3 in Table 4). On the other hand, if the added terms
did not occur in Di−1, they are the new search focus and
we increase their term weights (row 4 in Table 4). In an in-
teresting case (s37 q1 → q2), part of +∆q, ‘policy’, occurred
in D1 whereas the other part, ‘US’, did not. To respect the
user’s preference, we increase the weight of ‘US’ while de-
creasing that of ‘policy’ to penalize documents about other
‘polices’ including “Canada policy”.
For removed terms −∆q, if they appeared in Di−1, their
term weights are decreased since the user dislikes them by
deleting them (row 5 in Table 4). For example, in s28 q2 →
q3, ‘reaction’ existed in D2 and is removed in q3. However, if
the removed terms are not in Di−1, we do not change their
weights since they are already removed from qi by the user
(row 6 in Table 4).

In the sections below, we follow policies proposed for the
search engine agent as shown in Table 4 and incorporate
them into a novel query change retrieval model (QCM).

4. MODELING SESSION SEARCH
Markov Decision Process (MDP) [16, 28] models a state
space S and an action space A. Its states S = {s1, s2, ...}
change from one to another according to a transition model
T = P (si+1|si, ai), which models the dynamics of the entire

system. A policy π(s) = a indicates that at a state s, what
are the actions a can be taken by the agent.
In session
search, we employ queries as states. Particularly, we denote
q as state, T as the transition model P (qi|qi−1, ai−1), D
as documents, and A as actions. Actions include keeping,
adding, and removing query terms for the user agent and
increasing, decreasing, and maintaining the term weights for
the search engine agent.

In a MDP, each state is associated with a reward function
R that indicates possible positive reward or negative loss
that a state and an action may result. In session search, we
consider the reward function to be the relevance function.

Reinforcement learning [16] oﬀers general solutions to MDP
and seeks for the best policy for an agent. Each policy has
a value associated with the policy and denoted as Vπ(s),
which is the expected long-term reward starting from state
s and continuing with policy π from then on. In a MDP, it
is believed that a future reward is not worth quite as much
as a current reward and thus a discount factor γ ∈ (0, 1) is
applied to future rewards. By considering the discount fac-
tor, the value function starting from s0 for a policy π can be
written as: Vπ(s0) = Eπ[R(s0) + γR(s1) + γ2R(s2) + ...] =
t=0 γtR(si)]. The Bellman equation [16] describes the
optimal value V ∗ for a state s in the long run and is often
used to obtain the best value for a MDP:

Eπ[(cid:80)∞

∗

V

(s) = max

a

R(s, a) + γ

(cid:48)|s, a)V

∗

(cid:48)

)

(s

P (s

(cid:88)

s(cid:48)

where s(cid:48) is the next state after s, V ∗(s) and V ∗(s(cid:48)) are the
optimal values for s and s(cid:48).

For session search, we observe that the inﬂuence of previ-
ous queries and previous search results to the current queries,
becomes weaker and weaker. The user’s desire for novel doc-
uments also supports this argument. We hence propose to
employ the reinforcement learning model backwards. That
is, instead of discounting the future rewards, we discount
the past rewards, i.e. the relevant documents that appeared
in the previous search results.

We propose the query change retrieval model (QCM) as
the following. We consider the task of retrieving relevant
documents for qi as ranking documents based on the re-
ward, i.e., how relevant it is to qi. Inspired by the Bellman
equation, we model the relevance of a document d to the
current query qi as:
Score(qi, d) = P (qi|d)+γ

P (qi|qi−1, Di−1, a) max
Di−1

(cid:88)

a

P (qi−1|Di−1)

(1)
which recursively calculates the reward starting from q1 and
continues with the search engine agent’s policy until qi. γ ∈
(0, 1) is the discount factor, maxDi−1 P (qi−1|Di−1) is the
maximum of the past rewards, P (qi|d) is the current reward,
and P (qi|qi−1, Di−1, a) is the query transition model.
The ﬁrst component in Eq.1, P (qi|d), measures the rel-
evance between qi and a document d that is under evalu-
ation. This component can be estimated by the Bayesian
(1 − P (t|d)),
where P (t|d) is calculated by the multinomial query genera-
tion language model with Dirichlet smoothing [33]: P (t|d) =
#(t,d)+µP (t|C)
, where #(t, d) denotes the number of occur-
rences of term t in document d, P (t|C) calculates the prob-
ability that t appears in corpus C based on Maximum Like-

belief network model [27]: P (qi|d) = 1 −(cid:81)

|d|+µ

t∈qi

456lihood Estimation (MLE), |d| is the document length, and
µ is the Dirichlet smoothing parameter (set to 5000).
The remaining challenges of calculating Eq.1 include max-
imizing the reward function maxDi−1 P (qi−1|Di−1) and es-
timating the transition model P (qi|qi−1, Di−1, a). They are
described in Section 4.1 and Section 4.2, respectively.
4.1 Maximizing the Reward Function

When considering the past/future rewards, MDP uses only
the optimal (the maximum possible) values from those past
/future rewards. This is reﬂected in maxDi−1 P (qi−1|Di−1)
as part of Eq. 1.

, #(t, di−1)

i−1.

Prior research [10, 22] suggests that Satisfying (SAT) clicks,
i.e., clicked documents with dwell time longer than 30 sec-
onds [10, 22], are probably the only ones that are eﬀective
at predicting user behaviors and relevance judgments. Since
the user also skims snippets in search interactions, in this
work, we consider both the top 10 returned snippets and
SAT clicks as eﬀective previous search results and denote
them as De
To obtain an maximum reward from all possible reward
functions P (qi−1|di−1), i.e., the text relevance of previous
query qi−1 and all previous search results di−1 ∈ Di−1, we
propose to generate a maximum rewarding document, de-
noted as d∗
i−1. We further propose that the candidates for
the d∗
i−1 should only be selected from the eﬀective previ-
ous search results De
i−1 as the document(s)
that is the most relevant to qi−1. To discover d∗
i−1, we ﬁrst
rank all the documents (either a snippet or a document)
di−1 ∈ De
i−1 by measuring the relevance between qi−1 and
{1 − P (t|di−1)}, where

di−1 as: P (qi−1|di−1) = 1 −(cid:81)

i−1. We deﬁne d∗

t∈qi−1

P (t|di−1) is calculated by MLE: P (t|di−1) = #(t,di−1)
|di−1|
is the number of occurrences of term t in document di−1, and
|di−1| is the document length. We do not apply smoothing
here since P (t|di−1) can be zero, i.e., t /∈ De
i−1. In fact, we
rely on this property in later calculation.

i−1, we generate d∗

After ranking documents di−1 in De

i−1
by the following options: (1) using the document with the
largest P (qi−1|di−1), (2) concatenating the top k documents
i−1 with the largest P (qi−1|di−1), or (3) concatenating
in De
all documents in De
i−1. Experiments show that option (1)
works the best and we use this setting throughout the paper.
For notation simplicity, we use Di−1 from now on to denote
eﬀective previous search results.
4.2 Estimating the Transition Model

The transition model indicated in Eq. 1 is(cid:80)

a P (qi|qi−1,
Di−1, a). It includes the probabilities of query transitions
under various actions. We incorporate polices designed in
Table 4 to calculate it.

Search engine agent performs actions based on user agent’s
actions. We need to identify user’s actions, i.e. query change
∆q before search engine takes actions. Particularly, we rec-
ognize ∆q by the following procedure. First, we generate
qtheme based on the Longest Common Subsequence (LCS)
[11] in both qi−1 and qi. A subsequence is a sequence that
appears in two strings in the same relative order but is not
necessarily continuous. The LCS can be the common preﬁx
or the common suﬃx of the two queries; it can also consist
of several discontinuous common parts from the two queries.
Take s6 q6 → q7 as an example: q6=“pocono mountains
camelbeach hotel”, q7=“pocono mountains chateau resort”,
qtheme = LCS(q6, q7) = “pocono mountains”. Next, we rec-

ognize added terms +∆q and removed terms −∆q. Gener-
ally, the terms that occur in the current query but not in
the previous query constitute +∆q; while the terms occur in
the previous query but not in the current query constitute
−∆q. In the above example, −∆q7 = “camelbeach hotel”,
and +∆q7 = “chateau resort”.

The search engine actions are decreasing, increasing, and
maintaining term weights. According to Table 4 rows 3 and
5, we decrease a term’s weight if the query change, either
+∆q or −∆q, occurred in the eﬀective previous search re-
sults Di−1. We propose to deduct term t’s weight by P (t|d),
i.e. t’s default contribution to the relevance score between
qi and the document under evaluation (denoted as d). Fur-
thermore, since t already occurred in Di−1, for the sake of
novelty, we deduct more weight that is proportional to t’s
frequency in Di−1 such that the more frequently t occurred
in Di−1, the more heavily t’s weight is deducted from the
current query qi and d. We formulate this weight deduction
for a term t ∈ +∆q or t ∈ −∆q as:
log Pnew(t|d) = (1 − P (t|d
where d∗
i−1 denotes the maximum rewarded document, d is
the document under evaluation, and P (t|d) is calculated by
MLE. We apply the log function to avoid numeric underﬂow.
We notice that Eq. 2 has an interesting connection with

i−1)) log P (t|d)
∗

(2)

the Kullback-Leibler divergence (KL divergence) [33]:

− P (t|d

i−1) log P (t|d) = P (t|d
∗

∗
i−1) log

1

P (t|d)

(3)

rank= P (t|d

∗
i−1) log

rank= KLDt

(cid:16)

P (t|d∗
i−1)
P (t|d)

(cid:17)

||θd

(cid:16)

(cid:17)

θd∗
i−1
||θd

θd∗

i−1

i−1

where KLDt
denotes the contribution of term
t to the KL divergence between two documents’ language
models θd∗
and θd. In Eq. 3, the larger the divergence be-
tween θd∗
and θd, the more novel document d is compared
i−1
to Di−1, and the less deduction to the relevance score. In
this sense, Eq. 2 models novelty for the added terms and
the removed terms during a query transition.

According to Table 4 row 4, we increase a term’s weight if
it is an added term and did not occur in Di−1. We propose
to raise the term weight proportional to its inverse document
frequency (idf). This is to make sure that while increasing
a preferred term’s weight, we avoid increasing its weight
too much if it is a common term in many documents. We
formulate this weight increase for a novel added term t (t ∈
+∆q and t /∈ Di−1) as:

log Pnew(t|d) = (1 + idf (t)) log P (t|d)

(4)

where idf (t) is the inverse document frequency of t in Corpus
C and P (t|d) is calculated by MLE. Note that this term
weight adjustment is in a form of tf-idf.

The increasing in term weights also applies to theme terms,
which corresponds to rows 1 and 2 in Table 4. Theme terms
repeatedly appear in a session, which implies the impor-
tance of them. Similar to the novel added terms, we should
avoid increasing their weights too much. We could discount
the increment proportional to idf. However, theme terms
are topical/common terms within a session, not necessarily
common terms in the entire corpus. Therefore, idf may not
be applicable here. We hence employ the negation of the

457number of occurrences of t in previous maximum rewarding
document, 1 − P (t|d∗
i−1), to substitute idf. We formulate
this weight increase for a theme term t ∈ qtheme as:
i−1))) log P (t|d)
∗

log Pnew(t|d) = (1 + (1 − P (t|d
i−1 denotes the maximum rewarded document and

where d∗
P (t|d) is calculated by MLE.

(5)

For removed terms that did not appear in Di−1 (Table 4
row 6), the search agent does not change their term weights.
By considering all possible cases for the transition model
as deﬁned in Eq. 1, the relevance score between the current
query qi and a document d is represented as below:
Score(qi, d) = log P (qi|d) + α

[1 − P (t|d

(cid:88)

i−1)] log P (t|d)
∗

P (t|d

i−1) log P (t|d) + 
∗

idf (t) log P (t|d)

t∈qtheme

(cid:88)

t∈+∆q
t /∈d∗
i−1

(cid:88)
(cid:88)

t∈+∆q
t∈d∗
i−1

t∈−∆q

− β

− δ

P (t|d

i−1) log P (t|d)
∗

(6)
where α, β, , and δ are parameters for each types of actions.
Note that we apply diﬀerent parameters β and δ on +∆q and
−∆q, since added terms and removed terms may aﬀect the
retrieval diﬀerently. We report the parameter selection in
Section 6.
4.3 Scoring the Entire Session

It is worth noting that Eq. 6 is valid only when i > 1.
When i = 1, there is no previous result for q1. We thus use

Score(q1, d) = log P (q1|d)

(7)

as a base case. P (q1|d) is calculated by Eq. 4.

Using Eq. 7 as the base case for the recursive function
described in Eq. 1, we obtain the overall document relevance
score Scoresession(qn, d) for a session that starts at q1 and
ends at qn by considering all queries in the session:

Scoresession(qn, d) = Score(qn, d) + γScoresession(qn−1, d)
= Score(qn, d) + γ [Score(qn−1, d) + γScoresession(qn−2, d)]

n(cid:88)

=

γn−iScore(qi, d)

i=1

(8)
where q1, q2,··· , qn are in the same session, and γ ∈ (0, 1)
is the discount factor. Eq. 8 provides a form of aggregation
over the relevance functions of all the queries in a session.

5. DUPLICATED QUERIES

Duplicated queries sometimes occur in a search session.
Prior work shows that removing duplicated queries could
eﬀectively boost the search accuracy [8, 19]. Duplicated
queries often occur when a user is frustrated by irrelevant
documents in search results and comes back to one of the
previous queries for a fresh start. For example, in s6 (Ta-
ble 1), q2 and q4 are duplicates and both search for pocono
mountains pennsylvania hotels. The query between them is
q3: pocono mountains pennsylvania things to do. It suggests
that the user might dislike the search results for q3 and he
returns to q2 to search again (q2 = q4).

To detect query duplicates, we ﬁrst remove punctuations
and white spaces in queries, then apply stemming on them.

Table 5: Dataset statistics for TREC 2011 and 2012 Session.

2011

2012

#topics
#sessions
#queries
#dups

62
76
280
16

48
98
297

5

#queries/session
#sessions/topic
#pages judged
#sessions w/o rel. docs

2011
3.68
1.23

2012
3.03
2.04

19,413

17,861

2

4

Next we determine exact string matches between every query
pair. The exactly matched query pairs are identiﬁed as du-
plicated queries.

Since the user may dislike the queries and their corre-
sponding search results between two duplicated queries, we
propose to eliminate from the MDP the undesired queries
and their interactions. We achieve this by setting the dis-
count factor to zero for any interaction between two dupli-
cated queries as well as that for the earlier query in the two.
The new discount factor γ(cid:48) can be calculated as:
{i|i ∈ [j, k),∃qj = qk, j < k)}
otherwise

(cid:40)

(cid:48)
i =

(9)

0
γi

γ

where γi is the original discount factor for the ith query, γ(cid:48)
i
is the updated discount factor for the ith query after de-
duplication.

For the above example s6, the eﬀects from q2 and q3 on the
session are eliminated. The entire session is now equivalent
to q1, q4, q5, ..., q11.

6. EVALUATION

The evaluation datasets are from TREC 2011 and 2012
Session tracks [18, 19]. Table 5 lists the statistics about
these two datasets. Each search session includes several
queries and the corresponding search results. The users
(NIST assessors) were given a topic description about infor-
mation needs before they searched. For example, s85 (Table
1) are related to topic 43 “When is scientiﬁc glass blowing
used? What are the purposes? What organizations do sci-
entiﬁc glass blowing?” Multiple sessions can relate to the
same topic. The search engine used to create the sessions
was Yahoo! BOSS. The top 10 returned documents were
shown to the users and they clicked documents that were
interesting to them and interacted with the system. We use
TREC’s oﬃcial ground truth and oﬃcial evaluation metrics
nDCG@10 and MAP.

The corpus used in this evaluation is ClueWeb09 Cate-
gory B collection (CatB).4 CatB contains the ﬁrst 50 mil-
lion English pages crawled from the Web during January
to February 2009. We ﬁlter out the spam documents by
removing documents whose WateQCMoo’s “GroupX” spam
ranking scores [6] are less than 70.

We compare the following systems in this evaluation:

• Lemur : Directly submitting the current query qn (with
punctuations removed) to the Lemur search engine [21]
(language modeling + Dirichlet smoothing) and obtain
the returned documents.
• TREC best : The top TREC system as reported by NIST
[13, 14].
It adopts a query generation model with rele-
vance feedback and handles document novelty. CatB was
used in their TREC submissions. This system is used as
the baseline system in this evaluation.
• Nugget: Another top TREC 2012 session search system
groups semantically coherent query terms as nuggets and

4http://lemurproject.org/clueweb09/.

458Figure 2: nDCG@10 for TREC 2012 against the parameters. (a), (b), (c), and (d) are about α, β, , and δ respectively.

Table 6: nDCG@10, MAP, and their improvements over
the baseline (%chg) for TREC 2012 sessions. The runs are
sorted by nDCG@10. A statistical signiﬁcant improvement
over the baseline is indicated with a † at p < 0.05 level.
%chg

nDCG@10

Approach

Lemur
TREC median
Nugget
TREC best
QCM
QCM+Dup

0.2474
0.2608
0.3021
0.3221
0.3353
0.3368

%chg

MAP
-21.54% 0.1274
-17.29% 0.1440
−4.19% 0.1490
0.1559
0.00%
4.10%†
0.1529
4.56%†
0.1537

-18.28%
-7.63%
-4.43%
0.00%
-1.92%
-1.41%

Table 7: nDCG@10, MAP, and their improvements over
the baseline (%chg) for TREC 2011 sessions. The runs are
sorted by nDCG@10. A statistical signiﬁcant improvement
over the baseline is indicated with a † at p < 0.05 level.

Approach

nDCG@10

Lemur
TREC median
TREC best
QCM
QCM+Dup
Nugget

0.3378
0.3544
0.4409
0.4728
0.4821
0.4836

%chg

MAP
-23.38% 0.1118
-19.62% 0.1143
0.1508
0.00%
7.24%†
0.1713
9.34%†
0.1714
9.68%†
0.1724

%chg

-25.86%
-24.20%
0.00%
13.59%†
13.66%†
14.32%†

creates structured Lemur queries [8]. We re-implement
and apply it on both TREC 2011 and 2012.
• TREC median: The median TREC system as reported by
• QCM : The proposed query change retrieval model.
• QCM + De-Duplicate (Dup): The proposed query change

NIST [18, 19].

retrieval model with duplicated queries removed.

6.1 Search Accuracy

Table 6 and Table 7 demonstrate search accuracy for all
systems under comparison for TREC 2012 and TREC 2011,
respectively. The evaluation metrics are nDCG@10 and
MAP, the same as in the oﬃcial TREC evaluations. TREC
best serves as the baseline.

Table 6 shows that the proposed QCM approach outper-
forms the best TREC 2012 system on nDCG@10 by 4.1%,
which is statistically signiﬁcant (one sided t-test, p = 0.05).
The search accuracy is further improved by 0.46% through
removing the duplicated queries. The experimental results
strongly suggest that our approach is highly eﬀective.

Table 7 shows that for TREC 2011, our approach again
outperforms the baseline by a statistically signiﬁcant 7.24%
(one sided t-test, p = 0.05) and achieves a further improve-
ment of 9.34% by the QCM+Dup approach. For TREC
2011, the performance gain by performing de-dup is 2.1%,
which is bigger than that for TREC 2012 (0.46%). The
reason is probably because that TREC 2012 only has 5 du-
plicated queries while TREC 2011 has 16 (shown in Table

5). However, the best approach for TREC 2011 is the nugget
approach, which is slightly better than QCM+Dup.

Table 5 illustrates the dataset diﬀerences between TREC
2011 and 2012. These diﬀerences may aﬀect search accuracy.
The average number of sessions per topic is 2.04 in 2012,
that is more than that in 2011 (1.23). Moreover, on average,
TREC 2012 sessions contain less queries per session (3.03)
than 2011 (3.68). As a result, the shorter sessions in 2012
may make the search task more diﬃcult than 2011 since less
information are provided by previous interactions. Another
diﬀerence is that 2012 sessions have fewer (sometimes even
none) relevant documents than 2011 sessions in CatB ground
truth. It unavoidably hurts the performance for any retrieval
system. Generally, we observe lower search accuracy in 2012
(Table 6) than in 2011 (Table 7).

6.2 Parameter Tuning

We investigate good values for parameters in Eq. 6. A
supervised learning-to-rank method should be able to ﬁnd
the optimal values for those parameters. However, in this
paper, we take a step-by-step parameter tuning procedure
and leave the supervised learning method as future work.

We add each component, i.e., theme terms, added terms,
and removed terms, one by one into Eq. 6. The tuning
is performed for QCM only and the parameters are shared
between QCM and QCM+Dup.

First, we plot nDCG@10 against α while setting other
parameters to 0 (Figure 2(a)). α represents the parameter
for theme terms. α ranges over [1.1, 2.5] by an interval of 0.1.
We notice that nDCG@10 reaches its maximum at α = 2.2.
We ﬁnd 2 other local maximums at 1.6 and 1.2 for α.

Next, we ﬁx α to the above values and plot nDCG@10
against β (Figure 2(b)). β is the parameter for added terms
that appeared in eﬀective previous search results; we call
them old added terms. β ranges over [1.0, 2.4] by an interval
of 0.2. We choose the top 2 local values from each curve and
pick 6 combinations for (α, β) as indicated in Figure 2(c).

Then, we ﬁx (α, β) and plot nDCG@10 against  (Figure
2(c)).  is the parameter for added terms that did not appear
in eﬀective previous search results; we call them novel added
terms.  ranges over [0.05, 0.1] by an interval of 0.01. All the
curves show similar trends and reach the highest nDCG@10
at around 0.07. We hence ﬁx  to 0.07.

Finally, we plot nDCG@10 against δ (Figure 2(d)) with
the parameter combinations that we discover eerlier. Even-
tually, nDCG@10 reaches its peak 0.3353 at α = 2.2, β =
1.8,  = 0.07, and δ = 0.4. We apply this set of parameters
to both QCM and QCM+Dup.

As we can see, α, β, and δ are much larger than . This
is because that in Eq. 4, idf (t) = log(N/nd) falls in the
range of [1, 10], while in Eq. 2 and Eq. 5, P (t|d∗
i−1) falls

459by an interval of 0.02. Figure 3 illustrates the relationship
between nDCG@10 and γ. nDCG@10 climbs to its peak
0.3368 when γ = 0.92. The result suggests that a good
discount factor γ is very close to 1, implying that previous
queries contribute to the overall search accuracy nearly the
same as the last query. It suggests that in QCM, a discount
between two adjacent queries should be mild.

7. DISCUSSION
7.1 Advantages of Our Approach

A main contribution of our approach is that we treat a
search session as a continuous process by studying changes
among query transitions and modeling the dynamics in the
entire session. Through the reinforcement learning style
framework, our system provides the best aggregation scheme
for all queries in a session (Table 9). This allows us to better
handle sessions that demonstrate evolution and exploration
in nature than most existing systems do. On the contrary,
for sessions that are clear in search goals and lack of a ex-
ploratory nature, the advantage of our system over other
systems looks less signiﬁcant.

This can be seen in Table 10, which illustrates the search
accuracy for the TREC best, Nugget, and our system for
various classes of sessions. The TREC best is used as the
baseline and we also show the percentile improvement over it
in Table 10. TREC 2012 sessions were created by consider-
ing and hence can be classiﬁed into two facets: search target
(factual or intellectual) and goal quality (speciﬁc/good or
amorphous/ill) [19]. Table 10 shows that QCM works very
well for all classes of sessions. Speciﬁcally, QCM works even
better, i.e. outperforms the TREC best even more signiﬁ-
cantly, for sessions that search for intellectual targets as well
as sessions that search with amorphous goals. In our opin-
ion, this is due to that intellectual tasks produce new ideas
or new ﬁndings (e.g.
learn about a topic or make decision
based on the information collected so far) while searching.
Both intellectual and amorphous sessions rely more on pre-
vious search results. Thus, users reformulate queries based
more on what they have retrieved, not the vague informa-
tion need. This is a scenario where our approach is good at
since we employ previous search results to guide the search
engine’s action. For speciﬁc and factual sessions, users are
clearer in search goals, query changes may come less from the
previous search results. In summary, our good performance
on both intellectual task and amorphous task is consistent
with our eﬀorts of modeling query changes.

Moreover, we beneﬁt from term-level manipulation in var-
ious aspects in our system. The ﬁrst aspect is novelty. Both
the TREC best system and our system handle novelty in
a session. The TREC best system only deals with novelty
at the document level. They consider documents that have
been examined by the user in a previous interaction not
novel and the rest are novel [14]. That is, they determine
novelty purely based on document identiﬁcation number, not
the actual content. Through studying whether query terms
appeared in previous search results, our approach evaluates
and models novelty at the term level (or concept level),
which we believe better represents the evolving informa-
tion needs in a session. The second aspect is query han-
dling. The Nugget approach [8] treats queries at the phrase
level and formulates structured queries based on phrase-like
nuggets. The approach achieves good performance, espe-

Figure 3: Discount factor γ.

Figure 4: Error types.

Table 8: Aggregation schemes.

Approach
Query change model

qn
1

Aggregation

Scheme

Uniform
PvC
Distance-based

λn = 1

λn = 1 − λp
λn = 1 − λp

qi(i ∈ [1, n − 1])

γn−i
λi = 1
λi = λp
λi = λp
n−i

Table 9: nDCG@10 for various aggregation schemes. λp is
0.4 in PvC. γ is 0.92 in QCM and QCM+Dup. TREC 2012
best serves as the baseline. A signiﬁcant improvement over
the baseline is indicated with a † at p < 0.05 level.

Aggregation

Scheme

Distance-based
TREC best
Uniform
PvC
QCM
QCM+Dup

TREC 2011

TREC 2012

nDCG@10

0.4431
0.4540
0.4626
0.4713
0.4728
0.4821

%chg
-2.40%
0.00%
1.89%†
3.81%†
4.14%†
6.19%†

nDCG@10

0.3111
0.3221
0.3316
0.3351
0.3353
0.3368

%chg
−3.42%
0.00%
2.95%†
4.04%†
4.10%†
4.56%†

in the range of [0,0.1]. Therefore, the values of  are two
magnitudes less than that for the other parameters. Among
α, β, and δ, we ﬁnd that α and β are larger than δ, which
implies that theme terms and added terms may play more
important roles in session search than removed terms.
6.3 Aggregation for the Entire Session

QCM proposes an eﬀective way to aggregate all queries in
a session as in Eq.8. We compare how eﬀective it is to prior
query aggregation methods. A query aggregation scheme

can be represented as: Score(session, d) =(cid:80)n

i=1 λi · Score(qi, d),

where Score(qi, d) is the relevance scoring function of d and
qi and λi is the query weight for qi.

[8] proposed several aggregation schemes for TREC 2012
Session track. The schemes are: uniform (all queries are
equally weighted), previous vs.
current (known as PvC;
all previous queries are discounted by λp, while the current
query uses a complementary and higher coeﬃcient (1 − λp),
and distance-based (previous queries are discounted based
on a reciprocal function of queries’ positions in the session).
We express various query aggregation schemes in terms
of the discount factor γ in order to compare them with our
approach. From Table 8, we ﬁnd that QCM degenerates to
uniform when γ = 1. Previous queries in PvC and Distance-
based schemes are also discounted as they are in QCM, but
with diﬀerent decay functions.

The search accuracy for diﬀerent aggregation schemes are
compared in Table 9. QCM performs the best for both
TREC 2011 and 2012. The PvC scheme is the second best
scheme, which conﬁrms what is reported in [8]. The Distance-
based scheme gives the worst performance.

We explore the best discount factor γ for QCM over (0, 1)

460Table 10: nDCG@10 for diﬀerent classes of sessions in TREC 2012.

TREC best
Nugget
QCM
QCM+DUP

Intellectual

0.3369
0.3305
0.3870
0.3900

%chg
0.00%
-1.90%
14.87%
15.76%

Factual %chg
Speciﬁc %chg
Amorphous %chg
0.3138
0.00%
0.3007
0.00%
0.00%
-8.51%
-9.01% 0.2871
-2.80% 0.2736
-2.29%
0.3066
2.79%
0.3091
5.55%
5.64%
0.3114
3.56%
0.3072
-2.10%

0.3495
0.3397
0.3689
0.3692

cially for TREC 2011. However, due to complexity in nat-
ural language, nugget detection is sensitive to dataset and
the approach’s performance is not quite as stable as ours on
diﬀerent datasets.

Lastly, our system beneﬁts from trusting the user. Our ap-
proach does not use too much materials from other resources
such as anchor texts, meta data, or click orders, as many
other approaches do [8, 26]. We believe that the most direct
and valuable feedback is the next query that the user enters.
In this work, we manage to capture the query change and
investigate the reasons behind it. We use ourselves as users
to summarize possible human users’ reasoning and actions.
More detailed analysis about user intent might be useful for
researchers to understand web users, however, it might be
overwhelming (too ﬁne-grained or too much semantics) for
a search engine that essentially only counts words.

7.2 Error Analysis & Future Work

Our system retrieves nothing for 22 out of 98 sessions in
TREC 2012. To analyze the reason for the poor performance
for those sessions, we study their topic descriptions, queries,
and ground truth documents. We summarize the types of
errors as “two theme concepts”, “ill query”, “few relevant
documents”, and others. Figure 4 shows how many sessions
that we fail to retrieve under each error type.

We call the ﬁrst type of errors “two theme concepts”. It
comes from a type of session where the information need
cover more than one concepts. For instance, s17 and s18
share the the same topic “... To what extent can decisions
and policies of the Indian government be credited with these
wins?”. Queries in s17 and s18 ask about both concepts “in-
dian politics’ and “miss universe”. Unfortunately, very few
relevant documents about both theme concepts exist in the
corpus. The retrieved documents are about either concept,
but none is about both. Eight sessions belong to this type.
As future work, we can improve our system by incorporating
structures in queries, and enable more sophisticated opera-
tors such as Boolean and proximity search.

The second type of errors is “ill query”, where in such
sessions, queries themselves are ill formulated and do not
well-represent the information needs indicated in the given
topic. A common mistake is that the user misses some sub-
information need. For example, the topic for s16 is: “... you
want to reduce the use of air conditioning in your house ...
you could protect the roof being overly hot due to sun ex-
posure... Find information of ... how it could be done.” A
good query for this topic should include roof and air condi-
tioning. However, the queries that the user issued for s60,
“reduce airconditioning” and “attic insulation air condition-
ing costs”, do not mention roof at all. Because of this ill
query formulation, our system yields no relevant documents
for s60. On the other hand, for s59, which shares the same
information need with s60, our system achieves a nDCG@10
of 0.48 simply because s59 queries “cool roof”. It suggests
that ill queries mislead the search engine and yield poor re-
trieval performance. Four sessions belong to this type. As

future work, we will explore eﬀective query suggestion by
studying sessions that share the same topic.

The third type of errors is “too few relevant documents”.
For sessions with too few relevant documents in the ground
truth, our system do not perform well. In total 2,573 rele-
vant documents exist in CatB for all 48 TREC 2012 topics;
on average 53.6 relevant documents per topic. However,
topics 10, 45, 47 and 48, each has no more than 2 relevant
documents and topic 47 (s92 to s95) has no relevant docu-
ment in CatB (Table 5). This problem could be reduced if
we index the entire ClubWeb09 CatA collection.

Figure 4 also indicates in which classes of sessions these
errors lie. We ﬁnd that all “two theme concept” errors be-
long to sessions created with amorphous goals while all “too
few relevant documents” errors belong to those with speciﬁc
goals. Moreover, “ill queries” tend to occur more in sessions
with amorphous goals. Note that “ill query” and “few rel-
evant documents” are errors due to either the user or the
data. There might not be much room for our system to im-
prove over them. However, “two theme concepts” is where
our system can certainly make further improvements.

8. RELATED WORK

Session search is a challenging IR task [4, 8, 13, 14, 25, 32].
Existing approaches investigate session search from various
aspects such as semantic meanings of search tasks [23], doc-
ument novelty [14], and phrase structure in queries [8]. The
best TREC system [13, 14] employs an adaptive browsing
model by considering both relevance and novelty; however it
does not demonstrate improvement by handling novelty. In
this paper, we successfully model query and document nov-
elty by investigating the relationship between query change
and previous search results. Moreover, our analysis on query
change does not require knowledge of semantic types for the
sessions as [23] proposed.

Our proposed work is perhaps the most similar to the
problem of query formulation [1, 9, 12, 24] and query sug-
gestion [29]. [12] showed that certain query changes such as
adding/removing words, word substitution, acronym expan-
sion, and spelling correction are more likely to cause clicks,
especially on higher ranked results. The ﬁnding is generally
consistent with our view of query change. However, their
work only emphasized on understanding of query changes,
without showing how to apply it to help session search. [24]
examined the relationship between task types and how users
change queries. They classiﬁed query changes by semantic
types: Generalization, Specialization, Word Substitution,
Repeat, and New. Similar to [12], however, [24] stopped at
understanding query changes and didn’t apply their ﬁndings
to help session search. This probably makes us the ﬁrst to
utilize query changes in actual retrieval. [1] derived query-
ﬂow graph, a graph representation of user query behavior,
from user query logs. The approach detected query chains
in the graph and recommended queries based on maximum
weights, random walk, or just the previous query. Other
mining approaches [1, 29] identify the importance of query

461change in sessions; however, they require the luxury of large
user query logs.

This research is perhaps the ﬁrst to employ reinforcement
learning to solve the Markov Decision Process demonstrated
in session search. Reinforcement learning is complex and dif-
ﬁcult to solve. Its solutions include model-based approaches
and model-free approaches [16]. The former learn the transi-
tion model and the reward function for every possible states
and actions and mainly employ MLE to estimate the model
parameters. Others also use matrix inversion or linear pro-
gramming to solve the Bellman equation. It works well when
state spaces are small. However, in our case, the state space
is large since we use natural language queries as the states;
hence we could not easily apply model-based approaches in
practice. In this work, we eﬀectively reduce the search space
by summarizing users’ and search engine’s actions into a few
types and employ a model-free approach to learn value func-
tions directly.

9. CONCLUSION

This paper presents a novel session search approach (QCM)
by utilizing query change and modeling the dynamic of the
entire session as a Markov Decision Process. We assume
that query change is an important form of feedback. Based
on this assumption, through studying editing changes be-
tween adjacent queries, and their relationship with previous
retrieved documents, we propose corresponding search en-
gine actions to handle individual term weights for both the
query and the document.
In a reinforcement learning in-
spired framework, we incorporate various ingredients present
in session search, such as query changes, satisfactory clicks,
desire for document novelty, and duplicated queries. The
proposed framework provides a theoretically sound and gen-
eral foundation that allows more novel features to be incor-
porated. Experiments on both TREC 2011 and 2012 Ses-
sion tracks show that our approach is highly eﬀective and
outperforms the best session search systems in TREC. This
research is perhaps the ﬁrst to employ reinforcement learn-
ing in session search. Our MDP view of modeling session
search can potentially beneﬁt a wide range of IR tasks.

10. ACKNOWLEDGMENT

This research was supported by NSF grant CNS-1223825.
Any opinions, ﬁndings, conclusions, or recommendations ex-
pressed in this paper are of the authors, and do not neces-
sarily reﬂect those of the sponsor.

11. REFERENCES
[1] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and
S. Vigna. The query-ﬂow graph: model and applications. In
CIKM ’08.

[2] I. Bordino, C. Castillo, D. Donato, and A. Gionis. Query

similarity by projecting the query-ﬂow graph. In SIGIR ’10.
[3] P. Bruza, R. McArthur, and S. Dennis. Interactive internet

search: keyword, directory and query reformulation
mechanisms compared. In SIGIR ’00.

[4] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating

simple user behavior for system eﬀectiveness evaluation. In
CIKM ’11.

[5] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions

and attention in exploratory health search. In SIGIR’11.

[6] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Eﬃcient

and eﬀective spam ﬁltering and re-ranking for large web
datasets. Inf. Retr., 14(5), Oct. 2011.

[7] D. Guan and H. Yang. Increasing stability of result

organization for session search. In ECIR ’13.

[8] D. Guan, H. Yang, and N. Goharian. Eﬀective structured

query formulation for session search. In TREC ’12.
[9] J. Guo, G. Xu, H. Li, and X. Cheng. A uniﬁed and

discriminative model for query reﬁnement. In SIGIR ’08.

[10] Q. Guo and E. Agichtein. Ready to buy or just browsing?:

detecting web searcher goals from interaction data. In
SIGIR ’10.

[11] D. S. Hirschberg. Algorithms for the longest common

subsequence problem. J. ACM, 24(4), Oct. 1977.

[12] J. Huang and E. N. Efthimiadis. Analyzing and evaluating
query reformulation strategies in web search logs. In CIKM
’09.

[13] J. Jiang, S. Han, J. Wu, and D. He. Pitt at trec 2011

session track. In TREC ’11.

[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track.

In TREC ’12.

[15] R. Jones and K. L. Klinkner. Beyond the session timeout:

automatic hierarchical segmentation of search topics in
query logs. In CIKM ’08.

[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore.

Reinforcement learning: a survey. J. Artif. Int. Res., 4(1),
May 1996.

[17] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: the

state of the art. Knowl. Eng. Rev., 18(1), Jan. 2003.
[18] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and

M. Sanderson. Overview of the trec 2011 session track. In
TREC’11.

[19] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and

M. Sanderson. Overview of the trec 2012 session track. In
TREC’12.

[20] E. Kanoulas, P. D. Clough, B. Carterette, and

M. Sanderson. Session track at trec 2010. In TREC’10.
[21] Lemur Search Engine. http://www.lemurproject.org/.
[22] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of

search results using interaction behaviors in search sessions.
In SIGIR ’12.

[23] C. Liu, M. Cole, E. Baik, and J. N. Belkin. Rutgers at the

trec 2012 session track. In TREC’12.

[24] C. Liu, J. Gwizdka, J. Liu, T. Xu, and N. J. Belkin.

Analysis and evaluation of query reformulations in diﬀerent
task types. In ASIST ’10.

[25] J. Liu and N. J. Belkin. Personalizing information retrieval

for multi-session tasks: the roles of task stage and task
type. In SIGIR ’10.

[26] A. M-Dyaa, K. Udo, N. Nikolaos, N. Brendan, L. Deirdre,
and F. Maria. University of essex at the trec 2011 session
track. In TREC ’11.

[27] D. Metzler and W. B. Croft. Combining the language

model and inference network approaches to retrieval. Inf.
Process. Manage., 40(5), Sept. 2004.

[28] S. P. Singh. Learning to solve markovian decision processes.

Technical report, Amherst, MA, USA, 1993.

[29] Y. Song and L.-w. He. Optimal rare query suggestion with

implicit user feedback. In WWW ’10.

[30] Y. Song, D. Zhou, and L.-w. He. Query suggestion by

constructing term-transition graphs. In WSDM ’12.

[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize

or not to personalize: modeling queries with variation in
user intent. In SIGIR ’08.

[32] R. W. White, I. Ruthven, J. M. Jose, and C. J. V.

Rijsbergen. Evaluating implicit feedback models using
searcher simulations. ACM Trans. Inf. Syst., 23(3), July
2005.

[33] C. Zhai and J. Laﬀerty. A study of smoothing methods for

language models applied to information retrieval. ACM
Trans. Inf. Syst., 22(2):179–214, Apr. 2004.

462