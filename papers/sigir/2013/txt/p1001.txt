Estimating Topical Context by Diverging from External

Resources

Romain Deveaud

University of Avignon - LIA

Avignon, France

romain.deveaud@univ-

avignon.fr

Eric SanJuan

University of Avignon - LIA

Avignon, France

eric.sanjuan@univ-

avignon.fr

Patrice Bellot

Aix-Marseille University - LSIS

Marseille, France

patrice.bellot@lsis.org

ABSTRACT
Improving query understanding is crucial for providing the
user with information that suits her needs. To this end, the
retrieval system must be able to deal with several sources of
knowledge from which it could infer a topical context. The
use of external sources of information for improving docu-
ment retrieval has been extensively studied. Improvements
with either structured or large sets of data have been re-
ported. However, in these studies resources are often used
separately and rarely combined together. We experiment
in this paper a method that discounts documents based on
their weighted divergence from a set of external resources.
We present an evaluation of the combination of four re-
sources on two standard TREC test collections. Our pro-
posed method signiﬁcantly outperforms a state-of-the-art
Mixture of Relevance Models on one test collection, while
no signiﬁcant diﬀerences are detected on the other one.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Relevance feedback

Keywords
External resources, language models, topical context

1.

INTRODUCTION

When searching for speciﬁc information in a document col-
lection, users submit a query to the retrieval system. The
query is a representation or an interpretation of an under-
lying information need, and may not be accurate depending
on the background knowledge of the user. Automatically
retrieving documents that are relevant to this initial infor-
mation need may thus be challenging without additional in-
formation about the topical context of the query. One com-
mon approach to tackle this problem is to extract evidences
from query-related documents [8, 16]. The basic idea is to
expand the query with words or multi-word terms extracted

Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear  this  notice  and  the  full  citation  on  the  first  page.  Copyrights  for 
components  of  this  work  owned  by  others  than  ACM  must  be  honored. 
Abstracting  with  credit  is  permitted.  To  copy  otherwise,  or  republish,  to 
post on servers or to redistribute to lists, requires prior specific permission 
and/or a fee. Request permissions from permissions@acm.org.. 
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland. 
Copyright © 2013 ACM  978-1-4503-2034-4/13/07…$15.00. 

 
  
 
 
 
 
 
 

from feedback documents. This feedback set is composed of
documents that are relevant or pseudo-relevant to the ini-
tial query, and that are likely to carry important pieces of
information. Words that convey the most information or
that are the most relevant to the initial query are then used
to reformulate the query. They can come from the target
collection or from external sources and several sources can
be combined [1, 3]. These words usually are synonyms or
related concepts, and allow to infer the topical context of
the user search. Documents are then ranked based, among
others, on their similarity to the estimated topical context.
We explore the opposite direction and choose to carry ex-
periments with a method that discounts documents scores
based on their divergences from pseudo-relevant subsets of
external resources. We allow the method to take several
resources into account and to weight the divergences in or-
der to provide a comprehensive interpretation of the topical
context. More, our method equally considers sequences of
1, 2 or 3 words and chooses which terms best describe the
topical context without any supervision.

The use of external data sets had been extensively stud-
ied in the pseudo-relevance feedback setting, and proved to
be eﬀective at improving search performance when choos-
ing proper data. However studies mainly concentrated on
demonstrating how the use of a single resource could improve
performance. Data sources like Wikipedia [10, 15], Word-
Net [11, 15], news corpora or even the web itself [1, 3] were
used separately for enhancing search performances. Com-
bining several source of information was nonetheless studied
in [1]. However the authors used web anchor and heading
texts, which are very small units that are less likely to carry
a complete context. They also used the entire Wikipedia
but they did not report results of its contribution in the
information sources combination. Diaz and Metzler [3] in-
vestigated the use of larger and more general external re-
sources than those used in [1]. They present a Mixture of
Relevance Models (MoRM) that estimates the query model
using a news corpus and two web corpora as external sources,
and achieves state-of-the-art retrieval performance. To our
knowledge, this last approach is the closest one from the
method we experiment in this paper.

2. DIVERGENCE FROM RESOURCES

In this work, we use a language modeling approach to
information retrieval. Our goal is to accurately model the
topical context of a query by using external resources. We
use the Kullback-Leibler divergence to measure the infor-
mation gain (or drift) between a given resource R and a

1001(cid:2)

P (t|θR) log

document D. Formally, the KL divergence between two lan-
guage models θR and θD is written as:
P (t|θR)
KL(θR||θD) =
(cid:2)
(cid:2)
P (t|θD)
P (t|θR) log P (t|θR) −
(cid:2)
P (t|θR) log P (t|θD)

P (t|θR) log P (t|θD)

=
t∈V
∝ −

t∈V

t∈V

(1)

t∈V

where t is a term belonging to vocabulary V . The ﬁrst part
is the resource entropy and does not aﬀect ranking of doc-
uments, which allows us to simplify the KL divergence and
to obtain equation (1). In order to capture the topical con-
text from the resource, we estimate the θR model through
pseudo-relevance feedback. Given a ranked list RQ obtained
by retrieving the top N documents of R using query likeli-
hood, the feedback query model is estimated by:
P (t|ˆθR) ∝

P (w|θDF ) log P (w|θDF )

P (Q|θDF )

(cid:2)

(cid:2)

(cid:3)

−

(cid:4)

DF ∈RQ

w∈t

The right-hand expression of this estimation is actually equiv-
alent to computing the entropy of the term t in the pseudo-
relevant subset RQ. One advantage of doing so it that t may
not be necessarily a single term, like in traditional relevance
models approaches [3, 9], or a ﬁxed-length term [12]. When
forming the V set, we slide a window over the entire textual
content of RQ and consider all sequences of 1, 2 or 3 words.
Following equation (1), we compute the information di-
vergence between a resource R and a document D as:

D(ˆθR||θD) = −

P (t|ˆθR) log P (t|θD)

(cid:2)

t∈V

R∈S

(cid:2)

The ﬁnal score of a document D with respect to a given
user query Q is determined by the linear combination of
query word matches (standard retrieval) and the weighted
divergence from general resources. It is formally written as:
s(Q, D) = λ log P (Q|θD)−(1− λ)
ϕR· D(ˆθR||θD) (2)
where S is a set of resources, P (Q|θD) is standard query
likelihood with Dirichlet smoothing and ϕR represents the
weight given to resource R. We use here the information
divergence to reduce the score of a document: the greater
the divergence, the lower the score of the document will be.
Hence the combination of several resources intuitively acts
as a generalization of the topical context, and increasing
the number of resources will eventually improve the topi-
cal representation of the user information need. While we
chose to use traditional query likelihood for practical and re-
producibility reasons, it could entirely be substituted with
other state-of-the-art retrieval models (e.g. MRF-IR [12],
BM25 [13]...).

3. EXPERIMENTS
3.1 Experimental setup

We performed our evaluation using two main TREC1 col-
lections which represent two diﬀerent search contexts. The
ﬁrst one is the WT10g web collection and consists of 1,692,096
1

http://trec.nist.gov

web pages, as well as the associated TREC topics (451-550)
and judgments. The second data set is the Robust04 collec-
tion, which is composed of news articles coming from var-
ious newspapers.
It was used in the TREC 2004 Robust
track and is composed of standard corpora: FT (Financial
Times), FR (Federal Register 94), LA (Los Angeles Times)
and FBIS (i.e. TREC disks 4 and 5, minus the Congres-
sional Record). The test set contains 250 topics (301-450,
601-700) and relevance judgements of the Robust 2004 track.
Along with the test collections, we used a set of external re-
sources from which divergences are computed. This set is
composed of four general resources: Wikipedia as an ency-
clopedic source, the New York Times and GigaWord cor-
pora as sources of news data and the category B of the
ClueWeb092 collection as a web source. The English Gi-
gaWord LDC corpus consists of 4,111,240 news-wire articles
collected from four distinct international sources including
the New York Times [4]. The New York Times LDC corpus
contains 1,855,658 news articles published between 1987 and
2007 [14]. The Wikipedia collection is a recent dump from
May 2012 of the online encyclopedia that contains 3,691,092
documents3. We removed the spammed documents from the
category B of the ClueWeb09 according to a standard list of
spams for this collection4. We followed authors recommen-
dations [2] and set the “spamminess” threshold parameter to
70. The resulting corpus contains 29,038,220 web pages.

Indexing and retrieval were performed using Indri5. The
two test collections and the four external resources were in-
dexed with the exact same parameters. We use the standard
INQUERY english stoplist along with the Krovetz stemmer.
We employ a Dirichlet smoothing and set the μ parame-
ter to 1, 500. Documents are ranked using equation (2).
We compare the performance of the approach presented in
Section 2 (DfRes) with that of three baselines: Query Like-
lihood (QL), Relevance Models (RM3) [9] and Mixture of
Relevance Models (MoRM) [3]. In the results reported in
Table 1, the MoRM and DfRes approaches both perform
feedback using all external resources as well as the target
collection, while RM3 only performs feedback using the tar-
get collection. QL uses no additional information.

RM3, MoRM and DfRes depend on three free-parameters:
λ which controls the weight given to the original query, k
which is the number of terms and N which is the number
of feedback documents from which terms are extracted. We
performed leave-one-query-out cross-validation to ﬁnd the
best parameter setting for λ and averaged the performance
for all queries. Previous research by He and Ounis [5] showed
that doing PRF with the top 10 pseudo-relevant feedback
documents was as eﬀective as doing PRF with only relevant
documents present in the top 10, and that there are no sta-
tistical diﬀerences. Following these ﬁndings, we set N = 10
and also k = 20, which was found to be a good PRF set-
ting. DfRes depends on an additional parameter ϕR which
controls the weight given to each resource. We also per-
form leave-one-query-out cross-validation to learn the best
setting for each resource. Although the results in Table 1
correspond to this parameter setting, we explore in the fol-
lowing section the inﬂuence of the N and k parameters. In

2
3
4
5

http://boston.lti.cs.cmu.edu/clueweb09/
http://dumps.wikimedia.org/enwiki/20110722/
http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/
http://www.lemurproject.org/

1002QL

P@20
0.2429
0.3528

MAP
0.2026
0.2461

wt10g
robust

RM3

MAP
0.2035
0.2727α

P@20
0.2449
0.3677

MoRM

MAP

P@20

0.2339α,β
0.2869α,β

0.2833α,β
0.3799α,β

DfRes

MAP
0.2463α,β
0.3147α,β,γ

P@20
0.2954α,β
0.4024α,β,γ

Table 1: Document retrieval results reported in terms of Mean Average Precision and Precision at 20
documents. We use a two sided paired wise t-test to determine signiﬁcant diﬀerences over baselines. α, β
and γ indicate statistical improvements over QL, RM3 and MoRM respectively, with p <0 .05.

the following section, when discussing results obtained using
single sources of expansion with DfRes, we use the notation
DfRes-r where r ∈ (Web,Wiki,NYT,Gigaword).
3.2 Results

The main observation we can draw from the ad hoc re-
trieval results presented in Table 1 is that using a combina-
tion of external information sources performs always better
than only using the target collection. The numbers we re-
port vary from those presented in [3], however we could not
replicate the exact same experiments since the authors do
not detail indexing parameters. DfRes signiﬁcantly outper-
forms RM3 on both collections, which conﬁrms that state
that combining external resources improves retrieval.

We see from Figure 1 that DfRes-Gigaword is ineﬀective
on the WT10g collection, which is not in line with the re-
sults reported in [3] where the Gigaword was found to be
an interesting source of expansion. Another remarkable re-
sult is the ineﬀectiveness of the WT10g collection as a single
source of expansion. However we see from Table 2 that the
learned weight ϕR of this resource is very low (= 0.101),
which signiﬁcantly reduces its inﬂuence compared to other
best performing resources (such as NYT or Web).

nyt
0.303
0.309

wiki
0.162
0.076

gigaword web

0.121
0.281

0.313
0.149

wt10g
robust

robust wt10g
0.101

-

0.185

-

Table 2: ϕR weights learned for resources on the two
collections. We averaged weights over all queries.

Results are more coherent on the Robust collection. DfRes-
NYT and DfRes-Gigaword achieve very good results, while
the combination of all resources consistently achieves the
best results. The very high weights learned for these re-
sources hence reﬂect these good performances. As previ-
ously noted, the Robust collection is composed of news ar-
ticles coming from several newspapers (not including the
NYT). In this speciﬁc setting, it seems that the nature of
the good-performing resources is correlated with the nature
of the target collection. We observed that NYT and Gi-
gaword articles, which are focused contributions produced
by professional writers, are smaller on average (in unique
words) than Wikipedia or Web documents.

We explored the inﬂuence of the number of feedback doc-
uments used for the approximation of each resource. We
omit the plots of retrieval performances for the sake of space,
and also because they are not noteworthy. Performances in-
deed remain almost constant for all resources as N varies.
Changes in MAP are about ± 2% from N = 1 to N = 20
depending on the resource. However we also explored the
inﬂuence of the number of terms used to estimate each re-
source’s model. While we could expect that increasing the
number of terms would improve the granularity of the model

and maybe capture more contextual evidences, we see from
Figure 2 that using 100 terms is not really diﬀerent than us-
ing 20 terms. We even see that using only 5 terms achieves
the best results for DfRes on the WT10g collection.

Overall, these results show support for the principles of
polyrepresentation [6] and intentional redundancy [7] which
state that combining cognitively and structurally diﬀerent
representations of information needs and documents will in-
crease the likelihood of ﬁnding relevant documents. Since
we use several resources of very diﬀerent natures ranging
from news articles to web pages, DfRes takes advantage of
this variety to improve the estimation of the topical con-
text. Moreover, the most eﬀective values of λ tend to be
low, which means that DfRes is more eﬀective than the ini-
tial query. We even see on Figure 1 that only relying on the
divergence from resources (i.e. setting λ = 0) achieves bet-
ter results than only relying on the user query (i.e. setting
λ = 1). More, setting λ = 0 for DfRes also outperforms
MoRM (signiﬁcantly on the Robust collection). This sug-
gests that DfRes is actually better as estimating the topical
context of the information need than the user keyword query.
We also observe from Figure 1 and 2 that the NYT is the
resource that provides the best estimation of the topical con-
text for the two collections, despite being the smallest one.
This may be due to the fact that articles are well-written by
professionals and contain lots of synonyms to avoid repeti-
tion. Likewise, the failure of Wikipedia may be due to the
encyclopedic segmentation of articles. Since each Wikipedia
article covers a speciﬁc concept, it is likely that only concept-
related articles compose the pseudo-relevant set, which may
limit a larger estimation of the topical context. One of the
originality of the DfRes is that it can automatically take into
account n-grams without any supervision (such as setting
the size of the grams prior to retrieval). In practice, there is
on average 1.19 words per term, but most of the time arti-
cles like “the” are added to words that already were selected
(i.e. “the nativity scene”, where “nativity” and “scene” were
used before as single words).
4. CONCLUSION & FUTURE WORK

Accurately estimating the topical context of a query is
a challenging issue. We experimented a method that dis-
counts documents based on their average divergence from a
set of external resources. Results showed that, while rein-
forcing previous research, this method performs at least as
good as a state-of-the-art resource combination approach,
and sometimes achieves signiﬁcantly higher results. Per-
formances achieved by the NYT as a single resource are
very promising and need further exploration, as well as the
counter-performance of Wikipedia. More speciﬁcally, using
nominal groups or sub-sentences that rely on the good qual-
ity of NYT articles could be interesting and in line with
ongoing research in the Natural Language Processing ﬁeld.

1003P
A
M

4
2
0

.

2
2
0

.

0
2
0

.

8
1
0

.

6
1
0

.

4
1
0

.

2
1
0

.

wt10g

robust

P
A
M

0
3
0

.

8
2
0

.

6
2
0

.

4
2
0

.

2
2
0

.

0
2
0

.

wt10g
web
wiki
nyt
gigaword
all

robust
web
wiki
nyt
gigaword
all

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

lambda

lambda

Figure 1: Retrieval performance (in MAP) as a function of the λ parameter. The DfRes results reported
in Table 1 are depicted by curve “all”, while all other curves correspond to DfRes with a single resource.
Baselines are shown for reference: dashed lines represent RM3 and dotted lines represent MoRM.

wt10g

robust

4
2
0

.

2
2
0

.

0
2
0

.

8
1
0

.

6
1
0

.

4
1
0

.

P
A
M

P
A
M

0
3
0

.

5
2
0

.

0
2
0

.

5
1
0

.

wt10g
web
wiki
nyt
gigaword
all

robust
web
wiki
nyt
gigaword
all

0

20

40

60

80

100

0

20

40

60

80

100

Figure 2: Retrieval performance (in MAP) as a function of the number of terms k used for estimating the
resource language model. Legend is the same as in Figure 1.

k

k

5. ACKNOWLEDGMENTS

This work was supported by the French Agency for Sci-
entiﬁc Research (Agence Nationale de la Recherche) under
CAAS project (ANR 2010 CORD 001 02).

6. REFERENCES
[1] M. Bendersky, D. Metzler, and W. B. Croft. Eﬀective query

formulation with multiple information sources. In
Proceedings of WSDM, 2012.

[2] G. Cormack, M. Smucker, and C. Clarke. Eﬃcient and

eﬀective spam ﬁltering and re-ranking for large web
datasets. Information Retrieval, 2011.

[3] F. Diaz and D. Metzler. Improving the estimation of

relevance models using large external corpora. In
Proceedings of SIGIR, 2006.

[4] D. Graﬀ and C. Cieri. English Gigaword. Philadelphia:

Linguistic Data Consortium, LDC2003T05, 2003.

[5] B. He and I. Ounis. Finding good feedback documents. In

Proceedings of CIKM, 2009.

[6] P. Ingwersen. Polyrepresentation of information needs and

semantic entities: elements of a cognitive theory for
information retrieval interaction. In Proc. of SIGIR, 1994.

[7] K. Jones. Retrieving Information Or Answering

Questions? British Library annual research lecture. British
Library Research and Development Department, 1990.

[8] R. Kaptein and J. Kamps. Explicit extraction of topical

context. J. Am. Soc. Inf. Sci. Technol., 62(8):1548–1563,
Aug. 2011.

[9] V. Lavrenko and W. B. Croft. Relevance based language

models. In Proceedings of SIGIR, 2001.

[10] Y. Li, W. P. R. Luk, K. S. E. Ho, and F. L. K. Chung.

Improving weak ad-hoc queries using Wikipedia as external
corpus. In Proceedings of SIGIR, 2007.

[11] S. Liu, F. Liu, C. Yu, and W. Meng. An eﬀective approach

to document retrieval via utilizing WordNet and
recognizing phrases. In Proceedings of SIGIR, 2004.

[12] D. Metzler and W. B. Croft. Latent Concept Expansion
Using Markov Random Fields. In Proc. of SIGIR, 2007.

[13] S. E. Robertson and S. Walker. Some simple eﬀective

approximations to the 2-Poisson model for probabilistic
weighted retrieval. In Proceedings of SIGIR, 1994.

[14] E. Sandhaus. The New York Times Annotated Corpus.

Philadelphia: Linguistic Data Consortium, LDC2008T19,
2008.

[15] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core

of semantic knowledge. In Proceedings of WWW, 2007.

[16] R. W. White, P. Bailey, and L. Chen. Predicting user

interests from contextual information. In Proceedings of
SIGIR, 2009.

1004