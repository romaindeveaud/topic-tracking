Commodity Query by Snapping

Hao Huang†

Yunjun Gao‡

Kevin Chiew§

Qinming He‡

Lu Chen‡

†School of Computing, National University of Singapore, Singapore

‡College of Computer Science, Zhejiang University, China

§School of Engineering, Tan Tao University, Vietnam

†huanghao@comp.nus.edu.sg, ‡{gaoyj, hqm, chenl}@zju.edu.cn, §kevin.chiew@ttu.edu.vn

ABSTRACT
Commodity information such as prices and public reviews is
always the concern of consumers. Helping them conveniently
acquire these information as an instant reference is often of
practical signiﬁcance for their purchase activities. Nowa-
days, Web 2.0, linked data clouds, and the pervasiveness of
smart hand held devices have created opportunities for this
demand, i.e., users could just snap a photo of any commodity
that is of interest at anytime and anywhere, and retrieve the
relevant information via their Internet-linked mobile devices.
Nonetheless, compared with the traditional keyword-based
information retrieval, extracting the hidden information re-
lated to the commodities in photos is a much more com-
plicated and challenging task, involving techniques such as
pattern recognition, knowledge base construction, semantic
comprehension, and statistic deduction. In this paper, we
propose a framework to address this issue by leveraging on
various techniques, and evaluate the eﬀectiveness and eﬃ-
ciency of this framework with experiments on a prototype.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Information retrieval; commodity; snapping

1.

INTRODUCTION

Query by keywords is one of the most important and
prevalent mechanisms for information retrieval. Nonethe-
less, this mechanism is still far from perfect or convenient
sometimes. Take the following scenario as an example. When
users stumble across a “new” product in a supermarket, they
usually would like to refer to others’ reviews and comments
on the product, or to know whether there is a lower price or
any better alternatives. With the traditional information re-
trieval procedure, users have to visually extract and organize

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

the keywords to describe the product by themselves, and in-
put these keywords on their Internet-linked mobile devices,
followed by checking each search result if it contains review
information as expected. Such a process is not so easy since
the choosing of keywords aﬀects the search results. Instead,
it would be more eﬃcient and eﬀective for users to snap a
photo of the product, and let the system automatically gen-
erate the keywords as inputs to an existing search engine.
The idea of query by snapping is not new. It was introduced
in 2006 when camera became a part of a mobile phone.

In recent years, several APPs and APIs running on mobile
devices have been developed for query by snapping, such as
barcode scanners, image matching-based solutions [5], and
Google Goggles which combines image matching and OCR.
Nevertheless, these approaches still have a few minor ﬂaws
in eﬀectiveness or convenience when they are used to query
commodities. For example, (1) it is not convenient for a
user to take and turn a commodity to scan its barcode when
the commodity is not at hand or the user is in a hurry;
(2) commodities sharing similar appearances, such as se-
ries products, signiﬁcantly aﬀect the image matching results;
and (3) the results of image matching and OCR are directly
returned by Google Goggles without a further analysis to
determine the exact names of the objective commodities.

In this paper, we focus on a simple yet eﬀective mecha-
nism for commodity query by snapping. It enables users to
retrieve relevant and useful product information, such as the
exact name, brand information, recommended retail price,
reviews, and similar alternatives, by just snapping a photo
of a commodity and submitting it to our system. We pro-
pose a framework which addresses the following three issues:
(1) ﬁrst, extracting the information printed on the surface of
an objective commodity; (2) second, deducing the brand in-
formation which will help users locate the commodity more
accurately; and (3) third, generating the ﬁnal keywords that
exactly describe the commodity for the retrieval of the re-
lated product information from the Internet.

2. FRAMEWORK OVERVIEW

The framework contains two parts, i.e., (1) the oﬀ-line
part in which a knowledge base is constructed based on m
product names Nj (j = 1, . . . , m) crawled from e-commerce
web sites, such as Amazon and eBay; and (2) the on-line part
consisting of three phases, i.e., keyword extraction, brand
deduction, and information retrieval.

In the oﬀ-line knowledge base, the product names are
split into a set W of standard word items wi ∈ W , i =
1, . . . , n, and the three matrices below are constructed by

985statistics, namely (1) the word symbiosis matrix S where
the value of each element Sik (i, k ∈ {1, . . . , n}) denotes the
co-occurrence probability Pr(wk | wi) that word wk appears
in the product names containing word wi; (2) the product-
word matrix P of which each element Pji (j ∈ {1, . . . , m},
i ∈ {1, . . . , n}) represents the occurrence times of word wi
in product name Nj; and (3) the brand-product matrix B
of which each element Btj (t ∈ {1, . . . , b}, j ∈ {1, . . . , m})
indicates whether or not the commodity with product name
Nj belongs to the tth brand with 1 for yes and 0 for no.

Using the knowledge base, the on-line part carries out a

query by snapping through the following three phases.

• Keyword Extraction. On a commodity, there are usually
both useful information (e.g., product name, brand) and
trivial information (e.g., net weight, user guide). Thus this
phase aims to extract the informative keywords that describe
the commodity in the speciﬁed photo appropriately.

• Brand Deduction. To retrieve the product information
of a correct commodity for users, it is important to iden-
tify the commodity accurately. Although the extracted key-
words may express the most important information of the
commodity, sometimes they are not clear enough to ﬁnd
out the exact commodity, especially when there is no brand
information contained in the keywords. For example, key-
words “facial”, “foam”, and “cucumber” have delivered the
main information for a commodity named dove facial foam
of cucumber scent though, they are still ambiguous for the
commodity identiﬁcation without the brand “dove”1. Mo-
tivated by this, we deduce the brand of the commodity to
complement extracted keywords for the sake of clarity. In-
stead of using existing techniques such as image matching-
based Logo recognition [1], we infer the brand information
based on the extracted keywords, the product-word matrix
P and the brand-product matrix B, avoiding the extensive
cost of building a large-scale brand Logo database.

• Information Retrieval. In a traditional information ac-
quisition mechanism, users provide an exact description of a
commodity as the query keywords to obtain a better search
results. In our framework, this query keywords can be gen-
erated automatically using extracted keywords and the de-
duced brand with the help of Google. With our query key-
words which exactly describe an objective commodity, the
desired commodity information can be accurately retrieved
from the e-commerce and product-review websites.

3. KEYWORD EXTRACTION

There is plenteous information on a commodity’s package,
such as the product name, brand, producer, net weight, and
so on. Hence, it plays a key role in the commodity identiﬁca-
tion to extract the most useful keywords from the package.
To this end, we introduce a three-step approach, i.e., (1)
running OCR on the photo of a commodity to obtain what
are printed on its package, (2) standardizing the OCR re-
sults to a subset of the standard word items, and (3) ﬁltering
the standardized word items to remove the noise words that
are not regular collocations of the other words.

3.1 Optical Character Recognition

To utilize the existing information of a commodity in a
speciﬁed photo for the subsequent steps, ﬁrst of all, we have

1According to the search results on Google using keywords
“facial foam cucumber” and “dove facial foam cucumber”.

to recognize the characters printed on its package. Thanks
to the advanced OCR techniques (e.g., Tesseract-OCR), the
character recognition can handle characters from multiple
languages and with poly-fonts. Nonetheless, due to the var-
ious design styles of product package, such as ﬂourish letters,
and diﬀerent inter-word and inter-character spaces, OCR is
often unable to recognize every word on the commodity ac-
curately. Partial OCR results are useless string fragments
corresponding to errors in the recognition. To address this
problem, we employ the set W of standard word items to
emend the OCR results, and ﬁlter out the useless standard-
ized word items based on the word symbiosis matrix S.

3.2 Standardization

For each string fragment s in OCR results, we map it to
a standard word item wi ∈ W (i ∈ {1, . . . , n}) with the
maximal similarity to s. Here, the similarity is deﬁned as

Similarity(s, wi) = 1 −

Ed(s, wi)

max{Length(s), Length(wi)}

where Ed(s, wi) is the edit distance (a.k.a. Levenshtein dis-
tance, which is commonly used to evaluate the similarity be-
tween strings [3]) between s and wi, Length(s) & Length(wi)
are the string lengths of s and wi respectively. As Ed(s, wi) ∈

(cid:2)0, max{Length(s), Length(wi)}(cid:3), Similarity(s, wi) ∈ [0, 1].

Since a few string fragments are meaningless symbols (e.g.,
“?”, “||”, “–”) corresponding to errors and noises in OCR,
we make use of the following heuristic, i.e., for each string
fragment s, if Similarity(s, wi) is less than a threshold Ts
(Ts = 0.5 in this paper) for any wi, then the current s is
discarded without mapping it to any standard word item.

3.3 Filtering

Although standardization cleans up OCR results, it may

occasionally bring with several word items that are not printed
on the commodity. For example, the word “men” may be
transformed from a meaningless string fragment “ren”. In
order to improve the accuracy of the commodity identiﬁca-
tion, these unexpected word items should be removed.

Inspired by collaborative ﬁltering [4] which predicts the
interests of a user by collecting preferences from many users,
we predict the unexpected word items according to the co-
occurrence probability of each two standard word items, i.e.,
the elements of the word symbiosis matrix S. The detailed
steps for the ﬁltering process are as follows.

Step 1. If ∀wk, Sik = 0, then ﬁlter out this standardized

word item wi.

Step 2. For each remaining wi with maxk Sik < 1, if

Pk Sik

Pk I(Sik > 0)

< Tp

where I(·) is the indicator function, and Tp a threshold (Tp =
0.2 in this paper), then ﬁlter out this wi.

By ﬁltering out the unexpected words, we can obtain a
set of word items with much more cohesiveness in colloca-
tion. These ﬁltered word items are returned as the extracted
keywords for the objective commodity.

4. BRAND DEDUCTION

Product brand is important for commodity identiﬁcation
due to its uniqueness. However, sometimes there is no brand
information in the extracted keywords, especially in the cases

986that the commodity brands are images or in art fonts that
are very diﬃcult to be recognized by OCR. Instead of us-
ing Logo recognition, we deduce a product brand with the
extracted keywords, avoiding to hassle with constructing a
large-scale Logo database.

In fact, even if extracted keywords may not contain prod-
uct brands explicitly, their collocations can also imply brand
information. For example, keywords “pro-x” and “profes-
sional” imply that the commodity is very likely one of the
pro-x series products of Olay. Thus, we can deduce the prod-
uct brand by two steps, i.e., (1) ﬁnding out the most relevant
commodities of the extracted keywords, and (2) checking
which brand owns these relevant commodities.

4.1 Finding Relevant Commodities

In our knowledge base, there are n standard word items
wi (i = 1, . . . , n) extracted from the product names Nj
(j = 1, . . . , m) of m commodities. For each wi, the product-
word matrix P has recorded its occurrence times Pji in each
product name Nj. Let Yi be the occurrence times of wi in
the extracted keywords, we can evaluate each commodity’s
relevance αj (j ∈ {1, . . . , m}) to the extracted keywords by
solving the following minimization problem.

α = arg min

α

n

Xi=1 Yi −

m

Xj=1

αj Pji!2

, s.t.

α2

j 6 1

m

Xj=1

where α = {α1, . . . , αm}.

bias between Yi (i = 1, . . . , n) andPm

The reasons behind are as follows. To minimize the squares
j=1 αj Pji (j = 1, . . . , m),
(1) if Pji is close to Yi > 0, i.e., word wi appears in both
the extracted keywords and the jth commodity’s name with
about the same occurrence times, then the jth commodity
should have a higher relevance αj to the extracted keywords;
(2) otherwise, αj will be close to zero since the extracted
keywords include only a few standard word items such that
most of Yi are equal to 0. Therefore, by solving the mini-
mization problem with ridge regression [2], the relevance α
between the extracted keywords and the commodities in the
knowledge base will be revealed.

4.2 Deducing Brand

As the brand-product matrix B has recorded the aﬃlia-
tion between brands and commodities, we can combine it
with the relevance α to deduce the brand of the commodity
in the photo. Let βt (t ∈ {1, . . . , b}) be the probability of
that the tth brand is the true one, then the probability of
each brand can be estimated as

βt = Pm
Pb
k=1Pm

j=1 αj Btj

.

j=1 αj Bkj

By summing up the relevance between the extracted key-
words and the relevant commodities belonging to the tth
brand, each probability βt denotes the relevance between
the commodity in the photo and the tth brand. The brand
with the maximal probability is returned as the target one.

Figure 1: Example of the generated ﬁnal keywords

Step 1. Combine the brand with the extracted keywords
as the search terms k = {k1, . . . , kp}, where ki (1 6 i 6 p) is
the ith word in the search terms, and input them to Google.
Step 2. Extract a set t = {t1, . . . , tq} of the titles of the
top search results listed on the ﬁrst search result page, in
which tj (1 6 j 6 q) represents the jth title.

Step 3. Rank each title tj according to its consistency
C(tj) with the search terms k, where C(tj) denotes the total
number of matched word items and approximately matched
collocations in tj and k, and can be calculated as

I(ki ∈ tj , ki+1 ∈ tj).

C(tj) =Xp

i=1

I(ki ∈ tj) +Xp−1

i=1

Step 4. Select the title with the maximal consistency with

the search terms k as the ﬁnal keywords.

Fig. 1 illustrates an example of the ﬁnal keywords gen-
erated by our framework for a commodity photo, together
with the search results of these ﬁnal keywords on Google,
where the ﬁrst one corresponds to the correct commodity.

By using the generated ﬁnal keywords as the query terms,
we can retrieve the related product information, such as rec-
ommended retail price, reviews, and alternatives, from the
e-commerce and product-review web sites.

6. EXPERIMENTAL EVALUATION

We have implemented a prototype with Java for the pro-
posed framework. With the prototype, we verify the eﬀec-
tiveness and eﬃciency of our framework from the two aspects
below, i.e., (1) the performance study in terms of accuracy
of commodity identiﬁcation, and (2) the eﬃciency study in
terms of runtime, after we present the experimental setup.

6.1 Experimental Setup

The prototype focuses on query personal care commodi-
ties by snapping. It constructs a knowledge base with 830
standard word items, which are extracted from the prod-
uct names of 600 personal care commodities crawled from
Amazon.com. All these products come from 201 brands.

We adopt 100 photos of personal care commodities as the
testing set. These photos diﬀer from sizes, image-capturing
conditions, illuminations, pros and cons of the commodities,
etc. Furthermore, for the sake of fairness, about half of the
testing commodities are outside the knowledge base.

The prototype runs on a laptop PC (2GB RAM, Intel Core
2 Due CPU 1.80GHz) under a 54.0Mbps wireless network.

5.

INFORMATION RETRIEVAL

6.2 Accuracy Study

Based on the extracted keywords and the deduced brand,
we generate the ﬁnal keywords that describe the objective
commodity much more exactly with the help of Google. The
generation can be performed as follows.

We study the accuracy performance of our prototype in
terms of (1) the accuracy of commodity identiﬁcation (name
accuracy for short), and (2) the brand accuracy. We also
report the performance of Google Goggles, which is based

9871

0.9

0.8

0.7

0.6

0.5

0.4

0.3

y
c
a
r
u
c
c
A

 

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

y
c
a
r
u
c
c
A

Name(Prototype)

Brand(Prototype)

Name(Goggles)

Brand(Goggles)

0.2

 

100

200

300

400

500

600

Number m of Product Names in Knowledge Base

(a)

0.2

 

0.1

 

s
d
n
o
c
e
S

12

10

8

6

4

2

0

 

Name(Ts)

Brand(Ts)

Name(Tp)

Brand(Tp)

Name(Goggles)

Brand(Goggles)

0.2

0.3

0.5

0.4
0.6
Value of Threshold

0.7

0.8

(b)

 

Runtime

OCR Time

Computation Time

Communication Time

20

40

60

80

100

Individual Photos

(c)

0.6

0.5

0.4

f

d
p

0.3

0.2

0.1

0

 
0

 

Runtime

OCR Time

Computation Time

Communication Time

2

4

6

8

10

12

Seconds

(d)

Figure 2: Performance of the prototype. (a) Accuracy vs. knowledge base sizes. (b) Accuracy vs. thresholds
Ts & Tp. (c) Runtime on each tested photo. (d) Runtime distribution

on both OCR and image matching, just for a reference. The
detailed performance metrics are as follows.

• Our prototype.

(1) For the name accuracy, the ﬁrst
search result on Google using the generated ﬁnal keywords
should correspond to the true commodity; and (2) for the
brand accuracy, the brand information within the ﬁnal key-
words should be unique and exactly the true brand.

• Google Goggles. (1) The metric for its name accuracy is
relaxed to either the ﬁrst search result on Google using the
OCR results as the search terms or the returned similar im-
age corresponding to the true commodity; and (2) its brand
accuracy can be checked by its Logo recognition result.

The accuracy study consists of two parts, i.e., (1) the eﬀect
of the size of knowledge base, in which we randomly select
m (m = 100, 200, . . . , 600) product names from the original
600 product names to construct knowlege base with diﬀerent
sizes, and record the accuracy performance of the prototype.
Without loss of generality, we report its average performance
of 20 times of run for each m; and (2) the eﬀect of the two
parameters in our framework, i.e., the thresholds Ts (see
Section 3.2) and Tp (see Section 3.3).

Fig. 2(a) illustrates the accuracy performance of the pro-
totype using knowledge bases with various sizes, from which
we can observe that the name and brand accuracy increase
with the growth of the size of knowledge base. The rea-
sons behind are as follows. (1) With fewer product names
in the knowledge base, the number of the standard word
items is smaller, increasing the risk that the OCR results
cannot be mapped to the correct standard word items, and
thus aﬀecting the performance of the subsequent processes.
(2) Contrarily, if there are abundant standard word items
collected from many real product names, these word items
can cover most words, even those in a “new” product name
not included in the knowledge base, which better reﬂects
the word collocations in reality, leading to more reasonable
extracted keywords and deduced brand.

Fig. 2(b) depicts the accuracy performance of the proto-
type using diﬀerent thresholds Ts and Tp, from which we
can observe that the name and brand accuracy vary slightly
with the change of these two parameters. In other words,
the prototype is relatively robust to the parameters.

6.3 Efﬁciency Study

In this experiment, we investigate the eﬃciency perfor-
mance of our prototype in terms of runtime by illustrat-
ing the runtime for processing each photo in the testing
set, and the runtime distribution in Figs. 2(c) and 2(d),
respectively. Furthermore, since each runtime consists of

three parts, namely (1) the time for OCR; (2) the compu-
tation time for the standardization and ﬁltering processes,
the brand deduction, and the computation of local matching
degree during the product information acquisition via web,
and (3) the time consumed on Internet communication, the
OCR, computation, and communication times and their dis-
tribution are also depicted in the ﬁgures respectively.

We can observe that (1) the average runtime of the pro-
totype is about 4.5 seconds, comparable to approximately
5 seconds required by Google Goggles running on the same
wireless network; and (2) the computation and communi-
cation times change slightly in each execution, resulting in
that the runtime mostly depends on the time for OCR.

7. CONCLUSION

In this paper, we have proposed a simple yet eﬀective
framework for commodity query by snapping, which enables
users to obtain relevant commodity information via their
smart mobile devices by just taking a snapshot of an ob-
jective commodity and submitting it to our system. Ex-
periments on a prototype have veriﬁed the eﬀectiveness and
eﬃciency of the proposed framework.

8. ACKNOWLEDGMENTS

This work was partly supported by the National Key Tech-
nologies R&D Program (Grant No.: 20128AH94F01), NSFC
61003049, the Fundamental Research Funds for the Central
Universities (Grant No.: 2012QNA5018, 2013QNA5020), and
the Key Project of ZJU Excellent Young Teacher Fund.

9. REFERENCES
[1] J. Chen, L. Wang, and D. Chen. Logo Recognition:

Theory and Practice. CRC press, Boca Raton, 2011.

[2] T. Hastie, R. Tibshirani, and J. Friedman. The

elements of statistical learning: data mining, inference,
and prediction, 2nd ed. Springer, Heidelberg, 2009.

[3] Y. Li and B. Liu. A normalized levenshtein distance
metric. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 29(6):1091–1095, 2007.

[4] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl.

Item-based collaborative ﬁltering recommendation
algorithms. In WWW’01, pages 285–295, 2001.

[5] S.S. Tsai, D. Chen, V. Chandrasekhar, G. Takacs, N.M.

Cheung, R. Vedantham, R. Grzeszczuk, and B. Girod.
Mobile product recognition. In MM’10, pages
1587–1590, 2010.

988