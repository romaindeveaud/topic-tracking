A Novel TF-IDF Weighting Scheme for Effective Ranking

Jiaul H. Paik

Indian Statistical Institute, Kolkata, India

jia.paik@gmail.com

ABSTRACT
Term weighting schemes are central to the study of informa-
tion retrieval systems. This article proposes a novel TF-IDF
term weighting scheme that employs two diﬀerent within
document term frequency normalizations to capture two dif-
ferent aspects of term saliency. One component of the term
frequency is eﬀective for short queries, while the other per-
forms better on long queries. The ﬁnal weight is then mea-
sured by taking a weighted combination of these compo-
nents, which is determined on the basis of the length of the
corresponding query.

Experiments conducted on a large number of TREC news
and web collections demonstrate that the proposed scheme
almost always outperforms ﬁve state of the art retrieval
models with remarkable signiﬁcance and consistency. The
experimental results also show that the proposed model achieves
signiﬁcantly better precision than the existing models.

Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Re-
trieval : Retrieval Models

General Terms
Algorithm, Experimentation, Performance

Keywords
Document ranking, Retrieval model, Term weighting

1.

INTRODUCTION

Term weighting schemes are the central part of an infor-
mation retrieval system. Eﬀectiveness of IR systems are thus
crucially dependent on the underlying term weighting mech-
anism. Almost all retrieval models integrate three major
variables to determine the degree of importance of a term
for a document: (i) within document term frequency, (ii)
document length and (iii) the speciﬁcity of the term in the

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

collection. Term frequency and document length combina-
tion is used to infer the saliency of a term in a document, and
when a query contains more than one term, term speciﬁcity
is used to reward the documents that contain the terms rare
in the collection.

Retrieval models can be broadly classiﬁed into two ma-
jor families based on their term weight estimation princi-
ple. Vector space model casts queries and documents as
ﬁnite dimensional vectors, where the weight of an individual
component is computed using numerous variations of tf-idf
scores. On the other hand, probabilistic models [16, 17] pri-
marily focus on estimating the probabilities of the terms in
the documents, and the estimation techniques diﬀer from
one approach to the other. But in essence all of them use
the same basic principles that we have outlined before.

Most of the existing models (possibly all) employ a sin-
gle term frequency normalization mechanism that does not
take into account various aspects of a term’s saliency in a
document. For example, frequency of a term in a document
relative to the frequency of the other terms in the same doc-
ument gives us an important clue that can not be achieved
by the commonly used document length based normaliza-
tion scheme. On the contrary, length based normalization
can restrict the likelihood of retrieval of extremely long doc-
uments which can not be taken care of by the relative fre-
quency based term weighting.

Another major limitation of the present models is that
they do not balance well in preferring short and long doc-
uments. Such limitation makes a system to retrieve low
quality documents at the top of the ranked list when they
face queries of varying length. For example, in pivoted doc-
ument length normalization scheme, if the parameter is set
to a smaller value, it performs better for shorter queries, and
when the parameter value is larger, longer queries are bene-
ﬁted more than the shorter queries [10]. Similar observation
can be made for other models such as BM25, language model
or relatively recent divergence from randomness based mod-
els [13, 10].

The main reason is that when the parameter is set to
a static value, most of the models prefer either short doc-
uments or long documents.
If a weighting scheme prefers
long documents, it pulls up extremely long documents when
longer queries are encountered, since the longer documents
have higher verbosity level it matches more query terms[28].
On the other direction, preference of short documents may
degrade the overall retrieval performance, since it violates
the likelihood of relevance versus retrieval pattern suggested
by Singhal et al. [28].

343This article proposes a term weighting scheme that can ad-
dress these weaknesses in an eﬀective way. In particular, we
argue that the two aspects of term frequencies, when com-
bined appropriately, leads to signiﬁcant performance beneﬁt.
In this article we make the following contributions.

• It introduces a two aspect term frequency normaliza-
tion scheme, that combines relative tf weighting and
the tf normalization based on document length. One
component of the term frequency tends to prefer long
documents, while the other component prefers short
documents and therefore, it maintains a good balance
in preferring short and long documents.

• It uses the query length information to emphasize the
appropriate component. In particular, when the sys-
tem faces a long query, it down-weights the part that
prefers long documents in order to compensate the ef-
fect and vice versa.

• It modiﬁes the usual term discrimination measure (idf)
by integrating mean term frequency of a term in the
set of documents the term is contained in.

• Finally, we use asymptotically bounded function (sim-
ilar to Robertson and Walker [22]) to transform the tf
factors that better handles the term coverage issues in
the documents and also helps to combine the two tf
factors more easily. As a bi-product of such transfor-
mation, the ranking function easily produces the sim-
ilarity values in the range of [0-1].

In order to assess the eﬀectiveness of the proposed model
we carry out a set of experiments on a large number of stan-
dard test collections containing news and web data. The
experimental results show that the proposed weighting func-
tion consistently and signiﬁcantly outperforms ﬁve state of
the art retrieval models (from vector space as well as proba-
bilities families) measured in terms of the standard metrics
such as MAP and NDCG. The experimental outcomes also
attest that the model achieves signiﬁcantly better precision
than all the other models when measured in terms of a re-
cently popularized metric, namely, expected reciprocal rank
(ERR) [6].

The remainder of the article is organized as follows.

In
Section 2 we review the state of the art. Section 3 describes
the proposed weighing scheme. Description about the test
collections, evaluation metrics and the details of the base-
lines are given in Section 4. Experimental results are pre-
sented in Section 5, where we compare the performance of
the proposed model with the state of the art vector space
models, followed by the comparison with the probabilistic
models. Finally, we conclude in Section 6.

2. STATE OF THE ART

Information retrieval systems, when encounter a query,
tries to rank documents by their likelihood of relevance.
Most IR systems assign a numeric score to the documents
and then they are ranked based on these scores. Three
widely used models in IR are the vector space model [26,
25], the probabilistic models [21] and the inference network
based model [30]. In this section we review some of the state
of the art models.

In vector space model, queries and documents are repre-
sented as the vector of terms. To compute a score between a

document and a query the model measures the similarity be-
tween the query and document vector using cosine function.
The central part of the vector space model is to determine
the weight of the terms that are present in the query and
the documents. Three main factors that come into play to
compute the weight of a term are: (i) frequency of the term
in the document, (ii) document frequency of the term in the
collection (ﬁrst proposed in [29]) and (iii) the length of the
document that contains the term. Fang et al. [10] give a
comprehensive analysis of four retrieval models by deﬁning
a set of constraints that needs to be satisﬁed for eﬀective
retrieval. Using these constraints the strengths and weak-
nesses of some well known models are analyzed and some
of the models are modiﬁed. There are also a number of re-
cent works that focus on the constraint based analysis of the
retrieval models [8, 9].

Salton and Buckley [24] summarize a number of term
weighting approaches which use various types of normal-
ization. It is evident that document length is an important
component in eﬀective term weighting. Singhal et al. [28]
identify a number of weaknesses of cosine and maximum tf
normalization and they observe that a weighting formula
that retrieves documents with chances similar to their prob-
ability of relevance performs better. Following this obser-
vation, they propose a pivoted normalization scheme that
acts as a correction factor of old normalization and is one
of the most eﬀective term weighting schemes in the vector
space framework. The pivoted length normalization scheme
computes the term weight as follows [27]:

1 + ln(1 + ln(T F (t, D)))

1 − s + s

len(D)
ADL(C)

· T F (t, Q) · ln

N + 1
df (t)

(1)

X

t∈Q∩D

The parameter s controls the extent of normalization with
respect to the document. Typically, the term weighting
functions in vector space model are designed heuristically,
which are based on the researchers experience. Several work
tried to use the data to learn the patterns that satisfy the
data. For example, Greiﬀ [11] uses exploratory data analysis
to uncover some important relationship between the docu-
ment frequency and the relevance of a document.

The key part of the probabilistic models is to estimate the
probability of relevance of the documents for a query. This
is where most probabilistic model diﬀers from one another.
Binary independence model is perhaps the most widely ac-
cepted technique in the classical probabilistic model. A
number of weighting formula have been developed and BM25 [20]
has been the most eﬀective among the formulae. The major
diﬀerences between BM25 and the other commonly used TF-
IDF models are the slightly variant IDF formulation and the
use of the query term frequency. The length normalization
factor uses the average document length and a parameter
has been introduced to control the relative length eﬀect.

Probabilistic language modeling technique [19, 14] is an-
other eﬀective ranking model that is widely used today.
Typically, language modeling approaches compute the prob-
ability of generating a query from a document, assuming
that the query terms are chosen independently. Unlike TF-
IDF models, language modeling approaches do not explic-
itly use document length factor and the idf component. It
seems that the length of the document is an integral part of
this formula and that automatically takes care of the length
normalization issue. However, smoothing is crucial and it
has very similar eﬀect as the parameter that controls the

344length normalization components in pivoted normalization
or BM25 model. Three major smoothing techniques (Dirich-
let, Jelinek-Mercer and Two-stage) are commonly used in
this model [31].

Relatively recent, another probabilistic model is proposed
in [3] that computes the weight of a term by measuring the
informative content of a term by computing the amount of
divergence of the term frequency distribution from the dis-
tribution based on a random process. Like most of the well
known models, they also use the same basic components.
However, the integration of various component are derived
theoretically. This family of formula also uses the average
document length as an ideal length of the documents and the
term frequencies are normalized with respect to the average
document length.

In inference network, document retrieval is modeled as
an inference process [30]. A document instantiates a term
with a certain strength and given a query the credit from
multiple terms is accumulated to compute a relevance that is
very equivalent to the similarity score of vector space model.
From an operational angle, the strength of instantiation of
a term for a document can be considered as weight of the
term in a document. The strength of instantiation of a term
can be computed using any reasonable formula.

3. PROPOSED WORK
3.1 Preliminaries

Given a query Q and a document D, the main task of
a ranking function is to assign a score to D with respect
to the query Q. The main objective of a term weighting
scheme is to quantify the saliency of the query terms in
the document. This section describes a novel TF-IDF term
weighting scheme that serves above purpose. En-route to
the development, we are guided by a number of hypotheses
that are commonplace in quantifying the importance of a
term. Thus, before we give the main motivation behind our
work, let us ﬁrst revisit the three key hypotheses.

1. Term Frequency Hypothesis (TFH): The weight
of term in a document should increase with the in-
crease in term frequency (T F ). However, it seems un-
likely that the importance of a term grows linearly with
the increase in T F . Therefore, researchers have used
dampened TF instead of the raw TF for ranking. The
most widely used damping function has been log(T F )
and the basis of this damping can be best captured by
the following advanced hypothesis.

Advanced TF Hypothesis (AD-TFH): The mod-
iﬁed term frequency hypothesis captures the intuition
that the rate of change of weight of a term should de-
crease with the larger TF. For example, the change in
the weight caused by increasing TF from 2 to 3 should
be higher than that caused by increasing TF from 25
to 26 [10]. Thus, the raw T F has to be transformed to
fulﬁll the above goal. Formally, we hypothesize that,
a function Ft(T F ), that maps the original T F to the
resultant value (which will be used for ﬁnal weighting),
should possess the following two properties.

(a) F

(b) F

(cid:4)
t (T F ) > 0
(cid:4)(cid:4)
t (T F ) < 0

2. Document Length Hypothesis (DLH): This hy-
pothesis captures the relationship between the term
frequency and the document length. Long documents
tend to use a term repeatedly, thus term frequency
can be higher in a long document. Therefore, if T F
is considered in isolation (disregarding the document
length), long documents are given undue preference.
Thus, it is necessary to regulate the T F value in ac-
cordance with the document length. The general prin-
ciple is that if two documents have diﬀerent lengths
and the same T F values for a term t, then the con-
tribution of T F (of t) should be higher for the shorter
document.

3. Term Discrimination Hypothesis (TDH): If a
query contains more than one term, then a good weight-
ing scheme should prefer a document that contains the
rare term (in the collection).

3.2 Two Aspects of TF

Most existing weighting schemes employ the above heuris-
tics to quantify the term importance. However, they gen-
erally normalize the term frequency that captures a single
aspect of the saliency of the terms and hence disregards an-
other important aspect that we detail next. In particular,
we consider the following two aspects:

1. Relative Intra-document TF (RITF) : In this fac-
tor, the importance of a term is measured by consid-
ering its frequency relative to the average T F of the
document. Thus, a natural formulation for this could
be

RIT F (t, D) =

T F (t, D)

Avg.T F (D)

(2)

where T F (t, D) and Avg.T F (t, D) denote the frequency
of the term t in D and average term frequency of D re-
spectively. However, Equation 2 may too much prefer
excessively long documents, since the denominator is
close to 1 for a very long document [28]. Hence, a sub-
linear damping of T F seems to be a better choice over
the raw T F and thus we use the following function.

RIT F (t, D) =

log2(1 + T F (t, D))

log2(1 + Avg.T F (D))

(3)

Indeed, such a formula has been used by Singhal et
al. [28] in the pivoted length normalization framework
to normalize the tf values in accordance with the num-
ber of unique terms in the document.

2. Length Regularized TF (LRTF) : This factor nor-
malizes the term frequency by considering the number
of terms present in a document. Similar to Robert-
son’s [22] notion, we assume that the appropriate length
of a document should be the average document length
of the collection and the frequency of the terms of an
average length document should remain unchanged.
Thus, a reasonable starting point could be T F (t, D)×
len(D) , where ADL(C) is the average document length
ADL(C)

of the collection and len(D) is the length of the doc-
ument D. But once again, it seems unlikely that the
increase in term frequency follows a linear relationship
with the document length, and thus the above formula
over-penalizes the long documents. To overcome this

345bias, we employ the following function (used in [3]) to
achieve the length dependent normalization.
LRT F (t, D) = T F (t, D) × log2

„

«

(4)

ADL(C)
len(D)

1 +

Equation 4 still punishes the long documents but with
diminishing eﬀect.

However, we believe that any document length normaliza-
tion can be used to achieve this purpose. Some of the pos-
sible alternatives might be the length normalization compo-
nent of BM25 or that of the pivoted normalization scheme.

3.2.1 Motivation

In order to motivate the use of two T F factors, let us
consider the following two somewhat toy examples. We use
these examples just to introduce the basic idea.

Example 1 Let D1 and D2 be two documents of equal

lengths, with the following statistics.

1. len(D1) = 20, # distinct term of D1 = 5,

T F (t, D1)=4

2. len(D2) = 20, # distinct term of D2 = 15,

T F (t, D2)=4

In both of the cases, LRT F considers t equally important.
A little thought will convince us that this is not appropri-
ate, since the focus of the document D1 seems to be divided
equally among 5 terms and therefore t should not be consid-
ered salient, while t seems to be important for D2. Thus, in
the later case RIT F seems to be a better choice to LRT F .
Let us now turn to the other direction and consider the

second example.

Example 2 Let D1 and D2 be two documents with the

following statistics.

1. len(D1) = 20, # distinct term of D1 = 15,

T F (t, D1)=4

2. len(D2) = 200, # distinct term of D2 = 150,

T F (t, D2)=4

For this instance however, RIT F considers the term t equally
important for both D1 and D2, which is not right, since D2
contains more distinct terms and thus seems to cover many
other topics (also possibly uses t repeatedly). Therefore, in
this case, the use of LRT F seems to be a potential choice
over RIT F .

Motivated by the above examples, our main goal now is to
integrate the above two factors into our weighting scheme.
However, we do not use the T F factors as deﬁned in the
Equations 3 and 4. We transform these T F values for our
ﬁnal use that in some sense makes use of the hypothesis AD-
TFH. The next section details the transformation procedure
and the underlying motivation.

3.2.2 Transforming TF Factors

Recall that the main motivation behind the advanced hy-
pothesis on term frequency (AD-TFH) is that a good weight-
ing function, while emphasizing on term frequencies and
term discrimination factors, should also pay attention to the
term coverage issue (i. e number of match). For example, if
a document D1 contains a query term 10 times and another
document D2 contains two query terms (of the same query)

5 times each, then the assigned score should be higher for D2
(assuming that both the query terms have equal term dis-
crimination values). That is probably the most important
reason why raw T F does not work well in practice. Second,
another common trait of many weighting schemes (for exam-
ple, pivoted normalization) is that they use the T F functions
that are not bounded above. We transform the T F factors
using a function f (x) that possesses the following proper-
ties: (i) vanishes at 0, (ii) satisﬁes the two conditions of
AD-TFH hypothesis (f
(x) < 0), and (iii)
asymptotically upper bounded to 1.

(x) > 0 and f

(cid:4)(cid:4)

(cid:4)

One of the simplest functions that satisﬁes the above prop-
erties is f (x) = x
1+x . Indeed, similar functions have been
employed before in [22] and in [3]. Using this function, we
now transform the two T F factors as follows:

BRIT F (t, D) =

BLRT F (t, D) =

RIT F (t, D)

1 + RIT F (t, D)

LRT F (t, D)

1 + LRT F (t, D)

(5)

(6)

3.2.3 Combining Two TF Factors

Now the key question that we face: how should we com-
bine BRIT F (t, D) and BLRT F (t, D)? A natural way to do
this is as follows:
T F F (t, D) = w × BRIT F (t, D) + (1 − w) × BLRT F (t, D) (7)

where 0 < w <1. The next important issues that arise out
of Equation 7 are the following:

• Should we prefer BRIT F (t, D) (w > 0.5)?
• Should we prefer BLRT F (t, D) (w < 0.5)?

In order to answer these questions, we now analyze the prop-
erties of the two TF components. From Equation 5, it is
clear that BRIT F (t, D) has a tendency to prefer long doc-
uments, since for long documents the denominator part of
RIT F (t, D) is close to 1, and T F is usually larger. On the
other hand, BLRT F (t, D) tends to prefer short documents,
since LRT F (t, D) → 0 as len(D) → ∞. Therefore, when a
query is long, BRIT F (t, D) heavily prefers extremely long
documents, since the number of matches is more or less
proportional to the length of the document [28]. On the
contrary, since BLRT F (t, D) prefers short documents it
can penalize extremely long documents when it faces longer
queries, and thus it is preferable when longer queries are
encountered. Another interesting property of BRIT F (t, D)
is that it emphasizes on the number of matches, since the
main component of this formula RIT F (t, D) heavily pun-
ishes the term frequency, and thus important for the short
queries. Hence, the foregoing discussion suggests that, for
short queries BRIT F (t, D) should be preferred, while for
longer queries, BLRT F (t, D) should be given more weight
Based on the discussion given in the previous section,
we now turn to incorporate the query length information
into our weighting formula. The value of w should decrease
with the increase in query length, while it must lie between
[0-1]. Speciﬁcally, we characterize the query length factor
(QLF (Q)) by the following variables. (i) QLF (Q) = 1 for
|Q| = 1, (ii) QLF
(Q) < 0 and (iii) 0 < QLF (Q) < 1. Nu-
merous diﬀerent functions can be constructed that satisfy
the above conditions. We used the following three diﬀerent

(cid:4)

346functions.

tionship.

Sim(Q, D) <

|Q|X

i=1

T DF (qi,C)

(14)

QLF1(Q) =

QLF2(Q) =

QLF3(Q) =

1

2

log2(1 + |Q|)
1 + log2(1 + |Q|)
2 + log2(1 + |Q|)

3

(8)

(9)

(10)

The ﬁrst function descends more rapidly than the second
function, while the second function descends more rapidly
than the third function. Our experiments suggest that func-
tion 9 performs consistently better than the other two func-
tions on all the collections. Hence, we set w = QLF2(Q).
We leave this issue for further investigation.

3.3 Term Discrimination Factor

The goal of the term discrimination factor in weighting
is to assign higher score to the documents that contain the
terms which are rare in the collection.
Inverse document
frequency (IDF ) is a well known measure that serves the
above purpose. A number of IDF formulation are prevalent
in the IR literature, all of which essentially quantify the
above intuition. We use the following standard idf measure.

IDF (t,C) = log

„

«

CS(C) + 1
DF (t,C)

(11)

DF (t,C) , where CT F (t,C) denotes the
CT F (t,C)

The above IDF measure considers only the presence or
absence of a term in a document and does not take into ac-
count the document speciﬁc term occurrence. We hypoth-
esize that the term discrimination is a combination of the
above two factors. In particular, we hypothesize that if two
terms have equal document frequencies, then the term dis-
crimination should increase with the increase in average elite
set term frequency. The average elite set term frequency
(AEF ) is deﬁned as
total occurrence of the term t in the entire collection.
In
fact, Kwok [18] used AEF for term weighting, but the pur-
pose was diﬀerent. However, the combination of raw AEF
with IDF may disturb the overall term discrimination value,
since the IDF values are obtained by dampening through
log function. Hence, we employ a slowly increasing function
to transform the AEF values for this combination. Once
again, we use the function f (x) = x/(1 + x) to transform
the AEF values for the ﬁnal use. The ﬁnal term discrimi-
nation value of term t is computed as

T DF (t,C) = IDF (t,C) × AEF (t,C)
1 + AEF (t,C)

(12)

Therefore, we can easily modify Equation 13 to get the nor-
malized similarity scores (0 < Sim(Q, D) < 1) as follows:

Simnorm(Q, D) =

i=1

|Q|P

T F F (qi, D) × T DF (qi, C)

|Q|P

i=1

T DF (qi,C)

(15)

Equations 13 and 15 are equivalent in the sense that they
produce the same ranked lists. However, an application that
requires normalized scores, Equation 15 can be used as a
suitable alternative.

4. EXPERIMENTAL SETUP

In this section we describe the details of our experimen-
tal setup. First, in Section 1 we give the details of the test
collections used in our experiments. In Section 4.2 and Sec-
tion 4.3 we describe the evaluation measures and the baseline
retrieval models respectively.
4.1 Data

Table 1 summarizes the statistics on test collections used
in our experiments. The experiments are conducted on a
large number of standard test collections, that vary both by
type, the size of the document collections and the number
of queries.

TREC 6,7,8 and ROBUST are news collections contain-
ing 528,155 documents and supplemented by 150 (queries
301-450) and 100 (601-700) queries respectively. WT10G
is a web collection of moderate size supplemented by 100
queries (451-550), while GOV2 is another web collection of
larger size, which is crawled from .gov domain. There are
150 (queries 701-850) queries attached with GOV2 collection
which were used in TREC terabyte [4] track for three years.

Table 1: Test Collection Statistics
# of docs # of queries
Name
528,155
TREC 6,7,8
528,155
ROBUST
WT10G
1,692,096
25,205,179
GOV2
25,205,179
MQ-07
MQ-08
25,205,179

150
100
100
150
1778
784

Our experiments reveal that the use of the above term dis-
crimination has not very signiﬁcant eﬀect on the overall per-
formance. However, it is observed that the improvements,
although are small, consistent across the collections.

3.4 Final Formula

Integrating the above factors we now obtain the following

ﬁnal scoring formula.

|Q|X

Sim(Q, D) =

i=1

T F F (qi, D) × T DF (qi,C)

(13)

Again, since T F F (qi, D) < 1, we obtain the following rela-

The MQ-07 and MQ-08 set of queries are based on the
Million Query Track 2007 [2] and 2008 [1] respectively. This
track was designed to serve two purposes. First, it was an
exploration of ad-hoc retrieval on a large collection of doc-
uments. Second, it investigated questions of system evalua-
tion, particularly whether it is better to evaluate using many
shallow judgments or fewer thorough judgments. Both mil-
lion query track use GOV2 as document collection. Topics
for this task were drawn from a large collection of queries
that were collected by a large Internet search engine. The
queries also vary by their length, with short (2-3 words) to
long (6-10 words). Speciﬁcally, MQ-07 and MQ-08 collec-
tions contain 505 and 433 queries of length higher than 5

347respectively. Therefore, the test collections provide us a di-
verse experimental setup for assessing the eﬀectiveness of
the proposed weighting method.

Except TREC-6,7,8, all the test collections have three
scale graded relevance assessment. The grades are 0, 1 and
2- meaning non-relevant, relevant and highly relevant re-
spectively. TREC-6,7,8 collection uses binary relevance as-
sessment.
4.2 Evaluation Measures and IR System

All our experiments are carried out using TERRIER1 re-
trieval system (version 3.5). Terrier is a ﬂexible Information
retrieval system which provides the implementation of many
well known models. We use title ﬁeld of the topics (note that
two million query data contain more than 1000 queries that
contain more than 5 terms). From all the collections we re-
moved stopwords during indexing. Documents and queries
are stemmed using Porter stemmer. Statistical signiﬁcance
tests are done using two sided paired t-test at 95% conﬁ-
dence level (i,e p < 0.05).

We use the following metrics to evaluate the systems.
• Mean Average Precision (MAP): This is a standard

metric for binary relevance assessment.

• Normalized DCG at k (NDCG@k) [15]: Discounted
cumulative gain (DCG) is an evaluation measure that
can leverage the relevance judgment in terms of multi-
ple grades, and has an explicit position-wise discount
factor. NDCG is the normalized version of DCG.

• Expected Reciprocal Rank at k (ERR@k) [6]: To re-
lax the additive nature and the underlying indepen-
dent assumption in NDCG, another evaluation mea-
sure, namely, Expected Reciprocal Rank (ERR) is pro-
posed in [6].
It discounts the documents which are
shown below very relevant documents, and is deﬁned
as the expected reciprocal length of time that a user
will take to ﬁnd a relevant document. ERR@k is com-
puted as follows:

(1 − R(gi))

(16)

kX

ERR@k =

i−1Y

j=1

R(gi)

i=1

i

2hg , hg is the highest grade and g1, g2 . . . gk

where R(g) = 2g−1
are the relevance grades associated with the top k doc-
uments. The value of mg is 2 for all the collections,
except TREC 6,7&8 (for this mg = 1).

The ﬁrst two metrics are used to reﬂect the overall per-
formance of the systems, while the last evaluation measure
reﬂects better the precision of search results, thereby making
more important for the precision oriented systems. ERR has
been chosen as one of the oﬃcial metrics for recent TREC
web tracks [7].

Note that, two million query collections (MQ-07 and MQ-
08) have incomplete relevance assessment. Therefore, for
the sake of more reliable conclusions, we evaluate the mil-
lion query sets in two diﬀerent ways. First, we skip the
unjudged documents from the ranked lists to compute the
values of well known metrics following the recommendation
made in [23]. Additionally, we also present the statistical

1http://terrier.org/

average precision2 [5] which was one of the oﬃcial metrics
for the million query tracks [2].

4.3 Baselines

We have compared the performance of the proposed weight-
ing scheme with ﬁve state of the art retrieval models. Since
the proposed weighting function is a TF-IDF based formula,
we have taken two well known state of the art TF-IDF mod-
els. We have also chosen BM25, language model with Dirich-
let smoothing (LM), and relatively recent divergence from
randomness based formula (PL2) as the other state of the
art baselines. The choice of our baselines are primarily mo-
tivated by [10], which provides a thorough and detailed de-
scription of all the state of the art models along with the
parameter sensitivity issues.

The performances of all the baseline models are dependent
on the parameters they contain. Therefore, for the sake of
more reliable comparisons with the baselines, we carry out
two experiments by taking ﬁrst 50 judged queries from MQ-
07 and MQ-08 collections. We search parameters by opti-
mizing NDCG@20. The parameters values are given in the
description of the corresponding baselines. Our experiments
mostly agree with the ﬁndings reported by Fang et al. [10].
In particular, we ﬁnd that the performances of Pivoted TF-
IDF and PL2 are very sensitive with the variation of the
parameters. For example, the parameter value (s = 0.2)
suggested for pivoted TF-IDF in the original paper [28] gives
12% poorer MAP than that we ﬁnd here by training. Sim-
ilarly, the default PL2 parameter (c = 1) is 14% poorer
than the one we ﬁnd. Therefore, for fair comparisons, we
use these optimal parameter values for the baselines. The
details of the baselines are given below.

1. Pivoted length normalized TF-IDF model: This model
is one of the best performing TF-IDF formula in the
vector space model framework. The value of the pa-
rameter s is set to 0.05.

2. Lemur TF-IDF model: This model is another TF-IDF
model that uses Robertson’s tf and the standard idf .
The parameter of this model is set to its default value,
0.75.

3. Classical Probabilistic model (BM25): BM25 is cho-
sen as a state of the art representative of the classical
probabilistic model. The main diﬀerences with this
model and the previous model are that BM25 uses
query term frequency in a diﬀerent way and the idf
also diﬀers with the standard one. The parameters of
this model is set to k1 = 1.2, b = 0.6 and k3 = 1000.
Note that we found (on training data) slightly better
results for b = 0.6 than the default 0.75.

4. Dirichlet smooth language model (LM): Language model

is another probabilistic model that performs very ef-
fectively. For this model we set the value of Dirichlet
smoothing (µ) to 1700.

5. Divergence from Randomness model (PL2): Finally,

PL2 [12] represents the recently proposed non-parametric
probabilistic model from divergence from randomness

2the code available at TREC million query page is used to
compute stat AP

348(DFR) family. Similar to the previous models, its per-
formance also depends on a parameter value (c in the
formula). We conduct experiments for this model by
setting c = 13.

5. RESULTS

In this section we present the experimental results of our
proposed work and compare them with the state of the art
retrieval models. In Section 5.1 we compare the performance
of the proposed model (MATF for Multi Aspect TF) with
the two TF-IDF models, followed by the comparison with
three probabilistic models- BM25, language model (LM) and
PL2. We use three evaluation measures to evaluate the per-
formance of all the methods.
5.1 Comparison with TF-IDF Models

In this section we focus on to compare the performance
of the proposed model (MATF) with the Lemur TF-IDF
and and Pivoted TF-IDF models. Table 2 presents the ex-
perimental results for six test collections measured in terms
MAP, NDCG@20 and ERR@20.

First, we describe the results in terms of MAP. Table 2
clearly shows that MATF gains signiﬁcantly better MAP
than both of the TF-IDF models on two news collections.
MATF performs 12% and 15.7% better than Lemur TF-IDF
model on TREC-678 and ROBUST respectively. MATF is
also signiﬁcantly surpasses the Pivoted TF-IDF model on
these collections with a margin of 8.8% and 6.3% respec-
tively.

The behavior of MATF is similar when we see the results
for two web collections, namely, WT10G and GOV2. Once
again, MATF outperforms Lemur TF-IDF model by a mar-
gin of more than 20% in both of the occasions, which is
clearly highly signiﬁcant as conﬁrmed by the paired t test.
Like the previous two news collections, MATF maintains its
superior behavior over Pivoted TF-IDF in these web collec-
tions also.
In particular MATF gains more than 8% and
19% average precision than Pivoted TF-IDF for both of the
collections and paired t test once again attests the signiﬁ-
cance.

We now turn to describe the results on two million query
data sets. These two collections are particularly interesting,
since they contain real search queries collected from a com-
mercial search engine and also because of their variations
in length. Table 2 once again demonstrates that MATF un-
equivocally outperforms the two TF-IDF models with signif-
icantly large margin. The MAP achieved by MATF is nearly
11% and 7% better than that achieved by Lemur TF-IDF
on MQ-07 and MQ-08 collections respectively. Similarly,
MATF surpasses the Pivoted TF-IDF by more than 10%
margin on both of the occasions. Signiﬁcance tests show
that the performance diﬀerences are always statistically sig-
niﬁcant.

Among the two TF-IDF models, Lemur TF-IDF often
seems to perform poorer than Pivoted (except MQ-08 where
Lemur TF-IDF is nearly 4% better than pivoted). One po-
tentially interesting outcome that we can see from Table 2 is
that, when the document collection is larger MATF outper-
forms Pivoted TF-IDF with larger margin.
In particular,
MATF gains a MAP on GOV2 collection which is almost
20% better than the pivoted TF-IDF, which is a clear sign
of eﬀectiveness of MATF over the state of the TF-IDF mod-
els.

So far our discussion of experimental outcomes primar-
ily conﬁned on the basis of the binary relevance assessment.
Note that ﬁve out of six test collections used in our evalua-
tion have graded assessment in three scales (0,1,2). There-
fore, we now turn to describe the results measured in terms
of NDCG, that leverages the graded assessment.

The middle segment of Table 2 presents the results in
terms of NDCG@20. It is once again clear that the perfor-
mances are more or less consistent with MAP. Speciﬁcally,
MATF surpasses the Lemur TF-IDF models with consis-
tently and signiﬁcantly large margin on all six collections
and often the diﬀerences are higher or close to 10%, which
once again clearly demonstrates the eﬀectiveness of MATF.
Performance of pivoted TF-IDF is once again very similar
under the graded assessment and it achieves larger NDCG
than Lemur TF-IDF except in one occasion. MATF once
again is signiﬁcantly better than the pivoted TF-IDF on all
the collections and the diﬀerences are larger for larger web
collections.

Our ﬁnal comparison between the proposed model and
the TF-IDF models focus on precision enhancing capabili-
ties measured in terms of a metric ERR, that consider three
things simultaneously: rank of the document, quality con-
veyed by the assessor assigned grade (non-relevant, relevant
and highly relevant) and the quality of the documents that
have been seen before the document of our focus.

The last segment of Table 2 reports the ERR@20 values
achieved by the competing models on six collections. We
can easily infer that MATF once again unanimously beats
the two TF-IDF models. Only on WT10G, pivoted TF-IDF
performs slightly better than MATF. Consistent with the
previous measures, ERR@20 results demonstrate that on
larger web collections the performance diﬀerences between
MATF and the two TF-IDF models are larger.

Table 3: Comparison with TF-IDF models (statAP).
Lemur means Lemur TF-IDF. Superscripts have
their usual meaning.
Lemur
29.0
28.4

pivot MATF (% improv)
34.4lp (18.2, 15.8)
29.7
32.5lp (14.9, 17.8)
27.6

MQ-07
MQ-08

The performances of MATF and the two TF-IDF models
on two million query data, measured by statistical average
precision, are shown in Table 3. MATF transcends Lemur
TF-IDF by a margin of 18% and 15% on MQ-07 and MQ-
08 respectively, while it is better than pivoted TF-IDF with
more than 15% on both of the collections.

In summary, based on the results shown in Table 2 and Ta-
ble 3 we can infer that MATF outperforms two state of the
art TF-IDF models with remarkable signiﬁcance and con-
sistency, and the performance diﬀerences are often notice-
ably large. The performance measured by three evaluation
metrics unequivocally demonstrate that MATF is highly ef-
fective in ranked retrieval. Moreover, the results also show
that MATF is more eﬀective for larger web collections.
5.2 Comparison with Probabilistic Models

In the last section we compare the performance of our
model with two TF-IDF models. In this section we compare
the performance of MATF with three well known state of the

349Table 2: Comparison with the TF-IDF models measured in terms of MAP, NDCG@20 and ERR@20. MATF
denotes the proposed model. The best results are boldfaced. Superscripts l and p denote that the performance
diﬀerence is statistically signiﬁcant (p < 0.05) compared to Lemur TF-IDF and Pivot TF-IDF respectively.

Metric

MAP

Method
Lemur.TF-IDF
Pivot.TF-IDF
MATF

TREC-678 ROBUST WT10G GOV2 MQ-07 MQ-08
20.9
21.5
23.4lp

24.8
26.5
31.7lp
27.8
19.6

39.6
40.0
44.2lp
11.6
10.5

42.8
41.2
45.7lp
6.8
10.9

26.1
28.4
30.2lp
15.7
6.3

18.4
20.5
22.2lp
20.7
8.3

% better than Lemur.TF-IDF 12.0
% better than Pivot.TF-IDF

8.8

Lemur.TF-IDF
NDCG@20 Pivot.TF-IDF

MATF

40.0
41.5
44.6lp

% better than Lemur.TF-IDF 11.5
% better than Pivot.TF-IDF

7.5

ERR@20

Lemur.TF-IDF
Pivot.TF-IDF
MATF

40.7
41.9
43.9lp

art probabilistic retrieval models. Our evaluation strategy
is once again similar to the previous section. We compare
the performances of the models under MAP, NDCG@20 and
ERR@20.

First we compare the performance of MATF with the
BM25 model. Table 4 shows the summary of the retrieval
results on six test collections. It is clear from the table that
MATF is superior to BM25 model. This result holds for all
the collections and for all three evaluation measure. When
the performance diﬀerences between them are measured in
terms of MAP, we notice that MATF is signiﬁcantly eﬀective
for news as well as web corpora compared to BM25. In fact,
MATF is nearly 10% better than BM25 on two news data,
while on two web collections (WT10G and GOV2), MATF
achieves 17% and 12% more MAP than BM25. The diﬀer-
ences on MQ-07 and MQ-08 are similarly signiﬁcant with
substantial margins. The performance diﬀerences between
MATF and BM25 revealed by NDCG metric are consistent
with that revealed by MAP and once again, all the diﬀer-
ences are statistically signiﬁcant. ERR@20 depicts that for
all the collections, MATF remains consistently superior to
BM25, which clearly conﬁrms that MATF is very eﬀective
for precision oriented systems.

We now compare the eﬀectiveness of MATF and language
model with Dirichlet prior language model. From Table 4 we
clearly see that the performance diﬀerences between MATF
and LM are larger in three out of six cases than that we
had observed when comparing the performance of MATF
and BM25. Speciﬁcally, MATF achieves close to or more
than 10% MAP than LM on four out out six instances (ex-
cept WT10G). The performance measured on graded rel-
evance assessment also demonstrates that MATF unequiv-
ocally beats the Dirichlet prior language model based ap-
proach, and the diﬀerences are substantially large. On GOV2,
MQ-07 and MQ-08 data, MATF surpasses LM with a margin
of 14%, 8% and nearly 9% respectively. The comparison of
precision enhancing abilities of MATF and LM also clearly
indicates that MATF is always better than LM, which is
very concordant with the experimental ﬁndings captured by
MAP and NDCG.

37.5
40.2
41.5lp
10.7
3.2

45.7
46.3
48.5lp

31.6
33.4
34.6l
9.5
3.6

34.7
37.9
37.1l

43.8
46.8
51.0lp
16.4
9.0

48.3
49.4
53.4lp

46.8
48.3
51.1lp
9.2
5.8

40.6
42.9
44.9lp

50.1
48.7
52.6lp
5.0
8.0

44.5
44.6
47.3lp

We now compare the performance of the proposed model
with another probabilistic model from the divergence from
randomness family, namely, PL2. This model is relatively
recent compared to the previous two probabilistic models
and was also found to be better than BM25 in the exper-
iments reported in [3]. Table 4 reﬂects two major facts.
First, it appears from the table that PL2 is most eﬀective
among the probabilistic models and in particular only on
MQ-08 data it performs worse than BM25 as reﬂected by
both MAP and NDCG. The second major observation that
can be made from Table 4 is that MATF beats this model
also with harmonious consistency and performance diﬀer-
ences are statistically signiﬁcant on TREC-678, GOV2, MQ-
07 and MQ-08 data. Similar to the previous outcomes, on
web collections the performance diﬀerences between MATF
and PL2 are larger than that for the news collections. Lastly,
ERR metric depicts that MATF is better than PL2 across
all six collection.

Table 5: Comparison with probabilistic models
(statAP).

MQ-07
MQ-08

BM25 LM PL2 MATF (% improvement)
34.4blp (12.8, 15.8, 13.2)
30.6
32.5blp (10.5, 17.8, 18.6)
29.6

30.4
27.4

29.7
27.6

Table 5 compares the performance of four models for mil-
lion query collections measured in terms of statistical aver-
age precision. It is once again clearly evident that MATF
is consistently better than all three models and all the dif-
ferences are very large and it is very consistent with the
performance measured in terms other metrics presented in
Table 4.

Overall, the comparative analysis clearly shows that MATF
is the most eﬀective retrieval model, which unequivocally
outperforms all three probabilistic models, when the perfor-
mances are measured in terms of MAP, NDCG and a pre-
cision biased metric, namely, ERR. Also, the relative per-

350Table 4: Comparison with probabilistic models measured in terms of MAP, NDCG@20 and ERR@20. MATF
denotes the proposed model. The best results are boldfaced. Superscripts b, l and p denote that the perfor-
mance diﬀerences are statistically signiﬁcant compared to BM25, LM and PL2 respectively.

Metric

MAP

Method TREC-678 ROBUST WT10G GOV2 MQ-07 MQ-08
BM25
LM
PL2
MATF
% better than BM25
% better than LM
% better than PL2

41.2
40.1
40.9
44.2blp
7.3
10.2
8.1

28.3
29.1
29.7
31.7blp
12.1
8.9
6.7

21.3
21.3
22.7
23.4blp
9.9
9.9
3.1

27.7
28.4
29.5
30.2bl
9.0
6.9
2.4

18.9
21.3
21.3
22.2b
17.5
4.2
4.2

43.6
41.0
41.5
45.7blp
4.8
11.5
10.1

BM25

NDCG@20 LM
PL2
MATF
% better than BM25
% better than LM
% better than PL2

ERR@20

BM25
LM
PL2
MATF

41.2
40.2
42.9
44.6blp
8.3
10.9
4.0

41.1
41.2
43.0
43.9blp

39.3
39.3
41.1
41.5bl
5.6
5.6
1.0

45.6
46.5
47.0
48.5b

32.4
32.5
33.1
34.6bl
6.8
6.5
4.5

34.7
35.4
35.2
37.1b

45.2
44.6
46.1
51.0blp
12.8
14.3
10.6

48.2
47.6
47.7
53.4blp

48.1
47.2
48.0
51.1blp
6.2
8.3
6.5

41.3
39.9
40.7
44.9blp

50.9
48.3
49.0
52.6blp
3.3
8.9
7.3

44.8
42.7
42.7
47.3blp

Table 6: Performance of two TF factors on short
and long query. The values are MAP.

short

long

MQ-07 MQ-08 MQ-07 MQ-08

LRTF 43.5
RITF
45.4

43.3
45.0

39.9
37.7

44.3
41.8

formance diﬀerences are often substantially large and the
diﬀerences are even larger for the web collections that con-
tain large number of queries. Among the three probabilistic
models, PL2 and Dirichlet prior language model perform
almost equally, with PL2 having a marginal edge over LM.

5.3 Analysis

In this section we analyze the eﬀect of the two TF factors
on short and long queries. For this analysis we choose the
two million query collections, primarily because the collec-
tions have large number of queries. We divide the queries in
two sets. The queries having at least 5 terms are denoted as
short, while the rest of the queries (longer than 5 words) are
treated as long. The main goal of this section is to validate
the hypothesis made in the proposed section that relative
intra-document based TF (RITF) performs better on short
queries, while length regularized TF (RLTF) performs bet-
ter on long queries.

Table 6 presents the experimental results on two million
query data. The results seem to conﬁrm our aforesaid as-
sumption. LRTF always performs better than RITF on both
of the collections, while RITF does better for short queries.
However, the performance diﬀerences between the methods
on longer queries are noticeably better than that for shorter
queries.

6. CONCLUSION

In this paper, we present a novel TF-IDF term weighting
scheme. The proposed term weighting scheme employs two
aspects of within document term frequency normalization
to determine the importance of a term. One component of
the term frequency tends to prefer short documents, while
the other tends to prefer long documents. We then combine
these two TF components using the query length informa-
tion, that maintains a balanced trade-oﬀ in retrieving short
and long documents, when the ranking function faces queries
of varying lengths.

Experiments carried out on a set of news and web collec-
tions show that the proposed model outperforms two well
known state of the art TF-IDF baselines with signiﬁcantly
large margin, when measured in terms of MAP and NDCG.
The model also surpasses three state of the art probabilistic
models with remarkable signiﬁcance almost always. More-
over, the proposed model is also signiﬁcantly better than all
of the ﬁve baselines in improving precision.

Acknowledgments
I would like to thank Dipasree Pal, Mandar Mitra and Swa-
pan Parui for their comments, suggestions and help.

7. REFERENCES
[1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, and
E. Kanoulas. Million query track 2008 overview. In
E. M. Voorhees and L. P. Buckland, editors, The
Sixteenth Text REtrieval Conference Proceedings
(TREC 2008). National Institute of Standards and
Technology, December 2009.

[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam,

V. Pavlu, and E. Kanoulas. Million query track 2007
overview. In TREC, 2007.

351[3] G. Amati and C. J. Van Rijsbergen. Probabilistic

models of information retrieval based on measuring
the divergence from randomness. ACM Trans. Inf.
Syst., 20(4):357–389, Oct. 2002.

[4] S. B¨uttcher, C. L. A. Clarke, and I. Soboroﬀ. The trec

2006 terabyte track. In TREC, 2006.

[5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam,

and J. Allan. Evaluation over thousands of queries. In
Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ’08, pages 651–658, New
York, NY, USA, 2008. ACM.

[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.

Expected reciprocal rank for graded relevance. In
Proceedings of the 18th ACM conference on
Information and knowledge management, CIKM ’09,
pages 621–630, New York, NY, USA, 2009. ACM.

[7] C. L. A. Clarke, N. Craswell, I. Soboroﬀ, and E. M.

Voorhees. Overview of the trec 2011 web track. In
TREC, 2011.

development and comparative experiments - part 2.
Inf. Process. Manage., 36(6):809–840, 2000.

[18] K. L. Kwok. A new method of weighting query terms
for ad-hoc retrieval. In Proceedings of the 19th annual
international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’96, pages
187–195, New York, NY, USA, 1996. ACM.

[19] J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’98, pages 275–281, New York, NY, USA, 1998.
ACM.

[20] S. Robertson and H. Zaragoza. The probabilistic
relevance framework: BM25 and beyond. Found.
Trends Inf. Retr., 3(4):333–389, Apr. 2009.

[21] S. E. Robertson. Readings in information retrieval.

chapter The probability ranking principle in IR, pages
281–286. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 1997.

[8] S. Clinchant and E. Gaussier. Retrieval constraints

[22] S. E. Robertson and S. Walker. Some simple eﬀective

and word frequency distributions a log-logistic model
for ir. Inf. Retr., 14(1):5–25, Feb. 2011.

[9] R. Cummins and C. O’Riordan. A constraint to

automatically regulate document-length
normalisation. In Proceedings of the 21st ACM
international conference on Information and
knowledge management, CIKM ’12, pages 2443–2446,
New York, NY, USA, 2012. ACM.

[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of

information retrieval models. ACM Trans. Inf. Syst.,
29(2):7:1–7:42, Apr. 2011.

[11] W. R. Greiﬀ. A theory of term weighting based on

exploratory data analysis. In Proceedings of the 21st
annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’98, pages 11–19, New York, NY, USA, 1998.
ACM.

approximations to the 2-Poisson model for
probabilistic weighted retrieval. In Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’94, pages 232–241, New York, NY, USA, 1994.
Springer-Verlag New York, Inc.

[23] T. Sakai. Alternatives to bpref. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’07, pages 71–78, New York, NY, USA, 2007.
ACM.

[24] G. Salton and C. Buckley. Term-weighting approaches

in automatic text retrieval. Inf. Process. Manage.,
24(5):513–523, Aug. 1988.

[25] G. Salton and M. J. McGill. Introduction to Modern
Information Retrieval. McGraw-Hill, Inc., New York,
NY, USA, 1986.

[12] B. He and I. Ounis. A study of the dirichlet priors for

[26] G. Salton, A. Wong, and C. S. Yang. A vector space

term frequency normalisation. In Proceedings of the
28th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’05, pages 465–471, New York, NY, USA, 2005.
ACM.

[13] B. He and I. Ounis. On setting the hyper-parameters

of term frequency normalization for information
retrieval. ACM Trans. Inf. Syst., 25(3), July 2007.

[14] D. Hiemstra, S. Robertson, and H. Zaragoza.

Parsimonious language models for information
retrieval. In Proceedings of the 27th annual
international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’04, pages
178–185, New York, NY, USA, 2004. ACM.

[15] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based

evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, Oct. 2002.

[16] K. S. Jones, S. Walker, and S. E. Robertson. A

probabilistic model of information retrieval:
development and comparative experiments - part 1.
Inf. Process. Manage., 36(6):779–808, 2000.

[17] K. S. Jones, S. Walker, and S. E. Robertson. A

probabilistic model of information retrieval:

model for automatic indexing. Commun. ACM,
18(11):613–620, Nov. 1975.

[27] A. Singhal. Modern information retrieval: A brief

overview. IEEE Data Eng. Bull., 24(4):35–43, 2001.

[28] A. Singhal, C. Buckley, and M. Mitra. Pivoted

document length normalization. In Proceedings of the
19th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’96, pages 21–29, New York, NY, USA, 1996.
ACM.

[29] K. Sparck Jones. Document retrieval systems. chapter
A statistical interpretation of term speciﬁcity and its
application in retrieval, pages 132–142. Taylor
Graham Publishing, London, UK, UK, 1988.

[30] H. Turtle and W. B. Croft. Evaluation of an inference
network-based retrieval model. ACM Trans. Inf. Syst.,
9(3):187–222, July 1991.

[31] C. Zhai and J. Laﬀerty. A study of smoothing methods

for language models applied to information retrieval.
ACM Trans. Inf. Syst., 22(2):179–214, Apr. 2004.

352