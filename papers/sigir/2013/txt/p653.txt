Modeling Term Dependencies with
Quantum Language Models for IR

Alessandro Sordoni

sordonia@iro.umontreal.ca

Jian-Yun Nie

nie@iro.umontreal.ca

Yoshua Bengio

bengioy@iro.umontreal.ca

DIRO, Université de Montréal
Montréal, H3C 3J7, Québec

ABSTRACT
Traditional information retrieval (IR) models use bag-of-
words as the basic representation and assume that some form
of independence holds between terms. Representing term
dependencies and deﬁning a scoring function capable of in-
tegrating such additional evidence is theoretically and prac-
tically challenging. Recently, Quantum Theory (QT) has
been proposed as a possible, more general framework for IR.
However, only a limited number of investigations have been
made and the potential of QT has not been fully explored
and tested. We develop a new, generalized Language Model-
ing approach for IR by adopting the probabilistic framework
of QT. In particular, quantum probability could account for
both single and compound terms at once without having to
extend the term space artiﬁcially as in previous studies. This
naturally allows us to avoid the weight-normalization prob-
lem, which arises in the current practice by mixing scores
from matching compound terms and from matching single
terms. Our model is the ﬁrst practical application of quan-
tum probability to show signiﬁcant improvements over a ro-
bust bag-of-words baseline and achieves better performance
on a stronger non bag-of-words baseline.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Density Matrices; Language Modeling; Retrieval Models

1.

INTRODUCTION

The quest for the eﬀective modeling of term dependen-
cies has been of central interest in the information retrieval
(IR) community since the inception of ﬁrst retrieval models.
However, the gradual shift towards non bag-of-words mod-
els is strewn with modeling diﬃculties. One of the central

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

problems is to ﬁnd an eﬀective way of representing and scor-
ing documents based on such dependencies. As pointed out
by Gao et al. [9], dependencies can be handled in two ways.
The ﬁrst approach is to extend the dimensionality of the
representation space. In early geometrical retrieval models
such as the Vector Space Model (VSM), dependencies arising
from phrases (compound terms) are represented by deﬁning
additional dimensions in the space, i.e. both the phrase and
its component single terms are regarded as representation
features [8, 21, 28]. For example, computer architecture is
considered as disjoint from computer and architecture, which
is a strong modeling assumption, and does not take advan-
tage of the semantic relation that generally exists between
a compound phrase and its component terms.

The second approach is more principled in such that sim-
ple terms are kept as representational units and term de-
pendencies are modeled statistically as joint probabilities,
i.e. p(computer, architecture). Proposed dependence mod-
els such as n-gram Language Model (LM) for IR [30], biterm
LM [31] or the dependence LM [9] adopt such a represen-
tation. However, the gain from integrating dependencies
was smaller than hoped [35] and it came with higher com-
putational costs due to dependency parsing or n-gram mod-
els [13, 30], or unsupervised iterative methods for estimating
the joint probability [9].

Recently, non bag-of-words models such Markov random
ﬁeld (MRF) [19], quasi-synchronous dependence model [24]
and the query hypergraph model [2] have been proposed.
Most of these retrieval models take a log-linear form, which
oﬀers a very ﬂexible way of taking into account term depen-
dencies by integrating diﬀerent sources of evidence, such as
proximity heuristics and exact matching. However, the LM
is used as a black box to estimate single-term and compound-
term inﬂuences separately and then the model combines
them to compute the ﬁnal score. We believe that, from a
representational point of view, these models have implicitly
made a turn back to the ﬁrst VSM approach in the sense
that the dependencies are assumed to represent additional
concepts, i.e. atomic units for the purpose of document
and query representation, thus disjoint from the component
terms [2, 3]. This choice indeed allows for ﬂexible scoring
functions. However, the retrieval model boils down to a com-
bination of scores obtained separately from matching single
terms and from matching compound dependencies. This
is the main cause of the weight-normalization problem [9,
11] which is that a dependency may be counted twice, as
a compound and as component terms.
In the context of
phrases, Sparck Jones et al. note that “the weight of the

653phrase should reﬂect not the increased odds of relevance im-
plied by its presence as compared to its absence, as a whole
unit, but the increased odds compared to the presence of its
components words” [11]. When integrating the evidence, the
weights for the combination are usually estimated by opti-
mizing a retrieval measure such as mean average precision
(MAP). In this sense, a principled probabilistic interpreta-
tion of these models is diﬃcult.

The pioneering work by Van Rijsbergen [33] oﬃcially for-
malized the idea that Quantum Theory (QT) could be seen
as a “formal language that can be used to describe the ob-
jects and processes in information retrieval”. The idea of QT
as a framework for manipulating vector spaces and probabil-
ity is appealing. However, the methods that stem from this
initial intuition provided only limited evidence about the
usefulness and eﬀectiveness of the framework for IR tasks.
For example, Piwowarski et al. [25] test if acceptable perfor-
mance for ad-hoc tasks could be achieved with a quantum
approach to IR. The authors represent documents as sub-
spaces and queries as density operators. However, both doc-
uments and queries representations are estimated through
passage-retrieval like heuristics, i.e. a document is divided
into passages and is associated to a subspace spanned by the
vectors corresponding to document passages [25]. Diﬀerent
representations for the query density matrix are tested but
none of them led to good retrieval performance. Succes-
sively, a number of works took inspiration from quantum
phenomena in order to relax some common assumption in
IR [37, 38]. Zuccon and Azzopardi [38] introduce interfer-
ence eﬀects into the Probability Ranking Principle (PRP)
in order to rank interdependent documents. Although this
method achieves good results, it does not make principled
use of the quantum probability space and cannot be con-
sidered as evidence towards the usefulness of the enlarged
probabilistic space. In general, these methods made heuris-
tic use of the concepts of the theory and no clear probabilis-
tic interpretation can be given.

The intrinsic heuristic ﬂavor in preceding approaches mo-
tivated some authors to provide evidence to the hypothesis
that there exists an IR situation in which classical proba-
bilistic IR fails, or it is severely limited, and it is thus neces-
sary to switch to a more general probabilistic theory [16, 17,
34]. Although these works are theoretically grounded and
heavily inﬂuenced our general vision of the theory, no clue
is given on how to operationalize such results in real-world
applications.

In this paper, we propose a novel retrieval framework for
modeling term dependencies based on the probabilistic cal-
culus oﬀered by QT. In our model, both single terms and
compound dependencies are mathematically modeled as pro-
jectors in a vector space, i.e. elementary events in an en-
larged probabilistic space.
In particular, a compound de-
pendency is represented as a superposition event which is
a special kind of projector that is neither disjoint from its
component terms, nor a joint event. Documents and queries
are represented as a sequence of projectors associated to a
quantum language model (QLM), encapsulated in a partic-
ular matrix. The scoring function is a divergence between
query and document QLMs. We will show that our model
is a generalization of classical unigram LMs. To our knowl-
edge, this work can be seen as the ﬁrst work to use the quan-
tum probabilistic calculus in order to achieve improvements
over state-of-the-art models.

Our contributions are as follows:

1. We propose a novel application of quantum probability

to IR.

2. Using this approach, we show signiﬁcant improvements
over a strong baseline bag-of-words model and a strong
non bag-of-words model.

3. We propose a new way of representing dependencies
without artiﬁcially extending the term space and with-
out estimating expensive n-gram probabilities.

4. We show how the new representation of the depen-
dency permits to specify how the dependency behaves
with respect to its component terms.

5. In our model, the dependency information is not in-
tegrated in the scoring phase, but in the estimation
phase. Hence, our model does not suﬀer the weight-
normalization problem.

2. A BROADER VIEW ON PROBABILITY

2.1 The Quantum Sample Space

In quantum probability, the probabilistic space is natu-
rally encapsulated in a vector space, speciﬁcally a Hilbert
space, noted Hn, but for the sake of simplicity, in this pa-
per we limit ourselves to ﬁnite real spaces, noted Rn. We
will be using Dirac’s notation restricted to the real ﬁeld, for
which a unit vector ~u ∈ Rn, k~uk2 = 1 and its transpose ~u⊤
are respectively written as a ket |ui and a bra hu|. Using
this notation, the projector onto the direction u writes as
|uihu|. The inner product between two vectors writes as
hu|vi. Moreover, we note by |eii the elements of the stan-
dard basis in Rn, i.e. |eii = (δ1i, . . . , δni)⊤, where δij = 1 iﬀ
i = j.

Events are no more deﬁned as subsets but as subspaces,
more speciﬁcally as projectors onto subspaces [23, 34]. Given
a 1-dimensional subspace spanned by a ket |ui, the projector
onto the unit norm vector |ui, |uihu|, is an elementary event
of the quantum probability space, also called a dyad. A dyad
is always a projector onto a 1-dimensional space. Given the
bijection between subspaces and projectors, it is correct to
state that |ui is itself an elementary event. For example,
if n = 2, the quantum elementary events |e1i = (1, 0)⊤,
)⊤, can be represented by the following dyads:
|f1i = ( 1√2

, 1√2

|e1ihe1| =(cid:18)1 0

0 0(cid:19) , |f1ihf1| =(cid:18)0.5 0.5
0.5 0.5(cid:19) .

(1)

Generally, any ket |vi = Pi υi|uii is called a superposition

of the {|uii} where {|u1i, . . . , |uni} form an orthonormal
basis. In order to see the generalization that is taking place,
one has to consider that in Rn there is an inﬁnite number of
vectors even if the dimension n is ﬁnite. Hence, contrary to
the classical case, an inﬁnite number of elementary events
can be deﬁned.

2.2 Density Matrices

A quantum probability measure µ is the generalization of
a classical probability measure such that (i) for every dyad
|uihu|, µ(|uihu|) ∈ [0, 1] and (ii) it reduces to a classical prob-
ability measure for any orthonormal basis {|u1i, . . . , |uni},

i.e. Pi µ(|uiihui|) = 1. Gleason’s Theorem [10] states that,

654(a) ρ = ( 0.75
0

0
0.25 )

(b) ρ = ( 0.5 0.5
0.5 0.5 )

(c) ρ = ( 0.5 0.25
0.25 0.5 )

Figure 1: The ellipses depict the set of points {ρ|ui : |ui ∈ R2}. The eigenvalues of ρ deﬁne how much each ellipse is
stretched along the corresponding eigenvectors. To the left, ρ corresponds to a classical probability distribution. To
the center, ρ is a pure state, thus the ellipse degenerates along the eigenvector corresponding to its unit eigenvalue.
To the right, a general density matrix for which we vary both the eigenvalues and the eigensystem.

for any real vector space with dimension greater than 2,
there is a one-to-one correspondence between quantum prob-
ability measures µ and density matrices ρ. The form of this
correspondence is given by:

µρ(|vihv|) = tr(ρ|vihv|).

(2)

A real density matrix is symmetric, ρ = ρ⊤, positive semidef-
inite, ρ ≥ 0, and of trace 1, tr ρ = 11. From now on, the set
of n × n real density matrices would be noted S n.

By Gleason’s theorem, a density matrix can be seen as
the proper quantum generalization of a classical probability
distribution. It assigns a quantum probability to each one
of the inﬁnite dyads. For example, the density matrix:

ρ =(cid:18)0.5 0.5
0.5 0.5(cid:19) ,

(3)

assigns probabilities tr(ρ|e1ihe1|) = 0.5 and tr(ρ|f1ihf1|) =
1. Hence, the event |f1ihf1| is certain and still there is
non-classical uncertainty on |e1ihe1|. Only if {|u1i, . . . , |uni}
form an orthonormal system of Rn can the dyads |uiihui| be
understood as disjoints events of a classical sample space, i.e.
their probabilities sum to one. The relation that ties |e1ihe1|
and |f1ihf1| is purely geometrical and cannot be expressed
using set theoretic operations.

Any classical discrete probability distribution can be seen
as a mixture over n elementary points, i.e. a parameter ~θ =

trix is the straightforward generalization of this idea by con-

(θ1, . . . , θn), where θi ≥ 0 and Pi θi = 1. The density ma-
sidering a mixture over orthogonal dyads ρ = Pi υi|uiihui|
where υi ≥ 0 and Pi υi = 1. Given a density matrix ρ,
note such decomposition by ρ = RΛR⊤ = Pn

one can ﬁnd the components dyads by taking its eigende-
composition and building a dyad for each eigenvector. We
i=1 λi|riihri|,
where |rii are the eigenvectors and λi their corresponding
eigenvalues. This decomposition always exists for density
matrices [23].

Conventional probability distributions can be represented
by diagonal density matrices. The sample space corresponds
to the standard basis E = {|eiihei|}n
i=1. Hence, the density
matrix corresponding to the parameter ~θ above can be repre-

sented as a mixture over E, i.e. ρθ = diag(~θ) =Pi θi|eiihei|.

1The trace is equal to the sum of the diagonal terms in a matrix.

Consider a vocabulary of two terms V = {a, b}. A unigram
language model ~θ = (0.75, 0.25) deﬁned on V is represented
by:

|ebiheb| =(cid:18)0.75

0

ρθ =

3
4

|eaihea| +

1
4

0

0.25(cid:19) .

Hence, term projectors are orthogonal, i.e. terms correspond
to disjoint events. For example, the probability of the term
a is computed by tr(ρθ|eaihea|) = 0.75. As conventional
probability distributions are restricted to the identity eigen-
system, they diﬀer in their eigenvalues, which correspond to
diagonal entries. On the contrary, general density matrices
can diﬀer also in the eigensystem. For example, the density
)⊤ with
matrix ρ of Eq. 3 has eigenvector |f1i = ( 1√2
)⊤ with
eigenvalue 1 and the eigenvector |f2i = ( 1√2
eigenvalue 0. Hence, it can be represented as a one-element
mixture containing the projector ρ = |f1ihf1|. When the
mixture weights are concentrated into a single projector, the
corresponding density matrix is called pure state. Otherwise,
it is called mixed state.

, 1√2
, − 1√2

When deﬁned over Rn, density matrices can be seen as el-
lipsoids, i.e. deformations of the unit sphere (Figure 1) [34].
Classical probability distributions, i.e. diagonal density ma-
trices, are ellipsoids stretched along the identity eigensys-
tem. As quantum probability has access to an inﬁnite num-
ber of eigensystems, the ellipsoid can be “rotated”, i.e. de-
ﬁned on a diﬀerent eigensystem. In this work, we will use
this additional feature in order to build a more reliable rep-
resentation of documents and queries taking into account
more complex information than single terms.

3. QUANTUM LANGUAGE MODELS

The approach Quantum Language Modeling (QLM) re-
tains the classical Language Modeling for IR as a special
case. Hereafter, we will present in details the quantum
counterpart of unigram language models. Although it is not
explicitly developed in this paper, we argue that arbitrary
n-gram models could be modeled as well.

3.1 Representation

In classical bag-of-words language models, a document
term events, i.e.

d is represented by a sequence of i.i.d.

655Wd = {wi : i = 1, . . . , N }, where N is the document length.
Each wi belongs to a sample space V, corresponding to the
vocabulary, of size n. It is assumed that such sequences cor-
respond to a sample from an unknown distribution ~θ over
the vocabulary V, for which we want to gain insight.

A quantum language model assigns quantum probabilities
to arbitrary subsets of the vocabulary. It is parametrized by
an n × n density matrix ρ, ρ ∈ S n, where n is the size of
the vocabulary V. In QLM, a document d is considered as
a sequence of M quantum events associated with a density
matrix ρ:

Pd = {Πi : i = 1, . . . , M },

(4)

where each Πi is a general dyad |uihu| and represents a sub-
set of the vocabulary. Note that the number of dyads M can
be diﬀerent from N , the total number of terms in the doc-
ument. The sequence Pd is constructed from the observed
terms Wd: we have to deﬁne how to map subsets of terms to
projectors. Separating the observed text from the observed
projectors constitutes the main ﬂexibility of our model. In
what follows, we deﬁne a way of mapping single terms and
arbitrary dependencies to quantum elementary events. For-
mally, we seek to deﬁne a mapping m : P(V) → L(Rn),
where P(V) is the powerset of the vocabulary and L(Rn) is
the set of dyads on Rn. As an initial assumption, we set
m(∅) = O, where O is the projector onto the zero vector.

3.1.1 Representing Single Terms

In Section 2.2, we showed that unigram sample spaces
can be represented as the set of projectors on the standard
basis E = {|eiihei|}n
i=1 and unigram language models can
be represented as mixtures over E, i.e. diagonal matrices.
Therefore, a straightforward mapping from single terms to
quantum events is:

m({w}) = |ewihew|,

(5)

where w ∈ V. This choice associates the occurrence of
each term to a dyad |ewihew|, and these dyads form an or-
thonormal basis. Hence, occurrences of single terms are still
represented as disjoint events. Consider n = 3 and V =
{computer, architecture, games}. If Wd = {computer, archi-
tecture} and one applies m to each of the terms, the sequence
of corresponding projectors is Pd = {Ecomputer, Earchitecture}
where Ew = |ewihew|:

Ecomputer =


1 0 0
0 0 0
0 0 0

 , Earchitecture =



0 0 0
0 1 0
0 0 0


 . (6)

Note that if we decide to observe only single terms, Pd turns
out to be the quantum counterpart of classical observed
terms Wd, i.e. M = N .

3.1.2 Representing Dependencies

In this paper, by dependency, we mean a relationship
linking two or more terms and we represent such an en-
tity abstractly by a subset of the vocabulary,
i.e. κ =
{w1, . . . , wK}. We deﬁne the following mapping for an arbi-
trary dependency κ:

Figure 2: The dependency κca is modeled as a pro-
jector onto |κcai, i.e. as a superposition event.

|κi. The well-deﬁned dyad |κihκ| is a superposition event.
As we showed in Section 2.2, superposition events are justiﬁ-
able only in the quantum probabilistic space. They are nei-
ther disjoint from their constituents |ewi ihewi | nor do they
solely constitute joint events in the sense of n-grams: here,
the compound dependency is not considered as an additional
entity, as done in previous models [2, 3, 19, 21]. The pro-
posed mapping allows for the representation of relationships
within a group of terms by creating a new quantum event
in the same n-dimensional space.

In addition, superposition events come with a ﬂexible way
in quantifying how much evidence the observation of depen-
dency κ brings to its component terms. This is achieved by
changing the distribution of the σi: if one wants to attempt
a classical interpretation, the σi can be viewed as relative
pseudo-counts, i.e. observing |κihκ| adds fractional occur-
rence to the events of its component terms |ewi ihewi |. To
our knowledge, until now this feature has been only mod-
eled heuristically, or not modeled at all. In our framework,
it ﬁts nicely in the quantum probabilistic space by specify-
ing how a compound dependency event and its constituent
single terms events are related.

As an example, one could model the compound depen-
dency between computer and architecture, κca = {computer,
architecture}, by the dyad Kca = |κcaihκca|, where |κcai =

p2/3|eci +p1/3|eai (Figure 2). With respect to the exam-

ple taken above, the event is represented by the matrix:

Kca =


2
3
√2
3
0

√2
3
1
3
0


 .

0
0
0

(8)

The superposition coeﬃcients entail that observing Kca adds
more evidence to |ecihec| than to |eaihea|.

3.1.3 Choosing When and What to Observe

Once we have deﬁned the mapping m, one must ask three

questions:

1. Which compound dependencies to consider?

2. When does such a compound dependency hold in a

document?

m(κ) = m({w1, . . . , wK}) = |κihκ|, |κi =

KXi=1

σi|ewi i, (7)

3. When the compound dependency is detected, should
we also consider the projectors for its subsets as ob-
served events?

where the coeﬃcients σi ∈ R must be chosen such that
i = 1, in order to ensure the proper normalization of

Pi σ2

Regarding the ﬁrst question, one may (a) use a dictio-
nary of phrases or frequent n-grams, or (b) assume that any

656w1

w2

w3

w4

w5

w6

w7

w8

Sakamura

says

he

created

Tron

a

computer

architecture

Π1

Π2

Π3

Π4

Π5

ESakamura

Esays

Ehe

Ecreated

ETron

Π6

Ea

Π7

Π8

Π9

Π1

Π2

Π3

Π4

Π5

Ecomputer

Earchitecture

Kca

ESakamura

Esays

Ehe

Ecreated

ETron

Π6

Ea

Π7

Kca

Figure 3: Two possible quantum sequences P i
d of an excerpt Wd from a TREC collection. The observation of
computer architecture is associated to a superposition projector Kca = |κcaihκca| while Ew = |ewihew| are classical
projectors. For P 2

d we observed only the compound while in P 1

d we also added its subsets.

subset of terms that appear in short queries are candidate
compound dependencies to capture. In this paper, we want
to make the approach as independent as possible of any lin-
guistic resource. So the second approach (b) is used. This
will also allow us to make a fair comparison with the previ-
ous approaches using the same strategy (such as the MRF
model [19]).

The second question regards whether such selected com-
pound dependencies hold in a given document.
In other
words, one has to decide when to add the selected depen-
dency projector into a document sequence Pd. This can be
done for example by assuming that the components terms
in the dependency appear as a bigram in a document, as
biterm or in a unordered window of L terms. Convergent
evidence from diﬀerent works [1, 12, 14, 18, 31, 36] conﬁrms
that proximity is a strong indicator of dependence. There-
fore, in this work we choose to detect a dependency if its
component terms appear in a ﬁxed-window of length L.

The third question regards how to apply the mapping m
and can be more easily understood by a practical example.
Consider a document Wd = {computer, architecture} and
a query Wq = {computer, architecture}. Once the depen-
dency κca = {computer, architecture} has been detected
in the document, i.e. the component terms appear next to
each other, one can further decide:

1. to map only the dependency, i.e. Pd = {Kca},

2. to map both the dependency and the component terms,

i.e. Pd = {Ecomputer, Earchitecture, Kca}.

These two choices are illustrated in Figure 3. The ﬁrst choice
is a highly non-classical one because it completely steals the
occurrence of its component terms. Nevertheless, it becomes
a valid choice in our framework. Diﬀerently from classical
approaches, the fact that we only consider a count for the
compound computer architecture does not mean that we as-
sume that the terms computer and architecture do not occur.
The dependency event is not disjoint from the single term
events, and its occurrence partially entails the occurrence
of its component terms. However, this choice is more dan-
gerous because it over-penalizes the component terms: we
should know very precisely when such a strong dependency
is observed and which coeﬃcients to assign to it.

The second choice is implicitly done in current dependency
models and is at the basis of the weight-normalization prob-
lem. From this point of view, the sequence Pd could be seen
as composed by concepts as recently formalized by Bender-
sky et al. [2, 3]. However, there are crucial diﬀerences from

that work: (1) we give a clear probabilistic status to such
concepts and (2) we do not assume that concepts are atomic
units of information, completely unrelated from each other.
In classical dependence models, single terms and compound
dependencies are scored separately and then the scores are
combined together [2, 19, 35]. A critical aspect of such mod-
els is that the occurrence of the phrase computer architec-
ture will be counted twice - as single terms and as a com-
pound. That is why the score on compound dependencies
must be reweighed before integrating it with the indepen-
dence score [9, 11, 19]. Contrary to classical models, our
model does not suﬀer from such a problem because the ev-
idences brought by the compound dependency as a whole
and by its component terms are integrated in the estima-
tion phase. Even if not reported explicitly in the experi-
ments section, conducted experiments show that including
projectors for both the dependency and its subsets is much
more eﬀective for the ad-hoc task evaluated here and thus
this strategy will be preferred throughout this paper.
In
addition, an algorithm building the sequence of projectors
from the document sequence will be presented in Section
4.3.1.

3.2 Estimation

3.2.1 Maximum Likelihood Estimation

Given that a document is represented by a set of observed
projectors, one has to ﬁnd ways to learn a quantum lan-
guage model ρ to associate with a document.
In QT, a
number of objective functions have been proposed to esti-
mate an unknown density matrix from a set of projectors:
Linear Inversion [23] and Hedged ML [4] are notorious exam-
ples. In this work, we use the Maximum Likelihood (ML)
formulation proposed in [15], because (1) it can easily be
seen as a quantum generalization of a classical likelihood
function (2) contrary to linear inversion, ML generates a
well-deﬁned density matrix, i.e. ρ ∈ S n, and (3) proposed
estimation methods remain computationally aﬀordable in
high-dimensional spaces.

Given the observed projectors Pd = {Π1, . . . , ΠM } for
document d, we deﬁne as training criterion for the quan-
tum language model ρ the maximization of the following
product proposed in [15] and corresponding in the unigram
case to a proper likelihood:

LPd (ρ) =

MYi=1

tr(ρΠi).

(9)

657The estimate bρ can be obtained by approximately solving

the following maximization problem:

maximize

ρ

subject to

log LPd (ρ)
ρ ∈ S n.

(10)

This maximization is diﬃcult and must be approximated
by using iterative methods. In [15], the following iterative
scheme is proposed, also called the “RρR algorithm”. One
introduces the operator:

R(ρ) =

MXi=1

1

tr(ρΠi)

Πi,

(11)

and updates an initial density matrix bρ(0) by applying repet-

itive iterations:

1
Z

bρ(k+1) =

R(bρ(k))bρ(k)R(bρ(k)),

(12)

where, Z = tr(R(bρ(k))bρ(k)R(bρ(k))) is a normalization factor
in order to ensure that bρ(k+1) respects the constraint of uni-

tary trace [15]. Despite the RρR algorithm being a quantum
generalization of the well-behaving Expectation Maximiza-
tion (EM) algorithm, the likelihood is not guaranteed to
increase at each step because the nonlinear iteration may
overshoot, similarly to a gradient descent algorithm with a
too big step size. Characterizing such situations still remains
an open problem [27]. In this work, in order to ensure con-
vergence, if the likelihood is decreased at k + 1, we use the
following damped update:

(13)

eρ(k+1) = (1 − γ)bρ(k) + γbρ(k+1),

where γ ∈ [0, 1) controls the amount of damping and is op-
timized by linear search in order to ensure the maximum in-

crease of the training objective2. As S n is convex [23], eρ(k+1)

is a proper candidate density matrix. The process stops if
the change in the likelihood is below a certain threshold or
if a maximum number of iterations is attained.

From an IR point of view, the metric divergence prob-
lem [22] tells us that the maximization of the likelihood does
not mean that the evaluation metric under consideration,
such as mean average precision, is also maximized. In the
experiments section, we address the two following questions
from a perspective closer to IR concerns:

1. Which initial matrix bρ(0) to choose?

2. When to stop the update process?

As the estimation of a quantum document model requires an
iterative process, one may believe that the complexity will
make the process intractable.
In Section 4.5, we provide
an analysis of the complexity of the proposed computation,
which will show that the process is quite tractable.

3.2.2 Smoothing Density Matrices

The ML estimation presented above suﬀers from a gen-
eralization of the usual zero-probability problem of classi-
cal ML, i.e. the estimator assigns zero probability to un-
seen data [35]. This is also called the zero eigenvalue prob-
lem [4]. Bayesian smoothing for density matrices has not
yet been proposed. This may be because Bayesian inference

in the quantum setting has just started to be the subject
of intensive research [5, 34].
In this work, we propose to

a document quantum language model obtained by ML, its
smoothed version is obtained by interpolation with the ML

smooth density matrices by linear interpolation [35]. If bρd is
collection quantum language model bρc:
ρd = (1 − αd)bρd + αdbρc,

where αd ∈ [0, 1] controls the amount of smoothing. As
the set of density matrices S n is convex, the resulting ρd
is a proper density matrix.
In this work, we assume that
αd = µ
(µ+M ) , which is the well-known form of the parameter
for Dirichlet smoothing [35].

(14)

3.3 Scoring

The ﬂexibility of the Kullback Liebler (KL) divergence
approach in keeping distinct query and document represen-
tations makes it attractive for a candidate scoring function
in our new framework. The direct generalization of classi-
cal KL divergence was introduced by Umegaki in [32] and is
called quantum relative entropy or Von-Neumann (VN) di-
vergence. Given two quantum language models ρq and ρd for
the query and a document respectively, our scoring function
is the negative query-to-document VN divergence:

−∆V N (ρqkρd) = − tr(ρq(log ρq − log ρd))

rank=

tr(ρq log ρd),

(15)

where log applied to a matrix denotes the matrix logarithm,
i.e. the classical logarithm applied to the matrix eigenvalues.
Rank equivalence is obtained by noting that tr(ρq log ρq)
does not depend on the particular document. Denote by

ρq = Pi λqi |qiihqi|, ρd = Pi λdi |diihdi| the eigendecompo-

sitions of the density matrices ρq and ρd respectively. By
substituting into the above equation, the scoring function
rewrites as:

−∆V N (ρq||ρd) rank= Xi

λqiXj

log λdjhqi|dji2.

(16)

Compared to a classical KL divergence, the additional
term hqi|dji2 quantiﬁes the diﬀerence in the eigenvectors
between the two models. Following the representation in-
troduced in Section 2.2, the VN divergence compares two
ellipsoids not only by diﬀerences in the “shape” but also by
diﬀerences in the “rotation”.

If a VSM-like interpretation is attempted, one can think
about {|qii}, {|dji} as semantic concepts for the query and
the document respectively, whereas the vectors of eigenval-
ues ~λq, ~λd denote the importance of the corresponding se-
mantic concepts in the two models. The VN divergence
oﬀers a way of matching query concepts by analyzing how
much such concepts are related to documents concepts, i.e.

∀i, j, hqi|dji2. Particularly, Pjhqi|dji2 = 1. Thus, hqi|dji2

can be interpreted as the quantum probability associated
with the pure state |qiihqi| for the elementary event |djihdj|,
i.e. µqi (|djihdj|) = tr(|qiihqi|djihdj|) = hqi|dji2. Hence, one
could rewrite Eq. 16 as:

−∆V N (ρq||ρd) rank= Xi

λqi Eµqi hlog ~λdi .

(17)

2Similar damped updates were successfully used in [26] to im-
prove convergence and stability of the loopy belief propagation
algorithm.

Therefore, the VN divergence scores a document based on
the expectation of how important concept |qii is in document
d even if it does not appear in it explicitly.

658collections in order to vary (1) the collection size and (2)
collection type. This will produce a comprehensive test set
in order to verify the properties of our approach. All the

Name
Content # Docs
SJMN
Newswire 90,257
TREC7-8
Newswire 528,155
WT10g
Web
ClueWeb-B Web

1,692,096
50,220,423

Topic Numbers
51-150
351-450
451-550
51-200

Table 1: Summary of the TREC collections used to
support the experimental evaluation.

collections have been stemmed with the Krovetz stemmer.
Both documents and queries have been stopped using the
standard INQUERY stopword list. For all the methods, the
Dirichlet smoothing parameter µ is set to the default Indri
value (µ = 2500). The optimization of all the other free pa-
rameters for the proposed model and the baselines is done
using ﬁve-fold cross validation using coordinate ascent [18]
with mean average precision (MAP) as the target metric.
The performance is measured on the top 1000 ranked doc-
uments.
In addition to MAP, for newswire collections we
report the early precision metric @10 (precision at 10) and
for web collections with graded relevance judgements we re-
port the recent ERR@10, which correlates better with click
metrics than other editorial metrics [6]. The statistical sig-
niﬁcance of diﬀerences in the performance of tested meth-
ods is determined using a two-sided Fisher’s randomization
test [29] with 25,000 permutations evaluated at α < 0.05.

4.2 Methodology

Our experimental methodology goes as follows. In a ﬁrst
step, we compare our QLM approach to a unigram Language
Modeling baseline (denoted LM) based on Dirichlet smooth-
ing [35], which is a strong bag-of-words baseline. This com-
parison is done by assigning uniform superposition weights

to each dependency κ, i.e. σi = 1/p|κ|, where |κ| is the

cardinality of κ (denoted QLM-UNI). This step has two main
objectives: (1) to test if quantum probability can bring bet-
ter performance than a standard bag-of-words model and
(2) to test if uniform superposition weights are a reasonable
baseline setting.

As a second step, we test the proposed model against the
strong non bag-of-words MRF model, which has shown to be
highly eﬀective especially for large scale web collections [19,
20]. We test the full dependence version of the model (de-
noted MRF-FD) which captures dependencies between all the
query terms and thus is the most natural choice for a com-
parison with our model. However, MRF-FD exploits both
proximity (#uw) and exact matching (#1). As our model
only exploits proximity as an indicator of dependence, we
also propose to test the variant MRF-FD-U, which is a MRF
using only the proximity feature. This could provide inter-
esting insights on how the models score based upon the same
evidence.

Finally, we propose a slightly more elaborate version of
our model (denoted QLM-IDF) in which the superposition
weights are no more assumed to be uniform.
Instead, we
assign to each σi the normalized idf weight of the corre-
sponding term wi. The objective is to test if a more reason-
able parametrization of superposition weights can improve
the retrieval eﬀectiveness.

(a) −∆V N (ρqkρd1

) ∝ −.76

(b) −∆V N (ρqkρd2

) ∝ −1.06

o
q
d1
d2

Wo

Po

{computer, architecture}

{Ec, Ea, Kca}

{computer, architecture, and, games} {Ec, Ea, Kca, Eg}
{computer, games, and, architecture}

{Ec, Eg, Ea}

Figure 4: A synthetic example of QLM with a vocab-
ulary of n = 3 terms. The orthogonal rays are the
eigenvectors of the ellipsoids. ρq is not smoothed
thus degenerates onto a ray. ρd1 rotates towards the
direction of observed query dependencies and is thus
ranked higher.

3.4 Final Considerations

The estimation and scoring process of quantum language
models retains classical unigram LMs and KL divergence
as special cases. The classical unigram LM is recovered by
restricting the maximization in Eq. 10 to diagonal density
matrices and including into the sequence of projectors Pd
only an orthonormal basis, such as the elements of E. Clas-
sical KL divergence is recovered by noting that if ρq and ρd
are diagonal density matrices, they share the same eigensys-
tem. Hence, |qii = |dii and λqi = θqi, λdi = θdi, where ~θq,
~θd are the parameters of classical unigram LMs for the query
and the document respectively. In this setting, hqi|dji2 = 0
for i 6= j and the VN divergence reduces to classical KL, i.e.

−∆V N (ρqkρd) = −∆KL(~θqk~θd) rank= Pi θqi log θdi.

In Figure 4, we report a synthetic example of the appli-
cation of the model. We plot the density matrices obtained
by the MLE (Section 3.2.1) on the sequence of projectors
reported in the table. As usual in ad-hoc tasks, we smooth
only the QLMs of the documents. The model corresponding
to the query is a projector, i.e. it has two zero eigenvalues,
because we did not apply smoothing. If the dependencies
are included in the sequence Po, the MLE rotates the cor-
responding QLM towards the direction spanned by the ob-
served projector (i.e. Kca). This entails that the model ρd1
is considered more similar to the query than the model ρd2
which corresponds to a classical language model.

4. EVALUATION

4.1 Experimental Setup

All the experiments reported in this work were conducted
using the open source Indri search engine (version 5.3)3. The
test collections used are reported in Table 1. We choose the

3http://www.lemurproject.org

659Figure 5: Plots of MAP (QLM-UNI and LM) and MLE objective against the number of updates of the density
matrix for SJMN, TREC7-8 and WT10g (left, center and right).

All the results exposed in this paper have been obtained by
reranking. We rerank a pool of 20000 documents retrieved
using LM in order to make a fair comparison between our
method and the baselines.

4.3 Setting up QLM

4.3.1 Building the Sequence of Projectors

Very similarly to MRF-FD, given a query Q = {q1, . . . , qn},
we assume that the interesting dependencies to consider cor-
respond to the power set P(Q)4. In order to build the set of
projectors for the given document we apply Algorithm 1.

Algorithm 1 Builds the sequence Pd given Wd, Q
Require: Wd, Q

1: Pd ← ∅
2: for κ ∈ P(Q) do

3:

4:

5:

for #(κ, Wd) do

Pd ← Pd ⊕ m(κ) %Adds the projector to the sequence

end for

6: end for
7: return Pd

For each dependency κ in P(Q), the algorithm scans the
document sequence Wd. For each occurrence of κ, it adds a
projector m(κ) to the sequence Pd. The function #(κ, Wd)
returns how many times the dependency κ is observed in Wd.
Therefore, the algorithm adds as many projectors as the
number of detected compound dependencies. Note that by
looping on P(Q), we are actually implementing the strategy
exposed in Section 3.1.3, i.e. adding both the dependence
and all of its subsets. Following Section 3.1.3, we choose to
parametrize # as the unordered window operator in Indri
(#uwL). Therefore, a given dependency κ will be detected if
the component terms appear in any order in a ﬁxed-window
of length L = l|κ|. This kind of adaptive parametrization of
the window length is state-of-the-art for dependence models
such as MRF-FD [2, 19]. For all the dependence models, the
coordinate ascent for l spans {1, 2, 4, 8, 16, 32}, which is a
robust pool covering diﬀerent window lengths, including the
standard value (l = 4) for MRF-FD.

4In order to keep the retrieval complexity reasonable both for
MRF and QLM, we limit ourselves to query term subsets with at
most three terms.

4.3.2 MLE Convergence Analysis

Before doing any comparisons, we answer the questions
related to the construction of a quantum language model,

process? In order to help the maximum likelihood process to

i.e. (1) how to initialize bρ(0)? (2) when to stop the update
converge faster, we initialize the matrix bρ(0) to the density
sideration. This is a diagonal matrix bρ(0) = diag(~θM L). We

matrix corresponding to the classical maximum likelihood
language model ~θM L of the document or query under con-

also tested with the uniform density matrix, as suggested
in [15], but we found that the MAP was severely harmed.

In order to address the second question, we analyze the
variation of MAP with respect to the maximum number of
iterations nit ∈ [1, 50]. The damping factor γ is optimized
over the set of values Γ = {0, 0.1, ..., 0.9}. The iterative
process stops before nit if the change in the likelihood is
below 10−4.
In order to check for possible variations due
to the collection type, we plot the iteration-MAP curve for
two similar collections, i.e. SJMN and TREC7-8, and a web
collection, WT10g. We also plot the training objective in
Eq. 10 over the set of topics:
R is the multiset of retrieved documents. The trend is shown
in Figure 5. Generally, at any number of iterations, the
MAP stays signiﬁcantly above the baseline. It seems that
there is a good correlation between likelihood maximization
and MAP, although one can note some overﬁtting at high
number of iterations. Capping by 10 ≤ nit ≤ 20 seems a
good trade-oﬀ between likelihood maximization and MAP.
However, to provide a fair comparison with the baselines,
we choose to include nit as a free parameter to train by
coordinate ascent.

log LPd (bρd), where

|R|Pd∈R

1

4.4 Results

The results discussed in this section are compactly re-

ported in Table 2.

4.4.1 Language Modeling Baseline

From the comparisons with the LM baseline, one can see
that QLM-UNI outperforms LM signiﬁcantly, with relative im-
provements in MAP going up to 12.1% in the case of WT10g
collection and 19.2% for the ClueWeb-B collection. This
seems to be in line with the hypothesis formulated in [19],
for which dependence models may yield larger improvements
for large collections.

The weight-normalization problem seems to be addressed
automatically: our model does not need for any combina-

660SJMN

TREC7-8

WT10g

ClueWeb-B

P@10
.3064
.3138
.3074

.3181

MAP
.1995
.2071
.2061

.2077

P@10
.4230
.4350
.4460

.4480

MAP
.2120
.2228
.2243

.2240

ERR@10

.1068
.1136
.1147

.1162

MAP
.1975
.2097
.2146
.2215αβ

ERR@10

.0718
.0828
.0881

MAP
.1003
.1103
.1137

.1015αβ

.1196αβ

LM

MRF-FD-U

MRF-FD

QLM-UNI

(+1.4/+3.5)

(+0.3/+0.8)

(+3.0/+0.4)

(+0.5/-0.1)

(+2.2/+1.3)

(+5.6/+3.2)

(+22.6/+15.2)

(+8.4/+5.2)

QLM-IDF

.3170

.2093

.4450

.2254

.1176

.2264αβ

.0997αβ

.1189αβ

(+1.0/+3.1)

(+1.1/+1.6)

(+2.3/-0.2)

(+1.2/+0.5)

(+3.5/+2.6)

(+7.9/+5.5)

(+20.4/+13.1)

(+7.8/+4.5)

Table 2: Evaluation of the performance for the ﬁve methods tested. Best results are highlighted in boldface.
Numbers in parentheses indicate relative improvement (%) in MAP over MRF-FD-U/MRF-FD. All the results
for dependence models are signiﬁcant with respect to the baseline LM. The symbols α,β means statistical
signiﬁcance over MRF-FD-U, MRF-FD respectively.

tion weights. Moreover, it is robust across the folds. From
an analysis of the optimal values of the parameters obtained
across the diﬀerent folds, we found that optimal window
sizes were l ∈ {1, 2}. This can be explained by considering
that in the current version of QLM, it is possible to decide
if the dependency is detected or not, but the model cannot
discriminate its “importance”. If one decides to increase l,
more inaccurate dependencies will be detected and the per-
formance will be deteriorated. However, even with a larger
window size, statistical signiﬁcance over LM is maintained.
From these considerations, we suggest l = 2 as a default
setting for our model. Finally, the results endorse that our
QLM does not need an engineered estimation of superposi-
tion weights to perform well.

4.4.2 Markov Random Fields Baseline

As a second test, we report the results obtained for the
MRF-FD and MRF-FD-U baselines. These have proved to be
very robust non bag-of-words baselines [2, 19, 20]. Contrary
to our model, MRF does not handle dependency informa-
tion in the estimation phase. One has to specify the coeﬃ-
cients (λT , λO, λU ) for the combination of dependence and
independence scores. To limit per-fold overﬁtting, for the
dependence models, we ﬁrst train combination parameters
(λf ∈ {0, 0.01, ..., 1}) then l for each fold. For MRF-FD-U, we
set λO = 0.

Results show that for SJMN and TREC7-8, QLM-UNI, MRF-
FD and MRF-FD-U are essentially equivalent. However, for
the two Web collections, our model signiﬁcantly outperforms
both MRF variants. On ClueWeb-B, statistical signiﬁcance
is attained for the two reported measures. As conjectured
in [19], noisy web collections could be a more discriminative
testbed for dependence models. Optimal l values for MRF-
FD were very small for SJMN (l ∈ {1, 2}) in contrast to the
optimal setting for ClueWeb-B (l ∈ {16, 32}). In [19], the
authors suggest that for homogenous newswire collections
a small window is enough to capture useful dependencies,
while for large, noisy web collections, a larger span must be
set. However, the performances obtained by our model seem
to suggest that it can greatly beneﬁt from term dependen-
cies, on a variety of collections, even when a small window
size is used. This elucidates the fact that even short range
information can be extremely useful if integrated in the es-
timation phase. In order to get a more comprehensive view
on such issues, we trained on the entire set of ClueWeb-B
topics three versions of MRF-FD-U, each obtained by clamp-

ing a diﬀerent value of l ∈ {1, 2, 4}. The best performing
model obtained a MAP of 10.91. It seems that our model
can exploit this short range information in a better way than
MRF models.

4.4.3

Setting Superposition Weights

/Pi idfwi

set σi =qidfwi

Our last test aimed at verifying if a more reasonable set-
ting of the superposition weights could further improve re-
trieval performance. For a dependency {w1, . . . , wK }, we
. This has the eﬀect of attributing
a larger count to the more “important” term in the depen-
dency. QLM-IDF generally increases MAP. However, this is
not the case for ClueWeb-B. From a query-by-query analysis,
we noticed that QLM-IDF increases the performance for noisy
queries by promoting the most “important” terms in unnec-
essary subsets. For multiword expressions such as ClueWeb-
B topics continental plates and rock art, weighting by idf
may be misleading by assigning more weight to one of the
terms. In this cases, a uniform parametrization is far more
eﬀective. This demonstrates that there is still room for im-
provement by a clever tuning of superposition parameters,
for example by leveraging feature functions [2, 3].

4.5 Complexity Analysis

Complexity issues can be tackled by noting that it is not
necessary to manipulate n × n matrices. We associate a
dimension for each query term and an additional dimen-
sion for a “don’t care” term that will store the probability
mass for the other terms in the vocabulary. Therefore, a
multinomial over n points is reduced to a multinomial over
|Q| + 1 points, where |Q| is the number of unique terms
in the query and the additional dimension is simply a re-
labeling of the other term events.
In this way, the QLM
to manipulate is k × k, where k = |Q| + 1. The eigende-
composition generally requires O(k3). The iterative process
requires at most |P(Q)| = 2|Q| matrix multiplications for
the expectation step, where 2|Q| is the maximum number of
unique projectors in Pd and 2 matrix multiplications for the
maximization step. In the case the likelihood is decreased,
|Γ| more iterations are done giving a worst-case complexity
of O(nit|Γ|2k + k3), i.e.
if each iteration needs damping.
We showed that 10 ≤ nit ≤ 20 is enough; we use |Γ| = 10
and k is very small for title queries, which make the process
computationally tractable. In practice, we observed that the
damping process is very eﬀective and dramatically improves
convergence speed. As an example, the mean number of iter-

661ations for ClueWeb-B when nit = 15 is 7.02 which is orders
of magnitude less than nit|Γ| = 150. Finally, we conjecture
that such process could be executed at indexing time, thus
eliminating any additional on-line costs.

5. CONCLUSION

In this paper, we presented a principled application of
quantum probability for IR. We showed how the ﬂexibility
of vector spaces joined with the powerful tools of probabilis-
tic calculus can be mixed together for a ﬂexible, yet prin-
cipled account of term dependencies for IR. In our model,
dependencies are neither represented as additional dimen-
sions, nor stochastically as joint probabilities. They assume
a new status as superposition events. The relationship of
such an event to the traditional term events are encoded by
the oﬀ-diagonal values in the corresponding projection ma-
trix. Both documents and queries are associated to density
matrices estimated through the maximization of a product,
which in the classical case reduces to a likelihood. As our
model integrates the dependencies in the estimation phase,
it has no need for combination parameters. Experiments
showed that it performs equivalently to the existing depen-
dence models on newswire test collections and outperforms
the latter on web data.

To our knowledge, this work provides the ﬁrst experimen-
tal result showing the usefulness of this kind of probabilistic
calculus for IR. The marriage between vector spaces and
probability can be endlessly improved in the future. One
straightforward direction is to relax the assumption that sin-
gle terms represent orthogonal projectors. This could lead
to a new way of integrating latent directions as estimated by
purely geometric methods such as Latent Semantic Indexing
(LSI) [7] into a probabilistic model. In this work, we did not
exploit the full machinery of complex vector spaces. We do
not have a practical justiﬁcation for the use of the complex
ﬁeld for IR tasks. However, we speculate that this could
bring improved representational power and thus remains an
interesting direction to explore. At last, we believe that our
model could be potentially applied to other ﬁelds of natural
language processing only by means of a principled Bayesian
calculus capable of manipulating density matrices. We hope
that this work will foster future research in this direction.

6. ACKNOWLEDGMENTS

We would like to thank the anonymous reviewers for their

valuable comments and suggestions.

7. REFERENCES

[1] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and X. Li.

Investigation of partial query proximity in web search. In Proc.
of WWW, pages 1183–1184, 2008.

[2] M. Bendersky and W. B. Croft. Modeling higher-order term

dependencies in information retrieval using query hypergraphs.
In Proc. of SIGIR, pages 941–950, 2012.

[3] M. Bendersky, D. Metzler, and W. B. Croft. Parametrized

concept weighting in verbose queries. In Proc. of SIGIR, pages
605–614, 2011.

[4] R. Blume-Kohout. Hedged maximum likelihood estimation.

Phys. Rev. Lett., 105:200504, 2010.

[5] R. Blume-Kohout. Optimal, reliable estimation of quantum

states. New J. Phys., 12:043034, 2010.

[6] O. Chapelle, D. Metzler, Y. Zhang, P. Grinspan. Expected

reciprocal rank for graded relevance In Proc. of CIKM, 2009.
[7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,

and R. Harshman. Indexing by latent semantic analysis.
JASIST, 41:391–407, 1990.

[8] J. L. Fagan. Automatic phrase indexing for document retrieval.

In Proc. of SIGIR, pages 91–101, 1987.

[9] J. Gao, J. Y. Nie, G. Wu, and G. Cao. Dependence language

model for information retrieval. In Proc. of SIGIR, pages
170–177, 2004.

[10] A. Gleason. Measures on the closed subspaces of a hilbert

space. Journ. Math. Mech., 6:885–893, 1957.

[11] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic

model of information retrieval: development and comparative
experiments. Inf. Proc. Manag., pages 779–840, 2000.
[12] M. Lease. An improved markov random ﬁeld model for

supporting verbose queries. In Proc. of SIGIR, pages 476–483,
2009.

[13] C. Lee, G. G. Lee, and M. G. Jang. Dependency structure

applied to language modeling for information retrieval. ETRI,
28(3):337–346, 2006.

[14] Y. Lv and C. Zhai. Positional language models for information

retrieval. In Proc. of SIGIR, pages 299–306, 2009.

[15] A. I. Lvovsky. Iterative maximum-likelihood reconstruction in

quantum homodyne tomography. Journ. Opt. B6, pages
S556–S559, 2004.

[16] M. Melucci. Deriving a quantum information retrieval basis.

The Computer Journal, 2012.

[17] M. Melucci and K. Rijsbergen. Quantum mechanics and
information retrieval. Advanced Topics in Information
Retrieval, 33:125–155, 2011.

[18] D. Metzler and W. Bruce Croft. Linear feature-based models

for information retrieval. Inf. Retr., 10(3):257–274, 2007.

[19] D. Metzler and W. B. Croft. A markov random ﬁeld model for

term dependencies. In Proc. of SIGIR, pages 472–479, 2005.
[20] D. Metzler, T. Strohman, Y. Zhou, and W. B. Croft. Indri at

TREC 2005: Terabyte Track. In Proc. of TREC, 2005.

[21] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An analysis

of statistical and syntactic phrases. In Proc of RIAO, pages
200–217, 1997.

[22] W. Morgan, W. Greiﬀ, and J. Henderson. Direct maximization

of average precision by hill-climbing, with a comparison to a
maximum entropy approach. In Proc. of HLT-NAACL, pages
93–96, 2004.

[23] M. A. Nielsen and I. L. Chuang. Quantum Computation and

Quantum Information. Cambridge University Press, 2004.

[24] J. H. Park, W. B. Croft, and D. A. Smith. A

quasi-synchronous dependence model for information retrieval.
In Proc. of CIKM, pages 17–26, 2011.

[25] B. Piwowarski, I. Frommholz, M. Lalmas, and K. van

Rijsbergen. What can quantum theory bring to information
retrieval. In Proc. of CIKM, pages 59–68, 2010.

[26] M. Pretti. A message-passing algorithm with damping. J. Stat.

Mech., page P11008, 2005.

[27] J. ˘Reh´a˘cek, Z. Hradil, E. Knill, A. I. Lvovsky. Diluted

maximum-likelihood algorithm for quantum tomography. Phys.
Rev. A, 75:042108, 2007.

[28] G. Salton, C. S. Yang, and C. T. Yu. A Theory of Term

Importance in Automatic Text Analysis. JASIST, 26(1):33–44,
1975.

[29] M. D. Smucker, J. Allan, and B. Carterette. A comparison of

statistical signiﬁcance tests for information retrieval
evaluation. In Proc. of CIKM, pages 623–632, 2007.

[30] F. Song and W. B. Croft. A general language model for

information retrieval. In Proc. of SIGIR, pages 279–280, 1999.

[31] M. Srikanth and R. Srihari. Biterm language models for

document retrieval. In Proc. of SIGIR, pages 425–426, 2002.
[32] H. Umegaki. Conditional expectation in an operator algebra.

Kodai Mathematical Seminar Reports, 14(2):59–85, 1962.

[33] K. van Rijsbergen. The Geometry of Information Retrieval.

Cambridge University Press, 2004.

[34] M. K. Warmuth and D. Kuzmin. Bayesian generalized

probability calculus for density matrices. Machine Learning,
78(1-2):63–101, 2009.

[35] C. Zhai. Statistical language models for information retrieval a

critical review. Found. Trends Inf. Retr., 2(3):137–213, 2008.

[36] J. Zhao and Y. Yun. A proximity language model for

information retrieval. In Proc. of SIGIR, pages 291–298, 2009.

[37] X. Zhao, P. Zhang, D. Song, and Y. Hou. A novel re-ranking

approach inspired by quantum measurement. In Proc. of
ECIR, pages 721–724, 2011.

[38] G. Zuccon and L. Azzopardi. Using the quantum probability

ranking principle to rank interdependent documents. In Proc.
of ECIR, page 357–369, 2010.

662