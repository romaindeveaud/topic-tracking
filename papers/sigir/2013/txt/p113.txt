Leveraging Conceptual Lexicon: Query Disambiguation

using Proximity Information for Patent Retrieval

Shima Gerani†

Jimmy Xiangji Huang‡

Fabio Crestani†

Parvaz Mahdabi†

†University of Lugano, Faculty of Informatics, Lugano, Switzerland
{parvaz.mahdabi, shima.gerani, fabio.crestani}@usi.ch
‡ School of Information Technology, York University, Toronto, Canada

jhuang@yorku.ca

ABSTRACT

Keywords

Patent prior art search is a task in patent retrieval where
the goal is to rank documents which describe prior art work
related to a patent application. One of the main properties
of patent retrieval is that the query topic is a full patent
application and does not represent a focused information
need. This query by document nature of patent retrieval
introduces new challenges and requires new investigations
speciﬁc to this problem. Researchers have addressed this
problem by considering diﬀerent information resources for
query reduction and query disambiguation. However, previ-
ous work has not fully studied the eﬀect of using proximity
information and exploiting domain speciﬁc resources for per-
forming query disambiguation.

In this paper, we ﬁrst reduce the query document by tak-
ing the ﬁrst claim of the document itself. We then build
a query-speciﬁc patent lexicon based on deﬁnitions of the
International Patent Classiﬁcation (IPC). We study how to
expand queries by selecting expansion terms from the lexi-
con that are focused on the query topic. The key problem is
how to capture whether an expansion term is focused on the
query topic or not. We address this problem by exploiting
proximity information. We assign high weights to expansion
terms appearing closer to query terms based on the intuition
that terms closer to query terms are more likely to be related
to the query topic. Experimental results on two patent re-
trieval datasets show that the proposed method is eﬀective
and robust for query expansion, signiﬁcantly outperforming
the standard pseudo relevance feedback (PRF) and existing
baselines in patent retrieval.

Categories and Subject Descriptors

H.3.3 [Information Storage and Retrieval]: Retrieval
Models, Query Formulation

General Terms

Experimentation, Performance, Measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for proﬁt or commercial advantage and that copies bear
this notice and the full citation on the ﬁrst page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior speciﬁc permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

Patent Search, Query Expansion, Proximity Information

1.

INTRODUCTION

For any new invention, in order to be granted a valid
patent, the owner of the idea needs to ensure that the in-
vention is novel, i.e., it has not been previously patented by
someone else or has not been described in a scientiﬁc paper.
To this end, a search for documents relevant to the invention
is carried out to see if the idea in the patent application is
invalidating on some existing ideas. This type of search is
called prior art search.

This search is executed by the author himself or (someone
else hired by the author) using keyword-based searches from
the claims of the patent application prior to ﬁling the patent
application. The same type of search is performed by the
patent examiner in the patent oﬃce after the application is
ﬁled, to ensure if the ﬁled patent application can be granted
a valid patent.

These keyword-based searches are then completed using
other metadata associated with the patent applications such
as (International Patent Classiﬁcation) IPC classes1 and date
tags. Bibliographic information such as citations, both back-
ward and forward, can also be used to perform prior art
searches. These searches made from diﬀerent sources are
then merged to compose a unique rank list. The goal of
combining these complementary searches is to solve the term
mismatch problem which is due to the obscure style of writ-
ing a patent (patentese) and often leads to low retrieval ef-
fectiveness [11].

Prior art search can take a very long time (days or weeks)
because the searcher needs to ensure he is not missing on any
relevant documents as infringing on some existing patents
might result in a multi-million dollar lawsuit. Therefore,
prior art search is considered as a recall-oriented application.
Patent retrieval, similar to other domain speciﬁc IR tasks
[17, 19], has the following three major problems: the fre-
quent usage of non-standardized acronyms which are in-
vented by patent applicants, the presence of homonyms (the
same word referring to two or more diﬀerent entities), such
as bus2 and closet3, and synonyms (two or more words re-
ferring to the same entity) such as signal and wave.

1http://www.wipo.int/classiﬁcations/ipc/en/
2i) motor vehicle, ii) an electronic subsystem transferring
plurality of digits bits in group.
3i)water closet (ﬂush toilet), ii) a small cupboard used for
storing things.

113Previous research [15, 9] has tackled this term mismatch
problem by ﬁrst forming a keyword query from the patent
application (query patent) based on the frequency infor-
mation, and then enhancing the query through a knowl-
edge base such as Wikipedia or WordNet, exploiting this
enhanced query to disambiguate the occurrences of query
terms in the documents.

Using external resources has been shown to be more eﬀec-
tive compared to the performance of the initial query and
pseudo relevance feedback (PRF). In fact, the retrieval ef-
fectiveness of PRF has been shown to be disappointing in
patent retrieval mainly due to the low MAP of the initial
rank list [5].

Patent examiners use term proximity heuristics in their
searches in Boolean retrieval model in order to reward a
document where the matched query terms occur close to
each other. Two forms of adjacency operators are used in
Boolean retrieval model to address proximity. “ADJn” op-
erator which searches for terms within n words proximity in
the order speciﬁed, and“NEARn” operator, which searches
for the terms within n words, in either order. This usage
shows that proximity information plays an important role
in patent searching.

Previous work [15, 9] which uses external resources for
query expansion did not take into account proximity infor-
mation between query terms and related expansion concepts
to form high quality expansions. Expansion terms extracted
from these external resources are often general terms. Thus,
it is necessary to condition their occurrences on their neigh-
boring query terms which are more precise and consequently
ignore their appearance in isolation from a query term.

In this paper, our aim is to address the term mismatch
problem in the patent retrieval through query expansion.
Diﬀerent from previous research [15, 9], we intend to use a
domain-dependent resource. We believe that using a domain-
dependent resource leads to the extraction of more relevant
expansion concepts. To do this, we ﬁrst constructed a lex-
icon from IPC deﬁnition pages4. Deﬁnition of IPC classes
consists of the explanations regarding each IPC class which
can be used to identify the important concepts and subtopics
of the query. We extract expansion concepts speciﬁc to each
query from this lexicon for query expansion. We then use
term proximity information to calculate reliable importance
weights for the expansion concepts.

We propose a proximity-based query propagation method
to calculate the query term density at each point in the doc-
ument. Our proximity-based framework incorporates posi-
tional information into the estimation of importance of ex-
pansion concepts so that we can reward expansion concepts
occurring close to query terms. This way we can concentrate
on the terms that are associated with the query terms and
avoid the topic drift which is caused by taking into account
irrelevant terms.

Our proposed model consists of four steps.

In the ﬁrst
step, we shorten the query document by taking the ﬁrst
claim of the document itself. We use the ﬁrst claim as the
source for building the initial query since it contains the core
of the invention and serves as a source for extracting precise
initial query terms.

In the second step, a query-speciﬁc lexicon is built. In the
third step, query expansion is performed by deriving expan-

4http://web2.wipo.int/ipcpub/

sion concepts from the query-speciﬁc lexicon and positional
information is used to calculate weights for insuring high
quality expansions. To this end, we utilize kernel functions
to keep track of the distance of the expansion concepts from
the query terms. Thus words appearing within the neighbor-
hood of a given query term are more likely to be associated
with that query term. In the fourth step, the initial query
and feedback runs (made using proximity information) are
combined together to generate a unique rank list.

Our contributions are:

• Presenting an approach to construct a domain-dependent

lexicon for identifying expansion concepts.

• Presenting a proximity-based method for estimating
the probability that a speciﬁc query expansion term is
relevant to the query term.

• Investigating diﬀerent strategies for extracting concepts
from domain-dependent lexicon for query reformula-
tion.

We evaluate our work on two patent retrieval corpora,
CLEF-IP 2010 and CLEF-IP 2011, using baselines which
employ external resources for query expansion. The experi-
mental results show that our model achieves signiﬁcant im-
provement over these baselines in terms of recall. The re-
sults show the advantage of deploying a domain-dependent
resource for selecting expansion candidate terms in contrast
to systems which deploy external resources that are not de-
pendent to the domain.

The results also conﬁrm that our model outperforms sys-
tems which perform query expansion via PRF. Besides, the
results demonstrate that utilizing the proximity information
leads to the calculation of reliable weights for the expansion
concepts in the process of query expansion.

2. RELATED WORK

Patent retrieval faces many challenges among which we
focus on the following two. The ﬁrst challenge is to reduce
the original query topic in order to ﬁnd a focused informa-
tion need and remove the ambiguous and noisy terms. In
previous work, researchers explored diﬀerent sections of the
query patent to perform the query reduction [18, 4]. Some of
the previous work reported that eﬀective queries were built
from the query patent [18], while others obtained better re-
sults using single ﬁelds of the query patent such as claims
or description [4].

The second challenge is related to query disambiguation.
Previous work used diﬀerent external resources for query
expansion such as Wikipedia [9] and WordNet [15] with the
goal of query disambiguation. The goal of this task is to al-
leviate the term mismatch problem by expanding the query
with all words related to the concept of the query or syn-
onyms of the query terms.

Our proximity-based framework is inspired by the origi-
nal work of Lv and Zhai on positional language model and
positional relevance model [12, 13]. An advantage of Lv and
Zhai’s work is that they can capture passage level evidence in
a “soft” way by modeling proximity information via density
functions. Their experiments conﬁrmed that such approach
works better than applying a “hard” boundary of passages.
Proximity information has shown to be useful in diﬀerent
IR tasks such as opinion mining [6]. Authors investigate

114proximity information for capturing the opinion density at
each point in the document.

Term position and proximity cues were mostly ignored in
previous work in patent retrieval. Recently, Ganguly et al.’s
work on reducing query patent using PRF captures term
position and proximity evidence indirectly through the use
of appropriate passages [5]. This work provides a general
model for query reduction using PRF. In fact, this passage-
based feedback model is orthogonal to our approach, in the
sense that it can be used as a pre-processing for our query
expansion method. Our proximity-based query expansion
framework can be applied on their reduced query. This way
we can ensure a more precise set of query terms as the initial
query.

Another recent study, Bashir and Rauber’s work on im-
proving retrievability of patent documents [3], combined term
proximity heuristics with other features to select good query
expansion terms in the context of PRF. In this work diﬀer-
ent distance functions were considered from diﬀerent win-
dows surrounding query term occurrences. They reported
an increase in terms of retrievability [2] of individual patents
using proximity heuristics compared to the standard PRF.
However, they did not evaluate directly the performance of
their approach in terms of retrieval eﬀectiveness.

Finally, Mahdabi et al.’s work used a learning approach
to predict the quality of a query [16]. This work uses prox-
imity information loosely through the use of noun phrases.
This work is complementary with our ideas as our proximity-
based method for estimating the probability of query relat-
edness of an expansion term can be integrated as a feature
in their learning-based model to improve their performance.

3. MULTIPLE INFORMATION SOURCES

We identify diﬀerent information sources that can be used
as additional knowledge for query reformulation in patent
retrieval. In this section we summarize and categorize them.
• Query patent: This document is a structured document
which is composed of the following sections: title, abstract,
description, and claims. A claim which does not reference
any other claim is called an independent claim and others
are called dependent claims [10]. The independent items
in the claims of the patent document comprise the kernel
of the technical innovation of a patent document. Among
the claims the most important claim is the ﬁrst indepen-
dent claim which represents the essence of the technology
of the patent document. The other parts of the patent
document illustrate the reason, background, implementa-
tion and advantages, of the invention being described [11].
• IPC classiﬁcation: The International Patent Classiﬁ-
cation (IPC classiﬁcation) provides a hierarchical catego-
rization over diﬀerent technological ﬁelds such as com-
puter science, electronics, mechanics, materials science,
and bio-chemistry. Such classes are language independent
keywords assigned as metadata to the patent documents.
They categorize the content of a patent document and
describe the ﬁeld of technology that a patent document
belongs to. These IPC classes can be seen as concep-
tual tags assigned to the documents [11]. For each con-
ceptual tag there is textual descriptions (IPC deﬁnition
pages) available which is an additional source of informa-
tion, providing contextual cues about diﬀerent technical
ﬁelds.

• Retrieval corpus: Our retrieval corpus can be used as
another source for extracting expansion concepts via the
procedure of PRF.

The above sources have diﬀerent vocabulary usage. The
query patent itself has an obscure style of writing (paten-
tese) [11]. This characteristic might create term mismatch
problem for ﬁnding relevant documents in the existing ﬁled
patents, such that a query term may not be a good indicator
in referring to a technological concept as it is not frequently
used in the given context by other authors.

However, the two other resources provide a more estab-
lished vocabulary usage. The descriptions of IPC classiﬁca-
tion represent the standard vocabulary usage related to dif-
ferent domains. This source provides a general but accepted
vocabulary usage for explaining technological concepts. Per-
forming query expansion via PRF allows extracting a set of
terms which are not used by the author but are frequent in
the retrieval corpus. Thus, the vocabulary usage of the two
latter sources are complementary to the query itself.

It is worth to mention that previous work [8] used the
conceptual tags for ﬁltering the patent documents. In this
work, we propose to exploit the textual description of such
conceptual tags in addition to the tags themselves.

4. BUILDING DOMAIN-DEPENDENT LEX-

ICON

We now explain the process of building a lexicon from
IPC deﬁnition pages. We refer to this lexicon as a concep-
tual lexicon. We ﬁrst perform stop-word removal on the text
of IPC deﬁnition pages. We then build a language model for
each IPC class and detect terms with document frequency
higher than 10 (based on experiments). We refer to these
terms as patent-speciﬁc stop-words and we ﬁlter them out
to increase the accuracy of our lexicon. Examples of these
patent-speciﬁc stop-words are “method”, “device”, “appara-
tus”, “process”.

Each entry in our lexicon is composed of a key and a
value. The key is an IPC class and the value is a set of terms
representing the mentioned class. These terms are extracted
from the IPC deﬁnition pages as previously described. An
example of an entry in the conceptual lexicon is presented
in Table 1.

IPC Class

Representing Terms

C07D 279/24

hydrocarbon, radicals,

amino, ring, nitrogen, atom

Table 1: An entry in the conceptual lexicon.

The lexicon can be used to extract expansion concepts
related to the context of the information need of a given
query patent. To this end, the IPC class of the query is
searched in the lexicon and the terms matching this class
are considered as candidate expansion terms.

Query expansion using the lexicon will help us solve the
two following problems: The ﬁrst problem is related to the
fact that the usage of words is sensitive to the topic domain;
In diﬀerent domains, the same word may be used to indicate
diﬀerent meanings. We aim at ﬁnding the correct sense of a
word, by associating relevant terms from the topic domain
to the given query terms for each query patent.

115The second problem is related to the term mismatch. The
vocabulary of the query patent is tailored by the language
usage of the author (a non-standard terminology), while con-
ceptual lexicon provides a more standard terminology. We
try to combine these two complementary vocabularies, as we
believe this will help alleviate the term mismatch problem.
In the next sections we will explain with more detail how

the lexicon is used for query expansion.

5. A PROXIMITY-BASED FRAMEWORK

Our hypothesis is that an author will use standard termi-
nology from a conceptual lexicon for clarifying his invented
concepts. We note that a query term might belong to the
author terminology or some terminology that is commonly
used in a domain but is not as conventional as the vocabu-
lary of the conceptual lexicon.

We now focus our attention to identify expansion concepts
in the document that are referring to the concepts in the
query. These expansion concepts can be extracted from one
of the sources explained in the previous section. We need to
estimate the probability that an expansion term is referring
to a query term. To do this, we rely on the structure of the
document. We assume that an expansion term refer with
higher probability to the query terms closer to its position.
We thus regard the distance of an expansion term to the
query term as a measure of relatedness.

5.1 Estimating the Query Relatedness

In this section, we explain our method for estimating the
probability that an expansion term e at position i, is related
to the query term q. We calculate this probability as follows:

P (q|i, d) =

m!j=1

P (q|tj)P (j|i, d)

(1)

where d denotes a document, i denotes an expansion term
position and J = {1, 2, ..., m} denotes a set of query term
positions. P (q|i, d) indicates the probability that the ex-
pansion term at position i in document d is about the query
term q. We refer to this probability as the query relatedness
probability. To ﬁnd the query relatedness at position i, we
calculate the accumulated probability from all query posi-
tions at that position. For every position j in a document
we consider the query weight of the term at that position,
denoted by P (q|tj), and weight it by the probability that
the term at position j is about the expansion term at posi-
tion i, denoted by P (j|i, d). This probability is estimated as
follows:

P (j|i, d) =

k(j, i)
j!=1 k(j!, i)

"|d|

(2)

where K(i, j) is the kernel function which determines the
weight of propagated query-relatedness from tj to ti. We
model the query relatedness by placing a density kernel
function around query terms. We use the same notation
throughout the paper.

In the following, we present diﬀerent kernels used in our
experiments. We study three diﬀerent density functions,
namely Gaussian, Laplace, and Rectangle kernel. We se-
lected Gaussian and Laplace kernels as they have been shown
to be the best performing kernels among the kernel functions
tested in the previous work [12, 6]. We also chose Rectangle

kernel to simulate the eﬀect of imposing a hard boundary
over passages in contrast to the soft boundary introduced
by other kernels.

• Gaussian Kernel
k(i, j) =

1

√2πσ

exp [−(i − j)2)

2σ2

]

• Laplace Kernel

k(i, j) =

1
2b

exp [−|i − j|

b

]

where σ2 = 2b2

• Rectangle Kernel

k(i, j) =# 1

2a
0

if |i − j| ! a
otherwise

where σ2 =

a2
3

Our aim is to investigate whether it is better to use kernels
which favor expansion term occurrence in close proximity of
query terms or not.

5.2 Calculating Document Relevance Score

In this section, we intend to calculate the overall probabil-
ity that relevant expansion concepts (inside the document)
are directed towards the technical concept of the query. This
probability is denoted by P (q|d, e). P (q|d, e) is deﬁned as:

P (q|d, e) =

P (q, i|d, e) =

P (q|i, d, e)P (i|d, e)

(3)

|d|!i=1

|d|!i=1

We assume e and q are conditionally independent given
the position in the document. Thus, P (q|i, d, e) reduces to
P (q|i, d) which can be estimated using the query related-
ness probability. We now need to estimate the probability
P (i|d, e). We suggest two diﬀerent methods for estimating
P (i|d, e).
• Avg Position Strategy: All positions of expansion con-

cepts are equally important:

by substituting this in Equation 3 we have:

if ti ∈ e
otherwise

0

#P (i|d, e) = 1/|pos(e)|
P (q|d, e) = 1/|pos(e)| !i∈pos(e)

P (q|i, d)

(4)

• Max Position Strategy: As an alternative, we can as-
sume that only the expansion term position where P (q|i, d)
is maximum is important:

P (q|d, e) = max
i∈pos(e)

P (q|i, d)

(5)

116Example 1 (Topic ID: pac-1474)
Patent title: “Optical information recording medium”
Query terms extracted from ﬁrst independent item of claims: optical, layer, record, lens, light,
interlay, irradiation, wavelength
Expansion concepts selected from the conceptual lexicon related to the query: organic, dielectric,
sensitizing, record, reproduction
Retrieved docs for example 1
Number of retrieved relevant documents in the baseline run: 15/42
Number of retrieved relevant documents after using proximity-based method: 24/42
Example 2: (Topic ID: pac-552)
Patent title: “Power supplying apparatus, design method of the same, and power generation apparatus”
Query terms extracted from ﬁrst independent item of claims: power, supply, boost, transformer,
switch, resonance
Expansion concepts selected from the conceptual lexicon related to the query: conversion, semiconductor,
electrode, light, push-pull
Retrieved docs for example 2
Number of retrieved relevant documents in the baseline run: 6/15
Number of retrieved relevant documents after using proximity-based method: 12/15

Table 2: Examples where using proximity-based information with IEC method improves recall in patent
retrieval.

6. QUERY REFORMULATION
In this section, given an input query Q = {q1, q2, ..., qn},
we can identify a set of concepts CE = {e1, e2, ..., em} which
are selected from the conceptual lexicon. The set of CE is
associated to the query Q as the conceptual lexicon contains
explanations about the IPC classes to which a patent docu-
ment is assigned. Once the set of concepts CE is identiﬁed,
we determine the concept importance weights according to
their distance from the query terms based on the intuition
that concepts closer to query terms are more related to the
query. Equation 1 demonstrates the process for calculat-
ing the importance weight for expansion concepts. We can
then re-rank documents in the original rank list R using a
weighted combination of the matches of concepts in CE and
our original keyword query Q based on Equation 3.

We use four diﬀerent strategies for the expansion con-
cept selection process. We categorize and summarize these
strategies below.

Explicit Expansion Concepts
In this setting, we use
the concepts in our conceptual lexicon which match against
the IPC classiﬁcation of the query. However, we restrict our
attention to concepts that appear in DQ. This provides a
set of explicit expansion concepts (a subset of CE) which
serves as candidate expansion terms. We refer to this set
as XE. We utilize the proximity of query terms and expan-
sion terms inside query document DQ to assign importance
weights to the explicit expansion concepts. These weights
are then used to re-rank documents in the list R.

Implicit Expansion Concepts In this strategy, the ex-
pansion terms are not limited to the set of explicit expan-
sion concepts XE which were deﬁned previously. Instead,
our query expansion method includes all expansion concepts
in CE.
In this setting we extract proximity information
from the documents inside R for computing the importance
weights associated with the expansion terms. The advan-
tage of this strategy compared to the previous one is that
we are able to make use of all terms available in the CE and

Query Document

Conceptual Lexicon

Retrieval Corpus

acrylate, ink,

light-sensitive,

record, liquid,

jet, acid, polymer,
pigment, record, ...

duplicate, printer,
ink, sheet, mark, ...

surface, composition,
polymer, cartridge, ...

Table 3: Comparison between the list of expansion
terms derived from the information sources for the
query with title ”inkjet recording ink”.

we are not limited to the query document.

Combining Search Strategies In this strategy, instead
of query expansion, we ﬁrst calculate a relevance score based
on the original keyword query Q. We then calculate an IPC
score based on the expansion concepts in CE. We linearly
combine the two scores together. Our goal is to compare
whether having a uniﬁed query, as exists in the query ex-
pansion, is better than constructing two separate queries
and combining their results at the end. We introduce this
setting for the experiments in order to simulate the spe-
ciﬁc search strategies taken by professional searchers for re-
trieving relevant documents [11]. In such a search strategy,
searchers perform separate searches based on diﬀerent infor-
mation sources, such as the query document and IPC clas-
siﬁcation, and then combine the results of the runs together
to produce a unique rank list.

Proximity-based Pseudo Relevance Feedback As a
comparison baseline we also used the retrieval corpus as a
source for PRF where we used the feedback set for selecting
expansion terms and identifying their weights. We use the
distance between the query terms and the candidate expan-
sion concepts inside the feedback documents to calculate the
weight for the expansion concepts.

As an example, Table 3 shows the terms selected from
diﬀerent information resources. The terms from the query
document is selected from the ﬁrst item of the claims. The
terms from the retrieval corpus are selected via the proce-
dure of PRF.

1177. EXPERIMENTAL SETUP

In this section we ﬁrst explain our experimental setup for

evaluating the eﬀectiveness of our proposed approach.

Testing Collections We used Terrier Information Re-
trieval System5 to index the collection with the default stem-
ming and stop-word removal. We removed patent-speciﬁc
stop-words such as “device” and “method”.

We conducted our experiments over two years worth of

CLEF Intellectual Property (CLEF-IP) task, including CLEF-
IP 2010 and CLEF-IP 2011 datasets. CLEF-IP 2010 con-
tains 2.6 million patent documents and CLEF-IP 2011 con-
sists of 3 million patent documents. In our experiments we
used the English subsection of both collections. The English
test set of CLEF-IP 2010 corresponds to 1348 topics. The
English test set of CLEF-IP 2011 consists of 1351 topics. We
used the training topics of CLEF-IP 2010 for tuning some
parameters of our model. This training set consists of 300
topics.

Evaluation We used the relevance judgment for the En-
glish language provided by CLEF-IP for evaluation. We
report the Recall, Mean Average Precision, and Patent Re-
trieval Evaluation Score [14] which combines MAP and Re-
call in one single score. We report the evaluation metrics on
the top 1000 results. In the remainder of our experiments
we used the Wilcoxon signed ranked matched pairs test with
a conﬁdence level of 0.05 level for testing statistical signiﬁ-
cance improvements.

8. EXPERIMENTAL RESULTS
8.1 Building the Initial Query

We built keyword queries by extracting distinguishing terms

from the query patent document. To this end, we estimated
the importance of each term according to a weighted log-
likelihood based approach, as in [16]. We used the claims
and the ﬁrst independent claim for selecting query terms.
Table 4 summarizes the results we obtained for the topics in
the training and test set of CLEF-IP 2010 and the test set
of CLEF-IP 2011. We used top-10 query terms with higher
weights from the estimated query model in our experiments.
Results marked with † and ‡ achieve statistically signiﬁcant
improvement in terms of MAP and recall, respectively. Note
that this comparison is performed among runs belonging to
the same experimental settings.

The results of Table 4 demonstrate that the performance
of the runs obtained by issuing the query built from the ﬁrst-
claim is always stronger than the performance of the runs
where the query is built from the claims in terms of MAP.
However, the opposite holds for recall. The reason for the
high MAP is because the ﬁrst independent item of claims
is focused on the core invention of the patent document.
However, we are losing some information by ignoring the
text of the rest of the claims and this explains the low recall
of this setting.

To guarantee the assignment of reliable importance weights
to the expansion concepts in our proximity-based frame-
work, we need to start with a set of precise query terms.
This is because we rely on the distance between query terms
and expansion concepts to calculate importance weights for

5http://ir.dcs.gla.ac.uk/terrier/

CLEF-IP 2010 (training topics)

Method
C10TR
FC10TR ﬁrst-claim

Run description MAP
0.1211
claims
0.1530 †

CLEF-IP 2010 (test topics)

Method
C10TE
FC10TE ﬁrst-claim

Run description MAP
0.1293
claims
0.1445 †

CLEF-IP 2011 (test topics)

Method
C11TE
FC11TE ﬁrst-claim

Run description MAP
0.0823
claims
0.1198 †

Recall
0.5905 ‡
0.5360

Recall
0.6302 ‡
0.6015

Recall
0.6067 ‡
0.5624

PRES
0.5492
0.5479

PRES
0.5140
0.4911

PRES
0.4850
0.4538

Table 4: Choosing baseline on the two retrieval col-
lections.

expansion concepts. Obviously starting with focused and
less noisy query terms has a direct eﬀect on the quality of
calculated importance weights. Thus, in the remainder of
this paper, we focus on selecting query terms from the ﬁrst
independent item of the claims.

We used the Language Modeling approach with Dirichlet
smoothing [20] to score documents from both collections and
build the initial rank lists. We empirically set the value for
the smoothing parameter µ to1500. We also used Language
Modeling for the re-ranking of the results. We note that we
do not use citation information in our experiments.

8.2 Choosing the Baseline

In terms of our comparison baseline, we chose the strongest
conﬁguration in terms of PRES from Table 4, the retrieval
run where query terms are selected from the claims section
of the patent document. C10TR is chosen as the baseline on
the training topics of CLEF-IP 2010. C10TE is chosen as
the baseline on the test topics of CLEF-IP 2010 and C11TE
is chosen as the baseline over test topics of CLEF-IP 2011.
Note that the training set of CLEF-IP 2010 is only used for
tuning the parameters of the model, thus we will refer to
C10TR in such comparisons.

Table 5 shows the performance of strong baselines of the
previous work [5, 15] over CLEF-IP 2010. We presented
their performance evaluation in terms of MAP and PRES
as the recall values were not reported for the two baselines.

method
baseline1 [5]
baseline2 [15]

MAP
0.1278
0.1399

PRES
0.4604
0.4860

Table 5: Strong baselines of the previous work

As we can see from the results of Table 5, C10TE is as
strong as the baseline1 and baseline2 in terms of PRES. This
ensures the selection of a strong baseline which will be used
in evaluating the performance of our proposed model in the
rest of the paper.

8.3 Motivation for Using Proximity Informa-

tion

In order to test if closeness of expansion concepts to the
query terms is correlated with relevance, we carry out pre-
liminary experiments on the CLEF-IP 2010 collection.
In
these experiments, we selected 100 random queries.

For each query, we ﬁrst retrieved the top 100 documents

118kernel \ σ
Gaussian
Laplace
Rectangle

25
0.6443
0.6422
0.6398

IEC

75
0.6561 †
0.6556 †
0.6523

125
0.6676 †
0.6588 †
0.6559 †

150
0.6795 †
0.6709 †
0.6678 †

kernel \ σ
Gaussian
Laplace
Rectangle

25
0.6388
0.6362
0.6339

EEC

75
0.6418
0.6390
0.6375

125
0.6669 †
0.6685 †
0.6642 †

150
0.6637 †
0.6516
0.6497

Table 6: Recall results of diﬀerent settings of the kernel functions using query reformulation methods (IEC
and EEC) on the training topics of CLEF-IP 2010.

using a Language Modeling retrieval method. We separated
relevant and non-relevant documents according to relevance
judgements (qrels). We then looked at the average distance
between query terms and expansion concepts inside the set
of relevant documents, denoted by R, and the set of non-
relevant documents, denoted by ¯R. The distance in each of
the two mentioned sets is calculated as follows:

DIS(Q, R) = !D∈R"q∈Q mine∈E(Dis(q, e))
DIS(Q, ¯R) = !D∈ ¯R"q∈Q mine∈E(Dis(q, e))

|Q|| ¯R|

|Q||R|

where q denotes a query term drawn from the set of query
terms, denoted by Q, e denotes an expansion term, and
E denotes the set of expansion concepts selected from the
conceptual lexicon. The distance between two terms is cal-
culated in terms of the number of terms between them. The
minimum is calculated among all the occurrences of each
pair of query term and expansion term.
Figure 1 shows the average distance for relevant and non-
relevant documents for each query topic. For clarity pur-
poses, topics are sorted by the average distance DIS of their
relevant documents.

It can be seen from Figure 1 that the minimum distance
between an expansion term and a query term in relevant doc-
uments is less than their respective distance in non-relevant
documents. Therefore we can use this proximity information
to diﬀerentiate the relevant documents from non-relevant
documents and to improve the ranking of relevant docu-
ments.

8.4 Effect of Density Kernel

We are interested to investigate the eﬀectiveness of diﬀer-
ent query reformulation methods proposed in Section 6 for
scoring documents in our proximity-based framework. The
results of this comparison are summarized in Table 6.

In all the comparisons, our query expansion method which
uses explicit expansion concept is denoted as EEC. The query
expansion method which uses implicit expansion concept is
referred to as IEC.

Since the performance of these methods is directly deter-
mined by the eﬀectiveness of the kernel function used to esti-
mate the propagated query relatedness probabilities for the
expansion concepts, we ﬁrst need to compare three diﬀerent
proximity-based kernel functions to see which one performs
the best.

We place a density kernel around each occurrence of query
term positions in the document as previously explained in
Section 5. The query relatedness at each expansion term po-
sition is then calculated by counting the accumulated query

relevant
non-relevant

       1.0

       0.8

e
c
n
a

       0.6

t
s
D

i

       0.4

 
.

g
v
A

       0.2

       0.0

Topics

Figure 1: AverageDIS value for each topic using qrels
and top-retrieved non-relevant documents.

relatedness density from diﬀerent query terms at that posi-
tion. Therefore, an expansion term which occurs at a posi-
tion close to many query terms will receive high query re-
latedness and thus will obtain a higher importance weight.
Our proximity-based framework has two parameters: the
type of kernel function and its bandwidth parameter σ which
controls the degree of query relatedness propagation through-
out the entire document. To tune the parameters of our
model we used the training topics of CLEF-IP 2010.

The results of comparing diﬀerent kernel functions on the
training topics of CLEF-IP 2010 are shown in Table 6. A
† denotes statistical signiﬁcant improvement over C10TR
and the best result for each kernel type is highlighted. The
results show that the performance of EEC and IEC with all
kernel functions improve over C10TR.

It is also clear that among all the kernel functions, the
Gaussian kernel outperforms other types of kernels in most
cases. Since the Gaussian kernel performed the best in most
of the carried experiments, we use this kernel function for
our system evaluation in the rest of our experiments.

In order to ﬁnd the best value for the parameter σ we
tried a set of ﬁxed values in the range of [25, 200] with a
step of 25 similar to the previous work [12, 13]. Table 6
reports the performance of diﬀerent kernel functions using
varying values of σ. The results show that selecting a value
of 125 or 150 usually gives the best retrieval performance.

Overall, the results of Table 6 clearly demonstrate that
the results obtained with the σ value of 150 achieved better
performance in most cases, although the diﬀerence among
diﬀerent settings was not signiﬁcant. We thus use the σ
value of 150 in the rest of our experiments. In Section 8.5
we further study the performance of the query reformulation

119Collection

CLEF-IP 2010

CLEF-IP 2011

metric
MAP
Recall
PRES
MAP
Recall
PRES

IEC
0.1050
0.6595 †
0.5540
0.0772
0.6371 ‡
0.5288

EEC
0.1026
0.6437 †
0.5498
0.0761
0.6254 ‡
0.5249

CSS
0.0982
0.6241
0.5354
0.0738
0.6088
0.5127

PPRF
0.0705
0.5877
0.5023
0.0629
0.5632
0.4945

Table 7: The performance results of query reformulation approaches on two patent retrieval datasets on the
test topics of CLEF-IP 2010 and CLEF-IP 2011.

methods.

Comparison of Max and Avg Strategy We are in-
terested to evaluate the two strategies for calculating the
probability of relevance of a document as proposed in Sec-
tion 5.2. Table 8 shows the result of using avg and max
strategies for diﬀerent sigma values on the training topics of
CLEF-IP 2010 using the IEC reformulation method.

The results show that max strategy is statistically better
than the avg strategy. Thus, we use the max strategy in
all conﬁgurations of our experiments throughout this paper.
A † denotes the statistical signiﬁcant improvement over avg
method.

method \ σ
max
avg

25
0.6443 †
0.6164

75
0.6561 †
0.6198

125
0.6676 †
0.6207

150
0.6795 †
0.6238

Table 8: Recall of the Max and Avg method using
Gaussian kernel with IEC reformulation method on
training topics of CLEF-IP 2010.

8.5 Effect of Query Reformulation

In this section, we present the evaluation results of our
proposed approaches on the topics in the test set of CLEF-
IP 2010 and CLEF-IP 2011.

Table 7 reports the retrieval performance of query refor-
mulation methods described in Section 6. The symbols † and
‡ denote statistical signiﬁcant improvements over C10TE
and C11TE, respectively.
We now compare the performance of our query formu-
lation methods. In addition to EEC and IEC which were
introduced earlier, the results of the two other query refor-
mulation methods (described in Section 6) are presented in
Table 7. Our method which combines search strategies is
denoted as CSS. The last method in our comparisons is the
positional-based pseudo relevance feedback, which is denoted
by PPRF.

The main observation from Table 7 is that IEC is always
more eﬀective than the other three methods. In addition,
IEC improves the baseline in terms of recall on both collec-
tions signiﬁcantly.

Table 7 shows that a method which uses a conceptual lex-
icon for selecting expansion terms outperforms a method
which uses feedback documents for identifying expansion
terms. This is evident by comparing the performance of
EEC, IEC and CSS to the performance of PPRF, as the
ﬁrst three methods use the conceptual lexicon for query ex-
pansion. This result is consistent on both corpora used for
evaluation.

In addition, the results of Table 7 demonstrate that IEC
obtains improvement over EEC. In contrast to IEC, EEC
extracts a limited set of expansion terms from the conceptual
lexicon, the ones which are present in the query document
itself. This diminishes the power of EEC in contrast to IEC,
and explains the advantage of IEC. Results conﬁrm that the
unlimited usage of the conceptual lexicon is superior to the
limited usage of it.

Another observation which can be made from Table 7 is
that CSS attains worst results compared to both EEC and
IEC. This is perhaps due to the fact that some information
is lost during the combination of two separate runs made
from the query terms and expansion terms. While, in EEC
and IEC we use a uniﬁed query which is composed of query
terms and expansion terms.

Overall, the results of Table 7 show that using the con-
ceptual lexicon as a domain-dependent external resource is
eﬀective in terms of recall, although this improvement does
not hold for precision. Table 2 shows some queries where
using IEC reformulation method improved the recall.
We used 40 expansion terms (based on initial experiments)
in each of the query reformulation methods. We studied the
eﬀect of the number of expansion terms on the performance
of each method. We report the result of this study in Section
8.7.

8.6 Comparison to Standard PRF

Table 9 reports the retrieval performance of PPRF com-
pared to PRF. A ‡ indicates the statistical signiﬁcant im-
provement over the baseline which is built from the ﬁrst-
claim presented in Table 4. A † denotes the statistical sig-
niﬁcant improvement over standard PRF in terms of recall.
As previously explained in Section 6, PPRF is similar to
PRF since they both use feedback set for expansion term
selection. However, PPRF uses proximity information in-
side feedback set to calculate weight for expansion terms in
contrast to standard PRF.

The results show that PPRF performs signiﬁcantly better
than the standard PRF. This result conﬁrms the usefulness
of proximity information for identifying importance weights
for expansion terms as previously was shown in [13].

Note that PPRF and PRF does not achieve improvement
over the baseline, but a fair comparison is to compare the re-
trieval eﬀectiveness after query expansion with the retrieval
eﬀectiveness before query expansion. We thus need to com-
pare the results of Table 9 with the results of FC10TE and
FC11TE which correspond to the performance of the initial
query built from the ﬁrst claim.

Our results show that the performance obtained with PPRF
method achieves statistical signiﬁcant improvements in terms

120     0.660
     0.650
     0.640
     0.630
     0.620
     0.610
     0.600
     0.590
     0.580
     0.570

l
l

a
c
e
R

IEC
EEC
CSS
PPRF

 0

 0.2

 0.4

 0.6

 0.8

 1

(cid:104)

a) CLEF-IP 2010

     0.640
     0.630
     0.620
     0.610
     0.600
     0.590
     0.580
     0.570
     0.560
     0.550
     0.540

l
l

a
c
e
R

IEC
EEC
CSS
PPRF

 0

 0.2

 0.4

 0.6

 0.8

 1

(cid:104)

b) CLEF-IP 2011

Figure 2: Sensitivity to the λ coeﬃcient in the linear combination of results made from the initial and the
expanded query

of recall over the initial query (before expansion). This re-
sult is interesting as a recent study [5] pointed out that often
standard PRF method fails on the patent prior art search.
This poor performance is associated to the low MAP of the
initial rank list from which feedback documents are selected.
This comparison demonstrates the usefulness of aggregating
the proximity information in the calculation of the expansion
weights as performed in our proximity-based framework.

We ﬁxed the number of feedback documents used in both

PPRF and PRF to 10.

8.7 Inﬂuence of Different Parameter Settings

In this section, we are interested to study the inﬂuence
of diﬀerent parameters on the eﬀectiveness of our proposed
methods. We used the test topics of both corpora during
the evaluations.

Collection

CLEF-IP 2010

CLEF-IP 2011

metric PPRF
0.0705
MAP
0.5877 ‡†
Recall
0.5023
PRES
0.0629
MAP
0.5632 ‡†
Recall
0.4945
PRES

PRF
0.0650
0.5630
0.4961
0.0617
0.5346
0.4792

Table 9: The comparison of performance results of
PRF and PPRF.

Number of Expansion Terms To see the eﬀect of the
number of expansion terms on the eﬀectiveness of our pro-
posed methods we plot the sensitivity of diﬀerent query re-
formulation methods to the number of expansion terms over
CLEF-IP 2010 test topics. We change the number of ex-
pansion terms from 1 to 50. The recall results are shown in
Figure 3. We observe that all four methods achieve eﬀective
performance using around 40 expansion terms.

Eﬀect of Combination In all conﬁgurations of our exper-
iments we linearly combined the results we got from each

     0.660

     0.640

     0.620

     0.600

l
l

a
c
e
R

     0.580

     0.560

     0.540

IEC
EEC
CSS
PPRF

 5

 10  15  20  25  30  35  40  45  50

# of expansion terms

Figure 3: Sensitivity to the eﬀect of number of ex-
pansion terms on CLEF-IP 2010

of the reformulation methods with the initial query. The
weight of the interpolation λ controls the weight of the ini-
tial query. When λ = 0, the query expansion model is used
and when λ = 1 the initial query is used. λ was tuned based
on the training topics of CLEF-IP 2010.

Figure 2 shows the results of the sensitivity analysis over
the coeﬃcient λ on the test topics of CLEF-IP 2010 and
CLEF-IP 2011. We notice that IEC is more eﬀective than
other query reformulation methods for diﬀerent λ values.
The optimal value for the parameter λ seems to be in a
range around 0.4.

Eﬀect of Normalization

We now compare the eﬀect of diﬀerent normalization meth-
ods prior to linear combination using two score normaliza-
tion methods, MinMax [7] and HIS normalization [1], which
are used in distributed information retrieval or meta-search.

121MinMax normalization method shifts and scales scores to be
between zero and one. While, HIS normalization estimates
a single cumulative density function (CDF) for every search
engine based on historical queries.

We also experimented with a variation of score normal-
ization where we ﬁrst applied MinMax and then we applied
HIS normalization. We refer to this method as MinMax-HIS
throughout the experiments.

Table 10 shows the comparison among diﬀerent normal-
ization methods. These results correspond to the ﬁnal per-
formance of each run after the combination over test topics
of CLEF-IP 2010. The results are obtained with the IEC
method. The best results are highlighted although the dif-
ference is not statistically signiﬁcant.

metric MinMax HIS
MAP
Recall
PRES

0.0924
0.6520
0.5473

0.0991
0.6568
0.5522

MinMax-HIS
0.1050
0.6595
0.5540

Table 10: The comparison of diﬀerent normalization
methods over CLEF-IP 2010 using IEC method

We observe that IEC achieves the best performance us-
ing MinMax-HIS normalization. The results of other meth-
ods were also conﬁrming that applying normalization using
MinMax-HIS was better compared to either MinMax or HIS
alone, although not signiﬁcantly. We thus presented the re-
sults of normalization using MinMax-HIS method through-
out the paper.

9. CONCLUSION AND FUTURE WORK

In this paper we introduced a proximity based framework
for query expansion which utilizes a conceptual lexicon for
patent retrieval. To this end, we constructed a domain-
dependent conceptual lexicon which can be used as an ex-
ternal resource for query expansion. Our proximity-based
retrieval framework provides a principled way to calculate
the importance weight for expansion terms selected from the
conceptual lexicon. We showed that proximity of expansion
terms to query terms is a good indicator of the importance of
the expansion terms. In this paper we focused on performing
query expansion with single terms to ensure the eﬃciency of
the expansion concept selection process.

We have evaluated our proposed method on two patent re-
trieval corpora, namely CLEF-IP 2010 and CLEF-IP 2011.
Our query formulation method, IEC, was shown to outper-
form the strong baselines of CLEF-IP and a standard pseudo
relevance feedback method in terms of recall. Further anal-
ysis of the performance of the query reformulation methods
proposed in this paper showed the high quality of expansion
terms extracted from the conceptual lexicon.

10. ACKNOWLEDGMENTS

The work of Parvaz Mahdabi was funded by the Informa-
tion Retrieval Facility, through the research project “Inter-
active Patent Search (IPS)”. This research is also supported
in part by the research grant from Natural Sciences and En-
gineering Research Council (NSERC) of Canada. We thank
anonymous reviewers for their thorough review comments
on this paper.

11. REFERENCES

[1] A. Arampatzis and J. Kamps. A signal-to-noise

approach to score normalization. In CIKM, pages
797–806, 2009.

[2] L. Azzopardi and V. Vinay. Retrievability: an

evaluation measure for higher order information access
tasks. In CIKM, pages 561–570, 2008.

[3] S. Bashir and A. Rauber. Improving retrievability of
patents in prior-art search. In ECIR, pages 457–470,
2010.

[4] S. Cetintas and L. Si. Eﬀective query generation and
postprocessing strategies for prior art patent search.
JASIST, 63(3):512–527, 2012.

[5] D. Ganguly, J. Leveling, W. Magdy, and G. J. F.

Jones. Patent query reduction based on
pseudo-relevant documents. In CIKM, pages
1953–1956, 2011.

[6] S. Gerani, M. J. Carman, and F. Crestani.

Aggregation methods for proximity-based opinion
retrieval. TOIS, 30(4):26, 2012.

[7] J.-H. Lee. Analyses of multiple evidence combination.

In SIGIR, pages 267–276, 1997.

[8] P. Lopez and L. Romary. Patatras: Retrieval model

combination and regression models for prior art
search. In CLEF (Notebook
Papers/LABs/Workshops), pages 430–437, 2009.

[9] P. Lopez and L. Romary. Experiments with citation
mining and key-term extraction for prior art search.
CLEF (Notebook Papers/LABs/Workshops), 2010.

[10] M. Lupu and A. Hanbury. Patent retrieval.

Foundations and Trends R$ in Information Retrieval,
7(1):1–97, 2013.
[11] M. Lupu, K. Mayer, J. Tait, and A. Trippe. Current

Challenges in Patent Information Retrieval. Springer,
2011.

[12] Y. Lv and C. Zhai. Positional language models for

information retrieval. In SIGIR, pages 299–306, 2009.

[13] Y. Lv and C. Zhai. Positional relevance model for

pseudo-relevance feedback. In SIGIR, pages 579–586,
2010.

[14] W. Magdy and G. J. F. Jones. PRES: A score metric

for evaluating recall-oriented information retrieval
applications. In SIGIR, pages 611–618, 2010.

[15] W. Magdy and G. J. F. Jones. A study on query

expansion methods for patent retrieval. In PAIR 2011
- CIKM, pages 19–24, 2011.

[16] P. Mahdabi, L. Andersson, M. Keikha, and

F. Crestani. Automatic reﬁnement of patent queries
using concept importance predictors. In SIGIR, pages
505–514, 2012.

[17] P. Sondhi, V. G. V. Vydiswaran, and C. Zhai.

Reliability prediction of webpages in the medical
domain. In ECIR, pages 219–231, 2012.

[18] X. Xue and W. B. Croft. Automatic query generation

for patent search. CKIM, pages 2037–2040, 2009.
[19] X. Yin, X. Huang, and Z. Li. Promoting ranking

diversity for biomedical information retrieval using
wikipedia. In ECIR, pages 495–507, 2010.

[20] C. Zhai and J. D. Laﬀerty. A study of smoothing

methods for language models applied to ad hoc
information retrieval. In SIGIR, pages 334–342, 2001.

122