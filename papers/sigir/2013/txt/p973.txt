Question Retrieval with User Intent

Long Chen

DCSIS

Birkbeck, University of London

long@dcs.bbk.ac.uk

Dell Zhang

DCSIS

Birkbeck, University of London

dell.z@ieee.org

Mark Levene

DCSIS

Birkbeck, University of London

mark@dcs.bbk.ac.uk

ABSTRACT
Community Question Answering (CQA) services, such as
Yahoo! Answers and WikiAnswers, have become popular
with users as one of the central paradigms for satisfying
users’ information needs. The task of question retrieval in
CQA aims to resolve one’s query directly by ﬁnding the
most relevant questions (together with their answers) from
an archive of past questions. However, as users can ask
any question that they like, a large number of questions in
CQA are not about objective (factual) knowledge, but about
subjective (sentiment-based) opinions or social interactions.
The inhomogeneous nature of CQA leads to reduced perfor-
mance of standard retrieval models. To address this prob-
lem, we present a hybrid approach that blends several lan-
guage modelling techniques for question retrieval, namely,
the classic (query-likelihood) language model, the state-of-
the-art translation-based language model, and our proposed
intent-based language model. The user intent of each can-
didate question (objective/subjective/social) is given by a
probabilistic classiﬁer which makes use of both textual fea-
tures and metadata features. Our experiments on two real-
world datasets show that our approach can signiﬁcantly out-
perform existing ones.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation, Performance

Keywords
Community Question Answering, Question Retrieval, User
Intent, Language Modelling.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1.

INTRODUCTION

In recent years, Community Question Answering (CQA)
services, such as Yahoo! Answers, WikiAnswers, Quora,
Stack Overﬂow, and Baidu Zhidao, have become popular
knowledge sources for Internet users to access useful infor-
mation online. When a user submits a new question (i.e.,
query) in CQA, the system would usually check whether
similar questions have already been asked and answered be-
fore. If so, the user’s query could be resolved directly by re-
turning those archive questions (i.e., documents) with their
corresponding answers.

Identifying relevant questions in CQA repositories is a dif-
ﬁcult task, as the underlying intents of two questions could
be very diﬀerent even if they bear a close lexical resemblance.
For example, at the time of writing this paper, when sub-
mitting the question “What does licking your ﬁnger before
turning a page do?” to Google, the question “What would
you do if I licked my ﬁngers before turning the pages of your
books?” (with answers like “No, I wouldn’t care”) is deemed
as the second best match, since these two questions look
similar. However, the intent behind these two questions are
substantially diﬀerent: the former seeks facts while the latter
looks for social empathy from other people. In this paper,
we aim to develop an approach for question retrieval that
can strike the optimal balance between a question’s lexical
relevance and intent relevance.
2. RELATED WORK

The technique of language modelling has proved to be
eﬀective for question retrieval in CQA. The state-of-the-
art approach utilises the classic (query-likelihood) language
model [9] (see Section 3.1) together with the translation-
based language model [5,8] (see Section 3.2). Cao et al. have
proposed a category-based language model [2] for question
retrieval, but it requires users to manually assign one topic
category to each of their query questions, which is not al-
ways feasible. Furthermore, their category-based language
model cannot be applied straightforwardly to incorporate
user intent into question retrieval, because it assumes that
a question belongs to one and only one category, while we
believe that a question can have multiple intents (each to a
certain degree).
3. MODELS
3.1 Classic Language Model

Using the classic (query-likelihood) language model [9] for
information retrieval, we can measure the relevance of an

973archive question d with respect to the query question q as:

Pcla(q|d)=

Pcla(w|d)

(1)

(cid:89)

w∈q

assuming that each term w in the query q is generated in-
dependently by the unigram model of document d. The
probabilities Pcla(w|d) are estimated from the bag of words
in document d with Dirichlet prior smoothing.
3.2 Translation-based Language Model

There are often lexical gaps between a query question
and archive questions in CQA. For example, “Where can
I see movies for free online” and “Anyone share me a DVD
streaming link?” have the same meaning but are expressed
in quite diﬀerent words. It has been demonstrated that this
issue could be addressed by the translation-based language
model [5, 8]:

Ptra(q|d) =

Ptra(w|d)=

Ptra(w|d)

P (w|t)P (t|d)

(2)

(3)

(cid:89)
(cid:88)

w∈q

t∈d

where P (w|t) represents the probability of a document term
t being translated into a query term w. As in [8], we esti-
mate such word-to-word translation probabilities P (w|t) on
a parallel corpus that consists of 200,000 archived question-
answer pairs from Yahoo! Answers.
3.3 Intent-based Language Model

There could be diﬀerent user intents underlying diﬀerent
questions. For example, many questions in CQA are aﬀected
by the users’ individual interests (empathy, support, and
aﬀection, etc.)
In
this paper, we propose to take user intent into account for
question retrieval in the language modelling framework:

rather than just informational needs.

Pint(q|d) =

Pint(w|d)=

Pint(w|d)

P (w|Ck)P (Ck|d)

(4)

(5)

(cid:89)
N(cid:88)

w∈q

k=1

where Ck represents a category of user intent, P (w|Ck) is its
corresponding unigram language model (see Section 3.3.2)
and P (Ck|d) is the probability that the document d belongs
to that category (see Section 3.3.1).

Compared to the category-based language model of Cao
et al. [2], the intent-based model above is more general and
more robust, because, instead of imposing hard mutually-
exclusive classiﬁcations, it classiﬁes a question into multiple
(user intent) categories with certain probabilities.
3.3.1 Probabilistic Classiﬁcation of User Intent
When computing P (Ck|d) in the above, intent-based lan-
guage model, we adopt the question taxonomy proposed by
Chen et al. [4] which classiﬁes the user intent of a question
as objective, subjective, and social. A number of popular
machine learning algorithms have been tried for question
classiﬁcation, and ﬁnally Support Vector Machine (SVM) is
chosen due to its consistent outstanding performance. The
probabilistic outputs are obtained from SVM via Platt’s
method [7].

In addition to standard textual features (i.e., the bag of
words weighted by TFxIDF), a series of metadata features

Figure 1: The question topic feature.

Figure 2: The question time feature.

have been identiﬁed and exploited for training the proba-
bilistic classiﬁer. We found that question topic, question
time, and asker experience are particularly useful for our
task of intent-oriented question classiﬁcation.

Question Topic. Figure 1 shows the distribution of user
intent over the top-10 question topic categories in Yahoo!
Answers. Objective and subjective questions have similar
proportion of presence in most topic categories, except for
“Arts & Humanities” (which contains many questions about
history and genealogy). The distribution of social questions
looks quite diﬀerent: most social questions are about topics
like “Family Relationships,”“News Events,” and “Entertain-
ment & Music” on which people may be more inclined to
have social interaction. This suggests that question topic
features have discriminative power to separate social ques-
tions between objective or subjective ones.

Question Time. Figure 2 shows the distribution of user
intent over the hour of the day when the question was asked
on 2005-05-01. It seems that objective and subjective ques-
tions do not have apparent diﬀerences in terms of question
time.
In contrast, social questions show interesting pat-
terns: the peak time for social questions is at 18:00 (end
of a work day), 15:00 (after lunch), and 03:00 (lonely in the
late night). This suggests that question time features are
capable of distinguishing social questions from objective or
subjective ones.

Question Asker’s Experience. Figure 3 shows the dis-
tribution of user intent over the question asker’s experience,
i.e., the number of questions the user has asked before. It

974Table 1: The performance of supervised learning
with diﬀerent sets of features.

features

text

metadata

text+metadata

objective

subjective

social

0.693

0.609

0.731

0.689

0.642

0.693

0.152

0.378

0.412

Table 2: The performance of supervised learning vs
semi-supervised learning (co-training).

approach

supervised (text+metadata)

co-training (text+metadata)

miF1 maF1

0.712

0.757

0.510

0.534

linear combination:

Pmix(q|d)=αPcla(q|d) + βPtra(q|d) + γPint(q|d)

(7)

where α, β, and γ are three non-negative weight parame-
ters satisfying α + β + γ = 1. When γ = 0, the complete
mixture model backs oﬀ to the current state-of-the-art ap-
proach, i.e., the combination of the classic language model
and the translation-based language model only [8].

4. EXPERIMENTS

We conducted experiments on two real-world CQA datasets.

The ﬁrst dataset, YA, comes from Yahoo! Answers. It is
part of Yahoo! Labs’ Webscope1 L6 dataset that consists of
4,483,032 questions with their answers from 2005-01-01 to
2006-01-01. The second dataset, WA, comes from WikiAn-
swers. It contains 824,320 questions with their answers col-
lected from WikiAnswers2 from 2012-01-01 to 2012-05-01.

We ﬁrst experimented with question classiﬁcation on 1,539
questions that are randomly selected from the YA dataset
and manually labelled according to their user intents. Those
questions were split into training and testing sets with a
proportion of 2:1.

Table 1 shows the performance (miF1) of [binary] question
classiﬁcation through supervised learning (linear SVM) with
diﬀerent sets of features. One can ﬁnd that using both tex-
tual features and metadata features works better than using
either kind of features alone, for all question categories.

Table 2 depicts the performances (miF1 and maF1) of
question classiﬁcation via supervised learning and also semi-
supervised learning (co-training) based on both textual and
metadata features. It is clear that co-training that regards
text features and metadata features as two views works bet-
ter than supervised learning that simply pools these two
types of features together. The performance gain is proba-
bly due to the fact that co-training can make use of a large
amount of unlabelled questions in addition to the small set
of labelled questions.

We then experimented with question retrieval using a sim-
ilar set-up as in [8]: 50 questions were randomly sampled
from the YA and WA datasets respectively for testing (which
were excluded from the CQA retrieval repositories to en-
sure the impartiality of the evaluation), and the top archive

1http://webscope.sandbox.yahoo.com/
2http://wiki.answers.com/

Figure 3: The question asker’s experience feature.

seems that subjective and social questions are more likely
to come from experienced users than new ones, probably
because experienced users are relatively familiar with each
other. This suggests that the question asker’s experience
features allow objective questions to separate from subjec-
tive or social ones.

It is time-consuming and error-prone to manually label
archive questions according to their user intents. Usually,
we only have a small set of labelled questions, which would
signiﬁcantly limit the success of supervised learning for ques-
tion classiﬁcation. However, obtaining unlabelled questions
is quite easy and cheap. So it is promising to apply semi-
supervised learning [3] which can make use of a large amount
of unlabelled data in addition to the small set of labelled
data.

For this particular problem of user intent oriented ques-
tion classiﬁcation, we argue that the semi-supervised learn-
ing framework co-training [1] is particularly suitable. In the
co-training process, each example is described by two diﬀer-
ent feature sets (views) which provide diﬀerent and comple-
mentary information about it. Ideally, those two views are
conditionally independent (given the class) and each view is
suﬃcient (to be used for classiﬁcation alone). The textual
and metadata features mentioned above are both eﬀective in
detecting the user intent of questions but with quite diﬀer-
ent discriminative powers for diﬀerent question categories;
therefore, they can be considered as the two views for co-
training.

3.3.2 Estimating Unigram Models for User Intent
Given the probabilistic classiﬁcation results on all archive
questions, we can obtain the unigram language model for
each user intent category Ck through maximum-likelihood
estimation:

(cid:80)

w(cid:48)∈d

(cid:80)

(cid:80)

d∈Ck

P (w|Ck)=

tf (w, d)P (Ck|d)
d∈Ck

tf (w(cid:48), d)P (Ck|d)

(6)

where tf (w, d) is the term frequency of word w in docu-
ment d. It is possible to employ more advanced estimation
methods, which is left for future work.
3.4 Mixture Model

To exploit evidences from diﬀerent perspectives for ques-
tion retrieval, we can mix the above language models via

975Figure 4: The experimental results on Yahoo! Anaswers (left) and WikiAnswers (right) respectively

Table 3: The model parameters for diﬀerent ques-
tion retrieval approaches.

C C+T C+T+I

.

α

β

γ

1

0

0

0.3

0.7

0.0

0.18

0.42

0.40

questions (i.e., search results) returned for each test query
question were manually labelled as either relevant or not.
In order to see whether user intent relevance can improve
question retrieval performance, we compared the following
three approaches:

• the baseline approach which only employs the classic

language model (C);

• the state-of-the-art approach which combines the clas-
sic language model and the translation-based language
model (C+T) [8];

• the proposed hybrid approach which blends the classic
language model, the translation-based language model,
and the intent-based language model (C+T+I).

The model parameters were tuned on the training data to
achieve optimal results, as shown in Table 3. In the mixture
models (C+T) and (C+T+I), the ratio between parameter
values α and β was same as that in [8].

The retrieval performances of those approaches, measured
by Precision at 10 (P@10) [6] and Mean Average Precision
(MAP) [6], are reported in Figure 4. Consistent to the obser-
vation in [8], adding the translation-based language model
(C+T) brings substantial performance improvement to the
classic language model (C). More importantly, it is clear
that our proposed hybrid approach incorporating the intent-
based language model (C+T+I) outperforms the state-of-
the-art approach (C+T) signiﬁcantly, according to both P@10
and MAP on YA and WA.

5. CONCLUSIONS

The main contribution of this paper is to show the useful-
ness of user intent for question retrieval in CQA. Our pro-
posed intent-based language model supersedes the existing

category-based language model since one question can be-
long to multiple (user intent) categories to diﬀerent degrees,
and since a probabilistic question classiﬁer is built automat-
ically by taking into consideration of both textual features
and metadata features in the co-training framework.

6. ACKNOWLEDGEMENTS

We thank Xiaobing Xue (UMass) who kindly shared his
code for the translation-based language model with us. We
appreciate the eﬀorts of the three anonymous reviewers and
their useful comments and suggestions for improving this
work.

7. REFERENCES

[1] A. Blum and T. Mitchell. Combining labeled and unlabeled data
with co-training. In Proceedings of the 11th Annual Conference
on Computational Learning Theory (COLT), pages 92–100,
Madison, WI, USA, 1998.

[2] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. The use

of categorization information in language models for question
retrieval. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management (CIKM), pages
265–274, Hong Kong, China, 2009.

[3] O. Chapelle, B. Sch¨olkopf, and A. Zien, editors.

Semi-Supervised Learning. MIT Press, 2006.

[4] L. Chen, D. Zhang, and M. Levene. Understanding user intent in

community question answering. In Proceedings of the 21st
World Wide Web Conference (WWW), Companion Volume,
pages 823–828, Lyon, France, 2012.

[5] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in

large question and answer archives. In Proceedings of the 14th
ACM International Conference on Information and Knowledge
Management (CIKM), pages 84–90, Bremen, Germany, 2005.
[6] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to

Information Retrieval. Cambridge University Press, 2008.

[7] J. C. Platt. Probabilistic outputs for support vector machines

and comparisons to regularized likelihood methods. In Advances
in Large Margin Classiﬁers, pages 61–74. MIT Press, 1999.

[8] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question

and answer archives. In Proceedings of the 31st Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR), pages 475–482,
Singapore, 2008.

[9] C. Zhai. Statistical Language Models for Information Retrieval.

Morgan & Claypool Publishers, 2008.

976